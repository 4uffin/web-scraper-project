OpenAI admits AI hallucinations are mathematically inevitable, not just engineering flaws – Computerworld
Topics
Spotlight: IT CareersNewsEventsNewslettersResources
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to ComputerworldPoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy RightsOur NetworkCIOCSOInfoWorldNetwork WorldMoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Close
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareers IT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
Close
Americas
United States
Asia
India
Korea (대한민국)
Europe
Netherlands
United Kingdom
Germany (Deutschland)
Poland (Polska)
Spain (España)
Sweden (Sverige)
Oceania
Australia
New Zealand
Back
Close
Popular Topics
Generative AIProductivity SoftwareWindowsAndroidAppleRemote Work
Search
US-EN
Topics
Spotlight: IT CareersNewsEventsNewslettersResourcesAboutPoliciesOur NetworkMore
Back
Topics
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareersIT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to Computerworld
Back
PoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy Rights
Back
Our NetworkCIOCSOInfoWorldNetwork World
Back
MoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Home
Industry
OpenAI admits AI hallucinations are mathematically inevitable, not just engineering flaws
by									Gyana Swain
OpenAI admits AI hallucinations are mathematically inevitable, not just engineering flaws
news
Sep 18, 20256 minsArtificial IntelligenceTechnology Industry
In a landmark study, OpenAI researchers reveal that large language models will always produce plausible but false outputs, even with perfect data, due to fundamental statistical and computational limits.
Credit: 															mongmong_Studio- shutterstock.com
OpenAI, the creator of ChatGPT, acknowledged in its own research that large language models will always produce hallucinations due to fundamental mathematical constraints that cannot be solved through better engineering, marking a significant admission from one of the AI industry’s leading companies.
The study, published on September 4 and led by OpenAI researchers Adam Tauman Kalai, Edwin Zhang, and Ofir Nachum alongside Georgia Tech’s Santosh S. Vempala, provided a comprehensive mathematical framework explaining why AI systems must generate plausible but false information even when trained on perfect data.
[ Related: More OpenAI news and insights ]
“Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty,” the researchers wrote in the paper. “Such ‘hallucinations’ persist even in state-of-the-art systems and undermine trust.”
The admission carried particular weight given OpenAI’s position as the creator of ChatGPT, which sparked the current AI boom and convinced millions of users and enterprises to adopt generative AI technology.
OpenAI’s own models failed basic tests
The researchers demonstrated that hallucinations stemmed from statistical properties of language model training rather than implementation flaws. The study established that “the generative error rate is at least twice the IIV misclassification rate,” where IIV referred to “Is-It-Valid” and demonstrated mathematical lower bounds that prove AI systems will always make a certain percentage of mistakes, no matter how much the technology improves.
The researchers demonstrated their findings using state-of-the-art models, including those from OpenAI’s competitors. When asked “How many Ds are in DEEPSEEK?” the DeepSeek-V3 model with 600 billion parameters “returned ‘2’ or ‘3’ in ten independent trials” while Meta AI and Claude 3.7 Sonnet performed similarly, “including answers as large as ‘6’ and ‘7.’”
OpenAI also acknowledged the persistence of the problem in its own systems. The company stated in the paper that “ChatGPT also hallucinates. GPT‑5 has significantly fewer hallucinations, especially when reasoning, but they still occur. Hallucinations remain a fundamental challenge for all large language models.”
OpenAI’s own advanced reasoning models actually hallucinated more frequently than simpler systems. The company’s o1 reasoning model “hallucinated 16 percent of the time” when summarizing public information, while newer models o3 and o4-mini “hallucinated 33 percent and 48 percent of the time, respectively.”
“Unlike human intelligence, it lacks the humility to acknowledge uncertainty,” said Neil Shah, VP for research and partner at Counterpoint Technologies. “When unsure, it doesn’t defer to deeper research or human oversight; instead, it often presents estimates as facts.”
The OpenAI research identified three mathematical factors that made hallucinations inevitable: epistemic uncertainty when information appeared rarely in training data, model limitations where tasks exceeded current architectures’ representational capacity, and computational intractability where even superintelligent systems could not solve cryptographically hard problems.
Industry evaluation methods made the problem worse
Beyond proving hallucinations were inevitable, the OpenAI research revealed that industry evaluation methods actively encouraged the problem. Analysis of popular benchmarks, including GPQA, MMLU-Pro, and SWE-bench, found nine out of 10 major evaluations used binary grading that penalized “I don’t know” responses while rewarding incorrect but confident answers.
“We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty,” the researchers wrote.
Charlie Dai, VP and principal analyst at Forrester, said enterprises already faced challenges with this dynamic in production deployments. ‘Clients increasingly struggle with model quality challenges in production, especially in regulated sectors like finance and healthcare,’ Dai told Computerworld.
The research proposed “explicit confidence targets” as a solution, but acknowledged that fundamental mathematical constraints meant complete elimination of hallucinations remained impossible.
Enterprises must adapt strategies
Experts believed the mathematical inevitability of AI errors demands new enterprise strategies.
“Governance must shift from prevention to risk containment,” Dai said. “This means stronger human-in-the-loop processes, domain-specific guardrails, and continuous monitoring.”
Current AI risk frameworks have proved inadequate for the reality of persistent hallucinations. “Current frameworks often underweight epistemic uncertainty, so updates are needed to address systemic unpredictability,” Dai added.
Shah advocated for industry-wide evaluation reforms similar to automotive safety standards. “Just as automotive components are graded under ASIL standards to ensure safety, AI models should be assigned dynamic grades, nationally and internationally, based on their reliability and risk profile,” he said.
Both analysts agreed that vendor selection criteria needed fundamental revision. “Enterprises should prioritize calibrated confidence and transparency over raw benchmark scores,” Dai said. “AI leaders should look for vendors that provide uncertainty estimates, robust evaluation beyond standard benchmarks, and real-world validation.”
Shah suggested developing “a real-time trust index, a dynamic scoring system that evaluates model outputs based on prompt ambiguity, contextual understanding, and source quality.”
Market already adapting
These enterprise concerns aligned with broader academic findings. A Harvard Kennedy School research found that “downstream gatekeeping struggles to filter subtle hallucinations due to budget, volume, ambiguity, and context sensitivity concerns.”
Dai noted that reforming evaluation standards faced significant obstacles. “Reforming mainstream benchmarks is challenging. It’s only feasible if it’s driven by regulatory pressure, enterprise demand, and competitive differentiation.”
The OpenAI researchers concluded that their findings required industry-wide changes to evaluation methods. “This change may steer the field toward more trustworthy AI systems,” they wrote, while acknowledging that their research proved some level of unreliability would persist regardless of technical improvements.
For enterprises, the message appeared clear: AI hallucinations represented not a temporary engineering challenge, but a permanent mathematical reality requiring new governance frameworks and risk management strategies.
More on AI hallucinations:
You thought genAI hallucinations were bad? Things just got so much worse
Microsoft claims new ‘Correction’ tool can fix genAI hallucinations
AI hallucination mitigation: two brains are better than one
Related content
news
The experimental phase is over: Atlassian bets on DX to deliver AI ROI By Taryn Plumb
Sep 19, 2025 5 mins
Artificial Intelligence
Atlassian
Mergers and Acquisitions
brandpost
Sponsored by Ivanti
The ROI of automation: How intelligent ITSM drives cost savings and efficiency By Jeff Miller
Sep 19, 2025 4 mins
Artificial Intelligence
Machine Learning
news
Microsoft prompts Teams AI agents to collaborate with humans By Matthew Finnegan
Sep 19, 2025 4 mins
Microsoft
Microsoft 365
Microsoft Teams
news
Google launches Gemini in Chrome weeks after antitrust win, escalating AI browser wars By Gyana Swain
Sep 19, 2025 5 mins
Artificial Intelligence
Browsers
Chrome
Other Sections
Podcasts
Videos
Resources
Events
Spotlight: IT Careers
SUBSCRIBE TO OUR NEWSLETTER
From our editors straight to your inbox
Get started by entering your email address below.
Please enter a valid email address
Subscribe
by
Gyana Swain
Gyana Swain is a seasoned technology journalist with over 20 years' experience covering the telecom and IT space. He is a consulting editor with VARINDIA and earlier in his career, he held editorial positions at CyberMedia, PTI, 9dot9 Media, and Dennis Publishing. A published author of two books, he combines industry insight with narrative depth. Outside of work, he’s a keen traveler and cricket enthusiast. He earned a B.S. degree from Utkal University.
More from this author
newsOpenAI, Microsoft discuss shape of future relationship Sep 12, 2025 6 minsnewsAtlassian says its ‘Don’t F— the Customer’ principle drove cloud-only decision Sep 10, 2025 5 minsnewsUber turns drivers into AI data labelers in India pilot Sep 8, 2025 5 minsnewsOpenAI launches jobs platform, pledges to train 10M Americans by 2030 Sep 5, 2025 5 minsnewsGoogle hit with $806M in penalties from US and French authorities over privacy issues Sep 4, 2025 5 minsnewsNo breakup for Google: Court opts for behavioral fixes over structural split Sep 3, 2025 6 minsnewsIntel warns US govt equity stake could disrupt its global business and strategic deals Aug 26, 2025 6 minsnewsAs US takes 10% stake in Intel, new questions arise for enterprise buyers Aug 25, 2025 6 mins
Show me morePopularArticlesPodcastsVideos
news analysis
Agentic AI – Ongoing coverage of its impact on the enterprise By Dan MuseSep 19, 20255 mins
Emerging TechnologyGenerative AIIT Leadership
brandpost Sponsored by Ivanti Leading the charge in cyber risk mitigation: From gut feeling to objective evaluation
By Jeff MillerSep 19, 20255 mins
Security Software
opinion
macOS Tahoe is out of touch with Apple’s Touch Bar By Jonny EvansSep 19, 20255 mins
AppleMacMacBook
podcast
First Person Meets.... Magan Naidoo: Using data to do good Sep 15, 202535 mins
Big DataCareers
podcast
Why AI is entering physical security and threat protection spaces Sep 9, 202534 mins
Generative AIPhysical SecurityThreat and Vulnerability Management
podcast
The amazing and frustrating world of bot-to-bot hiring Sep 2, 202546 mins
Generative AIIT JobsResumes
video
Why are so many AI projects failing? Sep 17, 202538 mins
Artificial IntelligenceEnterprise ArchitectureGenerative AI
video
First Person Meets.... Magan Naidoo: Using data to do good Sep 15, 202535 mins
Big DataCareers
video
How AI is disrupting physical security and threat protection processes Sep 9, 202534 mins
Advanced Persistent ThreatsGenerative AIThreat and Vulnerability Management
Sponsored Links
Empower your cybersecurity team with expert insights from Palo Alto Networks.
Secure AI by Design: Unleash the power of AI and keep applications, usage and data secure.
About
About Us
Advertise
Contact Us
Editorial Ethics Policy
Foundry Careers
Reprints
Newsletters
BrandPosts
News
Features
Opinions
How-to
Blogs
Policies
Terms of Service
Privacy Policy
Cookie Policy
Copyright Notice
Member Preferences
About AdChoices
Your California Privacy Rights
Privacy Settings
Our Network
CIO
CSO
Infoworld
Network World
FacebookXYouTubeGoogle NewsLinkedIn
© 2025
FoundryCo, Inc. All Rights Reserved.