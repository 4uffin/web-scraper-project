DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk – Computerworld
Topics
Spotlight: IT CareersNewsEventsNewslettersResources
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to ComputerworldPoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy RightsOur NetworkCIOCSOInfoWorldNetwork WorldMoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Close
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareers IT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
Close
Americas
United States
Asia
India
Korea (대한민국)
Europe
Netherlands
United Kingdom
Germany (Deutschland)
Poland (Polska)
Spain (España)
Sweden (Sverige)
Oceania
Australia
New Zealand
Back
Close
Popular Topics
Generative AIProductivity SoftwareWindowsAndroidAppleRemote Work
Search
US-EN
Topics
Spotlight: IT CareersNewsEventsNewslettersResourcesAboutPoliciesOur NetworkMore
Back
Topics
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareersIT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to Computerworld
Back
PoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy Rights
Back
Our NetworkCIOCSOInfoWorldNetwork World
Back
MoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Home
Artificial Intelligence
DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk
by									Prasanth Aby Thomas
DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk
news
Sep 18, 20253 minsGenerative AITechnology Industry
The model’s flawed responses to prompts involving Tibet, Taiwan, and Falun Gong raise red flags about the influence of state-aligned censorship on AI reliability, with experts calling for stronger oversight and certification frameworks.
Credit: 															Shutterstock/Rokas Tenys
A new study has shown that DeepSeek AI may generate deliberately flawed code when prompts involve groups or regions deemed politically sensitive by Beijing, raising fresh concerns for enterprises about the security and reliability of Chinese AI systems.
Researchers at CrowdStrike tested DeepSeek by submitting a series of nearly identical programming requests, varying only the intended user or region, according to a report from the Washington Post.
[ Related: More DeepSeek news and analysis ]
While general requests for code to run industrial control systems already produced a notable share of flawed results, the error rate increased sharply when the projects were described as serving groups or regions deemed sensitive by Beijing, including Tibet, Taiwan, and Falun Gong.
This is not the first time DeepSeek has drawn scrutiny. Earlier this year, a senior US State Department official warned that the company has provided support to China’s military and intelligence operations and is likely to continue doing so.
Security risks from bias
In the report, CrowdStrike stated that the behavior could stem from the AI engine following Chinese government directives, weaker training data in certain regions, or the model itself generating flawed code when instructed to associate a region with rebels.
Industry experts warn that these patterns carry significant implications for enterprises.
“If AI models generate flawed or biased code influenced by political directives, enterprises face inherent risks from vulnerabilities in sensitive systems, particularly where neutrality is critical, potentially leading to operational, reputational, and regulatory consequences,” said Prabhu Ram, VP of industry research at Cybermedia Research.
Enterprises operating under national security or regulatory constraints must be especially cautious, according to Neil Shah, VP for research at Counterpoint Research.
“The use of foreign AI models in sensitive workflows should be subject to national-level AI certification programs and export control compliance as a first line of defense,” Shah said. “Ultimately, trust in AI systems must be earned through transparency, accountability, and continuous oversight irrespective of the model’s popularity or open-source status.”
Systemic gaps in oversight
Analysts point out that this is not just a DeepSeek issue but a systemic risk across the AI foundational model ecosystem, citing the lack of cross-border standardization and governance.
“As the number of foundation models proliferates and enterprises increasingly build applications or code on top of them, it becomes imperative for CIOs and IT leaders to establish and follow a robust multi-level due diligence framework,” Shah said. “That framework should ensure training data transparency, strong data privacy, security governance policies, and at the very least, rigorous checks for geopolitical biases, censorship influence, and potential IP violations.”
Experts recommend that CIOs review the transparency of training data and algorithms, account for geopolitical context, and use independent third-party assessments and controlled pilot testing before moving to large-scale integration. “There is also a growing need for certification and regulatory frameworks to guarantee AI neutrality, safety, and ethical compliance,” Ram said. “National and international standards could help enterprises trust AI outputs while mitigating risks from biased or politically influenced systems.”
Related content
opinion
The Microsoft-OpenAI divorce is coming. Who’s getting the best deal? By Preston Gralla
Sep 23, 2025 6 mins
Generative AI
Microsoft
Technology Industry
news analysis
DeepSeek — Latest news and insights By Dan Muse
Sep 18, 2025 9 mins
Emerging Technology
Generative AI
Technology Industry
news
OpenAI admits AI hallucinations are mathematically inevitable, not just engineering flaws By Gyana Swain
Sep 18, 2025 6 mins
Artificial Intelligence
Technology Industry
Other Sections
Podcasts
Videos
Resources
Events
Spotlight: IT Careers
SUBSCRIBE TO OUR NEWSLETTER
From our editors straight to your inbox
Get started by entering your email address below.
Please enter a valid email address
Subscribe
by
Prasanth Aby Thomas
Prasanth Aby Thomas is a freelance technology journalist who specializes in semiconductors, security, AI, and EVs. His work has appeared in DigiTimes Asia and asmag.com, among other publications.
Earlier in his career, Prasanth was a correspondent for Reuters covering the energy sector. Prior to that, he was a correspondent for International Business Times UK covering Asian and European markets and macroeconomic developments.
He holds a Master's degree in international journalism from Bournemouth University, a Master's degree in visual communication from Loyola College, a Bachelor's degree in English from Mahatma Gandhi University, and studied Chinese language at National Taiwan University.
More from this author
newsQualcomm targets enterprise PCs with AI and remote management push Sep 25, 2025 4 minsnewsMeta launches bipartisan super PAC to influence state AI laws amid rising regulatory pressure Sep 24, 2025 4 minsnewsNew Alibaba model Qwen3-Omni heightens competition in multimodal AI Sep 23, 2025 4 minsnewsGoogle unveils payments protocol for AI agents with major financial firms Sep 17, 2025 4 minsnewsSenator Cruz introduces an AI ‘sandbox’ bill to ease regulatory burdens Sep 11, 2025 3 minsnewsMicrosoft to tap Anthropic for Office 365 as enterprises weigh risks of AI lock-in Sep 10, 2025 4 minsnewsIntel announces leadership overhaul, underscoring long road to recovery Sep 9, 2025 5 minsnewsNew procedural memory framework promises cheaper, more resilient AI agents Aug 28, 2025 5 mins
Show me morePopularArticlesPodcastsVideos
feature
Microsoft 365: A guide to the updates By Preston GrallaSep 25, 202533 mins
Microsoft 365Microsoft OfficeOffice Suites
feature
Windows 11: A guide to the updates By Preston GrallaSep 25, 202551 mins
MicrosoftSmall and Medium BusinessWindows 11
feature
Windows 11 Insider Previews: What’s in the latest build? By Preston GrallaSep 25, 2025104 mins
MicrosoftSmall and Medium BusinessWindows 11
podcast
First Person Meets... Stephen Kaufman: Solving problems with technology and people Sep 22, 202527 mins
CareersCloud ManagementIT Leadership
podcast
First Person Meets.... Magan Naidoo: Using data to do good Sep 15, 202535 mins
Big DataCareers
podcast
Why AI is entering physical security and threat protection spaces Sep 9, 202534 mins
Generative AIPhysical SecurityThreat and Vulnerability Management
video
How agentic AI is expanding the attack surface Sep 23, 202544 mins
CyberattacksGenerative AISecurity Practices
video
First Person Meets... Stephen Kaufman: Solving problems with technology and people Sep 22, 202527 mins
CareersCloud ManagementIT Leadership
video
Why are so many AI projects failing? Sep 17, 202538 mins
Artificial IntelligenceEnterprise ArchitectureGenerative AI
Sponsored Links
Secure AI by Design: Unleash the power of AI and keep applications, usage and data secure.
Empower your cybersecurity team with expert insights from Palo Alto Networks.
About
About Us
Advertise
Contact Us
Editorial Ethics Policy
Foundry Careers
Reprints
Newsletters
BrandPosts
News
Features
Opinions
How-to
Blogs
Policies
Terms of Service
Privacy Policy
Cookie Policy
Copyright Notice
Member Preferences
About AdChoices
Your California Privacy Rights
Privacy Settings
Our Network
CIO
CSO
Infoworld
Network World
FacebookXYouTubeGoogle NewsLinkedIn
© 2025
FoundryCo, Inc. All Rights Reserved.