DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk – Computerworld
Topics
Spotlight: IT CareersNewsEventsNewslettersResources
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to ComputerworldPoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesE-commerce Affiliate RelationshipsYour California Privacy RightsOur NetworkCIOCSOInfoWorldNetwork WorldMoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Close
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareers IT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
Close
Americas
United States
Asia
India
Korea (대한민국)
Europe
Netherlands
United Kingdom
Germany (Deutschland)
Poland (Polska)
Spain (España)
Sweden (Sverige)
Oceania
Australia
New Zealand
Back
Close
Popular Topics
Generative AIProductivity SoftwareWindowsAndroidAppleRemote Work
Search
US-EN
Topics
Spotlight: IT CareersNewsEventsNewslettersResourcesAboutPoliciesOur NetworkMore
Back
Topics
Generative AIOffice SuitesCollaboration SoftwareProductivity SoftwareWindowsAndroidAppleAugmented RealityEmerging TechnologyMobileRemote WorkArtificial IntelligenceOperating SystemsCareersIT LeadershipIT ManagementIT OperationsIndustryCloud ComputingComputers and PeripheralsAnalyticsData CenterEnterprise ApplicationsNetworkingSecurityVendors and ProvidersEnterprise Buyer’s Guides
Back
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersReprintsNewslettersContribute to Computerworld
Back
PoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesE-commerce Affiliate RelationshipsYour California Privacy Rights
Back
Our NetworkCIOCSOInfoWorldNetwork World
Back
MoreNewsFeaturesOpinionBlogsBrandPostsEventsPodcastsVideosEnterprise Buyer’s Guides
Home
Artificial Intelligence
DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk
by									Prasanth Aby Thomas
DeepSeek AI’s code bias sparks alarm over politicized AI outputs and enterprise risk
news
Sep 18, 20253 minsGenerative AITechnology Industry
The model’s flawed responses to prompts involving Tibet, Taiwan, and Falun Gong raise red flags about the influence of state-aligned censorship on AI reliability, with experts calling for stronger oversight and certification frameworks.
Credit: 															Shutterstock/Rokas Tenys
A new study has shown that DeepSeek AI may generate deliberately flawed code when prompts involve groups or regions deemed politically sensitive by Beijing, raising fresh concerns for enterprises about the security and reliability of Chinese AI systems.
Researchers at CrowdStrike tested DeepSeek by submitting a series of nearly identical programming requests, varying only the intended user or region, according to a report from the Washington Post.
[ Related: More DeepSeek news and analysis ]
While general requests for code to run industrial control systems already produced a notable share of flawed results, the error rate increased sharply when the projects were described as serving groups or regions deemed sensitive by Beijing, including Tibet, Taiwan, and Falun Gong.
This is not the first time DeepSeek has drawn scrutiny. Earlier this year, a senior US State Department official warned that the company has provided support to China’s military and intelligence operations and is likely to continue doing so.
Security risks from bias
In the report, CrowdStrike stated that the behavior could stem from the AI engine following Chinese government directives, weaker training data in certain regions, or the model itself generating flawed code when instructed to associate a region with rebels.
Industry experts warn that these patterns carry significant implications for enterprises.
“If AI models generate flawed or biased code influenced by political directives, enterprises face inherent risks from vulnerabilities in sensitive systems, particularly where neutrality is critical, potentially leading to operational, reputational, and regulatory consequences,” said Prabhu Ram, VP of industry research at Cybermedia Research.
Enterprises operating under national security or regulatory constraints must be especially cautious, according to Neil Shah, VP for research at Counterpoint Research.
“The use of foreign AI models in sensitive workflows should be subject to national-level AI certification programs and export control compliance as a first line of defense,” Shah said. “Ultimately, trust in AI systems must be earned through transparency, accountability, and continuous oversight irrespective of the model’s popularity or open-source status.”
Systemic gaps in oversight
Analysts point out that this is not just a DeepSeek issue but a systemic risk across the AI foundational model ecosystem, citing the lack of cross-border standardization and governance.
“As the number of foundation models proliferates and enterprises increasingly build applications or code on top of them, it becomes imperative for CIOs and IT leaders to establish and follow a robust multi-level due diligence framework,” Shah said. “That framework should ensure training data transparency, strong data privacy, security governance policies, and at the very least, rigorous checks for geopolitical biases, censorship influence, and potential IP violations.”
Experts recommend that CIOs review the transparency of training data and algorithms, account for geopolitical context, and use independent third-party assessments and controlled pilot testing before moving to large-scale integration. “There is also a growing need for certification and regulatory frameworks to guarantee AI neutrality, safety, and ethical compliance,” Ram said. “National and international standards could help enterprises trust AI outputs while mitigating risks from biased or politically influenced systems.”
Related content
news analysis
DeepSeek — Latest news and insights By Dan Muse
Sep 18, 2025 9 mins
Emerging Technology
Generative AI
Technology Industry
news
OpenAI admits AI hallucinations are mathematically inevitable, not just engineering flaws By Gyana Swain
Sep 18, 2025 6 mins
Artificial Intelligence
Technology Industry
news
Microsoft, EC reach deal on Teams app bundling By Matthew Finnegan
Sep 12, 2025 3 mins
Microsoft 365
Microsoft Office
Regulation
Other Sections
Podcasts
Videos
Resources
Events
Spotlight: IT Careers
SUBSCRIBE TO OUR NEWSLETTER
From our editors straight to your inbox
Get started by entering your email address below.
Please enter a valid email address
Subscribe
by
Prasanth Aby Thomas
Prasanth Aby Thomas is a freelance technology journalist who specializes in semiconductors, security, AI, and EVs. His work has appeared in DigiTimes Asia and asmag.com, among other publications.
Earlier in his career, Prasanth was a correspondent for Reuters covering the energy sector. Prior to that, he was a correspondent for International Business Times UK covering Asian and European markets and macroeconomic developments.
He holds a Master's degree in international journalism from Bournemouth University, a Master's degree in visual communication from Loyola College, a Bachelor's degree in English from Mahatma Gandhi University, and studied Chinese language at National Taiwan University.
More from this author
newsGoogle unveils payments protocol for AI agents with major financial firms Sep 17, 2025 4 minsnewsSenator Cruz introduces an AI ‘sandbox’ bill to ease regulatory burdens Sep 11, 2025 3 minsnewsMicrosoft to tap Anthropic for Office 365 as enterprises weigh risks of AI lock-in Sep 10, 2025 4 minsnewsIntel announces leadership overhaul, underscoring long road to recovery Sep 9, 2025 5 minsnewsNew procedural memory framework promises cheaper, more resilient AI agents Aug 28, 2025 5 minsnewsAnthropic invites users to test letting Claude operate Chrome browser Aug 27, 2025 4 minsnewsUS threat of sanctions on EU officials over tech law raises risks for enterprises Aug 26, 2025 4 minsnewsChina’s DeepSeek launches V3.1, raising stakes for enterprise AI adoption Aug 20, 2025 4 mins
Show me morePopularArticlesPodcastsVideos
news
Intel will design CPUs with Nvidia NVLink in return for $5 billion investment By Evan SchumanSep 18, 20251 min
CPUs and ProcessorsIntelNvidia
news
OpenAI: Latest news and insights By CW staffSep 18, 202518 mins
Artificial IntelligenceGenerative AI
opinion
Tool sprawl is dangerous, warns Apple MDM vendor Kandji By Jonny EvansSep 18, 20255 mins
AppleIT ManagementMobile
podcast
First Person Meets.... Magan Naidoo: Using data to do good Sep 15, 202535 mins
Big DataCareers
podcast
Why AI is entering physical security and threat protection spaces Sep 9, 202534 mins
Generative AIPhysical SecurityThreat and Vulnerability Management
podcast
The amazing and frustrating world of bot-to-bot hiring Sep 2, 202546 mins
Generative AIIT JobsResumes
video
Why are so many AI projects failing? Sep 17, 202538 mins
Artificial IntelligenceEnterprise ArchitectureGenerative AI
video
First Person Meets.... Magan Naidoo: Using data to do good Sep 15, 202535 mins
Big DataCareers
video
How AI is disrupting physical security and threat protection processes Sep 9, 202534 mins
Advanced Persistent ThreatsGenerative AIThreat and Vulnerability Management
Sponsored Links
Empower your cybersecurity team with expert insights from Palo Alto Networks.
Secure AI by Design: Unleash the power of AI and keep applications, usage and data secure.
About
About Us
Advertise
Contact Us
Editorial Ethics Policy
Foundry Careers
Reprints
Newsletters
BrandPosts
News
Features
Opinions
How-to
Blogs
Policies
Terms of Service
Privacy Policy
Cookie Policy
Copyright Notice
Member Preferences
About AdChoices
E-commerce Affiliate Relationships
Your California Privacy Rights
Privacy Settings
Our Network
CIO
CSO
Infoworld
Network World
FacebookXYouTubeGoogle NewsLinkedIn
© 2025
FoundryCo, Inc. All Rights Reserved.