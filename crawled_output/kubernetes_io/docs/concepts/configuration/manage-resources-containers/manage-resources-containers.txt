Resource Management for Pods and Containers | Kubernetes
KubernetesDocumentationKubernetes BlogTrainingCareersPartnersCommunityVersionsRelease Information
v1.34
v1.33
v1.32
v1.31
v1.30English中文 (Chinese)
日本語 (Japanese)
한국어 (Korean)
Português (Portuguese)
Español (Spanish)
Englishবাংলা (Bengali)
中文 (Chinese)
Français (French)
Deutsch (German)
हिन्दी (Hindi)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
日本語 (Japanese)
한국어 (Korean)
Polski (Polish)
Português (Portuguese)
Русский (Russian)
Español (Spanish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)Kubernetes Documentation
Documentation
Available Documentation Versions
Getting started
Learning environment
Production environment
Container Runtimes
Installing Kubernetes with deployment tools
Bootstrapping clusters with kubeadm
Installing kubeadm
Troubleshooting kubeadm
Creating a cluster with kubeadm
Customizing components with the kubeadm API
Options for Highly Available Topology
Creating Highly Available Clusters with kubeadm
Set up a High Availability etcd Cluster with kubeadm
Configuring each kubelet in your cluster using kubeadm
Dual-stack support with kubeadm
Turnkey Cloud Solutions
Best practices
Considerations for large clusters
Running in multiple zones
Validate node setup
Enforcing Pod Security Standards
PKI certificates and requirements
Concepts
Overview
Kubernetes Components
Objects In Kubernetes
Kubernetes Object Management
Object Names and IDs
Labels and Selectors
Namespaces
Annotations
Field Selectors
Finalizers
Owners and Dependents
Recommended Labels
The Kubernetes API
Cluster Architecture
Nodes
Communication between Nodes and the Control Plane
Controllers
Leases
Cloud Controller Manager
About cgroup v2
Kubernetes Self-Healing
Container Runtime Interface (CRI)
Garbage Collection
Mixed Version Proxy
Containers
Images
Container Environment
Runtime Class
Container Lifecycle Hooks
Workloads
Pods
Pod Lifecycle
Init Containers
Sidecar Containers
Ephemeral Containers
Disruptions
Pod Hostname
Pod Quality of Service Classes
User Namespaces
Downward API
Workload Management
Deployments
ReplicaSet
StatefulSets
DaemonSet
Jobs
Automatic Cleanup for Finished Jobs
CronJob
ReplicationController
Autoscaling Workloads
Managing Workloads
Services, Load Balancing, and Networking
Service
Ingress
Ingress Controllers
Gateway API
EndpointSlices
Network Policies
DNS for Services and Pods
IPv4/IPv6 dual-stack
Topology Aware Routing
Networking on Windows
Service ClusterIP allocation
Service Internal Traffic Policy
Storage
Volumes
Persistent Volumes
Projected Volumes
Ephemeral Volumes
Storage Classes
Volume Attributes Classes
Dynamic Volume Provisioning
Volume Snapshots
Volume Snapshot Classes
CSI Volume Cloning
Storage Capacity
Node-specific Volume Limits
Volume Health Monitoring
Windows Storage
Configuration
Configuration Best Practices
ConfigMaps
Secrets
Liveness, Readiness, and Startup Probes
Resource Management for Pods and Containers
Organizing Cluster Access Using kubeconfig Files
Resource Management for Windows nodes
Security
Cloud Native Security
Pod Security Standards
Pod Security Admission
Service Accounts
Pod Security Policies
Security For Linux Nodes
Security For Windows Nodes
Controlling Access to the Kubernetes API
Role Based Access Control Good Practices
Good practices for Kubernetes Secrets
Multi-tenancy
Hardening Guide - Authentication Mechanisms
Hardening Guide - Scheduler Configuration
Kubernetes API Server Bypass Risks
Linux kernel security constraints for Pods and containers
Security Checklist
Application Security Checklist
Policies
Limit Ranges
Resource Quotas
Process ID Limits And Reservations
Node Resource Managers
Scheduling, Preemption and Eviction
Kubernetes Scheduler
Assigning Pods to Nodes
Pod Overhead
Pod Scheduling Readiness
Pod Topology Spread Constraints
Taints and Tolerations
Scheduling Framework
Dynamic Resource Allocation
Scheduler Performance Tuning
Resource Bin Packing
Pod Priority and Preemption
Node-pressure Eviction
API-initiated Eviction
Cluster Administration
Node Shutdowns
Swap memory management
Node Autoscaling
Certificates
Cluster Networking
Admission Webhook Good Practices
Good practices for Dynamic Resource Allocation as a Cluster Admin
Logging Architecture
Compatibility Version For Kubernetes Control Plane Components
Metrics For Kubernetes System Components
Metrics for Kubernetes Object States
System Logs
Traces For Kubernetes System Components
Proxies in Kubernetes
API Priority and Fairness
Installing Addons
Coordinated Leader Election
Windows in Kubernetes
Windows containers in Kubernetes
Guide for Running Windows Containers in Kubernetes
Extending Kubernetes
Compute, Storage, and Networking Extensions
Network Plugins
Device Plugins
Extending the Kubernetes API
Custom Resources
Kubernetes API Aggregation Layer
Operator pattern
Tasks
Install Tools
Install and Set Up kubectl on Linux
Install and Set Up kubectl on macOS
Install and Set Up kubectl on Windows
Administer a Cluster
Administration with kubeadm
Adding Linux worker nodes
Adding Windows worker nodes
Upgrading kubeadm clusters
Upgrading Linux nodes
Upgrading Windows nodes
Configuring a cgroup driver
Certificate Management with kubeadm
Reconfiguring a kubeadm cluster
Changing The Kubernetes Package Repository
Overprovision Node Capacity For A Cluster
Migrating from dockershim
Changing the Container Runtime on a Node from Docker Engine to containerd
Find Out What Container Runtime is Used on a Node
Troubleshooting CNI plugin-related errors
Check whether dockershim removal affects you
Migrating telemetry and security agents from dockershim
Generate Certificates Manually
Manage Memory, CPU, and API Resources
Configure Default Memory Requests and Limits for a Namespace
Configure Default CPU Requests and Limits for a Namespace
Configure Minimum and Maximum Memory Constraints for a Namespace
Configure Minimum and Maximum CPU Constraints for a Namespace
Configure Memory and CPU Quotas for a Namespace
Configure a Pod Quota for a Namespace
Install a Network Policy Provider
Use Antrea for NetworkPolicy
Use Calico for NetworkPolicy
Use Cilium for NetworkPolicy
Use Kube-router for NetworkPolicy
Romana for NetworkPolicy
Weave Net for NetworkPolicy
Access Clusters Using the Kubernetes API
Advertise Extended Resources for a Node
Autoscale the DNS Service in a Cluster
Change the Access Mode of a PersistentVolume to ReadWriteOncePod
Change the default StorageClass
Switching from Polling to CRI Event-based Updates to Container Status
Change the Reclaim Policy of a PersistentVolume
Cloud Controller Manager Administration
Configure a kubelet image credential provider
Configure Quotas for API Objects
Control CPU Management Policies on the Node
Control Topology Management Policies on a node
Customizing DNS Service
Debugging DNS Resolution
Declare Network Policy
Developing Cloud Controller Manager
Enable Or Disable A Kubernetes API
Encrypting Confidential Data at Rest
Decrypt Confidential Data that is Already Encrypted at Rest
Guaranteed Scheduling For Critical Add-On Pods
IP Masquerade Agent User Guide
Limit Storage Consumption
Migrate Replicated Control Plane To Use Cloud Controller Manager
Operating etcd clusters for Kubernetes
Reserve Compute Resources for System Daemons
Running Kubernetes Node Components as a Non-root User
Safely Drain a Node
Securing a Cluster
Set Kubelet Parameters Via A Configuration File
Share a Cluster with Namespaces
Upgrade A Cluster
Use Cascading Deletion in a Cluster
Using a KMS provider for data encryption
Using CoreDNS for Service Discovery
Using NodeLocal DNSCache in Kubernetes Clusters
Using sysctls in a Kubernetes Cluster
Utilizing the NUMA-aware Memory Manager
Verify Signed Kubernetes Artifacts
Configure Pods and Containers
Assign Memory Resources to Containers and Pods
Assign CPU Resources to Containers and Pods
Assign Devices to Pods and Containers
Set Up DRA in a Cluster
Allocate Devices to Workloads with DRA
Assign Pod-level CPU and memory resources
Configure GMSA for Windows Pods and containers
Resize CPU and Memory Resources assigned to Containers
Configure RunAsUserName for Windows pods and containers
Create a Windows HostProcess Pod
Configure Quality of Service for Pods
Assign Extended Resources to a Container
Configure a Pod to Use a Volume for Storage
Configure a Pod to Use a PersistentVolume for Storage
Configure a Pod to Use a Projected Volume for Storage
Configure a Security Context for a Pod or Container
Configure Service Accounts for Pods
Pull an Image from a Private Registry
Configure Liveness, Readiness and Startup Probes
Assign Pods to Nodes
Assign Pods to Nodes using Node Affinity
Configure Pod Initialization
Attach Handlers to Container Lifecycle Events
Configure a Pod to Use a ConfigMap
Share Process Namespace between Containers in a Pod
Use a User Namespace With a Pod
Use an Image Volume With a Pod
Create static Pods
Translate a Docker Compose File to Kubernetes Resources
Enforce Pod Security Standards by Configuring the Built-in Admission Controller
Enforce Pod Security Standards with Namespace Labels
Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller
Monitoring, Logging, and Debugging
Logging in Kubernetes
Monitoring in Kubernetes
Troubleshooting Applications
Debug Pods
Debug Services
Debug a StatefulSet
Determine the Reason for Pod Failure
Debug Init Containers
Debug Running Pods
Get a Shell to a Running Container
Troubleshooting Clusters
Troubleshooting kubectl
Resource metrics pipeline
Tools for Monitoring Resources
Monitor Node Health
Debugging Kubernetes nodes with crictl
Auditing
Debugging Kubernetes Nodes With Kubectl
Developing and debugging services locally using telepresence
Windows debugging tips
Manage Kubernetes Objects
Declarative Management of Kubernetes Objects Using Configuration Files
Declarative Management of Kubernetes Objects Using Kustomize
Managing Kubernetes Objects Using Imperative Commands
Imperative Management of Kubernetes Objects Using Configuration Files
Update API Objects in Place Using kubectl patch
Migrate Kubernetes Objects Using Storage Version Migration
Managing Secrets
Managing Secrets using kubectl
Managing Secrets using Configuration File
Managing Secrets using Kustomize
Inject Data Into Applications
Define a Command and Arguments for a Container
Define Dependent Environment Variables
Define Environment Variables for a Container
Define Environment Variable Values Using An Init Container
Expose Pod Information to Containers Through Environment Variables
Expose Pod Information to Containers Through Files
Distribute Credentials Securely Using Secrets
Run Applications
Run a Stateless Application Using a Deployment
Run a Single-Instance Stateful Application
Run a Replicated Stateful Application
Scale a StatefulSet
Delete a StatefulSet
Force Delete StatefulSet Pods
Horizontal Pod Autoscaling
HorizontalPodAutoscaler Walkthrough
Specifying a Disruption Budget for your Application
Accessing the Kubernetes API from a Pod
Run Jobs
Running Automated Tasks with a CronJob
Coarse Parallel Processing Using a Work Queue
Fine Parallel Processing Using a Work Queue
Indexed Job for Parallel Processing with Static Work Assignment
Job with Pod-to-Pod Communication
Parallel Processing using Expansions
Handling retriable and non-retriable pod failures with Pod failure policy
Access Applications in a Cluster
Deploy and Access the Kubernetes Dashboard
Accessing Clusters
Configure Access to Multiple Clusters
Use Port Forwarding to Access Applications in a Cluster
Use a Service to Access an Application in a Cluster
Connect a Frontend to a Backend Using Services
Create an External Load Balancer
List All Container Images Running in a Cluster
Set up Ingress on Minikube with the NGINX Ingress Controller
Communicate Between Containers in the Same Pod Using a Shared Volume
Configure DNS for a Cluster
Access Services Running on Clusters
Extend Kubernetes
Configure the Aggregation Layer
Use Custom Resources
Extend the Kubernetes API with CustomResourceDefinitions
Versions in CustomResourceDefinitions
Set up an Extension API Server
Configure Multiple Schedulers
Use an HTTP Proxy to Access the Kubernetes API
Use a SOCKS5 Proxy to Access the Kubernetes API
Set up Konnectivity service
TLS
Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest
Configure Certificate Rotation for the Kubelet
Manage TLS Certificates in a Cluster
Manual Rotation of CA Certificates
Manage Cluster Daemons
Building a Basic DaemonSet
Perform a Rolling Update on a DaemonSet
Perform a Rollback on a DaemonSet
Running Pods on Only Some Nodes
Networking
Adding entries to Pod /etc/hosts with HostAliases
Extend Service IP Ranges
Kubernetes Default Service CIDR Reconfiguration
Validate IPv4/IPv6 dual-stack
Extend kubectl with plugins
Manage HugePages
Schedule GPUs
Tutorials
Hello Minikube
Learn Kubernetes Basics
Create a Cluster
Using Minikube to Create a Cluster
Deploy an App
Using kubectl to Create a Deployment
Explore Your App
Viewing Pods and Nodes
Expose Your App Publicly
Using a Service to Expose Your App
Scale Your App
Running Multiple Instances of Your App
Update Your App
Performing a Rolling Update
Configuration
Updating Configuration via a ConfigMap
Configuring Redis using a ConfigMap
Adopting Sidecar Containers
Security
Apply Pod Security Standards at the Cluster Level
Apply Pod Security Standards at the Namespace Level
Restrict a Container's Access to Resources with AppArmor
Restrict a Container's Syscalls with seccomp
Stateless Applications
Exposing an External IP Address to Access an Application in a Cluster
Example: Deploying PHP Guestbook application with Redis
Stateful Applications
StatefulSet Basics
Example: Deploying WordPress and MySQL with Persistent Volumes
Example: Deploying Cassandra with a StatefulSet
Running ZooKeeper, A Distributed System Coordinator
Cluster Management
Running Kubelet in Standalone Mode
Configuring swap memory on Kubernetes nodes
Install Drivers and Allocate Devices with DRA
Namespaces Walkthrough
Services
Connecting Applications with Services
Using Source IP
Explore Termination Behavior for Pods And Their Endpoints
Reference
Glossary
API Overview
Declarative API Validation
Kubernetes API Concepts
Server-Side Apply
Client Libraries
Common Expression Language in Kubernetes
Kubernetes Deprecation Policy
Deprecated API Migration Guide
Kubernetes API health endpoints
API Access Control
Authenticating
Authenticating with Bootstrap Tokens
Authorization
Using RBAC Authorization
Using Node Authorization
Webhook Mode
Using ABAC Authorization
Admission Control
Dynamic Admission Control
Managing Service Accounts
Certificates and Certificate Signing Requests
Mapping PodSecurityPolicies to Pod Security Standards
Kubelet authentication/authorization
TLS bootstrapping
Mutating Admission Policy
Validating Admission Policy
Well-Known Labels, Annotations and Taints
Audit Annotations
Kubernetes API
Workload Resources
Pod
Binding
PodTemplate
ReplicationController
ReplicaSet
Deployment
StatefulSet
ControllerRevision
DaemonSet
Job
CronJob
HorizontalPodAutoscaler
HorizontalPodAutoscaler
PriorityClass
DeviceTaintRule v1alpha3
ResourceClaim
ResourceClaimTemplate
ResourceSlice
Service Resources
Service
Endpoints
EndpointSlice
Ingress
IngressClass
Config and Storage Resources
ConfigMap
Secret
CSIDriver
CSINode
CSIStorageCapacity
PersistentVolumeClaim
PersistentVolume
StorageClass
StorageVersionMigration v1alpha1
Volume
VolumeAttachment
VolumeAttributesClass
Authentication Resources
ServiceAccount
TokenRequest
TokenReview
CertificateSigningRequest
ClusterTrustBundle v1beta1
SelfSubjectReview
PodCertificateRequest v1alpha1
Authorization Resources
LocalSubjectAccessReview
SelfSubjectAccessReview
SelfSubjectRulesReview
SubjectAccessReview
ClusterRole
ClusterRoleBinding
Role
RoleBinding
Policy Resources
FlowSchema
LimitRange
ResourceQuota
NetworkPolicy
PodDisruptionBudget
PriorityLevelConfiguration
ValidatingAdmissionPolicy
ValidatingAdmissionPolicyBinding
MutatingAdmissionPolicy v1beta1
MutatingAdmissionPolicyBinding v1alpha1
Extend Resources
CustomResourceDefinition
DeviceClass
MutatingWebhookConfiguration
ValidatingWebhookConfiguration
Cluster Resources
APIService
ComponentStatus
Event
IPAddress
Lease
LeaseCandidate v1beta1
Namespace
Node
RuntimeClass
ServiceCIDR
Common Definitions
DeleteOptions
LabelSelector
ListMeta
LocalObjectReference
NodeSelectorRequirement
ObjectFieldSelector
ObjectMeta
ObjectReference
Patch
Quantity
ResourceFieldSelector
Status
TypedLocalObjectReference
Other Resources
MutatingAdmissionPolicyBindingList v1beta1
Common Parameters
Instrumentation
Service Level Indicator Metrics
CRI Pod & Container Metrics
Node metrics data
Understand Pressure Stall Information (PSI) Metrics
Kubernetes z-pages
Kubernetes Metrics Reference
Kubernetes Issues and Security
Kubernetes Issue Tracker
Kubernetes Security and Disclosure Information
CVE feed
Node Reference Information
Kubelet Checkpoint API
Linux Kernel Version Requirements
Articles on dockershim Removal and on Using CRI-compatible Runtimes
Node Labels Populated By The Kubelet
Local Files And Paths Used By The Kubelet
Kubelet Configuration Directory Merging
Kubelet Device Manager API Versions
Kubelet Systemd Watchdog
Node Status
Seccomp and Kubernetes
Linux Node Swap Behaviors
Networking Reference
Protocols for Services
Ports and Protocols
Virtual IPs and Service Proxies
Setup tools
Kubeadm
kubeadm init
kubeadm join
kubeadm upgrade
kubeadm upgrade phases
kubeadm config
kubeadm reset
kubeadm token
kubeadm version
kubeadm alpha
kubeadm certs
kubeadm init phase
kubeadm join phase
kubeadm kubeconfig
kubeadm reset phase
Implementation details
Command line tool (kubectl)
Introduction to kubectl
kubectl Quick Reference
kubectl reference
kubectl
kubectl annotate
kubectl api-resources
kubectl api-versions
kubectl apply
kubectl apply edit-last-applied
kubectl apply set-last-applied
kubectl apply view-last-applied
kubectl attach
kubectl auth
kubectl auth can-i
kubectl auth reconcile
kubectl auth whoami
kubectl autoscale
kubectl certificate
kubectl certificate approve
kubectl certificate deny
kubectl cluster-info
kubectl cluster-info dump
kubectl completion
kubectl config
kubectl config current-context
kubectl config delete-cluster
kubectl config delete-context
kubectl config delete-user
kubectl config get-clusters
kubectl config get-contexts
kubectl config get-users
kubectl config rename-context
kubectl config set
kubectl config set-cluster
kubectl config set-context
kubectl config set-credentials
kubectl config unset
kubectl config use-context
kubectl config view
kubectl cordon
kubectl cp
kubectl create
kubectl create clusterrole
kubectl create clusterrolebinding
kubectl create configmap
kubectl create cronjob
kubectl create deployment
kubectl create ingress
kubectl create job
kubectl create namespace
kubectl create poddisruptionbudget
kubectl create priorityclass
kubectl create quota
kubectl create role
kubectl create rolebinding
kubectl create secret
kubectl create secret docker-registry
kubectl create secret generic
kubectl create secret tls
kubectl create service
kubectl create service clusterip
kubectl create service externalname
kubectl create service loadbalancer
kubectl create service nodeport
kubectl create serviceaccount
kubectl create token
kubectl debug
kubectl delete
kubectl describe
kubectl diff
kubectl drain
kubectl edit
kubectl events
kubectl exec
kubectl explain
kubectl expose
kubectl get
kubectl kustomize
kubectl label
kubectl logs
kubectl options
kubectl patch
kubectl plugin
kubectl plugin list
kubectl port-forward
kubectl proxy
kubectl replace
kubectl rollout
kubectl rollout history
kubectl rollout pause
kubectl rollout restart
kubectl rollout resume
kubectl rollout status
kubectl rollout undo
kubectl run
kubectl scale
kubectl set
kubectl set env
kubectl set image
kubectl set resources
kubectl set selector
kubectl set serviceaccount
kubectl set subject
kubectl taint
kubectl top
kubectl top node
kubectl top pod
kubectl uncordon
kubectl version
kubectl wait
kubectl Commands
kubectl
JSONPath Support
kubectl for Docker Users
kubectl Usage Conventions
Kubectl user preferences (kuberc)
Component tools
Feature Gates
Feature Gates (removed)
kubelet
kube-apiserver
kube-controller-manager
kube-proxy
kube-scheduler
Debug cluster
Flow control
Configuration APIs
Client Authentication (v1)
Client Authentication (v1beta1)
Event Rate Limit Configuration (v1alpha1)
Image Policy API (v1alpha1)
kube-apiserver Admission (v1)
kube-apiserver Audit Configuration (v1)
kube-apiserver Configuration (v1)
kube-apiserver Configuration (v1alpha1)
kube-apiserver Configuration (v1beta1)
kube-controller-manager Configuration (v1alpha1)
kube-proxy Configuration (v1alpha1)
kube-scheduler Configuration (v1)
kubeadm Configuration (v1beta3)
kubeadm Configuration (v1beta4)
kubeconfig (v1)
Kubelet Configuration (v1)
Kubelet Configuration (v1alpha1)
Kubelet Configuration (v1beta1)
Kubelet CredentialProvider (v1)
kuberc (v1alpha1)
WebhookAdmission Configuration (v1)
External APIs
Kubernetes Custom Metrics (v1beta2)
Kubernetes External Metrics (v1beta1)
Kubernetes Metrics (v1beta1)
Scheduling
Scheduler Configuration
Scheduling Policies
Other Tools
Contribute
Contribute to Kubernetes Documentation
Contributing to Kubernetes blogs
Submitting articles to Kubernetes blogs
Blog guidelines
Blog article mirroring
Post-release communications
Helping as a blog writing buddy
Suggesting content improvements
Contributing new content
Opening a pull request
Documenting for a release
Case studies
Reviewing changes
Reviewing pull requests
For approvers and reviewers
Localizing Kubernetes documentation
Participating in SIG Docs
Roles and responsibilities
Issue Wranglers
PR wranglers
Documentation style overview
Content guide
Style guide
Diagram guide
Writing a new topic
Page content types
Content organization
Custom Hugo Shortcodes
Updating Reference Documentation
Quickstart
Contributing to the Upstream Kubernetes Code
Generating Reference Documentation for the Kubernetes API
Generating Reference Documentation for kubectl Commands
Generating Reference Documentation for Metrics
Generating Reference Pages for Kubernetes Components and Tools
Advanced contributing
Viewing Site Analytics
Docs smoke test pageKubernetes DocumentationConceptsConfigurationResource Management for Pods and ContainersResource Management for Pods and ContainersWhen you specify a Pod, you can optionally specify how much of each resource a
container needs. The most common resources to specify are CPU and memory
(RAM); there are others.When you specify the resource request for containers in a Pod, the
kube-scheduler uses this information to decide which node to place the Pod on.
When you specify a resource limit for a container, the kubelet enforces those
limits so that the running container is not allowed to use more of that resource
than the limit you set. The kubelet also reserves at least the request amount of
that system resource specifically for that container to use.Requests and limitsIf the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its request for that resource specifies.For example, if you set a memory request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.Limits are a different story. Both cpu and memory limits are applied by the kubelet (and
container runtime),
and are ultimately enforced by the kernel. On Linux nodes, the Linux kernel
enforces limits with
cgroups.
The behavior of cpu and memory limit enforcement is slightly different.cpu limits are enforced by CPU throttling. When a container approaches
its cpu limit, the kernel will restrict access to the CPU corresponding to the
container's limit. Thus, a cpu limit is a hard limit the kernel enforces.
Containers may not use more CPU than is specified in their cpu limit.memory limits are enforced by the kernel with out of memory (OOM) kills. When
a container uses more than its memory limit, the kernel may terminate it. However,
terminations only happen when the kernel detects memory pressure. Thus, a
container that over allocates memory may not be immediately killed. This means
memory limits are enforced reactively. A container may use more memory than
its memory limit, but if it does, it may get killed.Note:There is an alpha feature MemoryQoS which attempts to add more preemptive
limit enforcement for memory (as opposed to reactive enforcement by the OOM
killer). However, this effort is
stalled
due to a potential livelock situation a memory hungry can cause.Note:If you specify a limit for a resource, but do not specify any request, and no admission-time
mechanism has applied a default request for that resource, then Kubernetes copies the limit
you specified and uses it as the requested value for the resource.Resource typesCPU and memory are each a resource type. A resource type has a base unit.
CPU represents compute processing and is specified in units of Kubernetes CPUs.
Memory is specified in units of bytes.
For Linux workloads, you can specify huge page resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.For example, on a system where the default page size is 4KiB, you could specify a limit,
hugepages-2Mi: 80Mi. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.Note:You cannot overcommit hugepages-* resources.
This is different from the memory and cpu resources.CPU and memory are collectively referred to as compute resources, or resources. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
API resources. API resources, such as Pods and
Services are objects that can be read and modified
through the Kubernetes API server.Resource requests and limits of Pod and containerFor each container, you can specify resource limits and requests,
including the following:spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.limits.hugepages-<size>spec.containers[].resources.requests.cpuspec.containers[].resources.requests.memoryspec.containers[].resources.requests.hugepages-<size>Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
For a particular resource, a Pod resource request/limit is the sum of the
resource requests/limits of that type for each container in the Pod.Pod-level resource specificationFEATURE STATE:
Kubernetes v1.34 [beta] (enabled by default: true)Provided your cluster has the PodLevelResources
feature gate enabled,
you can specify resource requests and limits at
the Pod level. At the Pod level, Kubernetes 1.34
only supports resource requests or limits for specific resource types: cpu and /
or memory and / or hugepages. With this feature, Kubernetes allows you to declare an overall resource
budget for the Pod, which is especially helpful when dealing with a large number of
containers where it can be difficult to accurately gauge individual resource needs.
Additionally, it enables containers within a Pod to share idle resources with each
other, improving resource utilization.For a Pod, you can specify resource limits and requests for CPU and memory by including the following:spec.resources.limits.cpuspec.resources.limits.memoryspec.resources.limits.hugepages-<size>spec.resources.requests.cpuspec.resources.requests.memoryspec.resources.requests.hugepages-<size>Resource units in KubernetesCPU resource unitsLimits and requests for CPU resources are measured in cpu units.
In Kubernetes, 1 CPU unit is equivalent to 1 physical CPU core,
or 1 virtual core, depending on whether the node is a physical host
or a virtual machine running inside a physical machine.Fractional requests are allowed. When you define a container with
spec.containers[].resources.requests.cpu set to 0.5, you are requesting half
as much CPU time compared to if you asked for 1.0 CPU.
For CPU resource units, the quantity expression 0.1 is equivalent to the
expression 100m, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,
500m CPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.Note:Kubernetes doesn't allow you to specify CPU resources with a precision finer than
1m or 0.001 CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form
instead of the decimal form when using less than 1 CPU unit.For example, you have a Pod that uses 5m or 0.005 CPU and would like to decrease
its CPU resources. By using the decimal form, it's harder to spot that 0.0005 CPU
is an invalid value, while by using the milliCPU form, it's easier to spot that
0.5m is an invalid value.Memory resource unitsLimits and requests for memory are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these
quantity suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:128974848, 129e6, 129M,
128974848000m, 123Mi
Pay attention to the case of the suffixes. If you request 400m of memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)
or 400 megabytes (400M).Container resources exampleThe following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (226 bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.---
apiVersion: v1
kind: Pod
metadata:
name: frontend
spec:
containers:
- name: app
image: images.my-company.example/app:v4
resources:
requests:
memory: "64Mi"
cpu: "250m"
limits:
memory: "128Mi"
cpu: "500m"
- name: log-aggregator
image: images.my-company.example/log-aggregator:v6
resources:
requests:
memory: "64Mi"
cpu: "250m"
limits:
memory: "128Mi"
cpu: "500m"
Pod resources exampleFEATURE STATE:
Kubernetes v1.34 [beta] (enabled by default: true)This feature can be enabled by setting the PodLevelResources
feature gate.
The following Pod has an explicit request of 1 CPU and 100 MiB of memory, and an
explicit limit of 1 CPU and 200 MiB of memory. The pod-resources-demo-ctr-1
container has explicit requests and limits set. However, the
pod-resources-demo-ctr-2 container will simply share the resources available
within the Pod resource boundaries, as it does not have explicit requests and limits
set.pods/resource/pod-level-resources.yaml
apiVersion: v1
kind: Pod
metadata:
name: pod-resources-demo
namespace: pod-resources-example
spec:
resources:
limits:
cpu: "1"
memory: "200Mi"
requests:
cpu: "1"
memory: "100Mi"
containers:
- name: pod-resources-demo-ctr-1
image: nginx
resources:
limits:
cpu: "0.5"
memory: "100Mi"
requests:
cpu: "0.5"
memory: "50Mi"
- name: pod-resources-demo-ctr-2
image: fedora
command:
- sleep
- inf
How Pods with resource requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.How Kubernetes applies resource requests and limitsWhen the kubelet starts a container as part of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.On Linux, the container runtime typically configures
kernel cgroups that apply and enforce the
limits you defined.The CPU limit defines a hard ceiling on how much CPU time the container can use.
During each scheduling interval (time slice), the Linux kernel checks to see if this
limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.The CPU request typically defines a weighting. If several different containers (cgroups)
want to run on a contended system, workloads with larger CPU requests are allocated more
CPU time than workloads with small requests.The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
cgroups v2, the container runtime might use the memory request as a hint to set
memory.min and memory.low.The memory limit defines a memory limit for that cgroup. If the container tries to
allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
and, typically, intervenes by stopping one of the processes in the container that tried
to allocate memory. If that process is the container's PID 1, and the container is marked
as restartable, Kubernetes restarts the container.The memory limit for the Pod or container can also apply to pages in memory backed
volumes, such as an emptyDir. The kubelet tracks tmpfs emptyDir volumes as container
memory use, rather than as local ephemeral storage.　When using memory backed emptyDir,
be sure to check the notes below.If a container exceeds its memory request and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will be
evicted.A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.To determine whether a container cannot be scheduled or is being killed due to resource limits,
see the Troubleshooting section.Monitoring compute & memory resource usageThe kubelet reports the resource usage of a Pod as part of the Pod
status.If optional tools for monitoring
are available in your cluster, then Pod resource usage can be retrieved either
from the Metrics API
directly or from your monitoring tools.Considerations for memory backed emptyDir volumesCaution:If you do not specify a sizeLimit for an emptyDir volume, that volume may
consume up to that pod's memory limit (Pod.spec.containers[].resources.limits.memory).
If you do not set a memory limit, the pod has no upper bound on memory consumption,
and can consume all available memory on the node. Kubernetes schedules pods based
on resource requests (Pod.spec.containers[].resources.requests) and will not
consider memory usage above the request when deciding if another pod can fit on
a given node. This can result in a denial of service and cause the OS to do
out-of-memory (OOM) handling. It is possible to create any number of emptyDirs
that could potentially consume all available memory on the node, making OOM
more likely.From the perspective of memory management, there are some similarities between
when a process uses memory as a work area and when using memory-backed
emptyDir. But when using memory as a volume, like memory-backed emptyDir,
there are additional points below that you should be careful of:Files stored on a memory-backed volume are almost entirely managed by the
user application. Unlike when used as a work area for a process, you can not
rely on things like language-level garbage collection.The purpose of writing files to a volume is to save data or pass it between
applications. Neither Kubernetes nor the OS may automatically delete files
from a volume, so memory used by those files can not be reclaimed when the
system or the pod are under memory pressure.A memory-backed emptyDir is useful because of its performance, but memory
is generally much smaller in size and much higher in cost than other storage
media, such as disks or SSDs. Using large amounts of memory for emptyDir
volumes may affect the normal operation of your pod or of the whole node,
so should be used carefully.If you are administering a cluster or namespace, you can also set
ResourceQuota that limits memory use;
you may also want to define a LimitRange
for additional enforcement.
If you specify a spec.containers[].resources.limits.memory for each Pod,
then the maximum size of an emptyDir volume will be the pod's memory limit.As an alternative, a cluster administrator can enforce size limits for
emptyDir volumes in new Pods using a policy mechanism such as
ValidationAdmissionPolicy.Local ephemeral storageFEATURE STATE:
Kubernetes v1.25 [stable]Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount emptyDir
volumes into containers.The kubelet also uses this kind of storage to hold
node-level container logs,
container images, and the writable layers of running containers.Caution:If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.Note:To make the resource quota work on ephemeral-storage, two things need to be done:An admin sets the resource quota for ephemeral-storage in a namespace.A user needs to specify limits for the ephemeral-storage resource in the Pod spec.If the user doesn't specify the ephemeral-storage resource limit in the Pod spec,
the resource quota is not enforced on ephemeral-storage.Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.Configurations for local ephemeral storageKubernetes supports two ways to configure local ephemeral storage on a node:Single filesystemTwo filesystemsIn this configuration, you place all different kinds of ephemeral local data
(emptyDir volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.The kubelet also writes
node-level container logs
and treats these similarly to ephemeral local storage.The kubelet writes logs to files inside its configured log directory (/var/log
by default); and has a base directory for other locally stored data
(/var/lib/kubelet by default).Typically, both /var/lib/kubelet and /var/log are on the system root filesystem,
and the kubelet is designed with that layout in mind.Your node can have as many other filesystems, not used for Kubernetes,
as you like.You have a filesystem on the node that you're using for ephemeral data that
comes from running Pods: logs, and emptyDir volumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.The kubelet also writes
node-level container logs
into the first filesystem, and treats these similarly to ephemeral local storage.You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.The first filesystem does not hold any image layers or writeable layers.Your node can have as many other filesystems, not used for Kubernetes,
as you like.The kubelet can measure how much local storage it is using. It does this provided
that you have set up the node using one of the supported configurations for local
ephemeral storage.If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.Note:The kubelet tracks tmpfs emptyDir volumes as container memory use, rather
than as local ephemeral storage.Note:The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to /var/lib/kubelet or /var/lib/containers will not report ephemeral storage correctly.Setting requests and limits for local ephemeral storageYou can specify ephemeral-storage for managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:spec.containers[].resources.limits.ephemeral-storagespec.containers[].resources.requests.ephemeral-storageLimits and requests for ephemeral-storage are measured in byte quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following quantities all represent roughly the same value:128974848129e6129M123MiPay attention to the case of the suffixes. If you request 400m of ephemeral-storage, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)
or 400 megabytes (400M).In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and
a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be
consumed by the emptyDir volume.apiVersion: v1
kind: Pod
metadata:
name: frontend
spec:
containers:
- name: app
image: images.my-company.example/app:v4
resources:
requests:
ephemeral-storage: "2Gi"
limits:
ephemeral-storage: "4Gi"
volumeMounts:
- name: ephemeral
mountPath: "/tmp"
- name: log-aggregator
image: images.my-company.example/log-aggregator:v6
resources:
requests:
ephemeral-storage: "2Gi"
limits:
ephemeral-storage: "4Gi"
volumeMounts:
- name: ephemeral
mountPath: "/tmp"
volumes:
- name: ephemeral
emptyDir:
sizeLimit: 500Mi
How Pods with ephemeral-storage requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, see
Node Allocatable.The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.Ephemeral storage consumption managementIf the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:emptyDir volumes, except tmpfs emptyDir volumesdirectories holding node-level logswriteable container layersIf a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod's emptyDir
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.Caution:If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.However, if the filesystem space for writeable container layers, node-level logs,
or emptyDir volumes falls low, the node
taints itself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.See the supported configurations
for ephemeral local storage.The kubelet supports different ways to measure Pod storage use:Periodic scanningFilesystem project quotaThe kubelet performs regular, scheduled checks that scan each
emptyDir volume, container log directory, and writeable container layer.The scan measures how much space is used.Note:In this mode, the kubelet does not track open file descriptors
for deleted files.If you (or a container) create a file inside an emptyDir volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.FEATURE STATE:
Kubernetes v1.31 [beta] (enabled by default: false)Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the emptyDir volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.Note:Project quotas let you monitor storage use; they do not enforce limits.Kubernetes uses project IDs starting from 1048576. The IDs in use are
registered in /etc/projects and /etc/projid. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in /etc/projects and /etc/projid so that
Kubernetes does not use them.Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.To use quotas to track a pod's resource usage, the pod must be in
a user namespace. Within user namespaces, the kernel restricts changes
to projectIDs on the filesystem, ensuring the reliability of storage
metrics calculated by quotas.If you want to use project quotas, you should:Enable the LocalStorageCapacityIsolationFSQuotaMonitoring=true
feature gate
using the featureGates field in the
kubelet configuration.Ensure the UserNamespacesSupport
feature gate
is enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.# For ext4, with /dev/block-device not mounted
sudo tune2fs -O project -Q prjquota /dev/block-device
Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named prjquota.If you don't want to use project quotas, you should:Disable the LocalStorageCapacityIsolationFSQuotaMonitoring
feature gate
using the featureGates field in the
kubelet configuration.Extended resourcesExtended resources are fully-qualified resource names outside the
kubernetes.io domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.Managing extended resourcesNode-level extended resourcesNode-level extended resources are tied to nodes.Device plugin managed resourcesSee Device
Plugin
for how to advertise device plugin managed resources on each node.Other resourcesTo advertise a new node-level extended resource, the cluster operator can
submit a PATCH HTTP request to the API server to specify the available
quantity in the status.capacity for a node in the cluster. After this
operation, the node's status.capacity will include a new resource. The
status.allocatable field is updated automatically with the new resource
asynchronously by the kubelet.Because the scheduler uses the node's status.allocatable value when
evaluating Pod fitness, the scheduler only takes account of the new value after
that asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource can be scheduled on that node.Example:Here is an example showing how to use curl to form an HTTP request that
advertises five "example.com/foo" resources on node k8s-node-1 whose master
is k8s-master.curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
Note:In the preceding request, ~1 is the encoding for the character /
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
IETF RFC 6901, section 3.Cluster-level extended resourcesCluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.You can specify the extended resources that are handled by scheduler extenders
in scheduler configurationExample:The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.The scheduler sends a Pod to the scheduler extender only if the Pod requests
"example.com/foo".The ignoredByScheduler field specifies that the scheduler does not check
the "example.com/foo" resource in its PodFitsResources predicate.{
"kind": "Policy",
"apiVersion": "v1",
"extenders": [
{
"urlPrefix":"<extender-endpoint>",
"bindVerb": "bind",
"managedResources": [
{
"name": "example.com/foo",
"ignoredByScheduler": true
}
]
}
]
}
Extended resources allocation by DRAExtended resources allocation by DRA allows cluster administrators to specify an extendedResourceName
in DeviceClass, then the devices matching the DeviceClass can be requested from a pod's extended
resource requests. Read more about
Extended Resource allocation by DRA.Consuming extended resourcesUsers can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.The API server restricts quantities of extended resources to whole numbers.
Examples of valid quantities are 3, 3000m and 3Ki. Examples of
invalid quantities are 0.5 and 1500m (because 1500m would result in 1.5).Note:Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than kubernetes.io which is reserved.To consume an extended resource in a Pod, include the resource name as a key
in the spec.containers[].resources.limits map in the container spec.Note:Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the PENDING state
as long as the resource request cannot be satisfied.Example:The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: my-container
image: myimage
resources:
requests:
cpu: 2
example.com/foo: 1
limits:
example.com/foo: 1
PID limitingProcess ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. See
PID Limiting for information.TroubleshootingMy Pods are pending with event message FailedSchedulingIf the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An
Event is produced
each time the scheduler fails to find a place for the Pod. You can use kubectl
to view the events for a Pod; for example:kubectl describe pod frontend | grep -A 9999999999 Events
Events:
Type
Reason
Age
From
Message
----
------
----
----
-------
Warning
FailedScheduling
23s
default-scheduler
0/42 nodes available: insufficient cpu
In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:Add more nodes to the cluster.Terminate unneeded Pods to make room for pending Pods.Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity of cpu: 1, then a Pod with a request of cpu: 1.1 will
never be scheduled.Check for node taints. If most of your nodes are tainted, and the new Pod does
not tolerate that taint, the scheduler only considers placements onto the
remaining nodes that don't have that taint.You can check node capacities and amounts allocated with the
kubectl describe nodes command. For example:kubectl describe nodes e2e-test-node-pool-4lw4
Name:
e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
cpu:
2
memory:
7679792Ki
pods:
110
Allocatable:
cpu:
1800m
memory:
7474992Ki
pods:
110
[ ... lines removed for clarity ...]
Non-terminated Pods:
(5 in total)
Namespace
Name
CPU Requests
CPU Limits
Memory Requests
Memory Limits
---------
----
------------
----------
---------------
-------------
kube-system
fluentd-gcp-v1.38-28bv1
100m (5%)
0 (0%)
200Mi (2%)
200Mi (2%)
kube-system
kube-dns-3297075139-61lj3
260m (13%)
0 (0%)
100Mi (1%)
170Mi (2%)
kube-system
kube-proxy-e2e-test-...
100m (5%)
0 (0%)
0 (0%)
0 (0%)
kube-system
monitoring-influxdb-grafana-v4-z1m12
200m (10%)
200m (10%)
600Mi (8%)
600Mi (8%)
kube-system
node-problem-detector-v0.1-fj7m3
20m (1%)
200m (10%)
20Mi (0%)
100Mi (1%)
Allocated resources:
(Total limits may be over 100 percent, i.e., overcommitted.)
CPU Requests
CPU Limits
Memory Requests
Memory Limits
------------
----------
---------------
-------------
680m (34%)
400m (20%)
920Mi (11%)
1070Mi (13%)
In the preceding output, you can see that if a Pod requests more than 1.120 CPUs
or more than 6.23Gi of memory, that Pod will not fit on the node.By looking at the “Pods” section, you can see which Pods are taking up space on
the node.The amount of resources available to Pods is less than the node capacity because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a .status.allocatable field
(see NodeStatus
for details).The .status.allocatable field describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, see
Reserve Compute Resources for System Daemons.You can configure resource quotas
to limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.You should also consider what access you grant to that namespace:
full write access to a namespace allows someone with that access to remove any
resource, including a configured ResourceQuota.My container is terminatedYour container might get terminated because it is resource-starved. To check
whether a container is being killed because it is hitting a resource limit, call
kubectl describe pod on the Pod of interest:kubectl describe pod simmemleak-hra99
The output is similar to:Name:
simmemleak-hra99
Namespace:
default
Image(s):
saadali/simmemleak
Node:
kubernetes-node-tf0f/10.240.216.66
Labels:
name=simmemleak
Status:
Running
Reason:
Message:
IP:
10.244.2.75
Containers:
simmemleak:
Image:
saadali/simmemleak:latest
Limits:
cpu:
100m
memory:
50Mi
State:
Running
Started:
Tue, 07 Jul 2019 12:54:41 -0700
Last State:
Terminated
Reason:
OOMKilled
Exit Code:
137
Started:
Fri, 07 Jul 2019 12:54:30 -0700
Finished:
Fri, 07 Jul 2019 12:54:33 -0700
Ready:
False
Restart Count:
5
Conditions:
Type
Status
Ready
False
Events:
Type
Reason
Age
From
Message
----
------
----
----
-------
Normal
Scheduled
42s
default-scheduler
Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
Normal
Pulled
41s
kubelet
Container image "saadali/simmemleak:latest" already present on machine
Normal
Created
41s
kubelet
Created container simmemleak
Normal
Started
40s
kubelet
Started container simmemleak
Normal
Killing
32s
kubelet
Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
In the preceding example, the Restart Count: 5 indicates that the simmemleak
container in the Pod was terminated and restarted five times (so far).
The OOMKilled reason shows that the container tried to use more memory than its limit.Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.What's nextGet hands-on experience assigning Memory resources to containers and Pods.Get hands-on experience assigning CPU resources to containers and Pods.Read how the API reference defines a container
and its resource requirementsRead about project quotas in XFSRead more about the kube-scheduler configuration reference (v1)Read more about Quality of Service classes for PodsRead more about Extended Resource allocation by DRAFeedbackWas this page helpful?Yes
NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on
Stack Overflow.
Open an issue in the GitHub Repository if you want to
report a problem
or
suggest an improvement.Last modified August 06, 2025 at 3:40 PM PST: removed backtick around DeviceClass (3a894e1291) Edit this page
Create child page
Create an issue
Print entire sectionRequests and limitsResource typesResource requests and limits of Pod and containerPod-level resource specificationResource units in KubernetesCPU resource unitsMemory resource unitsContainer resources examplePod resources exampleHow Pods with resource requests are scheduledHow Kubernetes applies resource requests and limitsMonitoring compute & memory resource usageConsiderations for memory backed emptyDir volumesLocal ephemeral storageConfigurations for local ephemeral storageSetting requests and limits for local ephemeral storageHow Pods with ephemeral-storage requests are scheduledEphemeral storage consumption managementExtended resourcesManaging extended resourcesConsuming extended resourcesPID limitingTroubleshootingMy Pods are pending with event message FailedSchedulingMy container is terminatedWhat's next© 2025 The Kubernetes Authors | Documentation Distributed under CC BY 4.0© 2025 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage pageICP license: 京ICP备17074266号-3