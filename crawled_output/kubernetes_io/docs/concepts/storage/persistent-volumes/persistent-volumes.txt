Persistent Volumes | Kubernetes
KubernetesDocumentationKubernetes BlogTrainingCareersPartnersCommunityVersionsRelease Information
v1.34
v1.33
v1.32
v1.31
v1.30English中文 (Chinese)
Français (French)
Bahasa Indonesia (Indonesian)
日本語 (Japanese)
한국어 (Korean)
Português (Portuguese)
Español (Spanish)
Englishবাংলা (Bengali)
中文 (Chinese)
Français (French)
Deutsch (German)
हिन्दी (Hindi)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
日本語 (Japanese)
한국어 (Korean)
Polski (Polish)
Português (Portuguese)
Русский (Russian)
Español (Spanish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)Kubernetes Documentation
Documentation
Available Documentation Versions
Getting started
Learning environment
Production environment
Container Runtimes
Installing Kubernetes with deployment tools
Bootstrapping clusters with kubeadm
Installing kubeadm
Troubleshooting kubeadm
Creating a cluster with kubeadm
Customizing components with the kubeadm API
Options for Highly Available Topology
Creating Highly Available Clusters with kubeadm
Set up a High Availability etcd Cluster with kubeadm
Configuring each kubelet in your cluster using kubeadm
Dual-stack support with kubeadm
Turnkey Cloud Solutions
Best practices
Considerations for large clusters
Running in multiple zones
Validate node setup
Enforcing Pod Security Standards
PKI certificates and requirements
Concepts
Overview
Kubernetes Components
Objects In Kubernetes
Kubernetes Object Management
Object Names and IDs
Labels and Selectors
Namespaces
Annotations
Field Selectors
Finalizers
Owners and Dependents
Recommended Labels
The Kubernetes API
Cluster Architecture
Nodes
Communication between Nodes and the Control Plane
Controllers
Leases
Cloud Controller Manager
About cgroup v2
Kubernetes Self-Healing
Container Runtime Interface (CRI)
Garbage Collection
Mixed Version Proxy
Containers
Images
Container Environment
Runtime Class
Container Lifecycle Hooks
Workloads
Pods
Pod Lifecycle
Init Containers
Sidecar Containers
Ephemeral Containers
Disruptions
Pod Hostname
Pod Quality of Service Classes
User Namespaces
Downward API
Workload Management
Deployments
ReplicaSet
StatefulSets
DaemonSet
Jobs
Automatic Cleanup for Finished Jobs
CronJob
ReplicationController
Autoscaling Workloads
Managing Workloads
Services, Load Balancing, and Networking
Service
Ingress
Ingress Controllers
Gateway API
EndpointSlices
Network Policies
DNS for Services and Pods
IPv4/IPv6 dual-stack
Topology Aware Routing
Networking on Windows
Service ClusterIP allocation
Service Internal Traffic Policy
Storage
Volumes
Persistent Volumes
Projected Volumes
Ephemeral Volumes
Storage Classes
Volume Attributes Classes
Dynamic Volume Provisioning
Volume Snapshots
Volume Snapshot Classes
CSI Volume Cloning
Storage Capacity
Node-specific Volume Limits
Volume Health Monitoring
Windows Storage
Configuration
Configuration Best Practices
ConfigMaps
Secrets
Liveness, Readiness, and Startup Probes
Resource Management for Pods and Containers
Organizing Cluster Access Using kubeconfig Files
Resource Management for Windows nodes
Security
Cloud Native Security
Pod Security Standards
Pod Security Admission
Service Accounts
Pod Security Policies
Security For Linux Nodes
Security For Windows Nodes
Controlling Access to the Kubernetes API
Role Based Access Control Good Practices
Good practices for Kubernetes Secrets
Multi-tenancy
Hardening Guide - Authentication Mechanisms
Hardening Guide - Scheduler Configuration
Kubernetes API Server Bypass Risks
Linux kernel security constraints for Pods and containers
Security Checklist
Application Security Checklist
Policies
Limit Ranges
Resource Quotas
Process ID Limits And Reservations
Node Resource Managers
Scheduling, Preemption and Eviction
Kubernetes Scheduler
Assigning Pods to Nodes
Pod Overhead
Pod Scheduling Readiness
Pod Topology Spread Constraints
Taints and Tolerations
Scheduling Framework
Dynamic Resource Allocation
Scheduler Performance Tuning
Resource Bin Packing
Pod Priority and Preemption
Node-pressure Eviction
API-initiated Eviction
Cluster Administration
Node Shutdowns
Swap memory management
Node Autoscaling
Certificates
Cluster Networking
Admission Webhook Good Practices
Good practices for Dynamic Resource Allocation as a Cluster Admin
Logging Architecture
Compatibility Version For Kubernetes Control Plane Components
Metrics For Kubernetes System Components
Metrics for Kubernetes Object States
System Logs
Traces For Kubernetes System Components
Proxies in Kubernetes
API Priority and Fairness
Installing Addons
Coordinated Leader Election
Windows in Kubernetes
Windows containers in Kubernetes
Guide for Running Windows Containers in Kubernetes
Extending Kubernetes
Compute, Storage, and Networking Extensions
Network Plugins
Device Plugins
Extending the Kubernetes API
Custom Resources
Kubernetes API Aggregation Layer
Operator pattern
Tasks
Install Tools
Install and Set Up kubectl on Linux
Install and Set Up kubectl on macOS
Install and Set Up kubectl on Windows
Administer a Cluster
Administration with kubeadm
Adding Linux worker nodes
Adding Windows worker nodes
Upgrading kubeadm clusters
Upgrading Linux nodes
Upgrading Windows nodes
Configuring a cgroup driver
Certificate Management with kubeadm
Reconfiguring a kubeadm cluster
Changing The Kubernetes Package Repository
Overprovision Node Capacity For A Cluster
Migrating from dockershim
Changing the Container Runtime on a Node from Docker Engine to containerd
Find Out What Container Runtime is Used on a Node
Troubleshooting CNI plugin-related errors
Check whether dockershim removal affects you
Migrating telemetry and security agents from dockershim
Generate Certificates Manually
Manage Memory, CPU, and API Resources
Configure Default Memory Requests and Limits for a Namespace
Configure Default CPU Requests and Limits for a Namespace
Configure Minimum and Maximum Memory Constraints for a Namespace
Configure Minimum and Maximum CPU Constraints for a Namespace
Configure Memory and CPU Quotas for a Namespace
Configure a Pod Quota for a Namespace
Install a Network Policy Provider
Use Antrea for NetworkPolicy
Use Calico for NetworkPolicy
Use Cilium for NetworkPolicy
Use Kube-router for NetworkPolicy
Romana for NetworkPolicy
Weave Net for NetworkPolicy
Access Clusters Using the Kubernetes API
Advertise Extended Resources for a Node
Autoscale the DNS Service in a Cluster
Change the Access Mode of a PersistentVolume to ReadWriteOncePod
Change the default StorageClass
Switching from Polling to CRI Event-based Updates to Container Status
Change the Reclaim Policy of a PersistentVolume
Cloud Controller Manager Administration
Configure a kubelet image credential provider
Configure Quotas for API Objects
Control CPU Management Policies on the Node
Control Topology Management Policies on a node
Customizing DNS Service
Debugging DNS Resolution
Declare Network Policy
Developing Cloud Controller Manager
Enable Or Disable A Kubernetes API
Encrypting Confidential Data at Rest
Decrypt Confidential Data that is Already Encrypted at Rest
Guaranteed Scheduling For Critical Add-On Pods
IP Masquerade Agent User Guide
Limit Storage Consumption
Migrate Replicated Control Plane To Use Cloud Controller Manager
Operating etcd clusters for Kubernetes
Reserve Compute Resources for System Daemons
Running Kubernetes Node Components as a Non-root User
Safely Drain a Node
Securing a Cluster
Set Kubelet Parameters Via A Configuration File
Share a Cluster with Namespaces
Upgrade A Cluster
Use Cascading Deletion in a Cluster
Using a KMS provider for data encryption
Using CoreDNS for Service Discovery
Using NodeLocal DNSCache in Kubernetes Clusters
Using sysctls in a Kubernetes Cluster
Utilizing the NUMA-aware Memory Manager
Verify Signed Kubernetes Artifacts
Configure Pods and Containers
Assign Memory Resources to Containers and Pods
Assign CPU Resources to Containers and Pods
Assign Devices to Pods and Containers
Set Up DRA in a Cluster
Allocate Devices to Workloads with DRA
Assign Pod-level CPU and memory resources
Configure GMSA for Windows Pods and containers
Resize CPU and Memory Resources assigned to Containers
Configure RunAsUserName for Windows pods and containers
Create a Windows HostProcess Pod
Configure Quality of Service for Pods
Assign Extended Resources to a Container
Configure a Pod to Use a Volume for Storage
Configure a Pod to Use a PersistentVolume for Storage
Configure a Pod to Use a Projected Volume for Storage
Configure a Security Context for a Pod or Container
Configure Service Accounts for Pods
Pull an Image from a Private Registry
Configure Liveness, Readiness and Startup Probes
Assign Pods to Nodes
Assign Pods to Nodes using Node Affinity
Configure Pod Initialization
Attach Handlers to Container Lifecycle Events
Configure a Pod to Use a ConfigMap
Share Process Namespace between Containers in a Pod
Use a User Namespace With a Pod
Use an Image Volume With a Pod
Create static Pods
Translate a Docker Compose File to Kubernetes Resources
Enforce Pod Security Standards by Configuring the Built-in Admission Controller
Enforce Pod Security Standards with Namespace Labels
Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller
Monitoring, Logging, and Debugging
Logging in Kubernetes
Monitoring in Kubernetes
Troubleshooting Applications
Debug Pods
Debug Services
Debug a StatefulSet
Determine the Reason for Pod Failure
Debug Init Containers
Debug Running Pods
Get a Shell to a Running Container
Troubleshooting Clusters
Troubleshooting kubectl
Resource metrics pipeline
Tools for Monitoring Resources
Monitor Node Health
Debugging Kubernetes nodes with crictl
Auditing
Debugging Kubernetes Nodes With Kubectl
Developing and debugging services locally using telepresence
Windows debugging tips
Manage Kubernetes Objects
Declarative Management of Kubernetes Objects Using Configuration Files
Declarative Management of Kubernetes Objects Using Kustomize
Managing Kubernetes Objects Using Imperative Commands
Imperative Management of Kubernetes Objects Using Configuration Files
Update API Objects in Place Using kubectl patch
Migrate Kubernetes Objects Using Storage Version Migration
Managing Secrets
Managing Secrets using kubectl
Managing Secrets using Configuration File
Managing Secrets using Kustomize
Inject Data Into Applications
Define a Command and Arguments for a Container
Define Dependent Environment Variables
Define Environment Variables for a Container
Define Environment Variable Values Using An Init Container
Expose Pod Information to Containers Through Environment Variables
Expose Pod Information to Containers Through Files
Distribute Credentials Securely Using Secrets
Run Applications
Run a Stateless Application Using a Deployment
Run a Single-Instance Stateful Application
Run a Replicated Stateful Application
Scale a StatefulSet
Delete a StatefulSet
Force Delete StatefulSet Pods
Horizontal Pod Autoscaling
HorizontalPodAutoscaler Walkthrough
Specifying a Disruption Budget for your Application
Accessing the Kubernetes API from a Pod
Run Jobs
Running Automated Tasks with a CronJob
Coarse Parallel Processing Using a Work Queue
Fine Parallel Processing Using a Work Queue
Indexed Job for Parallel Processing with Static Work Assignment
Job with Pod-to-Pod Communication
Parallel Processing using Expansions
Handling retriable and non-retriable pod failures with Pod failure policy
Access Applications in a Cluster
Deploy and Access the Kubernetes Dashboard
Accessing Clusters
Configure Access to Multiple Clusters
Use Port Forwarding to Access Applications in a Cluster
Use a Service to Access an Application in a Cluster
Connect a Frontend to a Backend Using Services
Create an External Load Balancer
List All Container Images Running in a Cluster
Set up Ingress on Minikube with the NGINX Ingress Controller
Communicate Between Containers in the Same Pod Using a Shared Volume
Configure DNS for a Cluster
Access Services Running on Clusters
Extend Kubernetes
Configure the Aggregation Layer
Use Custom Resources
Extend the Kubernetes API with CustomResourceDefinitions
Versions in CustomResourceDefinitions
Set up an Extension API Server
Configure Multiple Schedulers
Use an HTTP Proxy to Access the Kubernetes API
Use a SOCKS5 Proxy to Access the Kubernetes API
Set up Konnectivity service
TLS
Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest
Configure Certificate Rotation for the Kubelet
Manage TLS Certificates in a Cluster
Manual Rotation of CA Certificates
Manage Cluster Daemons
Building a Basic DaemonSet
Perform a Rolling Update on a DaemonSet
Perform a Rollback on a DaemonSet
Running Pods on Only Some Nodes
Networking
Adding entries to Pod /etc/hosts with HostAliases
Extend Service IP Ranges
Kubernetes Default Service CIDR Reconfiguration
Validate IPv4/IPv6 dual-stack
Extend kubectl with plugins
Manage HugePages
Schedule GPUs
Tutorials
Hello Minikube
Learn Kubernetes Basics
Create a Cluster
Using Minikube to Create a Cluster
Deploy an App
Using kubectl to Create a Deployment
Explore Your App
Viewing Pods and Nodes
Expose Your App Publicly
Using a Service to Expose Your App
Scale Your App
Running Multiple Instances of Your App
Update Your App
Performing a Rolling Update
Configuration
Updating Configuration via a ConfigMap
Configuring Redis using a ConfigMap
Adopting Sidecar Containers
Security
Apply Pod Security Standards at the Cluster Level
Apply Pod Security Standards at the Namespace Level
Restrict a Container's Access to Resources with AppArmor
Restrict a Container's Syscalls with seccomp
Stateless Applications
Exposing an External IP Address to Access an Application in a Cluster
Example: Deploying PHP Guestbook application with Redis
Stateful Applications
StatefulSet Basics
Example: Deploying WordPress and MySQL with Persistent Volumes
Example: Deploying Cassandra with a StatefulSet
Running ZooKeeper, A Distributed System Coordinator
Cluster Management
Running Kubelet in Standalone Mode
Configuring swap memory on Kubernetes nodes
Install Drivers and Allocate Devices with DRA
Namespaces Walkthrough
Services
Connecting Applications with Services
Using Source IP
Explore Termination Behavior for Pods And Their Endpoints
Reference
Glossary
API Overview
Declarative API Validation
Kubernetes API Concepts
Server-Side Apply
Client Libraries
Common Expression Language in Kubernetes
Kubernetes Deprecation Policy
Deprecated API Migration Guide
Kubernetes API health endpoints
API Access Control
Authenticating
Authenticating with Bootstrap Tokens
Authorization
Using RBAC Authorization
Using Node Authorization
Webhook Mode
Using ABAC Authorization
Admission Control
Dynamic Admission Control
Managing Service Accounts
Certificates and Certificate Signing Requests
Mapping PodSecurityPolicies to Pod Security Standards
Kubelet authentication/authorization
TLS bootstrapping
Mutating Admission Policy
Validating Admission Policy
Well-Known Labels, Annotations and Taints
Audit Annotations
Kubernetes API
Workload Resources
Pod
Binding
PodTemplate
ReplicationController
ReplicaSet
Deployment
StatefulSet
ControllerRevision
DaemonSet
Job
CronJob
HorizontalPodAutoscaler
HorizontalPodAutoscaler
PriorityClass
DeviceTaintRule v1alpha3
ResourceClaim
ResourceClaimTemplate
ResourceSlice
Service Resources
Service
Endpoints
EndpointSlice
Ingress
IngressClass
Config and Storage Resources
ConfigMap
Secret
CSIDriver
CSINode
CSIStorageCapacity
PersistentVolumeClaim
PersistentVolume
StorageClass
StorageVersionMigration v1alpha1
Volume
VolumeAttachment
VolumeAttributesClass
Authentication Resources
ServiceAccount
TokenRequest
TokenReview
CertificateSigningRequest
ClusterTrustBundle v1beta1
SelfSubjectReview
PodCertificateRequest v1alpha1
Authorization Resources
LocalSubjectAccessReview
SelfSubjectAccessReview
SelfSubjectRulesReview
SubjectAccessReview
ClusterRole
ClusterRoleBinding
Role
RoleBinding
Policy Resources
FlowSchema
LimitRange
ResourceQuota
NetworkPolicy
PodDisruptionBudget
PriorityLevelConfiguration
ValidatingAdmissionPolicy
ValidatingAdmissionPolicyBinding
MutatingAdmissionPolicy v1beta1
MutatingAdmissionPolicyBinding v1alpha1
Extend Resources
CustomResourceDefinition
DeviceClass
MutatingWebhookConfiguration
ValidatingWebhookConfiguration
Cluster Resources
APIService
ComponentStatus
Event
IPAddress
Lease
LeaseCandidate v1beta1
Namespace
Node
RuntimeClass
ServiceCIDR
Common Definitions
DeleteOptions
LabelSelector
ListMeta
LocalObjectReference
NodeSelectorRequirement
ObjectFieldSelector
ObjectMeta
ObjectReference
Patch
Quantity
ResourceFieldSelector
Status
TypedLocalObjectReference
Other Resources
MutatingAdmissionPolicyBindingList v1beta1
Common Parameters
Instrumentation
Service Level Indicator Metrics
CRI Pod & Container Metrics
Node metrics data
Understand Pressure Stall Information (PSI) Metrics
Kubernetes z-pages
Kubernetes Metrics Reference
Kubernetes Issues and Security
Kubernetes Issue Tracker
Kubernetes Security and Disclosure Information
CVE feed
Node Reference Information
Kubelet Checkpoint API
Linux Kernel Version Requirements
Articles on dockershim Removal and on Using CRI-compatible Runtimes
Node Labels Populated By The Kubelet
Local Files And Paths Used By The Kubelet
Kubelet Configuration Directory Merging
Kubelet Device Manager API Versions
Kubelet Systemd Watchdog
Node Status
Seccomp and Kubernetes
Linux Node Swap Behaviors
Networking Reference
Protocols for Services
Ports and Protocols
Virtual IPs and Service Proxies
Setup tools
Kubeadm
kubeadm init
kubeadm join
kubeadm upgrade
kubeadm upgrade phases
kubeadm config
kubeadm reset
kubeadm token
kubeadm version
kubeadm alpha
kubeadm certs
kubeadm init phase
kubeadm join phase
kubeadm kubeconfig
kubeadm reset phase
Implementation details
Command line tool (kubectl)
Introduction to kubectl
kubectl Quick Reference
kubectl reference
kubectl
kubectl annotate
kubectl api-resources
kubectl api-versions
kubectl apply
kubectl apply edit-last-applied
kubectl apply set-last-applied
kubectl apply view-last-applied
kubectl attach
kubectl auth
kubectl auth can-i
kubectl auth reconcile
kubectl auth whoami
kubectl autoscale
kubectl certificate
kubectl certificate approve
kubectl certificate deny
kubectl cluster-info
kubectl cluster-info dump
kubectl completion
kubectl config
kubectl config current-context
kubectl config delete-cluster
kubectl config delete-context
kubectl config delete-user
kubectl config get-clusters
kubectl config get-contexts
kubectl config get-users
kubectl config rename-context
kubectl config set
kubectl config set-cluster
kubectl config set-context
kubectl config set-credentials
kubectl config unset
kubectl config use-context
kubectl config view
kubectl cordon
kubectl cp
kubectl create
kubectl create clusterrole
kubectl create clusterrolebinding
kubectl create configmap
kubectl create cronjob
kubectl create deployment
kubectl create ingress
kubectl create job
kubectl create namespace
kubectl create poddisruptionbudget
kubectl create priorityclass
kubectl create quota
kubectl create role
kubectl create rolebinding
kubectl create secret
kubectl create secret docker-registry
kubectl create secret generic
kubectl create secret tls
kubectl create service
kubectl create service clusterip
kubectl create service externalname
kubectl create service loadbalancer
kubectl create service nodeport
kubectl create serviceaccount
kubectl create token
kubectl debug
kubectl delete
kubectl describe
kubectl diff
kubectl drain
kubectl edit
kubectl events
kubectl exec
kubectl explain
kubectl expose
kubectl get
kubectl kustomize
kubectl label
kubectl logs
kubectl options
kubectl patch
kubectl plugin
kubectl plugin list
kubectl port-forward
kubectl proxy
kubectl replace
kubectl rollout
kubectl rollout history
kubectl rollout pause
kubectl rollout restart
kubectl rollout resume
kubectl rollout status
kubectl rollout undo
kubectl run
kubectl scale
kubectl set
kubectl set env
kubectl set image
kubectl set resources
kubectl set selector
kubectl set serviceaccount
kubectl set subject
kubectl taint
kubectl top
kubectl top node
kubectl top pod
kubectl uncordon
kubectl version
kubectl wait
kubectl Commands
kubectl
JSONPath Support
kubectl for Docker Users
kubectl Usage Conventions
Kubectl user preferences (kuberc)
Component tools
Feature Gates
Feature Gates (removed)
kubelet
kube-apiserver
kube-controller-manager
kube-proxy
kube-scheduler
Debug cluster
Flow control
Configuration APIs
Client Authentication (v1)
Client Authentication (v1beta1)
Event Rate Limit Configuration (v1alpha1)
Image Policy API (v1alpha1)
kube-apiserver Admission (v1)
kube-apiserver Audit Configuration (v1)
kube-apiserver Configuration (v1)
kube-apiserver Configuration (v1alpha1)
kube-apiserver Configuration (v1beta1)
kube-controller-manager Configuration (v1alpha1)
kube-proxy Configuration (v1alpha1)
kube-scheduler Configuration (v1)
kubeadm Configuration (v1beta3)
kubeadm Configuration (v1beta4)
kubeconfig (v1)
Kubelet Configuration (v1)
Kubelet Configuration (v1alpha1)
Kubelet Configuration (v1beta1)
Kubelet CredentialProvider (v1)
kuberc (v1alpha1)
WebhookAdmission Configuration (v1)
External APIs
Kubernetes Custom Metrics (v1beta2)
Kubernetes External Metrics (v1beta1)
Kubernetes Metrics (v1beta1)
Scheduling
Scheduler Configuration
Scheduling Policies
Other Tools
Contribute
Contribute to Kubernetes Documentation
Contributing to Kubernetes blogs
Submitting articles to Kubernetes blogs
Blog guidelines
Blog article mirroring
Post-release communications
Helping as a blog writing buddy
Suggesting content improvements
Contributing new content
Opening a pull request
Documenting for a release
Case studies
Reviewing changes
Reviewing pull requests
For approvers and reviewers
Localizing Kubernetes documentation
Participating in SIG Docs
Roles and responsibilities
Issue Wranglers
PR wranglers
Documentation style overview
Content guide
Style guide
Diagram guide
Writing a new topic
Page content types
Content organization
Custom Hugo Shortcodes
Updating Reference Documentation
Quickstart
Contributing to the Upstream Kubernetes Code
Generating Reference Documentation for the Kubernetes API
Generating Reference Documentation for kubectl Commands
Generating Reference Documentation for Metrics
Generating Reference Pages for Kubernetes Components and Tools
Advanced contributing
Viewing Site Analytics
Docs smoke test pageKubernetes DocumentationConceptsStoragePersistent VolumesPersistent VolumesThis document describes persistent volumes in Kubernetes. Familiarity with
volumes, StorageClasses
and VolumeAttributesClasses is suggested.IntroductionManaging storage is a distinct problem from managing compute instances.
The PersistentVolume subsystem provides an API for users and administrators
that abstracts details of how storage is provided from how it is consumed.
To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.A PersistentVolume (PV) is a piece of storage in the cluster that has been
provisioned by an administrator or dynamically provisioned using
Storage Classes. It is a resource in
the cluster just like a node is a cluster resource. PVs are volume plugins like
Volumes, but have a lifecycle independent of any individual Pod that uses the PV.
This API object captures the details of the implementation of the storage, be that
NFS, iSCSI, or a cloud-provider-specific storage system.A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar
to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can
request specific levels of resources (CPU and Memory). Claims can request specific
size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany,
ReadWriteMany, or ReadWriteOncePod, see AccessModes).While PersistentVolumeClaims allow a user to consume abstract storage resources,
it is common that users need PersistentVolumes with varying properties, such as
performance, for different problems. Cluster administrators need to be able to
offer a variety of PersistentVolumes that differ in more ways than size and access
modes, without exposing users to the details of how those volumes are implemented.
For these needs, there is the StorageClass resource.See the detailed walkthrough with working examples.Lifecycle of a volume and claimPVs are resources in the cluster. PVCs are requests for those resources and also act
as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:ProvisioningThere are two ways PVs may be provisioned: statically or dynamically.StaticA cluster administrator creates a number of PVs. They carry the details of the
real storage, which is available for use by cluster users. They exist in the
Kubernetes API and are available for consumption.DynamicWhen none of the static PVs the administrator created match a user's PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
storage class and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class "" effectively disable
dynamic provisioning for themselves.To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the DefaultStorageClass
admission controller
on the API server. This can be done, for example, by ensuring that DefaultStorageClass is
among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of
the API server component. For more information on API server command-line flags,
check kube-apiserver documentation.BindingA user creates, or in the case of dynamic provisioning, has already created,
a PersistentVolumeClaim with a specific amount of storage requested and with
certain access modes. A control loop in the control plane watches for new PVCs, finds
a matching PV (if possible), and binds them together. If a PV was dynamically
provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,
the user will always get at least what they asked for, but the volume may be in
excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,
regardless of how they were bound. A PVC to PV binding is a one-to-one mapping,
using a ClaimRef which is a bi-directional binding between the PersistentVolume
and the PersistentVolumeClaim.Claims will remain unbound indefinitely if a matching volume does not exist.
Claims will be bound as matching volumes become available. For example, a
cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.
The PVC can be bound when a 100Gi PV is added to the cluster.UsingPods use claims as volumes. The cluster inspects the claim to find the bound
volume and mounts that volume for a Pod. For volumes that support multiple
access modes, the user specifies which mode is desired when using their claim
as a volume in a Pod.Once a user has a claim and that claim is bound, the bound PV belongs to the
user for as long as they need it. Users schedule Pods and access their claimed
PVs by including a persistentVolumeClaim section in a Pod's volumes block.
See Claims As Volumes for more details on this.Storage Object in Use ProtectionThe purpose of the Storage Object in Use Protection feature is to ensure that
PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)
that are bound to PVCs are not removed from the system, as this may result in data loss.Note:PVC is in active use by a Pod when a Pod object exists that is using the PVC.If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.
PVC removal is postponed until the PVC is no longer actively used by any Pods. Also,
if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.
PV removal is postponed until the PV is no longer bound to a PVC.You can see that a PVC is protected when the PVC's status is Terminating and the
Finalizers list includes kubernetes.io/pvc-protection:kubectl describe pvc hostpath
Name:
hostpath
Namespace:
default
StorageClass:
example-hostpath
Status:
Terminating
Volume:
Labels:
<none>
Annotations:
volume.beta.kubernetes.io/storage-class=example-hostpath
volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:
[kubernetes.io/pvc-protection]
...
You can see that a PV is protected when the PV's status is Terminating and
the Finalizers list includes kubernetes.io/pv-protection too:kubectl describe pv task-pv-volume
Name:
task-pv-volume
Labels:
type=local
Annotations:
<none>
Finalizers:
[kubernetes.io/pv-protection]
StorageClass:
standard
Status:
Terminating
Claim:
Reclaim Policy:
Delete
Access Modes:
RWO
Capacity:
1Gi
Message:
Source:
Type:
HostPath (bare host directory volume)
Path:
/tmp/data
HostPathType:
Events:
<none>
ReclaimingWhen a user is done with their volume, they can delete the PVC objects from the
API that allows reclamation of the resource. The reclaim policy for a PersistentVolume
tells the cluster what to do with the volume after it has been released of its claim.
Currently, volumes can either be Retained, Recycled, or Deleted.RetainThe Retain reclaim policy allows for manual reclamation of the resource.
When the PersistentVolumeClaim is deleted, the PersistentVolume still exists
and the volume is considered "released". But it is not yet available for
another claim because the previous claimant's data remains on the volume.
An administrator can manually reclaim the volume with the following steps.Delete the PersistentVolume. The associated storage asset in external infrastructure
still exists after the PV is deleted.Manually clean up the data on the associated storage asset accordingly.Manually delete the associated storage asset.If you want to reuse the same storage asset, create a new PersistentVolume with
the same storage asset definition.DeleteFor volume plugins that support the Delete reclaim policy, deletion removes
both the PersistentVolume object from Kubernetes, as well as the associated
storage asset in the external infrastructure. Volumes that were dynamically provisioned
inherit the reclaim policy of their StorageClass, which
defaults to Delete. The administrator should configure the StorageClass
according to users' expectations; otherwise, the PV must be edited or
patched after it is created. See
Change the Reclaim Policy of a PersistentVolume.RecycleWarning:The Recycle reclaim policy is deprecated. Instead, the recommended approach
is to use dynamic provisioning.If supported by the underlying volume plugin, the Recycle reclaim policy performs
a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
reference.
The custom recycler Pod template must contain a volumes specification, as
shown in the example below:apiVersion: v1
kind: Pod
metadata:
name: pv-recycler
namespace: default
spec:
restartPolicy: Never
volumes:
- name: vol
hostPath:
path: /any/path/it/will/be/replaced
containers:
- name: pv-recycler
image: "registry.k8s.io/busybox"
command: ["/bin/sh", "-c", "test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*
&& test -z \"$(ls -A /scrub)\" || exit 1"]
volumeMounts:
- name: vol
mountPath: /scrub
However, the particular path specified in the custom recycler Pod template in the
volumes part is replaced with the particular path of the volume that is being recycled.PersistentVolume deletion protection finalizerFEATURE STATE:
Kubernetes v1.33 [stable] (enabled by default: true)Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes
having Delete reclaim policy are deleted only after the backing storage are deleted.The finalizer external-provisioner.volume.kubernetes.io/finalizer(introduced
in v1.31) is added to both dynamically provisioned and statically provisioned
CSI volumes.The finalizer kubernetes.io/pv-controller(introduced in v1.31) is added to
dynamically provisioned in-tree plugin volumes and skipped for statically
provisioned in-tree plugin volumes.The following is an example of dynamically provisioned in-tree plugin volume:kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
Name:
pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
Labels:
<none>
Annotations:
kubernetes.io/createdby: vsphere-volume-dynamic-provisioner
pv.kubernetes.io/bound-by-controller: yes
pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume
Finalizers:
[kubernetes.io/pv-protection kubernetes.io/pv-controller]
StorageClass:
vcp-sc
Status:
Bound
Claim:
default/vcp-pvc-1
Reclaim Policy:
Delete
Access Modes:
RWO
VolumeMode:
Filesystem
Capacity:
1Gi
Node Affinity:
<none>
Message:
Source:
Type:
vSphereVolume (a Persistent Disk resource in vSphere)
VolumePath:
[vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk
FSType:
ext4
StoragePolicyName:
vSAN Default Storage Policy
Events:
<none>
The finalizer external-provisioner.volume.kubernetes.io/finalizer is added for CSI volumes.
The following is an example:Name:
pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d
Labels:
<none>
Annotations:
pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com
Finalizers:
[kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer]
StorageClass:
fast
Status:
Bound
Claim:
demo-app/nginx-logs
Reclaim Policy:
Delete
Access Modes:
RWO
VolumeMode:
Filesystem
Capacity:
200Mi
Node Affinity:
<none>
Message:
Source:
Type:
CSI (a Container Storage Interface (CSI) volume source)
Driver:
csi.vsphere.vmware.com
FSType:
ext4
VolumeHandle:
44830fa8-79b4-406b-8b58-621ba25353fd
ReadOnly:
false
VolumeAttributes:
storage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com
type=vSphere CNS Block Volume
Events:
<none>
When the CSIMigration{provider} feature flag is enabled for a specific in-tree volume plugin,
the kubernetes.io/pv-controller finalizer is replaced by the
external-provisioner.volume.kubernetes.io/finalizer finalizer.The finalizers ensure that the PV object is removed only after the volume is deleted
from the storage backend provided the reclaim policy of the PV is Delete. This
also ensures that the volume is deleted from storage backend irrespective of the
order of deletion of PV and PVC.Reserving a PersistentVolumeThe control plane can bind PersistentVolumeClaims to matching PersistentVolumes
in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding
between that specific PV and PVC. If the PersistentVolume exists and has not reserved
PersistentVolumeClaims through its claimRef field, then the PersistentVolume and
PersistentVolumeClaim will be bound.The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that storage class,
access modes, and requested storage size are valid.apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: foo-pvc
namespace: foo
spec:
storageClassName: "" # Empty string must be explicitly set otherwise default StorageClass will be set
volumeName: foo-pv
...
This method does not guarantee any binding privileges to the PersistentVolume.
If other PersistentVolumeClaims could use the PV that you specify, you first
need to reserve that storage volume. Specify the relevant PersistentVolumeClaim
in the claimRef field of the PV so that other PVCs can not bind to it.apiVersion: v1
kind: PersistentVolume
metadata:
name: foo-pv
spec:
storageClassName: ""
claimRef:
name: foo-pvc
namespace: foo
...
This is useful if you want to consume PersistentVolumes that have their persistentVolumeReclaimPolicy set
to Retain, including cases where you are reusing an existing PV.Expanding Persistent Volumes ClaimsFEATURE STATE:
Kubernetes v1.24 [stable]Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand
the following types of volumes:csi (including some CSI migrated
volme types)flexVolume (deprecated)portworxVolume (deprecated)You can only expand a PVC if its storage class's allowVolumeExpansion field is set to true.apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: example-vol-default
provisioner: vendor-name.example/magicstorage
parameters:
resturl: "http://192.168.10.100:8080"
restuser: ""
secretNamespace: ""
secretName: ""
allowVolumeExpansion: true
To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.Warning:Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.
If you edit the capacity of a PersistentVolume, and then edit the .spec of a matching
PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,
then no storage resize happens.
The Kubernetes control plane will see that the desired state of both resources matches,
conclude that the backing volume size has been manually
increased and that no resize is necessary.CSI Volume expansionFEATURE STATE:
Kubernetes v1.24 [stable]Support for expanding CSI volumes is enabled by default but it also requires a
specific CSI driver to support volume expansion. Refer to documentation of the
specific CSI driver for more information.Resizing a volume containing a file systemYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in ReadWrite mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the
RequiresFSResize capability to true. The FlexVolume can be resized on Pod restart.Resizing an in-use PersistentVolumeClaimFEATURE STATE:
Kubernetes v1.24 [stable]In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.Note:FlexVolume resize is possible only when the underlying driver supports resize.Recovering from Failure when Expanding VolumesIf a user specifies a new size that is too big to be satisfied by underlying
storage system, expansion of PVC will be continuously retried until user or
cluster administrator takes some action. This can be undesirable and hence
Kubernetes provides following methods of recovering from such failures.Manually with Cluster Administrator accessBy requesting expansion to smaller sizeIf expanding underlying storage fails, the cluster administrator can manually
recover the Persistent Volume Claim (PVC) state and cancel the resize requests.
Otherwise, the resize requests are continuously retried by the controller without
administrator intervention.Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)
with Retain reclaim policy.Delete the PVC. Since PV has Retain reclaim policy - we will not lose any data
when we recreate the PVC.Delete the claimRef entry from PV specs, so as new PVC can bind to it.
This should make the PV Available.Re-create the PVC with smaller size than PV and set volumeName field of the
PVC to the name of the PV. This should bind new PVC to existing PV.Don't forget to restore the reclaim policy of the PV.If expansion has failed for a PVC, you can retry expansion with a
smaller size than the previously requested value. To request a new expansion attempt with a
smaller proposed size, edit .spec.resources for that PVC and choose a value that is less than the
value you previously tried.
This is useful if expansion to a higher value did not succeed because of capacity constraint.
If that has happened, or you suspect that it might have, you can retry expansion by specifying a
size that is within the capacity limits of underlying storage provider. You can monitor status of
resize operation by watching .status.allocatedResourceStatuses and events on the PVC.Note that,
although you can specify a lower amount of storage than what was requested previously,
the new value must still be higher than .status.capacity.
Kubernetes does not support shrinking a PVC to less than its current size.Types of Persistent VolumesPersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:csi - Container Storage Interface (CSI)fc - Fibre Channel (FC) storagehostPath - HostPath volume
(for single node testing only; WILL NOT WORK in a multi-node cluster;
consider using local volume instead)iscsi - iSCSI (SCSI over IP) storagelocal - local storage devices
mounted on nodes.nfs - Network File System (NFS) storageThe following types of PersistentVolume are deprecated but still available.
If you are using these volume types except for flexVolume, cephfs and rbd,
please install corresponding CSI drivers.awsElasticBlockStore - AWS Elastic Block Store (EBS)
(migration on by default starting v1.23)azureDisk - Azure Disk
(migration on by default starting v1.23)azureFile - Azure File
(migration on by default starting v1.24)cinder - Cinder (OpenStack block storage)
(migration on by default starting v1.21)flexVolume - FlexVolume
(deprecated starting v1.23, no migration plan and no plan to remove support)gcePersistentDisk - GCE Persistent Disk
(migration on by default starting v1.23)portworxVolume - Portworx volume
(migration on by default starting v1.31)vsphereVolume - vSphere VMDK volume
(migration on by default starting v1.25)Older versions of Kubernetes also supported the following in-tree PersistentVolume types:cephfs
(not available starting v1.31)flocker - Flocker storage.
(not available starting v1.25)glusterfs - GlusterFS storage.
(not available starting v1.26)photonPersistentDisk - Photon controller persistent disk.
(not available starting v1.15)quobyte - Quobyte volume.
(not available starting v1.25)rbd - Rados Block Device (RBD) volume
(not available starting v1.31)scaleIO - ScaleIO volume.
(not available starting v1.21)storageos - StorageOS volume.
(not available starting v1.25)Persistent VolumesEach PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
DNS subdomain name.apiVersion: v1
kind: PersistentVolume
metadata:
name: pv0003
spec:
capacity:
storage: 5Gi
volumeMode: Filesystem
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Recycle
storageClassName: slow
mountOptions:
- hard
- nfsvers=4.1
nfs:
path: /tmp
server: 172.17.0.2
Note:Helper programs relating to the volume type may be required for consumption of
a PersistentVolume within a cluster. In this example, the PersistentVolume is
of type NFS and the helper program /sbin/mount.nfs is required to support the
mounting of NFS filesystems.CapacityGenerally, a PV will have a specific storage capacity. This is set using the PV's
capacity attribute which is a Quantity value.Currently, storage size is the only resource that can be set or requested.
Future attributes may include IOPS, throughput, etc.Volume ModeFEATURE STATE:
Kubernetes v1.18 [stable]Kubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block.volumeMode is an optional API parameter.
Filesystem is the default mode used when volumeMode parameter is omitted.A volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kubernetes creates a filesystem
on the device before mounting it for the first time.You can set the value of volumeMode to Block to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See Raw Block Volume Support
for an example on how to use a volume with volumeMode: Block in a Pod.Access ModesA PersistentVolume can be mounted on a host in any way supported by the resource
provider. As shown in the table below, providers will have different capabilities
and each PV's access modes are set to the specific modes supported by that particular
volume. For example, NFS can support multiple read/write clients, but a specific
NFS PV might be exported on the server as read-only. Each PV gets its own set of
access modes describing that specific PV's capabilities.The access modes are:ReadWriteOncethe volume can be mounted as read-write by a single node. ReadWriteOnce access
mode still can allow multiple pods to access (read from or write to) that volume when the pods are
running on the same node. For single pod access, please see ReadWriteOncePod.ReadOnlyManythe volume can be mounted as read-only by many nodes.ReadWriteManythe volume can be mounted as read-write by many nodes.ReadWriteOncePodFEATURE STATE:
Kubernetes v1.29 [stable]the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod
access mode if you want to ensure that only one pod across the whole cluster can
read that PVC or write to it.Note:The ReadWriteOncePod access mode is only supported for
CSI volumes and Kubernetes version
1.22+. To use this feature you will need to update the following
CSI sidecars
to these versions or greater:csi-provisioner:v3.0.0+csi-attacher:v3.3.0+csi-resizer:v1.3.0+In the CLI, the access modes are abbreviated to:RWO - ReadWriteOnceROX - ReadOnlyManyRWX - ReadWriteManyRWOP - ReadWriteOncePodNote:Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.
In some cases, the volume access modes also constrain where the PersistentVolume can be mounted.
Volume access modes do not enforce write protection once the storage has been mounted.
Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,
they don't set any constraints on the volume. For example, even if a PersistentVolume is
created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes
are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.Important! A volume can only be mounted using one access mode at a time,
even if it supports many.Volume PluginReadWriteOnceReadOnlyManyReadWriteManyReadWriteOncePodAzureFile✓✓✓-CephFS✓✓✓-CSIdepends on the driverdepends on the driverdepends on the driverdepends on the driverFC✓✓--FlexVolume✓✓depends on the driver-HostPath✓---iSCSI✓✓--NFS✓✓✓-RBD✓✓--VsphereVolume✓-- (works when Pods are collocated)-PortworxVolume✓-✓-ClassA PV can have a class, which is specified by setting the
storageClassName attribute to the name of a
StorageClass.
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no storageClassName has no class and can only be bound
to PVCs that request no particular class.In the past, the annotation volume.beta.kubernetes.io/storage-class was used instead
of the storageClassName attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.Reclaim PolicyCurrent reclaim policies are:Retain -- manual reclamationRecycle -- basic scrub (rm -rf /thevolume/*)Delete -- delete the volumeFor Kubernetes 1.34, only nfs and hostPath volume types support recycling.Mount OptionsA Kubernetes administrator can specify additional mount options for when a
Persistent Volume is mounted on a node.Note:Not all Persistent Volume types support mount options.The following volume types support mount options:csi (including CSI migrated volume types)iscsinfsMount options are not validated. If a mount option is invalid, the mount fails.In the past, the annotation volume.beta.kubernetes.io/mount-options was used instead
of the mountOptions attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.Node AffinityNote:For most volume types, you do not need to set this field.
You need to explicitly set this for local volumes.A PV can specify node affinity to define constraints that limit what nodes this
volume can be accessed from. Pods that use a PV will only be scheduled to nodes
that are selected by the node affinity. To specify node affinity, set
nodeAffinity in the .spec of a PV. The
PersistentVolume
API reference has more details on this field.PhaseA PersistentVolume will be in one of the following phases:Availablea free resource that is not yet bound to a claimBoundthe volume is bound to a claimReleasedthe claim has been deleted, but the associated storage resource is not yet reclaimed by the clusterFailedthe volume has failed its (automated) reclamationYou can see the name of the PVC bound to the PV using kubectl describe persistentvolume <name>.Phase transition timestampFEATURE STATE:
Kubernetes v1.31 [stable] (enabled by default: true)The .status field for a PersistentVolume can include an alpha lastPhaseTransitionTime field. This field records
the timestamp of when the volume last transitioned its phase. For newly created
volumes the phase is set to Pending and lastPhaseTransitionTime is set to
the current time.PersistentVolumeClaimsEach PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
DNS subdomain name.apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: myclaim
spec:
accessModes:
- ReadWriteOnce
volumeMode: Filesystem
resources:
requests:
storage: 8Gi
storageClassName: slow
selector:
matchLabels:
release: "stable"
matchExpressions:
- {key: environment, operator: In, values: [dev]}
Access ModesClaims use the same conventions as volumes when requesting
storage with specific access modes.Volume ModesClaims use the same convention as volumes to indicate the
consumption of the volume as either a filesystem or block device.Volume NameClaims can use the volumeName field to explicitly bind to a specific PersistentVolume. You can also leave
volumeName unset, indicating that you'd like Kubernetes to set up a new PersistentVolume
that matches the claim.
If the specified PV is already bound to another PVC, the binding will be stuck
in a pending state.ResourcesClaims, like Pods, can request specific quantities of a resource. In this case,
the request is for storage. The same
resource model
applies to both volumes and claims.Note:For Filesystem volumes, the storage request refers to the "outer" volume size
(i.e. the allocated size from the storage backend).
This means that the writeable size may be slightly lower for providers that
build a filesystem on top of a block device, due to filesystem overhead.
This is especially visible with XFS, where many metadata features are enabled by default.SelectorClaims can specify a
label selector
to further filter the set of volumes.
Only the volumes whose labels match the selector can be bound to the claim.
The selector can consist of two fields:matchLabels - the volume must have a label with this valuematchExpressions - a list of requirements made by specifying key, list of values,
and operator that relates the key and values.
Valid operators include In, NotIn, Exists, and DoesNotExist.All of the requirements, from both matchLabels and matchExpressions, are
ANDed together – they must all be satisfied in order to match.ClassA claim can request a particular class by specifying the name of a
StorageClass
using the attribute storageClassName.
Only PVs of the requested class, ones with the same storageClassName as the PVC,
can be bound to the PVC.PVCs don't necessarily have to request a class. A PVC with its storageClassName set
equal to "" is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to "").
A PVC with no storageClassName is not quite the same and is treated differently
by the cluster, depending on whether the
DefaultStorageClass admission plugin
is turned on.If the admission plugin is turned on, the administrator may specify a default StorageClass.
All PVCs that have no storageClassName can be bound only to PVs of that default.
Specifying a default StorageClass is done by setting the annotation
storageclass.kubernetes.io/is-default-class equal to true in a StorageClass object.
If the administrator does not specify a default, the cluster responds to PVC creation
as if the admission plugin were turned off.
If more than one default StorageClass is specified, the newest default is used when
the PVC is dynamically provisioned.If the admission plugin is turned off, there is no notion of a default StorageClass.
All PVCs that have storageClassName set to "" can be bound only to PVs
that have storageClassName also set to "".
However, PVCs with missing storageClassName can be updated later once default StorageClass becomes available.
If the PVC gets updated it will no longer bind to PVs that have storageClassName also set to "".See retroactive default StorageClass assignment for more details.Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.When a PVC specifies a selector in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.Note:Currently, a PVC with a non-empty selector can't have a PV dynamically provisioned for it.In the past, the annotation volume.beta.kubernetes.io/storage-class was used instead
of storageClassName attribute. This annotation is still working; however,
it won't be supported in a future Kubernetes release.Retroactive default StorageClass assignmentFEATURE STATE:
Kubernetes v1.28 [stable]You can create a PersistentVolumeClaim without specifying a storageClassName
for the new PVC, and you can do so even when no default StorageClass exists
in your cluster. In this case, the new PVC creates as you defined it, and the
storageClassName of that PVC remains unset until default becomes available.When a default StorageClass becomes available, the control plane identifies any
existing PVCs without storageClassName. For the PVCs that either have an empty
value for storageClassName or do not have this key, the control plane then
updates those PVCs to set storageClassName to match the new default StorageClass.
If you have an existing PVC where the storageClassName is "", and you configure
a default StorageClass, then this PVC will not get updated.In order to keep binding to PVs with storageClassName set to ""
(while a default StorageClass is present), you need to set the storageClassName
of the associated PVC to "".This behavior helps administrators change default StorageClass by removing the
old one first and then creating or setting another one. This brief window while
there is no default causes PVCs without storageClassName created at that time
to not have any default, but due to the retroactive default StorageClass
assignment this way of changing defaults is safe.Claims As VolumesPods access storage by using the claim as a volume. Claims must exist in the
same namespace as the Pod using the claim. The cluster finds the claim in the
Pod's namespace and uses it to get the PersistentVolume backing the claim.
The volume is then mounted to the host and into the Pod.apiVersion: v1
kind: Pod
metadata:
name: mypod
spec:
containers:
- name: myfrontend
image: nginx
volumeMounts:
- mountPath: "/var/www/html"
name: mypd
volumes:
- name: mypd
persistentVolumeClaim:
claimName: myclaim
A Note on NamespacesPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are
namespaced objects, mounting claims with "Many" modes (ROX, RWX) is only
possible within one namespace.PersistentVolumes typed hostPathA hostPath PersistentVolume uses a file or directory on the Node to emulate
network-attached storage. See
an example of hostPath typed volume.Raw Block Volume SupportFEATURE STATE:
Kubernetes v1.18 [stable]The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:CSI (including some CSI migrated volume types)FC (Fibre Channel)iSCSILocal volumePersistentVolume using a Raw Block VolumeapiVersion: v1
kind: PersistentVolume
metadata:
name: block-pv
spec:
capacity:
storage: 10Gi
accessModes:
- ReadWriteOnce
volumeMode: Block
persistentVolumeReclaimPolicy: Retain
fc:
targetWWNs: ["50060e801049cfd1"]
lun: 0
readOnly: false
PersistentVolumeClaim requesting a Raw Block VolumeapiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: block-pvc
spec:
accessModes:
- ReadWriteOnce
volumeMode: Block
resources:
requests:
storage: 10Gi
Pod specification adding Raw Block Device path in containerapiVersion: v1
kind: Pod
metadata:
name: pod-with-block-volume
spec:
containers:
- name: fc-container
image: fedora:26
command: ["/bin/sh", "-c"]
args: [ "tail -f /dev/null" ]
volumeDevices:
- name: data
devicePath: /dev/xvda
volumes:
- name: data
persistentVolumeClaim:
claimName: block-pvc
Note:When adding a raw block device for a Pod, you specify the device path in the
container instead of a mount path.Binding Block VolumesIf a user requests a raw block volume by indicating this using the volumeMode
field in the PersistentVolumeClaim spec, the binding rules differ slightly from
previous releases that didn't consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for
requesting a raw block device. The table indicates if the volume will be bound or
not given the combinations: Volume binding matrix for statically provisioned volumes:PV volumeModePVC volumeModeResultunspecifiedunspecifiedBINDunspecifiedBlockNO BINDunspecifiedFilesystemBINDBlockunspecifiedNO BINDBlockBlockBINDBlockFilesystemNO BINDFilesystemFilesystemBINDFilesystemBlockNO BINDFilesystemunspecifiedBINDNote:Only statically provisioned volumes are supported for alpha release. Administrators
should take care to consider these values when working with raw block devices.Volume Snapshot and Restore Volume from Snapshot SupportFEATURE STATE:
Kubernetes v1.20 [stable]Volume snapshots only support the out-of-tree CSI volume plugins.
For details, see Volume Snapshots.
In-tree volume plugins are deprecated. You can read about the deprecated volume
plugins in the
Volume Plugin FAQ.Create a PersistentVolumeClaim from a Volume SnapshotapiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: restore-pvc
spec:
storageClassName: csi-hostpath-sc
dataSource:
name: new-snapshot-test
kind: VolumeSnapshot
apiGroup: snapshot.storage.k8s.io
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 10Gi
Volume CloningVolume Cloning
only available for CSI volume plugins.Create PersistentVolumeClaim from an existing PVCapiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: cloned-pvc
spec:
storageClassName: my-csi-plugin
dataSource:
name: existing-src-pvc-name
kind: PersistentVolumeClaim
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 10Gi
Volume populators and data sourcesFEATURE STATE:
Kubernetes v1.24 [beta]Kubernetes supports custom volume populators.
To use custom volume populators, you must enable the AnyVolumeDataSource
feature gate for
the kube-apiserver and kube-controller-manager.Volume populators take advantage of a PVC spec field called dataSourceRef. Unlike the
dataSource field, which can only contain either a reference to another PersistentVolumeClaim
or to a VolumeSnapshot, the dataSourceRef field can contain a reference to any object in the
same namespace, except for core objects other than PVCs. For clusters that have the feature
gate enabled, use of the dataSourceRef is preferred over dataSource.Cross namespace data sourcesFEATURE STATE:
Kubernetes v1.26 [alpha]Kubernetes supports cross namespace volume data sources.
To use cross namespace volume data sources, you must enable the AnyVolumeDataSource
and CrossNamespaceVolumeDataSource
feature gates for
the kube-apiserver and kube-controller-manager.
Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.Enabling the CrossNamespaceVolumeDataSource feature gate allows you to specify
a namespace in the dataSourceRef field.Note:When you specify a namespace for a volume data source, Kubernetes checks for a
ReferenceGrant in the other namespace before accepting the reference.
ReferenceGrant is part of the gateway.networking.k8s.io extension APIs.
See ReferenceGrant
in the Gateway API documentation for details.
This means that you must extend your Kubernetes cluster with at least ReferenceGrant from the
Gateway API before you can use this mechanism.Data source referencesThe dataSourceRef field behaves almost the same as the dataSource field. If one is
specified while the other is not, the API server will give both fields the same value. Neither
field can be changed after creation, and attempting to specify different values for the two
fields will result in a validation error. Therefore the two fields will always have the same
contents.There are two differences between the dataSourceRef field and the dataSource field that
users should be aware of:The dataSource field ignores invalid values (as if the field was blank) while the
dataSourceRef field never ignores values and will cause an error if an invalid value is
used. Invalid values are any core object (objects with no apiGroup) except for PVCs.The dataSourceRef field may contain different types of objects, while the dataSource field
only allows PVCs and VolumeSnapshots.When the CrossNamespaceVolumeDataSource feature is enabled, there are additional differences:The dataSource field only allows local objects, while the dataSourceRef field allows
objects in any namespaces.When namespace is specified, dataSource and dataSourceRef are not synced.Users should always use dataSourceRef on clusters that have the feature gate enabled, and
fall back to dataSource on clusters that do not. It is not necessary to look at both fields
under any circumstance. The duplicated values with slightly different semantics exist only for
backwards compatibility. In particular, a mixture of older and newer controllers are able to
interoperate because the fields are the same.Using volume populatorsVolume populators are controllers that can
create non-empty volumes, where the contents of the volume are determined by a Custom Resource.
Users create a populated volume by referring to a Custom Resource using the dataSourceRef field:apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: populated-pvc
spec:
dataSourceRef:
name: example-name
kind: ExampleDataSource
apiGroup: example.storage.k8s.io
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 10Gi
Because volume populators are external components, attempts to create a PVC that uses one
can fail if not all the correct components are installed. External controllers should generate
events on the PVC to provide feedback on the status of the creation, including warnings if
the PVC cannot be created due to some missing component.You can install the alpha volume data source validator
controller into your cluster. That controller generates warning Events on a PVC in the case that no populator
is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the
responsibility of that populator controller to report Events that relate to volume creation and issues during
the process.Using a cross-namespace volume data sourceFEATURE STATE:
Kubernetes v1.26 [alpha]Create a ReferenceGrant to allow the namespace owner to accept the reference.
You define a populated volume by specifying a cross namespace volume data source
using the dataSourceRef field. You must already have a valid ReferenceGrant
in the source namespace:apiVersion: gateway.networking.k8s.io/v1beta1
kind: ReferenceGrant
metadata:
name: allow-ns1-pvc
namespace: default
spec:
from:
- group: ""
kind: PersistentVolumeClaim
namespace: ns1
to:
- group: snapshot.storage.k8s.io
kind: VolumeSnapshot
name: new-snapshot-demo
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: foo-pvc
namespace: ns1
spec:
storageClassName: example
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1Gi
dataSourceRef:
apiGroup: snapshot.storage.k8s.io
kind: VolumeSnapshot
name: new-snapshot-demo
namespace: default
volumeMode: Filesystem
Writing Portable ConfigurationIf you're writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:Include PersistentVolumeClaim objects in your bundle of config (alongside
Deployments, ConfigMaps, etc).Do not include PersistentVolume objects in the config, since the user instantiating
the config may not have permission to create PersistentVolumes.Give the user the option of providing a storage class name when instantiating
the template.If the user provides a storage class name, put that value into the
persistentVolumeClaim.storageClassName field.
This will cause the PVC to match the right storage
class if the cluster has StorageClasses enabled by the admin.If the user does not provide a storage class name, leave the
persistentVolumeClaim.storageClassName field as nil. This will cause a
PV to be automatically provisioned for the user with the default StorageClass
in the cluster. Many cluster environments have a default StorageClass installed,
or administrators can create their own default StorageClass.In your tooling, watch for PVCs that are not getting bound after some time
and surface this to the user, as this may indicate that the cluster has no
dynamic storage support (in which case the user should create a matching PV)
or the cluster has no storage system (in which case the user cannot deploy
config requiring PVCs).What's nextLearn more about Creating a PersistentVolume.Learn more about Creating a PersistentVolumeClaim.Read the Persistent Storage design document.API referencesRead about the APIs described in this page:PersistentVolumePersistentVolumeClaimFeedbackWas this page helpful?Yes
NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on
Stack Overflow.
Open an issue in the GitHub Repository if you want to
report a problem
or
suggest an improvement.Last modified August 05, 2025 at 9:52 AM PST: Update docs for recover from expansion failure feature (4cb930c989) PersistentVolume API reference PersistentVolumeClaim API reference
Edit this page
Create child page
Create an issue
Print entire sectionIntroductionLifecycle of a volume and claimProvisioningBindingUsingStorage Object in Use ProtectionReclaimingPersistentVolume deletion protection finalizerReserving a PersistentVolumeExpanding Persistent Volumes ClaimsTypes of Persistent VolumesPersistent VolumesCapacityVolume ModeAccess ModesClassReclaim PolicyMount OptionsNode AffinityPhasePersistentVolumeClaimsAccess ModesVolume ModesVolume NameResourcesSelectorClassClaims As VolumesA Note on NamespacesPersistentVolumes typed hostPathRaw Block Volume SupportPersistentVolume using a Raw Block VolumePersistentVolumeClaim requesting a Raw Block VolumePod specification adding Raw Block Device path in containerBinding Block VolumesVolume Snapshot and Restore Volume from Snapshot SupportCreate a PersistentVolumeClaim from a Volume SnapshotVolume CloningCreate PersistentVolumeClaim from an existing PVCVolume populators and data sourcesCross namespace data sourcesData source referencesUsing volume populatorsUsing a cross-namespace volume data sourceWriting Portable ConfigurationWhat's nextAPI references© 2025 The Kubernetes Authors | Documentation Distributed under CC BY 4.0© 2025 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage pageICP license: 京ICP备17074266号-3