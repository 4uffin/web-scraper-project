tencent/SRPO ¬∑ Hugging Face
Hugging Face
Models
Datasets
Spaces
Community
Docs
Enterprise
Pricing
Log In
Sign Up
tencent
/
SRPO
like
834
Follow
Tencent
6.15k
Text-to-Image
Diffusers
Safetensors
arxiv:
2509.06942
License:
tencent-hunyuan-community
Model card
Files
Files and versions
xet
Community
7
Use this model
Abstract
Acknowledgement
Checkpoints
üîë Inference
Using ComfyUI
Quick start
License
Citation
Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference
Xiangwei Shen1,2*,
Zhimin Li1*,
Zhantao Yang1,
Shiyi Zhang3,
Yingfang Zhang1,
Donghao Li1,
Chunyu Wang1,
Qinglin Lu1,
Yansong Tang3,‚úù
1Hunyuan, Tencent
2School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen
3Shenzhen International Graduate School, Tsinghua University
*Equal contribution
‚úùCorresponding author
Abstract
Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.
Acknowledgement
We sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.
8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0
bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO
GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF
‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.
Checkpoints
The diffusion_pytorch_model.safetensors is online version of SRPO based on FLUX.1 Dev, trained on HPD dataset with HPSv2
üîë Inference
Using ComfyUI
You can use it in ComfyUI.
Load the following image in ComfyUI to get the workflow, or load the JSON file directly SRPO-workflow:
Tip: The workflow JSON info was added to the image file.
Quick start
from diffusers import FluxPipeline
from safetensors.torch import load_file
prompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'
pipe = FluxPipeline.from_pretrained('./data/flux',
torch_dtype=torch.bfloat16,
use_safetensors=True
).to("cuda")
state_dict = load_file("./srpo/diffusion_pytorch_model.safetensors")
pipe.transformer.load_state_dict(state_dict)
image = pipe(
prompt,
guidance_scale=3.5,
height=1024,
width=1024,
num_inference_steps=50,
max_sequence_length=512,
generator=generator
).images[0]
License
SRPO is licensed under the License Terms of SRPO. See ./License.txt for more details.
Citation
If you use SRPO for your research, please cite our paper:
@misc{shen2025directlyaligningdiffusiontrajectory,
title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference},
author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},
year={2025},
eprint={2509.06942},
archivePrefix={arXiv},
primaryClass={cs.AI},
url={https://arxiv.org/abs/2509.06942},
}
Downloads last month5,843
Inference Providers
NEW
Text-to-Image
This model isn't deployed by any Inference Provider.
üôã
2
Ask for provider support
Model tree for tencent/SRPO
Adapters
1 model
Finetunes
2 models
Quantizations
4 models
Spaces using
tencent/SRPO
5
‚ö°
umint/searchgpt
üèÉ
akhaliq/SRPO
‚ö°
umint/image
üñº
xenomirant/diffusion-xenomirant-t2i
‚ö°
umint/qwen3-0.6b
System theme
Company
TOS
Privacy
About
Jobs
Website
Models
Datasets
Spaces
Pricing
Docs