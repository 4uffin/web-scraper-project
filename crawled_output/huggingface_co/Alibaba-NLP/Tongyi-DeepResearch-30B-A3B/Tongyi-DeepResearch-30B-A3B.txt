Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face
Hugging Face
Models
Datasets
Spaces
Community
Docs
Enterprise
Pricing
Log In
Sign Up
Alibaba-NLP
/
Tongyi-DeepResearch-30B-A3B
like
629
Follow
Alibaba-NLP
1.18k
Text Generation
Transformers
Safetensors
English
qwen3_moe
conversational
License:
apache-2.0
Model card
Files
Files and versions
xet
Community
14
Train
Deploy
Use this model
Introduction
Key Features
Download
Introduction
We present
Tongyi DeepResearch, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for long-horizon, deep information-seeking tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch and FRAMES.
More details can be found in our üì∞ Tech Blog.
Key Features
‚öôÔ∏è Fully automated synthetic data generation pipeline: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.
üîÑ Large-scale continual pre-training on agentic data: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.
üîÅ End-to-end reinforcement learning: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.
ü§ñ Agent Inference Paradigm Compatibility: At inference, Tongyi-DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.
Download
You can download the model then run the inference scipts in https://github.com/Alibaba-NLP/DeepResearch.
@misc{tongyidr,
author={Tongyi DeepResearch Team},
title={Tongyi-DeepResearch},
year={2025},
howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
Downloads last month11,501
Safetensors
Model size
30.5B params
Tensor type
BF16
¬∑
Chat template
Files info
Inference Providers
NEW
Text Generation
This model isn't deployed by any Inference Provider.
üôã
38
Ask for provider support
Model tree for Alibaba-NLP/Tongyi-DeepResearch-30B-A3B
Finetunes
2 models
Quantizations
16 models
Spaces using
Alibaba-NLP/Tongyi-DeepResearch-30B-A3B
5
‚ö°
umint/ai
‚ö°
umint/image
üëÄ
Spestly/Tongyi-DeepResearch-30B-A3B
‚ö°
umint/qwen3-0.6b
‚ö°
jairwaal/image
System theme
Company
TOS
Privacy
About
Jobs
Website
Models
Datasets
Spaces
Pricing
Docs