Bring us your LLMs: why peer review is good for AI models
Skip to main content
Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
and JavaScript.
Advertisement
View all journals
Search
Log in
Explore content
About the journal
Publish with us
Subscribe
Sign up for alerts
RSS feed
nature
editorials
article
Bring us your LLMs: why peer review is good for AI models
Download PDF
EDITORIAL
17 September 2025
Bring us your LLMs: why peer review is good for AI models
Deepseek’s R1 model has been peer reviewed. Others should follow the firm’s example.
Twitter
Facebook
Email
You have full access to this article via your institution.
Download PDF
Some of the risks of large language models can be mitigated if the tools are subjected to peer review.Credit: Matteo Della Torre/NurPhoto/GettyNone of the most widely used large language models (LLMs) that are rapidly upending how humanity is acquiring knowledge has faced independent peer review in a research journal. It’s a notable absence. Peer-reviewed publication aids clarity about how LLMs work, and helps to assess whether they do what they purport to do.Read the paper: DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learningThat changes with the publication1 in Nature of details regarding R1, a model produced by DeepSeek, a technology firm based in Hangzhou, China. R1 is an open-weight model, meaning that, although researchers and the public do not get all of its source code and training data, they can freely download, use, test and build on it without restriction. The value of open-weight artificial intelligence (AI) is becoming more widely recognized. In July, US President Donald Trump’s administration said that such models are “essential for academic research”. More firms are releasing their own versions.Since R1’s release in January on Hugging Face, an AI community platform, it has become the platform’s most-downloaded model for complex problem-solving. Now, the model has been reviewed by eight specialists to assess the originality, methodology and robustness of the work. The paper is being published alongside the reviewer reports and author responses. All of this is a welcome step towards transparency and reproducibility in an industry in which unverified claims and hype are all too often the norm.AI can learn to show its workings through trial and errorDeepSeek’s paper focuses on the technique that the firm used to train R1 to ‘reason’. The researchers applied an efficient and automated version of a ‘trial, error and reward’ process called reinforcement learning. In this, the model learns reasoning strategies, such as verifying its own working out, without being influenced by human ideas about how to do so.In January, DeepSeek also published a preprint that outlined the researchers’ approach and the model’s performance on an array of benchmarks2. Such technical documents, which are often called model or system cards, can vary wildly in the information they contain.In peer review, by contrast, rather than receive a one-way flow of information, external experts can ask questions and request more information in a collaborative process overseen and managed by an independent third party: the editor. That process improves a paper’s clarity, ensuring that authors justify their claims. It won’t always lead to major changes, but it improves trust in a study. For AI developers, this means that their work is strengthened and therefore more credible to different communities.The peer-review crisis: how to fix an overloaded systemPeer review also provides a counterbalance to the practice of AI developers marking their own homework by choosing benchmarks that show their models in the best light. Benchmarks can be gamed to overestimate a model’s capabilities, for instance, by training on data that includes example questions and answers, allowing the model to learn the correct response3.In DeepSeek’s case, referees raised the question of whether this practice might have occurred. The firm provided details of its attempts to mitigate data contamination and included extra evaluations using benchmarks published only after the model had been released.Peer review led to other important changes to the paper. One was to ensure that the authors had addressed the model’s safety. Safety in AI means avoiding unintended harmful consequences, from mitigating inbuilt biases in outputs to adding guardrails that prevent AIs from enabling cyberattacks. Some see open models as less secure than proprietary models, because, once downloaded by users, they are outside of the developers’ control (that said, open models also allow a wider community to understand and fix flaws).How China created AI model DeepSeek and shocked the worldReviewers of R1 pointed out a lack of information about safety tests: for example, there were no estimates of how easy it would be to build on R1 to create an unsafe model. In response, DeepSeek’s researchers added important details to the paper, including a section outlining how they evaluated the model’s safety and compared it with rival models.Firms are starting to recognize the value of external scrutiny. Last month, OpenAI and Anthropic, both based in San Francisco, California, tested each other’s models using their own internal evaluation processes. Both found issues that had been missed by their developers. In July, Paris-based Mistral AI released results of an environmental assessment of its model, in collaboration with external consultants. Mistral hopes that this will improve transparency of reporting standards across the industry.Given the rapid pace at which AI is developing and being unleashed on society, these efforts are important steps. But most lack the independence of peer-reviewed research, which, despite its limitations, represents a gold standard for validation.Scientists flock to DeepSeek: how they’re using the blockbuster AI modelSome companies worry that publishing could give away intellectual property — a risk, given the huge financial investment that such models have received. But, as shown with Nature’s publication of Google’s medical LLM Med-PaLM, peer review is possible for proprietary models4.Peer reviews relying on independent researchers is a way to dial back hype in the AI industry. Claims that cannot be verified are a real risk for society, given how ubiquitous this technology has become. We hope, for this reason, that more AI firms will submit their models to the scrutiny of publication. Review doesn’t mean giving outsiders access to company secrets. But it does mean being prepared to back up statements with evidence and ensuring that claims are validated and clarified.
Nature 645, 559 (2025)
doi: https://doi.org/10.1038/d41586-025-02979-9
ReferencesGuo, D. et al. Nature 645, 633–638 (2025).Article
Google Scholar
DeepSeek–AI et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2501.12948 (2025).Eriksson, M. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2502.06559 (2025).Singhal, K. et al. Nature 620, 172–180 (2023).Article
PubMed
Google Scholar
Download references
Reprints and permissions
Related Articles
Read the paper: DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning
AI can learn to show its workings through trial and error
How China created AI model DeepSeek and shocked the world
Scientists flock to DeepSeek: how they’re using the blockbuster AI model
The peer-review crisis: how to fix an overloaded system
Subjects
Machine learning
Computer science
Latest on:
Machine learning
Can AI chatbots trigger psychosis? What the science says
News 18 SEP 25
AI is helping to decode animals’ speech. Will it also let us talk with them?
News Feature 17 SEP 25
People are more likely to cheat when they delegate tasks to AI
News & Views 17 SEP 25
Computer science
People are more likely to cheat when they delegate tasks to AI
News & Views 17 SEP 25
AI can learn to show its workings through trial and error
News & Views 17 SEP 25
Secrets of DeepSeek AI model revealed in landmark paper
News 17 SEP 25
Jobs
Research specialist in integrated photonics
Seeking a Research Specialist in integrated photonics to support experiments, labs, training, and collaboration on cutting-edge research projects.
Gothenburg (Stad), Västra Götaland (SE)
Chalmers University of Technology - MC2
FAFU Global Faculty Recruitment
FAFU welcomes applicants of all nationalities to apply for faculty positions at any rank (full professor, associate professor, assistant professor).
Fuzhou, Fujian (CN)
Fujian Agriculture and Forestry University
Global Talent Recruitment-Hospital of Stomatology Xi’an Jiaotong University
Leading Talent, Excellent Young Scholars (Overseas), Young Top Talents, Postdoctoral Fellow
Xi'an, Shaanxi (CN)
Hospital of Stomatology Xi’an Jiaotong University
Tenure-track Positions at The HIT Center for Life Sciences, China
Harbin, Heilongjiang (CN)
The HIT Center for Life Sciences (HCLS)
Tenure-Track/Tenured Faculty Positions
Suzhou, Jiangsu, China
School of Sustainable Energy and Resources at Nanjing University
You have full access to this article via your institution.
Download PDF
Related Articles
Read the paper: DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning
AI can learn to show its workings through trial and error
How China created AI model DeepSeek and shocked the world
Scientists flock to DeepSeek: how they’re using the blockbuster AI model
The peer-review crisis: how to fix an overloaded system
Subjects
Machine learning
Computer science
Sign up to Nature Briefing
An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday.
Email address
Yes! Sign me up to receive the daily Nature Briefing email. I agree my information will be processed in accordance with the Nature and Springer Nature Limited Privacy Policy.
Sign up
Close banner
Close
Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.
Email address
Sign up
I agree my information will be processed in accordance with the Nature and Springer Nature Limited Privacy Policy.
Close banner
Close
Get the most important science stories of the day, free in your inbox.
Sign up for Nature Briefing
Explore content
Research articles
News
Opinion
Research Analysis
Careers
Books & Culture
Podcasts
Videos
Current issue
Browse issues
Collections
Subjects
Follow us on Facebook
Follow us on Twitter
Subscribe
Sign up for alerts
RSS feed
About the journal
Journal Staff
About the Editors
Journal Information
Our publishing models
Editorial Values Statement
Journal Metrics
Awards
Contact
Editorial policies
History of Nature
Send a news tip
Publish with us
For Authors
For Referees
Language editing services
Open access funding
Submit manuscript
Search
Search articles by subject, keyword or author
Show results from
All journals
Search
Advanced search
Quick links
Explore articles by subject
Find a job
Guide to authors
Editorial policies
Nature
(Nature)
ISSN 1476-4687 (online)
ISSN 0028-0836 (print)
nature.com sitemap
About Nature Portfolio
About us
Press releases
Press office
Contact us
Discover content
Journals A-Z
Articles by subject
protocols.io
Nature Index
Publishing policies
Nature portfolio policies
Open access
Author & Researcher services
Reprints & permissions
Research data
Language editing
Scientific editing
Nature Masterclasses
Research Solutions
Libraries & institutions
Librarian service & tools
Librarian portal
Open research
Recommend to library
Advertising & partnerships
Advertising
Partnerships & Services
Media kits
Branded
content
Professional development
Nature Awards
Nature Careers
Nature
Conferences
Regional websites
Nature Africa
Nature China
Nature India
Nature Japan
Nature Middle East
Privacy
Policy
Use
of cookies
Your privacy choices/Manage cookies
Legal
notice
Accessibility
statement
Terms & Conditions
Your US state privacy rights
© 2025 Springer Nature Limited