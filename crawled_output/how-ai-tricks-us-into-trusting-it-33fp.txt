How AI Tricks Us Into Trusting It - DEV Community
Forem Feed
Follow new Subforems to improve your feed
DEV Community
Follow
A space to discuss and keep up software development and manage your software career
Gamers Forem
Follow
An inclusive community for gaming enthusiasts
Future
Follow
News and discussion of science and technology such as AI, VR, cryptocurrency, quantum computing, and more.
Music Forem
Follow
From composing and gigging to gear, hot music takes, and everything in between.
DUMB DEV Community
Follow
Memes and software development shitposting
Vibe Coding Forem
Follow
Discussing AI software development, and showing off what we're building.
Popcorn Movies and TV
Follow
Movie and TV enthusiasm, criticism and everything in-between.
Design Community
Follow
Web design, graphic design and everything in-between
Maker Forem
Follow
A community for makers, hobbyists, and professionals to discuss Arduino, Raspberry Pi, 3D printing, and much more.
Scale Forem
Follow
For engineers building software at scale. We discuss architecture, cloud-native, and SRE‚Äîthe hard-won lessons you can't just Google
Forem Core
Follow
Discussing the core forem open source software project ‚Äî features, bugs, performance, self-hosting.
Crypto Forem
Follow
A collaborative community for all things Crypto‚Äîfrom Bitcoin to protocol development and DeFi to NFTs and market analysis.
Dropdown menu
Dropdown menu
Skip to content
Navigation menu
Search
Powered by Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Moderate
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Agustin V. Startari
Posted on Sep 12
How AI Tricks Us Into Trusting It
#ai
#devops
#react
#discuss
_Identity without expression. Authority without proof.
_
*Why This Matters Now
*
Every day, decisions about healthcare, policy, finance and education are increasingly shaped by texts that nobody has verified. These texts are produced by large language models designed to sound authoritative, even when they are completely wrong.
In our latest paper,
‚ÄúEthos Ex Machina: Identity Without Expression in Compiled Syntax‚Äù,
we demonstrate how AI-generated language creates synthetic trust by exploiting how our brains interpret structure before meaning.
SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5401687
Zenodo: https://zenodo.org/records/16927104
The conclusion is alarming: we are not verifying what AI says, because we are already convinced by how it says it.
The Core Problem: Trust Without Truth
Large language models are trained to predict words, not to check facts.
They are optimizers of plausibility, not validators of reliability.
Readers, however, are wired to respond to syntactic cues that signal credibility. These include passive voice, balanced coordination, enumerations and references. We believe structure equals authority.
This creates what the paper defines as a non-expressive ethos: credibility generated by form rather than substance.
How AI Creates the Illusion of Authority
AI uses predictable linguistic strategies that make us trust without questioning. Here are the five most critical ones, with expanded, relatable examples:
Passive Voice Removes Accountability
AI Output: ‚ÄúIt has been demonstrated that treatment A is superior.‚Äù
What It Hides: Who demonstrated it? Which study? Without an agent, the statement feels neutral and universal, but no responsibility exists.
Real-world risk: A hospital adopts a protocol based on an AI-generated clinical note. The phrasing implies medical consensus, but the evidence does not exist.
Balanced Coordination Creates False Neutrality
AI Output: ‚ÄúBoth treatment A and treatment B provide significant benefits.‚Äù
Reality: Treatment A has six controlled trials supporting it. Treatment B has none.
Real-world risk: A patient chooses the less effective treatment because the language creates symmetry where none exists.
Nominalizations Disguise Agency
AI Output: ‚ÄúThe implementation of the framework was executed.‚Äù
Reality: Who executed it? Which framework? Instead of stating ‚ÄúThe board approved the framework‚Äù, the agent disappears into abstract nouns.
Real-world risk: In financial reports, vague formulations like these are used to hide responsibility when performance targets are missed.
Calibrated Modality Feigns Scientific Caution
AI Output: ‚ÄúThe evidence may suggest an increase in efficiency.‚Äù
Reality: This phrasing sounds cautious and evidence-based, yet it communicates no measurable claim.
Real-world risk: Companies base strategies on claims that sound responsible but lack statistical validation.
Reference Scaffolding Simulates Depth
AI Output: ‚ÄúAs shown in Section 4.2 and supported by Smith (2021), our conclusions remain consistent.‚Äù
Reality: There is no Section 4.2. Smith (2021) does not exist.
Real-world risk: Governments accept AI-generated policy drafts filled with fake citations because the format looks official.
*Institutional Impacts: A Silent Crisis
*
This is no longer speculative. It is already happening.
Healthcare: AI-generated clinical notes from systems like Epic Scribe are entering patient records. Doctors assume correctness because the reports are structured and neutral. Yet the information is often incomplete, missing context or outdated.
Policy and Governance: Government agencies increasingly circulate AI-generated regulations. Their official tone and formal structure mean drafts move forward without verification.
Academia: AI-generated literature reviews dominate submission pipelines. They follow academic conventions perfectly but regularly cite non-existent studies, reshaping the academic record with fabricated authority.
Corporate Risk: Legal, compliance and financial decisions are now made using AI-written summaries that appear professional. The structure is polished, but the content is unverifiable.
*The Structural Inversion
*
This is not an occasional bug. It is a systemic inversion of authority.
Traditionally, credibility came from content: claims, data, sources, peer review.
Now, credibility is increasingly derived from form: tone, format, apparent neutrality.
This inversion has three dangerous consequences:
Institutions begin outsourcing legitimacy to machines.
Readers stop questioning structured outputs that look authoritative.
AI starts shaping what counts as authoritative knowledge by controlling the appearance of discourse.
*Three Scenarios Anyone Can Relate To
*
Scenario 1: Medical Reports
AI Note: ‚ÄúIt has been determined that further imaging is warranted.‚Äù
Hidden Reality: No human radiologist made this determination. The phrasing is statistical, not clinical.
Scenario 2: Corporate Risk Assessments
AI Summary: ‚ÄúBoth investment options present significant opportunities.‚Äù
Hidden Reality: Only one investment has supporting data. The balance is fabricated.
Scenario 3: Policy Recommendations
AI Output: ‚ÄúSection 3.4 confirms an efficiency gain of 24 percent.‚Äù
Hidden Reality: Section 3.4 does not exist. The number was generated to sound ‚Äúscientific.‚Äù
Key Takeaways
Insight Real-World Impact
Form dominates meaning
Syntax triggers trust before content is analyzed.
Credibility is simulated
AI uses structural cues to bypass critical review.
Institutions are exposed
Hospitals, courts and universities act on unverified outputs.
We need syntactic literacy
Verifying how AI speaks is as important as verifying what it says.
Read the Full Paper
SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5401687
Zenodo: https://zenodo.org/records/16927104
Author
Agustin V. Startari
Linguistic theorist and researcher in historical studies.
Author of Grammars of Power, Executable Power, and The Grammar of Objectivity.
ORCID: https://orcid.org/0000-0001-4714-6539
ResearcherID: K-5792-2016
Website: https://www.agustinvstartari.com
**
Ethos**
We do not use artificial intelligence to write what we do not know.
We use it to challenge what we do.
We write to reclaim the voice in an age of automated neutrality.
Our work is not outsourced. It is authored.
‚Äî Agustin V. Startari
Top comments (0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
‚Ä¢
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or reporting abuse
Agustin V. Startari
Follow
Agustin V. Startari is a researcher in linguistics, structural epistemology, and algorithmic power. He publishes on language, AI, and authority in international academic platforms.
Location
Nassau, Bahamas
Education
Universidad de la Republica - Universidad de la Empresa - Universidad de Palermo
Work
Manager
Joined
Jun 19, 2025
More from Agustin V. Startari
The Disappearance of Responsibility: AI Decisions Nobody Signs
#ai
#react
#opensource
#discuss
Who Really Decides When Nobody Is in Charge?
#ai
#productivity
#react
#discuss
How AI Is Rewiring Law and Authority
#ai
#productivity
#react
#discuss
üíé DEV Diamond Sponsors
Thank you to our Diamond Sponsors for supporting the DEV Community
Google AI is the official AI Model and Platform Partner of DEV
Neon is the official database partner of DEV
Algolia is the official search partner of DEV
DEV Community ‚Äî A space to discuss and keep up software development and manage your software career
Home
DEV++
Reading List
Podcasts
Videos
Tags
DEV Education Tracks
DEV Challenges
DEV Help
Advertise on DEV
DEV Showcase
About
Contact
Free Postgres Database
Software comparisons
Forem Shop
Code of Conduct
Privacy Policy
Terms of Use
Built on Forem ‚Äî the open source software that powers DEV and other inclusive communities.
Made with love and Ruby on Rails. DEV Community ¬© 2016 - 2025.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account