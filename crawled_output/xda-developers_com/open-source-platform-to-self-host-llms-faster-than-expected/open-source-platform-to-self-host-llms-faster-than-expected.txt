I tried this open-source platform to self-host LLMs, and it’s faster than I expected
Menu
Sign in now
Close
News
PC Hardware
Submenu
CPU
GPU
Storage
Monitors
Keyboards & Mice
Software
Submenu
Productivity
Other Software
Operating Systems
Submenu
Windows
Linux
macOS
Devices
Submenu
Single-Board Computers
Laptops
Gaming Handheld
Prebuilt PC
Home
Submenu
Networking
Smart Home
Gaming
Submenu
Game Reviews
Sign in
Newsletter
Menu
Follow
Followed
Like
10
Threads
10
More Action
Summary
Generate a summary of this story
Sign in now
Switch 2
RTX 5060
Windows 11
Gaming
Forums
Close
I tried this open-source platform to self-host LLMs, and it’s faster than I expected
By
Yash Patel
Published 4 days ago
Beginning his professional journey in the tech industry in 2018, Yash spent over three years as a Software Engineer. After that, he shifted his focus to empowering readers through informative and engaging content on his tech blog – DiGiTAL BiRYANi. He has also published tech articles for MakeTechEasier. He loves to explore new tech gadgets and platforms.  When he is not writing, you’ll find him exploring food. He is known as Digital Chef Yash among his readers because of his love for Technology and Food.
Sign in to your XDA account
Summary
Generate a summary of this story
follow
Follow
followed
Followed
Like
Like
10
Thread
10
Log in
Here is a fact-based summary of the story contents:
Try something different:
Show me the facts
Explain it like I’m 5
Give me a lighthearted recap
My AI world was limited to the big names like ChatGPT and Gemini. A few months back, I started exploring the world of self-hosted LLMs. I started using multiple self-hosted LLMs for different tasks. My experiments with self-hosted LLMs were primarily limited to Ollama and LM Studio. While both are excellent tools, I always wanted something that perfectly balanced an easy setup with deep customization. That's when I discovered Koboldcpp, and it completely changed my perspective. I set up Koboldcpp on my laptop, which is equipped with an Intel Core Ultra 9 processor, 32GB of RAM, and an NVIDIA GeForce RTX 4050 GPU. Not only was the setup incredibly simple, but the performance was so much faster than I ever thought possible.
What is Koboldcpp?
Installation is a breeze
Koboldcpp is an all-in-one software that enables you to run large language models (LLMs) directly on your own PC. It's built on the llama.cpp project, which makes it well-optimized for both CPU and GPU. With a customizable web interface featuring multiple themes, Koboldcpp allows you to easily manage persistent stories, characters, and even integrate image, speech-to-text, and text-to-speech features. The platform bundles everything you need into a single, straightforward executable. Honestly, I was expecting a complicated process involving command lines and compiling, but the installation was a complete breeze. I just went to their GitHub page and downloaded a single .exe file. No messy dependencies, no multi-step wizard; just one file to download. I ran it, and a small settings window popped up. In just a few minutes, I was ready to load my first model. The key to getting started is finding the right model file. Koboldcpp primarily works with GGML and the newer models. The recommended model type is GGUF. They are easily available on HuggingFace. I simply searched for "GGUF" along with the model, which was Meta-Llma-3.1. Once I had downloaded a model file, using it was incredibly easy. From the Koboldcpp startup window, I just clicked "Browse," found my .gguf file, selected it, and hit "Start." The web interface loaded instantly, opening a clean and customizable environment. It even has different themes, so I could make it feel just right. From there, I could just start typing. The whole process, from finding the model to generating my first text, took less time than I spent brewing my morning coffee.
I can play with texts, images, and audio
It is faster than I expected
The most exciting part of using Koboldcpp for me was realizing it's so much more than a simple text generator. After I got my text model running and was happily creating stories, I discovered it could do a lot more. I could even do image generation with models like Stable Diffusion. With the suitable model files, I was able to create pictures directly from my text prompts. I also experimented with its audio features. By loading a Whisper model for speech-to-text, I could talk into my mic and have my words instantly typed out. For an even better experience, I used a text-to-speech model to have the AI "read" its answers back to me. All of this happens locally, without any internet connection. My earlier attempts at running LLMs on my own computer were a bit slow. I'd wait for the model to process my prompts, which felt more like a chore than a conversation. That's why I felt pretty impressed with Koboldcpp. The moment I hit "Generate," the words just started appearing on the screen. The speed was incredible. It completely changed my mind about what’s possible with a local LLM setup.
It has many interesting power user features
Packed with features, built for everyone
What I appreciate most about Koboldcpp is that it's designed for everyone, from beginners to power users. Beyond the simple setup, it's packed with advanced features. It has a ton of customization options. I can fine-tune things like repetition penalty, temperature, and Top-P to control how the AI generates text, making it more creative or more focused. This level of control is something you just don't get with online platforms. Another huge benefit is its broad hardware support. It works with various GPUs, including NVIDIA and AMD. It means you can achieve great performance regardless of the type of graphics card you have. When I load a model, I can specify how many "GPU layers" I want to offload. This allows me to utilize my GPU's VRAM efficiently, striking a perfect balance between speed and memory usage. It’s also built to be used with other front-ends like SillyTavern, which opens up a new world of customization and role-playing features. This flexibility and user-friendly interface for managing advanced settings really make it stand out.
Is it better than competitors?
Having tried platforms like Ollama and LM Studio, I can say that Koboldcpp stands out for its unique blend of simplicity and power. While Ollama is great for a command-line approach and LM Studio has a polished all-in-one GUI, Koboldcpp feels like it hits the sweet spot. It features an easy, single-file setup and a user-friendly interface, but it doesn't compromise the deep customization that power users require. Of course, it’s not perfect. It primarily works with limited model types, so you can't just use any model file you download from Hugging Face, unlike other tools. This means you'll have to make sure you're downloading a compatible file, which can be a minor extra step. However, for a user who wants full control over the local AI and appreciates the ability to fine-tune every little setting for the fastest possible performance, Koboldcpp is worth trying. Despite any minor limitations, its speed and flexibility make it a useful tool for anyone interested in self-hosting LLMs.
Koboldcpp
See at Github
Expand
Collapse
Software and Services
Artificial Intelligence
Self-Hosting
Follow
Followed
Like
10
Share
Facebook
X
LinkedIn
Reddit
Flipboard
Copy link
Email
Close
Thread
10
Sign in to your XDA account
We want to hear from you! Share your opinions in the thread below and remember to keep it respectful.
Reply / Post
Images
Attachment(s)
Please respect our community guidelines. No links, inappropriate language, or spam.
Your comment has not been saved
Send confirmation email
Sort by:
Popular
Oldest
Newest
Henk
Henk
Henk
#SK493185
Member since 2025-09-20
0
Threads
4
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Thanks for the awesome review!I hope this comment box notifies me of any replies but if users do run into issues or need assistance setting it up or if you simply have further questions we are always happy to help.In case I don't get notified here the KoboldAI discord is another outlet for one on one help. We are always very welcoming to those for who AI is new and overwhelming so don't hesitate to ask.- Henk from KoboldAI
2025-09-20 19:39:48
Upvote
4
Downvote
Reply
Copy
Fredrik
Fredrik
Fredrik
#QK896669
Member since 2025-09-21
0
Threads
1
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Would you like to share how many TPS you get on that hardware? And what size was the model you used?What models won't work? What sizes, what quantz?
2025-09-21 02:40:45
Upvote
2
Downvote
Reply
1
Copy
Henk
Henk
Henk
#SK493185
Member since 2025-09-20
0
Threads
4
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Expect the TPS to be similar to llamacpp as we share the same engine (although a forked version), we do have our own context shifting implementation though so the times where it does not need to reprocess will be different.
2025-09-21 07:23:22
Upvote
1
Downvote
Reply
Copy
Eternal_Tech
Eternal_Tech
Eternal_Tech
#RD134310
Member since 2024-06-17
0
Threads
3
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
I have used web-based LLMs to write science fiction stories. However, there have been two major limitations:The guardrails on the web-based LLMs often prevented the exploration of certain violent or sexual situations. As Koboldcpp runs LLMs locally, are these guardrails able to be bypassed?Once stories reached a certain length, the web-based LLMs would refuse to write more chapters. Will using Koboldcpp prevent these length limitations?
2025-09-21 14:57:02
Upvote
1
Downvote
Reply
1
Copy
OddEven
OddEven
OddEven
#PZ078801
Member since 2025-09-21
0
Threads
1
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Regarding guardrails, some models are censored during their training, but there are models which are not, so it depends on what model you load. Generally, you can expect local LLMs to be much less censored than web-based LLMs since there are no banned words that will trigger the deletion of a generated response.Regarding length limitations, Koboldcpp doesn't limit lengths AFAIK, I don't believe it limits context size either, but models tend to become forgetful with large contexts, so while there isn't any limitation on length, the story may not remain consistent. Depending on the front end that you use, this forgetfulness can be mitigated with lorebooks or memories, but if you're writing a novel, I can only wish you luck.
2025-09-21 19:52:25
Upvote
Downvote
Reply
Copy
Sunish
Sunish
Sunish
#EO995007
Member since 2025-06-21
0
Threads
1
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Is it possible to upload PDFs or use files in the OS filesystem and ask questions based on that? Or something like NotebookLM that's completely offline.
2025-09-21 03:40:34
Upvote
1
Downvote
Reply
1
Copy
Henk
Henk
Henk
#SK493185
Member since 2025-09-20
0
Threads
4
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Not using the bundled UI but KoboldCpp has a fully featured OpenAI emulator on board for third party UI's.Its a limitation for us because our UI is aimed to be more light weight and the PDF parsing libraries would triple the size.What you can do however is copy and paste the plain text from the PDF into our TextDB field (This is in the context menu). That has a mode designed for the english language that can parse relevant strings using a search algorithm, or alternatively you can load an embedding model for proper embeddings for any language the embedding model supports.
2025-09-21 07:25:21
Upvote
1
Downvote
Reply
Copy
AdamBomb
AdamBomb
AdamBomb
#HG566214
Member since 2024-07-01
0
Threads
40
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
Try Open WebUI.
It works like ChatGPT.
You can scale it from basic to enterprise grade.
Has APIs available which are openai compatibile, and you can integrate any of the large players into it. Additionaly you can develop your own plugins and tools for the AI to use and specify MCP servers... The list goes on, like a mile.
It's basically the core of my AI and works with other open source projects like Home Assistant, Frigate, and can power the backend of any AI projects you make.
2025-09-21 16:26:03
Upvote
Downvote
Reply
Copy
alpha754293
alpha754293
alpha754293
#HK743600
Member since 2024-08-19
0
Threads
84
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
That's a pity that websearch is not supported.
2025-09-22 14:10:36
Upvote
Downvote
Reply
1
Copy
Henk
Henk
Henk
#SK493185
Member since 2025-09-20
0
Threads
4
Posts
Following
0
Stories
0
Topics
0
Authors
0
Users
Follow
Followed
0
Followers
View
It has a basic websearch, but can also be combined with UI's that do this more in depth.
2025-09-23 08:03:49
Upvote
Downvote
Reply
Copy
Terms
Privacy
Feedback
Recommended
Sep 17, 2025
This free and open-source app helps me keep track of all the stuff I usually forget
7 days ago
The Windows 11 Snipping Tool is getting a killer time-saving feature
6 days ago
Elevate your workspace with this sleek and feature-packed docking station
7 days ago
I use these Obsidian keyboard shortcuts to boost my productivity
Today's best deals
Grab the iPad Air with an M3 chip for one of its best prices at just $539
55 minutes ago
This sleek soundbar delivers bold sound without the big price
2 hours ago
This legendary farming sim with 98% positive Steam reviews is just $2 for a limited time
21 hours ago
See More
Trending Now
Netdata is an open source dashboard that shows all my servers and services in one place
Forget VRAM — these 5 GPU trends are way more disturbing
6 of the coolest HACS integrations for Home Assistant users
Join Our Team
Our Audience
About Us
Press & Events
Contact Us
Follow Us
Advertising
Careers
Terms
Privacy
Policies
XDA is part of the
Valnet Publishing Group
Copyright © 2025 Valnet Inc.