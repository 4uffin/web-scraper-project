python - pippy examples: torch._dynamo.exc.UserError: It looks like one of the outputs with type <class transformers.cache_utils.DynamicCache> is not supported - Stack Overflow
Skip to main content
Stack Overflow
About
Products
For Teams
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising
Reach devs & technologists worldwide about your product, service or employer brand
Knowledge Solutions
Data licensing offering for businesses to build and improve AI tools and models
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up or log in to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
AI Assist
Labs
Tags
Challenges
Chat
Articles
Users
Jobs
Companies
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
pippy examples: torch._dynamo.exc.UserError: It looks like one of the outputs with type <class transformers.cache_utils.DynamicCache> is not supported
Ask Question
Asked
2 days ago
Modified
yesterday
Viewed
62 times
Part of NLP Collective
0
when the program starts to initialize pipeline object, a unexpected error was thrown:
[rank0]: Traceback (most recent call last):
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/distributed/pipelining/_IR.py", line 1007, in _trace_with_export
[rank0]:
ep = torch.export.export(
[rank0]:
^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/__init__.py", line 270, in export
[rank0]:
return _export(
[rank0]:
^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 1017, in wrapper
[rank0]:
raise e
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 990, in wrapper
[rank0]:
ep = fn(*args, **kwargs)
[rank0]:
^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/exported_program.py", line 114, in wrapper
[rank0]:
return fn(*args, **kwargs)
[rank0]:
^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 1880, in _export
[rank0]:
export_artifact = export_func(
# type: ignore[operator]
[rank0]:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 1224, in _strict_export
[rank0]:
return _strict_export_lower_to_aten_ir(
[rank0]:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 1252, in _strict_export_lower_to_aten_ir
[rank0]:
gm_torch_level = _export_to_torch_ir(
[rank0]:
^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/export/_trace.py", line 560, in _export_to_torch_ir
[rank0]:
gm_torch_level, _ = torch._dynamo.export(
[rank0]:
^^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1575, in inner
[rank0]:
graph = rewrite_signature(
[rank0]:
^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1073, in rewrite_signature
[rank0]:
check_user_input_output(flat_results_traced, UserErrorType.INVALID_OUTPUT)
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1059, in check_user_input_output
[rank0]:
raise UserError(
[rank0]: torch._dynamo.exc.UserError: It looks like one of the outputs with type `<class 'transformers.cache_utils.DynamicCache'>` is not supported or pytree-flattenable.
[rank0]: Exported graphs outputs can only contain the following supported types: [<class 'torch.Tensor'>, <class 'torch.SymInt'>, <class 'torch.SymFloat'>, <class 'torch.SymBool'>, <class 'torch.ScriptObject'>, <class 'torch.device'>, <class 'torch.layout'>, <class 'code'>, <class 'triton.language.core.dtype'>, <class 'str'>, <class 'int'>, <class 'float'>, <class 'bytes'>, <class 'torch.dtype'>, <class 'bool'>, <class 'complex'>, <class 'NoneType'>, <class 'torch.memory_format'>, <class 'ellipsis'>].
[rank0]: If you are using a custom class object, please register a pytree_flatten/unflatten function using `torch.utils._pytree.register_pytree_node` or `torch.export.register_dataclass`.
[rank0]: The above exception was the direct cause of the following exception:
[rank0]: Traceback (most recent call last):
[rank0]:
File "/share/xxx/pippy_llama.py", line 36, in <module>
[rank0]:
pipe = pipeline(llama, mb_args=(mb_inputs["input_ids"],))
[rank0]:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/distributed/pipelining/_IR.py", line 1238, in pipeline
[rank0]:
return Pipe.from_tracing(
[rank0]:
^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/distributed/pipelining/_IR.py", line 1045, in from_tracing
[rank0]:
exported_program = Pipe._trace_with_export(
[rank0]:
^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:
File "/root/anaconda3/envs/polar/lib/python3.12/site-packages/torch/distributed/pipelining/_IR.py", line 1013, in _trace_with_export
[rank0]:
raise RuntimeError(
[rank0]: RuntimeError: It seems that we cannot capture your model as a full graph. Typical reasons include graph breaks, data/shape-dependent control flow, or missing meta kernels for custom operators. You can use our manual pipeline interfaces, or try to fix the graph breaks, see https://pytorch.org/docs/stable/export.html
the source code from torch/pippy/examples is below:
# $ torchrun --nproc-per-node 4 pippy_llama.py
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.distributed.pipelining import SplitPoint, pipeline, ScheduleGPipe
# Grab the model
llama = AutoModelForCausalLM.from_pretrained(
"meta-llama/Llama-2-7b-chat-hf", low_cpu_mem_usage=True
)
print(llama)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
tokenizer.pad_token = tokenizer.eos_token
mb_prompts = (
"How do you", "I like to",
)
# microbatch size = 2
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
device = torch.device(f"cuda:{rank % torch.cuda.device_count()}")
torch.distributed.init_process_group(rank=rank, world_size=world_size)
llama.to(device).eval()
# Cut model by equal number of layers per rank
layers_per_rank = llama.config.num_hidden_layers // world_size
print(f"layers_per_rank = {layers_per_rank}")
split_spec = {
f"model.layers.{i * layers_per_rank}": SplitPoint.BEGINNING
for i in range(1, world_size)
}
# Create a pipeline representation from the model
mb_inputs = tokenizer(mb_prompts, return_tensors="pt", padding=True).to(device)
pipe = pipeline(llama, mb_args=(mb_inputs["input_ids"],))
# Create pipeline stage for each rank
stage = pipe.build_stage(rank, device=device)
# Run time inputs
full_batch_prompts = (
"How do you", "I like to", "Can I help", "You need to",
"The weather is", "I found a", "What is your", "You are so",
)
# full batch size = 8
inputs = tokenizer(full_batch_prompts, return_tensors="pt", padding=True).to(device)
# Attach to a schedule
# number of microbatches = 8 // 2 = 4
num_mbs = 4
schedule = ScheduleGPipe(stage, num_mbs)
# Run
if rank == 0:
args = inputs["input_ids"]
else:
args = None
output = schedule.step(args)
# Decode
if output is not None:
next_token_logits = output[0][:, -1, :]
next_token = torch.argmax(next_token_logits, dim=-1)
print(tokenizer.batch_decode(next_token))
you can also find this file @ https://github.com/pytorch/PiPPy/blob/main/examples/llama/pippy_llama.py
I tried several methods like setting model.config.use_cache = False, but none of them worked.
I have no idea what happened. My environment is:
torch=='2.5.1'
transformers=='4.56.2'
Do I need to change to a new model rather than llama? It seems like a bug.
Further efforts: I upgraded and downgraded the torch version, but still not working
pythonpytorchhuggingface-transformersllama
Share
Improve this question
Follow
edited yesterday
agilgur5
2,03144 gold badges4444 silver badges6363 bronze badges
asked 2 days ago
AerithAerith
111 bronze badge
New contributor
Aerith is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our Code of Conduct.
1
Please provide enough code so others can better understand or reproduce the problem.
Community
–
Community
Bot
2025-09-24 00:08:58 +00:00
Commented
2 days ago
Add a comment
|
Related questions
1370
How do I type hint a method with the type of the enclosing class?
1
Apple M2 RuntimeError: Placeholder storage has not been allocated on MPS device
0
Using PyTorch's DDP for multi-GPU training with mp.spawn() doesn't work
Related questions
1370
How do I type hint a method with the type of the enclosing class?
1
Apple M2 RuntimeError: Placeholder storage has not been allocated on MPS device
0
Using PyTorch's DDP for multi-GPU training with mp.spawn() doesn't work
5
ERROR: vars() argument must have __dict__ attribute when trying to use trainer.train() on custom HF dataset?
0
Pytorch torchvision.transforms execute randomly?
Load 2 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this question via email, Twitter, or Facebook.
Your Answer
Thanks for contributing an answer to Stack Overflow!Please be sure to answer the question. Provide details and share your research!But avoid …Asking for help, clarification, or responding to other answers.Making statements based on opinion; back them up with references or personal experience.To learn more, see our tips on writing great answers.
Draft saved
Draft discarded
Sign up or log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our terms of service and acknowledge you have read our privacy policy.
Start asking to get answers
Find the answer to your question by asking.
Ask question
Explore related questions
pythonpytorchhuggingface-transformersllama
See similar questions with these tags.
NLP
Collective
See more
This question is in a collective: a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
Democratizing your data access with AI agents
The history and future of software development (part 1)
Featured on Meta
Spevacus has joined us as a Community Manager
Introducing a new proactive anti-spam measure
New and improved coding challenges
New comment UI experiment graduation
Policy: Generative AI (e.g., ChatGPT) is banned
Hot Network Questions
Using Switch to return a value depending on the sign of a real number
What is an aggravating factor and why is kinship considered one?
Changing size of math environments
What happened to all of the energy created at the Big Bang when the matter and antimatter annihilated each other?
What is this glass device I found next to drinking glasses in the south of France
What does it mean for work to be done ON a system?
What conditions on an Earthlike planet would allow for a massive, Pando-esque forest that’s a single organism?
Lock icon to convey disabled but has a clickable feature?
Separating trefoil knot on torus
Simplification of an analytically evaluated integral expressed in terms of elliptic integrals
Why would disembarking a few passengers delay a flight by 3 hours?
Time traveler named Tom/John forgets device, Penelope Weschler, machine harvests Earth's spin
Convex sets, tangent cones and convergence
Which driving direction syncs with Quebec’s and Maritime Canada’s autumn leaf colors?
Space Princess Space Tours: Black Holes merging - what would you visually see?
SF short-story written perhaps 30 - 40 years ago about female-infected creatures who are called males
What license to use when extending a pre-existing library?
Can the price of a gap option be negative?
Plotting functions without sampling artefacts
3 variables cyclic rearrangement inequality.
In the U.S., what protections are in place to help (under)graduate students whose entire department is removed?
What does "my left a** cheek" mean?
Regulator circuit outputs negative current
Short story about a metal-eating alien
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Your Privacy Choices
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2025 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2025.9.25.34480