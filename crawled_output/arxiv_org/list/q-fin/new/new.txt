Quantitative Finance
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-fin
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Finance
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 23 September 2025
Total of 30 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 9 of 9 entries)
[1]
arXiv:2509.16334
[pdf, html, other]
Title:
Volatility Calibration via Automatic Local Regression
Ruozhong Yang, Hao Qin, Charlie Che, Liming Feng
Comments:
38 pages,30 figures
Subjects:
Computational Finance (q-fin.CP)
Managing exotic derivatives requires accurate mark-to-market pricing and stable Greeks for reliable hedging. The Local Volatility (LV) model distinguishes itself from other pricing models by its ability to match observable market prices across all strikes and maturities with high accuracy. However, LV calibration is fundamentally ill-posed: finite market observables must determine a continuously-defined surface with infinite local volatility parameters. This ill-posed nature often causes spiky LV surfaces that are particularly problematic for finite-difference-based valuation, and induces high-frequency oscillations in solutions, thus leading to unstable Greeks. To address this challenge, we propose a pre-calibration smoothing method that can be integrated seamlessly into any LV calibration workflow. Our method pre-processes market observables using local regression that automatically minimizes asymptotic conditional mean squared error to generate denoised inputs for subsequent LV calibration. Numerical experiments demonstrate that the proposed pre-calibration smoothing yields significantly smoother LV surfaces and greatly improves Greek stability for exotic options with negligible additional computational cost, while preserving the LV model's ability to fit market observables with high fidelity.
[2]
arXiv:2509.16707
[pdf, html, other]
Title:
Increase Alpha: Performance and Risk of an AI-Driven Trading Framework
Sid Ghatak, Arman Khaledian, Navid Parvini, Nariman Khaledian
Comments:
To get access to the data, please contact this http URL@increasealpha.com
Subjects:
Portfolio Management (q-fin.PM); Machine Learning (cs.LG)
There are inefficiencies in financial markets, with unexploited patterns in price, volume, and cross-sectional relationships. While many approaches use large-scale transformers, we take a domain-focused path: feed-forward and recurrent networks with curated features to capture subtle regularities in noisy financial data. This smaller-footprint design is computationally lean and reliable under low signal-to-noise, crucial for daily production at scale. At Increase Alpha, we built a deep-learning framework that maps over 800 U.S. equities into daily directional signals with minimal computational overhead.
The purpose of this paper is twofold. First, we outline the general overview of the predictive model without disclosing its core underlying concepts. Second, we evaluate its real-time performance through transparent, industry standard metrics. Forecast accuracy is benchmarked against both naive baselines and macro indicators. The performance outcomes are summarized via cumulative returns, annualized Sharpe ratio, and maximum drawdown. The best portfolio combination using our signals provides a low-risk, continuous stream of returns with a Sharpe ratio of more than 2.5, maximum drawdown of around 3\%, and a near-zero correlation with the S\&P 500 market benchmark. We also compare the model's performance through different market regimes, such as the recent volatile movements of the US equity market in the beginning of 2025. Our analysis showcases the robustness of the model and significantly stable performance during these volatile periods.
Collectively, these findings show that market inefficiencies can be systematically harvested with modest computational overhead if the right variables are considered. This report will emphasize the potential of traditional deep learning frameworks for generating an AI-driven edge in the financial market.
[3]
arXiv:2509.16734
[pdf, html, other]
Title:
Multigenerational Inequality
Jan Stuhler
Comments:
This paper has previously been published in the Research Handbook on Intergenerational Inequality, Edward Elgar Publishing Ltd, this https URL
Journal-ref:
Research Handbook on Intergenerational Inequality, Edward Elgar Publishing Ltd (2024)
Subjects:
General Economics (econ.GN)
A growing literature provides evidence on multigenerational inequality -- the extent to which socio-economic advantages persist across three or more generations. This chapter reviews its main findings and implications. Most studies find that inequality is more persistent than a naive iteration of conventional parent-child correlations would suggest. We discuss potential interpretations of this new ``fact'' related to (i) latent, (ii) non-Markovian or (iii) non-linear transmission processes, empirical strategies to discriminate between them, and the link between multigenerational and assortative associations.
[4]
arXiv:2509.16912
[pdf, html, other]
Title:
Analysis of the Impact of an Execution Algorithm with an Order Book Imbalance Strategy on a Financial Market Using an Agent-based Simulation
Shuto Endo, Takanobu Mizuta, Isao Yagi
Journal-ref:
The Journal of the Japanese Society for Artificial Intelligence (Vol. 39, No. 4, 2024)
Subjects:
Computational Finance (q-fin.CP); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)
Order book imbalance (OBI) - buy orders minus sell orders near the best quote - measures supply-demand imbalance that can move prices. OBI is positively correlated with returns, and some investors try to use it to improve performance. Large orders placed at once can reveal intent, invite front-running, raise volatility, and cause losses. Execution algorithms therefore split parent orders into smaller lots to limit price distortion. In principle, using OBI inside such algorithms could improve execution, but prior evidence is scarce because isolating OBI's effect in real markets is nearly impossible amid many external factors.
Multi-agent simulation offers a way to study this. In an artificial market, individual actors are agents whose rules and interactions form the model. This study builds an execution algorithm that accounts for OBI, tests it across several market patterns in artificial markets, and analyzes mechanisms, comparing it with a conventional (OBI-agnostic) algorithm.
Results: (i) In stable markets, the OBI strategy's performance depends on the number of order slices; outcomes vary with how the parent order is partitioned. (ii) In markets with unstable prices, the OBI-based algorithm outperforms the conventional approach. (iii) Under spoofing manipulation, the OBI strategy is not significantly worse than the conventional algorithm, indicating limited vulnerability to spoofing.
Overall, OBI provides a useful signal for execution. Incorporating OBI can add value - especially in volatile conditions - while remaining reasonably robust to spoofing; in calm markets, benefits are sensitive to slicing design.
[5]
arXiv:2509.17236
[pdf, html, other]
Title:
An Ambit Field Framework for the Full Panel of Day-ahead Electricity Prices
Thomas K. Kloster
Comments:
34 pages, 6 figures
Subjects:
Mathematical Finance (q-fin.MF)
This paper considers the often overlooked fact that electricity spot prices in individual European generation zones evolve as a high dimensional panel structure. A general continuous time framework is developed by formulating the panel as an ambit field indexed by a cylinder surface, where the cross sectional dimension is represented by a circle. This requires a treatment of ambit fields on manifolds, but the departure from Euclidean space allows for embedding intrinsic dependence structures into the index set in a flexible and parameter-free way, where the daily delivery periods have a canonical mapping onto the circle. The model is a natural space-time extension of volatility modulated LÃ©vy-driven Volterra processes, which have previously been studied in the context of energy markets, and the pricing of electricity derivatives turns out to be essentially as analytically tractable as in the null-spatial setting. The space-time framework extends the scope of possible derivatives to products written on individual delivery periods, where spreads between these constitute an interesting example. We establish useful formulas for the pricing of various derivatives along with a simulation scheme, and study specifications of the dependence structure in detail.
[6]
arXiv:2509.17303
[pdf, html, other]
Title:
Trade, Political Distance and the World Trade Organization
Samuel Hardwick
Comments:
36 pages, 5 figures
Subjects:
General Economics (econ.GN)
Trade agreements are often understood as shielding commerce from fluctuations in political relations. This paper provides evidence that World Trade Organization membership reduces the penalty of political distance on trade at the extensive margin. Using a structural gravity framework covering 1948 to 2023 and two measures of political distance, based on high-frequency events data and UN General Assembly votes, GATT/WTO status is consistently associated with a wider range of products traded between politically distant partners. The association is strongest in the early WTO years (1995 to 2008). Events-based estimates also suggest attenuation at the intensive margin, while UN vote-based estimates do not. Across all specifications, GATT/WTO membership increases aggregate trade volumes. The results indicate that a function of the multilateral trading system has been to foster new trade links across political divides, while raising trade volumes among both close and distant partners.
[7]
arXiv:2509.17875
[pdf, html, other]
Title:
Invariance of finite-dimensional realisations of Heath-Jarrow-Morton models under diffusion estimation
Andreas Celary, Paul KrÃ¼hner
Subjects:
Mathematical Finance (q-fin.MF)
We identify all smooth manifolds of curves for Heath-Jarrow-Morton models that are consistent with any tangential diffusion coefficient. In fact, we show that these manifolds cannot be affine but must be of linear-rational type.
[8]
arXiv:2509.17919
[pdf, html, other]
Title:
Economic Complexity Alignment and Sustainable Development
Quinten De Wettinck, Karolien De Bruyne, Wouter Bam, CÃ©sar A. Hidalgo
Subjects:
General Economics (econ.GN)
Economic complexity has been linked to sustainability outcomes, such as income inequality and greenhouse gas emissions. Yet, it is unclear whether the pursuit of complex and/or related activities naturally aligns with these outcomes, or whether meeting sustainability goals requires policy interventions that pursue unrelated diversification. Here, we exploit multidimensional social and environmental sustainability indicators to quantify the alignment between a country's closest diversification opportunities and sustainability goals. We find that high- and upper-middle-income countries face significantly better environmentally aligned diversification opportunities than poorer economies. This means that, while richer countries enjoy diversification opportunities that align complexity, relatedness and environmental performance, this alignment is weaker for developing economies. These findings underscore the value of evaluating future diversification trajectories through a multidimensional sustainability framework, and emphasise the strategic relevance of unrelated diversification for less developed economies to foster sustainable development.
[9]
arXiv:2509.17964
[pdf, html, other]
Title:
FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance
Yang Li, Zhi Chen, Steve Y. Yang, Ruixun Zhang
Comments:
4 pages, 1 figure, 1 table
Subjects:
Computational Finance (q-fin.CP)
Traditional stochastic control methods in finance rely on simplifying assumptions that often fail in real world markets. While these methods work well in specific, well defined scenarios, they underperform when market conditions change. We introduce FinFlowRL, a novel framework for financial stochastic control that combines imitation learning with reinforcement learning. The framework first pretrains an adaptive meta policy by learning from multiple expert strategies, then finetunes it through reinforcement learning in the noise space to optimize the generation process. By employing action chunking, that is generating sequences of actions rather than single decisions, it addresses the non Markovian nature of financial markets. FinFlowRL consistently outperforms individually optimized experts across diverse market conditions.
Cross submissions (showing 5 of 5 entries)
[10]
arXiv:2509.16655
(cross-list from cs.SE)
[pdf, html, other]
Title:
Incentives and Outcomes in Bug Bounties
Serena Wang, Martino Banchio, Krzysztof Kotowicz, Katrina Ligett, R. Preston McAfee, Eduardo' Vela'' Nava
Subjects:
Software Engineering (cs.SE); Cryptography and Security (cs.CR); General Economics (econ.GN)
Bug bounty programs have contributed significantly to security in technology firms in the last decade, but little is known about the role of reward incentives in producing useful outcomes. We analyze incentives and outcomes in Google's Vulnerability Rewards Program (VRP), one of the world's largest bug bounty programs. We analyze the responsiveness of the quality and quantity of bugs received to changes in payments, focusing on a change in Google's reward amounts posted in July, 2024, in which reward amounts increased by up to 200% for the highest impact tier. Our empirical results show an increase in the volume of high-value bugs received after the reward increase, for which we also compute elasticities. We further break down the sources of this increase between veteran researchers and new researchers, showing that the reward increase both redirected the attention of veteran researchers and attracted new top security researchers into the program.
[11]
arXiv:2509.16925
(cross-list from cs.CY)
[pdf, other]
Title:
Tenure Under Pressure: Simulating the Disruptive Effects of AI on Academic Publishing
Shan Jiang
Subjects:
Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); General Economics (econ.GN)
Generative artificial intelligence (AI) has begun to reshape academic publishing by enabling the rapid production of submission-ready manuscripts. While such tools promise to enhance productivity, they also raise concerns about overwhelming journal systems that have fixed acceptance capacities. This paper uses simulation modeling to investigate how AI-driven surges in submissions may affect desk rejection rates, review cycles, and faculty publication portfolios, with a focus on business school journals and tenure processes. Three scenarios are analyzed: a baseline model, an Early Adopter model where a subset of faculty boosts productivity, and an AI Abuse model where submissions rise exponentially. Results indicate that early adopters initially benefit, but overall acceptance rates fall sharply as load increases, with tenure-track faculty facing disproportionately negative outcomes. The study contributes by demonstrating the structural vulnerabilities of the current publication system and highlights the need for institutional reform in personnel evaluation and research dissemination practices.
[12]
arXiv:2509.16955
(cross-list from quant-ph)
[pdf, html, other]
Title:
Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance
Chi-Sheng Chen, Aidan Hung-Wen Tsai
Subjects:
Quantum Physics (quant-ph); Machine Learning (cs.LG); Computational Finance (q-fin.CP)
We formulate automated market maker (AMM) \emph{rebalancing} as a binary detection problem and study a hybrid quantum--classical self-attention block, \textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum queries/keys/values via variational quantum circuits (VQCs) and applies standard softmax attention over Pauli-$Z$ expectation vectors, yielding a drop-in attention module for financial time-series decision making. Using daily data for \textbf{BTCUSDC} over \textbf{Jan-2024--Jan-2025} with a 70/15/15 time-series split, we compare QASA against classical ensembles, a transformer, and pure quantum baselines under Return, Sharpe, and Max Drawdown. The \textbf{QASA-Sequence} variant attains the \emph{best single-model risk-adjusted performance} (\textbf{13.99\%} return; \textbf{Sharpe 1.76}), while hybrid models average \textbf{11.2\%} return (vs.\ 9.8\% classical; 4.4\% pure quantum), indicating a favorable performance--stability--cost trade-off.
[13]
arXiv:2509.17555
(cross-list from math.PR)
[pdf, html, other]
Title:
The randomly distorted Choquet integrals with respect to a G-randomly distorted capacity and risk measures
Ohood Aldalbahi, Miryana Grigorova
Subjects:
Probability (math.PR); Mathematical Finance (q-fin.MF); Risk Management (q-fin.RM)
We study randomly distorted Choquet integrals with respect to a capacity c on a measurable space ({\Omega},F), where the capacity c is distorted by a G-measurable random distortion function (with G a sub-{\sigma}-algebra of F). We establish some fundamental properties, including the comonotonic additivity of these integrals under suitable assumptions on the underlying capacity space. We provide a representation result for comonotonic additive conditional risk measures which are monotone with respect to the first-order stochastic dominance relation (with respect to the capacity c) in terms of these randomly distorted Choquet integrals. We also present the case where the random distortion functions are concave. In this case, the G-randomly distorted Choquet integrals are characterised in terms of comonotonic additive conditional risk measures which are monotone with respect to the stop-loss stochastic dominance relation (with respect to the capacity c). We provide examples, extending some well-known risk measures in finance and insurance, such as the Value at Risk and the Average Value at Risk.
[14]
arXiv:2509.17715
(cross-list from quant-ph)
[pdf, html, other]
Title:
Enhanced fill probability estimates in institutional algorithmic bond trading using statistical learning algorithms with quantum computers
Axel Ciceri, Austin Cottrell, Joshua Freeland, Daniel Fry, Hirotoshi Hirai, Philip Intallura, Hwajung Kang, Chee-Kong Lee, Abhijit Mitra, Kentaro Ohno, Das Pemmaraju, Manuel Proissl, Brian Quanz, Del Rajan, Noriaki Shimada, Kavitha Yograj
Comments:
16 pages, 13 Figures, 4 Tables
Subjects:
Quantum Physics (quant-ph); Trading and Market Microstructure (q-fin.TR)
The estimation of fill probabilities for trade orders represents a key ingredient in the optimization of algorithmic trading strategies. It is bound by the complex dynamics of financial markets with inherent uncertainties, and the limitations of models aiming to learn from multivariate financial time series that often exhibit stochastic properties with hidden temporal patterns. In this paper, we focus on algorithmic responses to trade inquiries in the corporate bond market and investigate fill probability estimation errors of common machine learning models when given real production-scale intraday trade event data, transformed by a quantum algorithm running on IBM Heron processors, as well as on noiseless quantum simulators for comparison. We introduce a framework to embed these quantum-generated data transforms as a decoupled offline component that can be selectively queried by models in low-latency institutional trade optimization settings. A trade execution backtesting method is employed to evaluate the fill prediction performance of these models in relation to their input data. We observe a relative gain of up to ~ 34% in out-of-sample test scores for those models with access to quantum hardware-transformed data over those using the original trading data or transforms by noiseless quantum simulation. These empirical results suggest that the inherent noise in current quantum hardware contributes to this effect and motivates further studies. Our work demonstrates the emerging potential of quantum computing as a complementary explorative tool in quantitative finance and encourages applied industry research towards practical applications in trading.
Replacement submissions (showing 16 of 16 entries)
[15]
arXiv:2303.14486
(replaced)
[pdf, html, other]
Title:
Exposure to World War II and Its Labor Market Consequences over the Life Cycle
Sebastian T. Braun, Jan Stuhler
Comments:
JEL Code: J24, J26, N34, Keywords: World War II; labor market careers; war injuries; prisoners of war, displacement; life-cycle models
Subjects:
General Economics (econ.GN)
With 70 million dead, World War II remains the most devastating conflict in history. Among the survivors, millions were displaced, returned maimed from the battlefield, or endured years of captivity. We examine the effects of such war exposures on labor market careers, showing that they often become apparent only at certain life stages. While war injuries reduced employment in old age, former prisoners of war prolonged their time in the workforce before retiring. Many displaced workers, especially women, never returned to employment. These responses align with standard life-cycle theory and thus likely hold relevance for other conflicts.
[16]
arXiv:2308.00016
(replaced)
[pdf, html, other]
Title:
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment
Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel M. Ni, Heung-Yeung Shum, Jian Guo
Comments:
EMNLP 2025 System Demonstration Track
Subjects:
Computational Finance (q-fin.CP); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
[17]
arXiv:2310.02163
(replaced)
[pdf, html, other]
Title:
Navigating Uncertainty in ESG Investing
Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth
Comments:
36 pages, 2 figures, presented at Fields - Institute's Mathematics for Climate Change (MfCC) Network & Waterloo Institute for Complexity and Innovation (WICI): Math for Complex Climate Challenges Workshop, Waterloo, Canada; 26th International Congress on Insurance: Mathematics and Economics, Edinburgh, UK; and the 58th Actuarial Research Conference (ARC), Des Moines, Iowa, USA
Subjects:
Portfolio Management (q-fin.PM); Statistical Finance (q-fin.ST)
The widespread confusion among investors regarding Environmental, Social, and Governance (ESG) rankings assigned by rating agencies has underscored a critical issue in sustainable investing. To address this uncertainty, our research has devised methods that not only recognize this ambiguity but also offer tailored investment strategies for different investor profiles. By developing ESG ensemble strategies and integrating ESG scores into a Reinforcement Learning (RL) model, we aim to optimize portfolios that cater to both financial returns and ESG-focused outcomes. Additionally, by proposing the Double-Mean-Variance model, we classify three types of investors based on their risk preferences. We also introduce ESG-adjusted Capital Asset Pricing Models (CAPMs) to assess the performance of these optimized portfolios. Ultimately, our comprehensive approach provides investors with tools to navigate the inherent ambiguities of ESG ratings, facilitating more informed investment decisions.
[18]
arXiv:2401.08064
(replaced)
[pdf, other]
Title:
A mechanistic model of trust based on neural information processing
Scott E. Allen, RenÃ© F. Kizilcec, A. David Redish
Subjects:
General Economics (econ.GN); Human-Computer Interaction (cs.HC); Neurons and Cognition (q-bio.NC)
Trust is central to human social interactions, manifesting in actions that make one vulnerable to another. We argue that trust will thus depend on the decision-making processes that arise in neural systems. Building on advances in the cognitive neuroscience of decision making, we propose a mechanistic model of trust arising from multiple parallel systems that perform distinct, complementary information processing. Because each system learns via different mechanisms, trust can be created (or destroyed) in multiple ways. This systems-level taxonomy of information representations provides a principled basis for differentiating forms of trust, linking them to specific learning processes, and generating testable predictions about their expression in behavior. By situating trust within a broader theory of neural decision systems, our account unifies diverse findings across psychology, neuroscience, and the social sciences, and offers a foundation for explaining how humans develop, maintain, and repair trust in a complex social world.
[19]
arXiv:2407.12924
(replaced)
[pdf, html, other]
Title:
Concentration-Based Inference for Evaluating Horizontal Mergers
Paul S. Koh
Subjects:
General Economics (econ.GN)
Antitrust authorities routinely rely on market concentration measures to assess the potential adverse effects of mergers on consumer welfare. Using a first-order approximation argument with logit and CES demand, I derive the relationship between the welfare effect of a merger on consumer surplus and the change in the Herfindahl-Hirschman Index (HHI). My results suggest that merger harm is correlated with the merger-induced change in HHI, and the proportionality coefficient depends on the price responsiveness parameter, market size, and the distribution of market shares within and across the merging firms. I present numerical validation of my formula along with an empirical illustration.
[20]
arXiv:2410.17154
(replaced)
[pdf, other]
Title:
Estimating Spillovers from Sampled Connections
Kieran Marray
Subjects:
General Economics (econ.GN)
Empirical researchers often estimate spillover effects by fitting linear or non-linear regression models to sampled network data. We show that common sampling schemes bias these estimates, potentially upwards, and derive biased-corrected estimators that researchers can construct from aggregate network statistics. Our results apply under different assumptions on the relationship between observed and unobserved links, allow researchers to bound true effect sizes, and to determine robustness to mismeasured links. As an application, we estimate the propagation of climate shocks between U.S. public firms from self-reported supply links, building a new dataset of county-level incidence of large climate shocks.
[21]
arXiv:2501.12600
(replaced)
[pdf, html, other]
Title:
Pontryagin-Guided Deep Policy Learning for Constrained Dynamic Portfolio Choice
Jeonggyu Huh, Jaegi Jeon, Hyeng Keun Koo, Byung Hwa Lim
Subjects:
Portfolio Management (q-fin.PM)
We present a Pontryagin-Guided Direct Policy Optimization (PG-DPO) framework for \emph{constrained} continuous-time portfolio--consumption problems that scales to hundreds of assets. The method couples neural policies with Pontryagin's Maximum Principle and enforces feasibility via a lightweight log-barrier stagewise solve; a \emph{manifold-projection} variant (P--PGDPO) projects controls onto the PMP/KKT manifold using stabilized adjoints. We prove a barrier--KKT correspondence with $O(\epsilon)$ policy error and $O(\epsilon^2)$ instantaneous Hamiltonian gap, and extend the BPTT--PMP match to constrained settings. On short-sale (nonnegativity, floating cash) and wealth-proportional consumption caps, P--PGDPO reduces risky-weight errors by orders of magnitude versus baseline PG-DPO, while the one-dimensional consumption control shows smaller but consistent gains near binding caps. The approach remains effective when closed-form benchmarks are unavailable, and is readily extensible to transaction costs and interacting limits -- promising even greater benefits under time-varying investment opportunities where classical solutions are scarce.
[22]
arXiv:2503.15443
(replaced)
[pdf, other]
Title:
Are Elites Meritocratic and Efficiency-Seeking? Evidence from MBA Students
Marcel Preuss, GermÃ¡n Reyes, Jason Somerville, Joy Wu
Comments:
JEL codes: D63, C91, H23
Subjects:
General Economics (econ.GN)
Elites disproportionately influence policymaking, yet little is known about their fairness and efficiency preferences -- key determinants of support for redistributive policies. We investigate these preferences in an incentivized lab experiment with a group of future elites -- Ivy League MBA students. We find that MBA students implement substantially more unequal earnings distributions than the average American, regardless of whether inequality stems from luck or merit. Their redistributive choices are also highly responsive to efficiency costs, with an effect that is an order of magnitude larger than that found in representative U.S. samples. Analyzing fairness ideals, we find that MBA students are less likely to be strict meritocrats than the broader population. These findings provide novel insights into how elites' redistributive preferences may shape high levels of inequality in the U.S.
[23]
arXiv:2506.12976
(replaced)
[pdf, other]
Title:
Does Medicaid Expansion Lead to Income Adjustment? Evidence from the Survey of Income Program Participation
Mingjian Li
Comments:
43 pages
Subjects:
General Economics (econ.GN)
Using monthly Survey of Income Program Participation (SIPP) data and a regression discontinuity (RD) design at the 138 percent Federal Poverty Line (FPL) threshold, this paper shows that childless adults in Medicaid expansion states lowered their lowest monthly earnings by about 39 FPL percentage points, equal to 700 dollars in 2025 for a two person household, to qualify for Medicaid. The response was largest when the individual mandate penalty was in effect and declined after the penalty was reduced to zero in 2019. Evidence from zero earnings months and hours worked indicates adjustments along both extensive and intensive margins. These results validate lowest monthly earnings as a practical eligibility measure and provide new evidence of substantial labor supply responses to the Affordable Care Act Medicaid notch.
[24]
arXiv:2507.00575
(replaced)
[pdf, html, other]
Title:
Scale-Dependent Multifractality in Bitcoin Realised Volatility: Implications for Rough Volatility Modelling
Milan Pontiggia (MAGEFI - University of Bordeaux, France)
Comments:
41 pages, 7 figures, 15 tables. Code and supplementary diagnostics available upon request
Subjects:
Statistical Finance (q-fin.ST); Mathematical Finance (q-fin.MF)
We assess the applicability of rough volatility models to Bitcoin realised volatility using the normalised p-variation framework of Cont and Das (2024). Applying this model free estimator to high-frequency Bitcoin data from 2017 to 2024 across multiple sampling resolutions, we find that the normalised statistic remains strictly negative throughout, precluding the estimation of a valid roughness index. Stationarity tests and robustness checks reveal no significant evidence of non-stationarity or structural breaks as explanatory factors. Instead, convergent evidence from three complementary diagnostics, namely multifractal detrended fluctuation analysis, log-log moment scaling, and wavelet leaders, reveals a multifractal structure in Bitcoin volatility. This scale-dependent behaviour violates the homogeneity assumptions underlying rough volatility estimation and accounts for the estimator's systematic failure. These findings suggest that while rough volatility models perform well in traditional markets, they are structurally misaligned with the empirical features of Bitcoin volatility.
[25]
arXiv:2508.01880
(replaced)
[pdf, html, other]
Title:
Time-Varying Factor-Augmented Models for Volatility Forecasting
Duo Zhang, Jiayu Li, Junyi Mo, Elynn Chen
Subjects:
Statistical Finance (q-fin.ST); Mathematical Finance (q-fin.MF)
Accurate volatility forecasts are vital in modern finance for risk management, portfolio allocation, and strategic decision-making. However, existing methods face key limitations. Fully multivariate models, while comprehensive, are computationally infeasible for realistic portfolios. Factor models, though efficient, primarily use static factor loadings, failing to capture evolving volatility co-movements when they are most critical. To address these limitations, we propose a novel, model-agnostic Factor-Augmented Volatility Forecast framework. Our approach employs a time-varying factor model to extract a compact set of dynamic, cross-sectional factors from realized volatilities with minimal computational cost. These factors are then integrated into both statistical and AI-based forecasting models, enabling a unified system that jointly models asset-specific dynamics and evolving market-wide co-movements. Our framework demonstrates strong performance across two prominent asset classes-large-cap U.S. technology equities and major cryptocurrencies-over both short-term (1-day) and medium-term (7-day) horizons. Using a suite of linear and non-linear AI-driven models, we consistently observe substantial improvements in predictive accuracy and economic value. Notably, a practical pairs-trading strategy built on our forecasts delivers superior risk-adjusted returns and profitability, particularly under adverse market conditions.
[26]
arXiv:2508.10585
(replaced)
[pdf, html, other]
Title:
Racial bias, colorism, and overcorrection
Kenneth Colombe, Alex Krumer, Rosa Lavelle-Hill, Tim Pawlowski
Subjects:
General Economics (econ.GN)
This paper examines whether increased awareness can affect racial bias and colorism. We exploit a natural experiment from the widespread publicity of Price and Wolfers (2010), which intensified scrutiny of racial bias in men's basketball officiating. We investigate refereeing decisions in the Women's National Basketball Association (WNBA), an organization with a long-standing commitment to diversity, equity, and inclusion (DEI). We apply machine learning techniques to predict player race and to measure skin tone. Our empirical strategy exploits the quasi-random assignment of referees to games, combined with high-dimensional fixed effects, to estimate the relationship between referee-player racial and skin tone compositions and foul-calling behavior. We find no racial bias before the intense media coverage. However, we find evidence of overcorrection, whereby a player receives fewer fouls when facing more referees from the opposite race and skin tone. This overcorrection wears off over time, returning to zero-bias levels. We highlight the need to consider baseline levels of bias before applying any prescription with direct relevance to policymakers and organizations given the recent discourse on DEI.
[27]
arXiv:2509.10586
(replaced)
[pdf, other]
Title:
Stabilising Lifetime PD Models under Forecast Uncertainty
Vahab Rostampour
Subjects:
Risk Management (q-fin.RM); Systems and Control (eess.SY)
Estimating lifetime probabilities of default (PDs) under IFRS~9 and CECL requires projecting point--in--time transition matrices over multiple years. A persistent weakness is that macroeconomic forecast errors compound across horizons, producing unstable and volatile PD term structures. This paper reformulates the problem in a state--space framework and shows that a direct Kalman filter leaves non--vanishing variability. We then introduce an anchored observation model, which incorporates a neutral long--run economic state into the filter. The resulting error dynamics exhibit asymptotic stochastic stability, ensuring convergence in probability of the lifetime PD term structure. Simulation on a synthetic corporate portfolio confirms that anchoring reduces forecast noise and delivers smoother, more interpretable projections.
[28]
arXiv:2509.14057
(replaced)
[pdf, html, other]
Title:
Machines are more productive than humans until they aren't, and vice versa
Riccardo Zanardelli
Comments:
Results enriched by experiment focusing on machine skill achieving high performance across all task difficulties; results of the primary experiment unchanged; data analysis section expanded; conclusions enriched and re-organized; abstract perfected; example in section A.4.1 enhanced; corrections to Table 17 (now Table 21); minor typos corrected
Subjects:
General Economics (econ.GN); Artificial Intelligence (cs.AI)
With the growth of artificial skills, organizations are increasingly confronting with the problem of optimizing skill policy decisions guided by economic principles. This paper addresses the underlying complexity of this challenge by developing an in-silico framework based on Monte Carlo simulations grounded in empirical realism to analyze the economic impact of human and machine skills, individually or jointly deployed in the execution of tasks presenting varying levels of complexity. Our results provide quantitative support for the established notions that automation tends to be the most economically-effective strategy for tasks characterized by low-to-medium generalization difficulty, while automation may struggle to match the economic utility of human skills in more complex scenarios. Critically, our simulations highlight that, when high level of generalization is required and the cost of errors is high, combining human and machine skills can be the most effective strategy, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine policy is severely penalized by the inherent costs of its dual skill structure, causing it to destroy value and becoming the worst choice from an economic perspective. The takeaway for decision-makers is unambiguous: in complex and critical contexts, simply allocating human and machine skills to a task may be insufficient, and a human-machine skill policy is neither a silver-bullet solution nor a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
[29]
arXiv:2409.10096
(replaced)
[pdf, html, other]
Title:
Robust Reinforcement Learning with Dynamic Distortion Risk Measures
Anthony Coache, Sebastian Jaimungal
Comments:
27 pages, 3 figures
Subjects:
Machine Learning (cs.LG); Computational Finance (q-fin.CP); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM); Machine Learning (stat.ML)
In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.
[30]
arXiv:2509.05760
(replaced)
[pdf, html, other]
Title:
Rethinking Beta: A Causal Take on CAPM
Naftali Cohen
Subjects:
Theoretical Economics (econ.TH); Pricing of Securities (q-fin.PR); Statistical Finance (q-fin.ST); Applications (stat.AP)
The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.
Total of 30 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack