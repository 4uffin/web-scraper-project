Quantitative Finance
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-fin
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Finance
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Wednesday, 24 September 2025
Total of 14 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 5 of 5 entries)
[1]
arXiv:2509.18099
[pdf, html, other]
Title:
Path-dependent, ESG-valued, option pricing in the Bachelier-Black-Scholes-Merton model
Bhathiya Divelgama, Nancy Asare Nyarko, W. Brent Lindquist, Svetlozar T. Rachev, Blessing Omotade
Subjects:
Pricing of Securities (q-fin.PR)
We extend the application of the Cherny-Shiryaev-Yor invariance principle to a unified Bachelier-Black-Scholes-Merton (BBSM) dynamic pricing model. This extension incorporates the influence of the history of the dynamics (i.e., the path dynamics) of a market index on stock price changes. We add an ESG rating component to the price of the risky asset (stock), in such a manner that the impact of the ESG rating on the stock valuation can be explored through variation in the value of a single parameter. We develop discrete, binary tree, option pricing under this extended model. Using an empirical data set of 10 stocks chosen from the Nasdaq-100, we fit the model to stock price changes and compare model-based and published European call option prices.
[2]
arXiv:2509.18468
[pdf, html, other]
Title:
The Role of Informal Care in Cognitive Outcome and Healthcare Utilization Among Older Adults with Dementia
Mohammad Abdullah Al Faisal
Subjects:
General Economics (econ.GN)
This paper examines the relationship between informal caregiving and both cognitive functioning and healthcare utilization among older adults with dementia. Using data from the RAND version of the Health and Retirement Study (HRS), a nationally representative longitudinal panel of U.S. adults over age 50, covering the years 2010 to 2022, I estimate Ordinary Least Squares (OLS) and Instrumental Variables (IV) models to address potential endogeneity in caregiving decisions. The number of children is employed as an instrument for informal care intensity. While OLS estimates suggest a negative association between informal caregiving and cognition, IV estimates show no significant causal effect after controlling for demographic, socioeconomic, and lagged cognition variables. In contrast, IV results indicate that informal care significantly reduces the likelihood of nursing home use, the number of institutional nights, and the probability of institutionalization. No robust causal effects are found for hospital use, doctor visits, or outpatient surgery, although there is some suggestive evidence of a complementary relationship between informal care and home health services. These findings highlight the role of informal caregiving in substituting for institutional care and underscore its importance in long-term care policy for dementia patients. Keywords: Informal Caregiving; Cognitive Decline; Instrumental Variables; Healthcare Utilization: Dementia Patients.
[3]
arXiv:2509.18820
[pdf, html, other]
Title:
Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market
Marcin Wątorek, Marija Bezbradica, Martin Crane, Jarosław Kwapień, Stanisław Drożdż
Subjects:
Statistical Finance (q-fin.ST); Computational Engineering, Finance, and Science (cs.CE); Econometrics (econ.EM); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)
Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
[4]
arXiv:2509.18837
[pdf, html, other]
Title:
Fair Volatility: A Framework for Reconceptualizing Financial Risk
Sergio Bianchi, Daniele Angelini
Comments:
16 figures, 25 pages, 5 tables
Subjects:
Mathematical Finance (q-fin.MF)
Volatility is the canonical measure of financial risk, a role largely inherited from Modern Portfolio Theory. Yet, its universality rests on restrictive efficiency assumptions that render volatility, at best, an incomplete proxy for true risk. This paper identifies three fundamental inconsistencies: (i) volatility is path-independent and blind to temporal dependence and non-stationarity; (ii) its relevance collapses in derivative-intensive strategies, where volatility often represents opportunity rather than risk; and (iii) it lacks an absolute benchmark, providing no guidance on what level of volatility is economically ``fair'' in efficient markets. To address these limitations, we propose a new paradigm that reconceptualizes risk in terms of predictability rather than variability. Building on a general class of stochastic processes, we derive an analytical link between volatility and the Hurst-Holder exponent within the Multifractional Process with Random Exponent (MPRE) framework. This relationship yields a formal definition of ``fair volatility'', namely the volatility implied under market efficiency, where prices follow semi-martingale dynamics. Extensive empirical analysis on global equity indices supports this framework, showing that deviations from fair volatility provide a tractable measure of market inefficiency, distinguishing between momentum-driven and mean-reverting regimes. Our results advance both the theoretical foundations and empirical assessment of financial risk, offering a definition of volatility that is efficiency-consistent and economically interpretable.
[5]
arXiv:2509.19042
[pdf, other]
Title:
Predicting Credit Spreads and Ratings with Machine Learning: The Role of Non-Financial Data
Yanran Wu, Xinlei Zhang, Quanyi Xu, Qianxin Yang, Chao Zhang
Subjects:
General Economics (econ.GN)
We build a 167-indicator comprehensive credit risk indicator set, integrating macro, corporate financial, bond-specific indicators, and for the first time, 30 large-scale corporate non-financial indicators. We use seven machine learning models to construct a bond credit spread prediction model, test their spread predictive power and economic mechanisms, and verify their credit rating prediction effectiveness. Results show these models outperform Chinese credit rating agencies in explaining credit spreads. Specially, adding non-financial indicators more than doubles their out-of-sample performance vs. traditional feature-driven models. Mechanism analysis finds non-financial indicators far more important than traditional ones (macro-level, financial, bond features)-seven of the top 10 are non-financial (e.g., corporate governance, property rights nature, information disclosure evaluation), the most stable predictors. Models identify high-risk traits (deteriorating operations, short-term debt, higher financing constraints) via these indicators for spread prediction and risk identification. Finally, we pioneer a credit rating model using predicted spreads (predicted implied rating model), with full/sub-industry models achieving over 75% accuracy, recall, F1. This paper provides valuable guidance for bond default early warning, credit rating, and financial stability.
Cross submissions (showing 4 of 4 entries)
[6]
arXiv:2509.18394
(cross-list from cs.CY)
[pdf, other]
Title:
An Artificial Intelligence Value at Risk Approach: Metrics and Models
Luis Enriquez Alvarez
Subjects:
Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Risk Management (q-fin.RM)
Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.
The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.
[7]
arXiv:2509.18614
(cross-list from quant-ph)
[pdf, html, other]
Title:
Connecting Quantum Computing with Classical Stochastic Simulation
Jose Blanchet, Mark S. Squillante, Mario Szegedy, Guanyang Wang
Comments:
15 pages, tutorial paper prepared for the 2025 Winter Simulation Conference
Subjects:
Quantum Physics (quant-ph); Numerical Analysis (math.NA); Computational Finance (q-fin.CP); Computation (stat.CO)
This tutorial paper introduces quantum approaches to Monte Carlo computation with applications in computational finance. We outline the basics of quantum computing using Grover's algorithm for unstructured search to build intuition. We then move slowly to amplitude estimation problems and applications to counting and Monte Carlo integration, again using Grover-type iterations. A hands-on Python/Qiskit implementation illustrates these concepts applied to finance. The paper concludes with a discussion on current challenges in scaling quantum simulation techniques.
[8]
arXiv:2509.18633
(cross-list from cs.AI)
[pdf, html, other]
Title:
Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents
Yara Mohajerani
Comments:
Submitted and accepted to Tackling Climate Change with Machine Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and documentation available at this https URL
Subjects:
Artificial Intelligence (cs.AI); Risk Management (q-fin.RM)
Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.
[9]
arXiv:2509.19151
(cross-list from math.PR)
[pdf, html, other]
Title:
Sharp Large Deviations and Gibbs Conditioning for Threshold Models in Portfolio Credit Risk
Fengnan Deng, Anand N. Vidyashankar, Jeffrey F. Collamore
Subjects:
Probability (math.PR); Statistics Theory (math.ST); Mathematical Finance (q-fin.MF); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM)
We obtain sharp large deviation estimates for exceedance probabilities in dependent triangular array threshold models with a diverging number of latent factors. The prefactors quantify how latent-factor dependence and tail geometry enter at leading order, yielding three regimes: Gaussian or exponential-power tails produce polylogarithmic refinements of the Bahadur-Rao $n^{-1/2}$ law; regularly varying tails yield index-driven polynomial scaling; and bounded-support (endpoint) cases lead to an $n^{-3/2}$ prefactor. We derive these results through Laplace-Olver asymptotics for exponential integrals and conditional Bahadur-Rao estimates for the triangular arrays. Using these estimates, we establish a Gibbs conditioning principle in total variation: conditioned on a large exceedance event, the default indicators become asymptotically i.i.d., and the loss-given-default distribution is exponentially tilted (with the boundary case handled by an endpoint analysis). As illustrations, we obtain second-order approximations for Value-at-Risk and Expected Shortfall, clarifying when portfolios operate in the genuine large-deviation regime. The results provide a transferable set of techniques-localization, curvature, and tilt identification-for sharp rare-event analysis in dependent threshold systems.
Replacement submissions (showing 5 of 5 entries)
[10]
arXiv:2407.12924
(replaced)
[pdf, html, other]
Title:
Concentration-Based Inference for Evaluating Horizontal Mergers
Paul S. Koh
Subjects:
General Economics (econ.GN)
Antitrust authorities routinely rely on market concentration measures to assess the potential adverse effects of mergers on consumer welfare. Using a first-order approximation argument with logit and CES demand, I derive the relationship between the welfare effect of a merger on consumer surplus and the change in the Herfindahl-Hirschman Index (HHI). My results suggest that merger harm is correlated with the merger-induced change in HHI, and the proportionality coefficient depends on the price responsiveness parameter, market size, and the distribution of market shares within and across the merging firms. I present numerical validation of my formula along with an empirical illustration.
[11]
arXiv:2508.17407
(replaced)
[pdf, html, other]
Title:
General Social Agents
Benjamin S. Manning, John J. Horton
Subjects:
General Economics (econ.GN)
Useful social science theories predict behavior across settings. However, applying a theory to make predictions in new settings is challenging: rarely can it be done without ad hoc modifications to account for setting-specific factors. We argue that AI agents put in simulations of those novel settings offer an alternative for applying theory, requiring minimal or no modifications. We present an approach for building such "general" agents that use theory-grounded natural language instructions, existing empirical data, and knowledge acquired by the underlying AI during training. To demonstrate the approach in settings where no data from that data-generating process exists--as is often the case in applied prediction problems--we design a heterogeneous population of 883,320 novel games. AI agents are constructed using human data from a small set of conceptually related but structurally distinct "seed" games. In preregistered experiments, on average, agents predict initial human play in a random sample of 1,500 games from the population better than (i) a cognitive hierarchy model, (ii) game-theoretic equilibria, and (iii) out-of-the-box agents. For a small set of separate novel games, these simulations predict responses from a new sample of human subjects better even than the most plausibly relevant published human data.
[12]
arXiv:2509.16334
(replaced)
[pdf, html, other]
Title:
Volatility Calibration via Automatic Local Regression
Ruozhong Yang, Hao Qin, Charlie Che, Liming Feng
Comments:
38 pages,30 figures
Subjects:
Computational Finance (q-fin.CP)
Managing exotic derivatives requires accurate mark-to-market pricing and stable Greeks for reliable hedging. The Local Volatility (LV) model distinguishes itself from other pricing models by its ability to match observable market prices across all strikes and maturities with high accuracy. However, LV calibration is fundamentally ill-posed: finite market observables must determine a continuously-defined surface with infinite local volatility parameters. This ill-posed nature often causes spiky LV surfaces that are particularly problematic for finite-difference-based valuation, and induces high-frequency oscillations in solutions, thus leading to unstable Greeks. To address this challenge, we propose a pre-calibration smoothing method that can be integrated seamlessly into any LV calibration workflow. Our method pre-processes market observables using local regression that automatically minimizes asymptotic conditional mean squared error to generate denoised inputs for subsequent LV calibration. Numerical experiments demonstrate that the proposed pre-calibration smoothing yields significantly smoother LV surfaces and greatly improves Greek stability for exotic options with negligible additional computational cost, while preserving the LV model's ability to fit market observables with high fidelity.
[13]
arXiv:2509.17919
(replaced)
[pdf, html, other]
Title:
Economic Complexity Alignment and Sustainable Development
Quinten De Wettinck, Karolien De Bruyne, Wouter Bam, César A. Hidalgo
Subjects:
General Economics (econ.GN)
Economic complexity has been linked to sustainability outcomes, such as income inequality and greenhouse gas emissions. Yet, it is unclear whether the pursuit of complex and/or related activities naturally aligns with these outcomes, or whether meeting sustainability goals requires policy interventions that pursue unrelated diversification. Here, we exploit multidimensional social and environmental sustainability indicators to quantify the alignment between a country's closest diversification opportunities and sustainability goals. We find that high- and upper-middle-income countries face significantly better environmentally aligned diversification opportunities than poorer economies. This means that, while richer countries enjoy diversification opportunities that align complexity, relatedness and environmental performance, this alignment is weaker for developing economies. These findings underscore the value of evaluating future diversification trajectories through a multidimensional sustainability framework, and emphasise the strategic relevance of unrelated diversification for less developed economies to foster sustainable development.
[14]
arXiv:2506.17720
(replaced)
[pdf, html, other]
Title:
Wealth Thermalization Hypothesis and Social Networks
Klaus M. Frahm, Dima L. Shepelyansky
Comments:
72 pages, 49 figures: The replacement is an extension of the first version which is now "part I" and Appendix A (which corresponds to Suppmat of the first version). Part II and Appendix B are new and correspond to an extension to social networks
Subjects:
Statistical Mechanics (cond-mat.stat-mech); Statistical Finance (q-fin.ST)
In 1955 Fermi, Pasta, Ulam and Tsingou performed first numerical studies with the aim to obtain the thermalization in a chain of nonlinear oscillators from dynamical equations of motion. This model happend to have several specific features and the dynamical thermalization was established only later in other studies. In this work we study more generic models based on Random Matrix Theory and social networks with a nonlinear perturbation leading to dynamical thermalization above a certain chaos border. These systems have two integrals of motion being total energy and norm so that the theoretical Rayleigh-Jeans thermal distribution depends on temperature and chemical potential. We introduce the wealth thermalization hypothesis according to which the society wealth is associated with energy in the Rayleigh-Jeans distribution. At relatively small values of total wealth or energy there is a formation of the Rayleigh-Jeans condensate, well studied in physical systems such as multimode optical fibers. This condensation leads to a huge fraction of poor households at low wealth and a small oligarchic fraction which monopolizes a dominant fraction of total wealth thus generating a strong inequality in human society. We show that this thermalization gives a good description of real data of Lorenz curves of US, UK, the whole world and capitalization of companies at Stock Exchange of New York SE (NYSE), London and Hong Kong. It is also shown that above a chaos border the dynamical Rayleigh-Jeans thermalization takes place also in social networks with the Lorenz curves being similar to those of wealth distribution in world countries. Possible actions for inequality reduction are briefly discussed.
Total of 14 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack