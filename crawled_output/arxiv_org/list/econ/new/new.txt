Economics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
econ
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Economics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 23 September 2025
Total of 36 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 7 of 7 entries)
[1]
arXiv:2509.16396
[pdf, other]
Title:
Bundling against Learning
Agathe Pernoud, Frank Yang
Comments:
125 pages, 10 figures
Subjects:
Theoretical Economics (econ.TH)
A monopolist sells multiple goods to an uninformed buyer. The buyer chooses to learn any one-dimensional linear signal of their values for the goods, anticipating the seller's mechanism. The seller designs an optimal mechanism, anticipating the buyer's learning choice. In a generalized Gaussian environment, we show that every equilibrium has vertical learning where the buyer's posterior means are comonotonic, and every equilibrium is outcome-equivalent to nested bundling where the seller offers a menu of nested bundles. In equilibrium, the buyer learns more about a higher-tier good, resulting in a higher posterior variance on the log scale.
[2]
arXiv:2509.16734
[pdf, html, other]
Title:
Multigenerational Inequality
Jan Stuhler
Comments:
This paper has previously been published in the Research Handbook on Intergenerational Inequality, Edward Elgar Publishing Ltd, this https URL
Journal-ref:
Research Handbook on Intergenerational Inequality, Edward Elgar Publishing Ltd (2024)
Subjects:
General Economics (econ.GN)
A growing literature provides evidence on multigenerational inequality -- the extent to which socio-economic advantages persist across three or more generations. This chapter reviews its main findings and implications. Most studies find that inequality is more persistent than a naive iteration of conventional parent-child correlations would suggest. We discuss potential interpretations of this new ``fact'' related to (i) latent, (ii) non-Markovian or (iii) non-linear transmission processes, empirical strategies to discriminate between them, and the link between multigenerational and assortative associations.
[3]
arXiv:2509.17225
[pdf, html, other]
Title:
Mean-tail Gini framework for optimal portfolio selection
Jinghui Chen, Edward Furman, Stephano Ricci, Judeto Shanthirajah
Comments:
26 pages, 2 figures and 7 tables
Subjects:
Theoretical Economics (econ.TH)
The limitations of the traditional mean-variance (MV) efficient frontier, as introduced by Markowitz (1952), have been extensively documented in the literature. Specifically, the assumptions of normally distributed returns or quadratic investor preferences are often unrealistic in practice. Moreover, variance is not always an appropriate risk measure, particularly for heavy-tailed and highly volatile distributions, such as those observed in insurance claims and cryptocurrency markets, which may exhibit infinite variance. To address these issues, Shalit and Yitzhaki (2005) proposed a mean-Gini (MG) framework for portfolio selection, which requires only finite first moments and accommodates non-normal return distributions.
However, downside risk measures - such as tail variance - are generally considered more appropriate for capturing risk managers' risk preference than symmetric measures like variance or Gini. In response, we introduce a novel portfolio optimization framework based on a downside risk metric: the tail Gini. In the first part of the paper, we develop the mean-tail Gini (MTG) efficient frontier. Under the assumption of left-tail exchangeability, we derive closed-form solutions for the optimal portfolio weights corresponding to given expected returns. In the second part, we conduct an empirical study of the mean-tail variance (MTV) and MTG frontiers using data from equity and cryptocurrency markets. By fitting the empirical data to a generalized Pareto distribution, the estimated tail indices provide evidence of infinite-variance distributions in the cryptocurrency market. Additionally, the MTG approach demonstrates superior performance over MTV strategy by mitigating the amplification distortions induced by $\mathrm{L}^2$-norm risk measures. The MTG framework helps avoid overly aggressive investment strategies, thereby reducing exposure to unforeseen losses.
[4]
arXiv:2509.17248
[pdf, html, other]
Title:
Stochastic Non-Tâtonnement Processes and the Attraction Principle
Leandro Lyra Braga Dognini
Subjects:
Theoretical Economics (econ.TH)
I characterize stochastic non-tâtonnement processes (SNTP) and argue that they are a natural outcome of General Equilibrium Theory. To do so, I revisit the classical demand theory to define a normalized Walrasian demand and a diffeomorphism that flattens indifference curves. These diffeomorphisms are applied on the three canonical manifolds in the consumption domain (i.e., the indifference and the offer hypersurfaces and the trade hyperplane) to analyze their images in the normalized and the flat domains. In addition, relations to the set of Pareto optimal allocations on Arrow-Debreu and overlapping generations economies are discussed. Then, I derive, for arbitrary non-tâtonnement processes, an Attraction Principle based on the dynamics of marginal substitution rates seen in the "floor" of the flat domain. This motivates the definition of SNTP and, specifically, of Bayesian ones (BSNTP). When all utility functions are attractive and sharp, these BSNTP are particularly well behaved and lead directly to the calculation of stochastic trade outcomes over the contract curve, which are used to model price stickiness and markets' responses to sustained economic disequilibrium, and to prove a stochastic version of the First Welfare Theorem.
[5]
arXiv:2509.17303
[pdf, html, other]
Title:
Trade, Political Distance and the World Trade Organization
Samuel Hardwick
Comments:
36 pages, 5 figures
Subjects:
General Economics (econ.GN)
Trade agreements are often understood as shielding commerce from fluctuations in political relations. This paper provides evidence that World Trade Organization membership reduces the penalty of political distance on trade at the extensive margin. Using a structural gravity framework covering 1948 to 2023 and two measures of political distance, based on high-frequency events data and UN General Assembly votes, GATT/WTO status is consistently associated with a wider range of products traded between politically distant partners. The association is strongest in the early WTO years (1995 to 2008). Events-based estimates also suggest attenuation at the intensive margin, while UN vote-based estimates do not. Across all specifications, GATT/WTO membership increases aggregate trade volumes. The results indicate that a function of the multilateral trading system has been to foster new trade links across political divides, while raising trade volumes among both close and distant partners.
[6]
arXiv:2509.17919
[pdf, html, other]
Title:
Economic Complexity Alignment and Sustainable Development
Quinten De Wettinck, Karolien De Bruyne, Wouter Bam, César A. Hidalgo
Subjects:
General Economics (econ.GN)
Economic complexity has been linked to sustainability outcomes, such as income inequality and greenhouse gas emissions. Yet, it is unclear whether the pursuit of complex and/or related activities naturally aligns with these outcomes, or whether meeting sustainability goals requires policy interventions that pursue unrelated diversification. Here, we exploit multidimensional social and environmental sustainability indicators to quantify the alignment between a country's closest diversification opportunities and sustainability goals. We find that high- and upper-middle-income countries face significantly better environmentally aligned diversification opportunities than poorer economies. This means that, while richer countries enjoy diversification opportunities that align complexity, relatedness and environmental performance, this alignment is weaker for developing economies. These findings underscore the value of evaluating future diversification trajectories through a multidimensional sustainability framework, and emphasise the strategic relevance of unrelated diversification for less developed economies to foster sustainable development.
[7]
arXiv:2509.17949
[pdf, other]
Title:
Local Projections Bootstrap Inference
María Dolores Gadea, Òscar Jordà
Subjects:
Econometrics (econ.EM)
Bootstrap procedures for local projections typically rely on assuming that the data generating process (DGP) is a finite order vector autoregression (VAR), often taken to be that implied by the local projection at horizon 1. Although convenient, it is well documented that a VAR can be a poor approximation to impulse dynamics at horizons beyond its lag length. In this paper we assume instead that the precise form of the parametric model generating the data is not known. If one is willing to assume that the DGP is perhaps an infinite order process, a larger class of models can be accommodated and more tailored bootstrap procedures can be constructed. Using the moving average representation of the data, we construct appropriate bootstrap procedures.
Cross submissions (showing 8 of 8 entries)
[8]
arXiv:2509.16450
(cross-list from cs.GT)
[pdf, html, other]
Title:
On the Existence and Complexity of Core-Stable Data Exchanges
Jiaxin Song, Pooja Kulkarni, Parnian Shahkar, Bhaskar Ray Chaudhury
Comments:
28 pages, 5 figures, accepted by NeurIPS'25
Subjects:
Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)
The rapid growth of data-driven technologies and the emergence of various data-sharing paradigms have underscored the need for efficient and stable data exchange protocols. In any such exchange, agents must carefully balance the benefit of acquiring valuable data against the cost of sharing their own. Ensuring stability in these exchanges is essential to prevent agents -- or groups of agents -- from departing and conducting local (and potentially more favorable) exchanges among themselves. To address this, we study a model where agents participate in a data exchange. Each agent has an associated payoff for the data acquired from other agents and a cost incurred during sharing its own data. The net utility of an agent is payoff minus the cost. We adapt the classical notion of core-stability from cooperative game theory to data exchange. A data exchange is core-stable if no subset of agents has any incentive to deviate to a different exchange. We show that a core-stable data exchange is guaranteed to exist when agents have concave payoff functions and convex cost functions -- a setting typical in domains like PAC learning and random discovery models. We show that relaxing either of the foregoing conditions may result in the nonexistence of core-stable data exchanges. Then, we prove that finding a core-stable exchange is PPAD-hard, even when the potential blocking coalitions are restricted to constant size. To the best of our knowledge, this provides the first known PPAD-hardness result for core-like guarantees in data economics. Finally, we show that data exchange can be modelled as a balanced $n$-person game. This immediately gives a pivoting algorithm via Scarf's theorem \cite{Scarf1967core}. We show that the pivoting algorithm works well in practice through our empirical results.
[9]
arXiv:2509.16655
(cross-list from cs.SE)
[pdf, html, other]
Title:
Incentives and Outcomes in Bug Bounties
Serena Wang, Martino Banchio, Krzysztof Kotowicz, Katrina Ligett, R. Preston McAfee, Eduardo' Vela'' Nava
Subjects:
Software Engineering (cs.SE); Cryptography and Security (cs.CR); General Economics (econ.GN)
Bug bounty programs have contributed significantly to security in technology firms in the last decade, but little is known about the role of reward incentives in producing useful outcomes. We analyze incentives and outcomes in Google's Vulnerability Rewards Program (VRP), one of the world's largest bug bounty programs. We analyze the responsiveness of the quality and quantity of bugs received to changes in payments, focusing on a change in Google's reward amounts posted in July, 2024, in which reward amounts increased by up to 200% for the highest impact tier. Our empirical results show an increase in the volume of high-value bugs received after the reward increase, for which we also compute elasticities. We further break down the sources of this increase between veteran researchers and new researchers, showing that the reward increase both redirected the attention of veteran researchers and attracted new top security researchers into the program.
[10]
arXiv:2509.16925
(cross-list from cs.CY)
[pdf, other]
Title:
Tenure Under Pressure: Simulating the Disruptive Effects of AI on Academic Publishing
Shan Jiang
Subjects:
Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); General Economics (econ.GN)
Generative artificial intelligence (AI) has begun to reshape academic publishing by enabling the rapid production of submission-ready manuscripts. While such tools promise to enhance productivity, they also raise concerns about overwhelming journal systems that have fixed acceptance capacities. This paper uses simulation modeling to investigate how AI-driven surges in submissions may affect desk rejection rates, review cycles, and faculty publication portfolios, with a focus on business school journals and tenure processes. Three scenarios are analyzed: a baseline model, an Early Adopter model where a subset of faculty boosts productivity, and an AI Abuse model where submissions rise exponentially. Results indicate that early adopters initially benefit, but overall acceptance rates fall sharply as load increases, with tenure-track faculty facing disproportionately negative outcomes. The study contributes by demonstrating the structural vulnerabilities of the current publication system and highlights the need for institutional reform in personnel evaluation and research dissemination practices.
[11]
arXiv:2509.17103
(cross-list from math.OC)
[pdf, html, other]
Title:
Clarke Differentials and the Envelope Theorem in Dynamic Programming
Yuhki Hosoya
Subjects:
Optimization and Control (math.OC); Theoretical Economics (econ.TH)
In this paper, we consider a deterministic dynamic programming model, and derive the envelope theorem using the Clarke differential. Compared with past research, the requirements for our result are weaker, and do not include differentiability, convexity, and boundedness.
[12]
arXiv:2509.17147
(cross-list from nlin.AO)
[pdf, html, other]
Title:
A game played by tandem-running ants: Hint of procedural rationality
Joy Das Bairagya, Udipta Chakraborti, Sumana Annagiri, Sagar Chakraborty
Subjects:
Adaptation and Self-Organizing Systems (nlin.AO); Theoretical Economics (econ.TH); Biological Physics (physics.bio-ph); Populations and Evolution (q-bio.PE)
Navigation through narrow passages during colony relocation by the tandem-running ants, $\textit{Diacamma}$ $\textit{indicum}$, is a tour de force of biological traffic coordination. Even on one-lane paths, the ants tactfully manage a bidirectional flow: Informed individuals (termed leaders) guide nest-mates (termed followers) from a suboptimal nest to a new optimal nest, and then return to recruit additional followers. We propose that encounters between the ants moving in opposite directions can be modelled within the framework of game theory leading to an understanding of the mechanism behind observed behaviours. Our experiments reveal that, upon encountering a tandem pair (a leader and its follower) on a narrow path, the returning leader reverses her direction and proceeds toward the new nest again as if she becomes the leader guiding a follower. This observed behaviour is consistent with game-theoretic predictions, provided the assumption of perfect rationality is relaxed in favour of bounded rationality -- specifically, procedural rationality. In other words, the experimental outcomes are consistent with sampling equilibrium but not with Nash equilibrium. Our work, which strives to induct the essence of behavioural game theory into the world of ants, is first ever report of realizing sampling equilibrium in scenarios not involving human players.
[13]
arXiv:2509.17180
(cross-list from cs.LG)
[pdf, html, other]
Title:
Regularizing Extrapolation in Causal Inference
David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, Avi Feller
Subjects:
Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)
Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.
[14]
arXiv:2509.17385
(cross-list from stat.ME)
[pdf, html, other]
Title:
Bayesian Semi-supervised Inference via a Debiased Modeling Approach
Gözde Sert, Abhishek Chakrabortty, Anirban Bhattacharya
Comments:
60 pages (including supplement); to appear in Econometrics and Statistics
Journal-ref:
Econometrics and Statistics (2025)
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)
Inference in semi-supervised (SS) settings has gained substantial attention in recent years due to increased relevance in modern big-data problems. In a typical SS setting, there is a much larger-sized unlabeled data, containing only observations of predictors, and a moderately sized labeled data containing observations for both an outcome and the set of predictors. Such data naturally arises when the outcome, unlike the predictors, is costly or difficult to obtain. One of the primary statistical objectives in SS settings is to explore whether parameter estimation can be improved by exploiting the unlabeled data. We propose a novel Bayesian method for estimating the population mean in SS settings. The approach yields estimators that are both efficient and optimal for estimation and inference. The method itself has several interesting artifacts. The central idea behind the method is to model certain summary statistics of the data in a targeted manner, rather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying appropriate summary statistics crucially relies on a debiased representation of the population mean that incorporates unlabeled data through a flexible nuisance function while also learning its estimation bias. Combined with careful usage of sample splitting, this debiasing approach mitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from the posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete theoretical results, via Bernstein--von Mises theorems, are established, validating all claims, and are further supported through extensive numerical studies. To our knowledge, this is possibly the first work on Bayesian inference in SS settings, and its central ideas also apply more broadly to other Bayesian semi-parametric inference problems.
[15]
arXiv:2509.18047
(cross-list from stat.ML)
[pdf, html, other]
Title:
Functional effects models: Accounting for preference heterogeneity in panel data with machine learning
Nicolas Salvadé, Tim Hillel
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)
In this paper, we present a general specification for Functional Effects Models, which use Machine Learning (ML) methodologies to learn individual-specific preference parameters from socio-demographic characteristics, therefore accounting for inter-individual heterogeneity in panel choice data. We identify three specific advantages of the Functional Effects Model over traditional fixed, and random/mixed effects models: (i) by mapping individual-specific effects as a function of socio-demographic variables, we can account for these effects when forecasting choices of previously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of functional effects avoids the incidental parameters problem of the fixed effects model, even when the number of observed choices per individual is small; and (iii) we do not rely on the strong distributional assumptions of the random effects model, which may not match reality. We learn functional intercept and functional slopes with powerful non-linear machine learning regressors for tabular data, namely gradient boosting decision trees and deep neural networks. We validate our proposed methodology on a synthetic experiment and three real-world panel case studies, demonstrating that the Functional Effects Model: (i) can identify the true values of individual-specific effects when the data generation process is known; (ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual heterogeneity in terms of predictive performance, as well as traditional static panel choice models in terms of learning inter-individual heterogeneity. The results indicate that the FI-RUMBoost model, which combines the individual-specific constants of the Functional Effects Model with the complex, non-linear utilities of RUMBoost, performs marginally best on large-scale revealed preference panel data.
Replacement submissions (showing 21 of 21 entries)
[16]
arXiv:2107.09235
(replaced)
[pdf, html, other]
Title:
Distributional Effects with Two-Sided Measurement Error: An Application to Intergenerational Income Mobility
Brantly Callaway, Tong Li, Irina Murtazashvili, Emmanuel Tsyawo
Subjects:
Econometrics (econ.EM)
This paper considers identification and estimation of distributional effect parameters that depend on the joint distribution of an outcome and another variable of interest ("treatment") in a setting with "two-sided" measurement error -- that is, where both variables are possibly measured with error. Examples of these parameters in the context of intergenerational income mobility include transition matrices, rank-rank correlations, and the poverty rate of children as a function of their parents' income, among others. Building on recent work on quantile regression (QR) with measurement error in the outcome (particularly, Hausman, Liu, Luo, and Palmer (2021)), we show that, given (i) two linear QR models separately for the outcome and treatment conditional on other observed covariates and (ii) assumptions about the measurement error for each variable, one can recover the joint distribution of the outcome and the treatment. Besides these conditions, our approach does not require an instrument, repeated measurements, or distributional assumptions about the measurement error. Using recent data from the 1997 National Longitudinal Study of Youth, we find that accounting for measurement error notably reduces several estimates of intergenerational mobility parameters.
[17]
arXiv:2303.12667
(replaced)
[pdf, other]
Title:
Don't (fully) exclude me, it's not necessary! Causal inference with semi-IVs
Christophe Bruneel-Zupanc
Subjects:
Econometrics (econ.EM)
This paper proposes semi-instrumental variables (semi-IVs) as an alternative to instrumental variables (IVs) to identify the causal effect of a binary (or discrete) endogenous treatment. A semi-IV is a less restrictive form of instrument: it affects the selection into treatment but is excluded only from one, not necessarily both, potential outcomes. Having two continuously distributed semi-IVs, one excluded from the potential outcome under treatment and the other from the potential outcome under control, is sufficient to nonparametrically point identify marginal treatment effect (MTE) and local average treatment effect (LATE) parameters. In practice, semi-IVs provide a solution to the challenge of finding valid IVs because they are often easier to find: many selection-specific shocks, policies, prices, costs, or benefits are valid semi-IVs. As an application, I estimate the returns to working in the manufacturing sector on earnings using sector-specific characteristics as semi-IVs.
[18]
arXiv:2303.14486
(replaced)
[pdf, html, other]
Title:
Exposure to World War II and Its Labor Market Consequences over the Life Cycle
Sebastian T. Braun, Jan Stuhler
Comments:
JEL Code: J24, J26, N34, Keywords: World War II; labor market careers; war injuries; prisoners of war, displacement; life-cycle models
Subjects:
General Economics (econ.GN)
With 70 million dead, World War II remains the most devastating conflict in history. Among the survivors, millions were displaced, returned maimed from the battlefield, or endured years of captivity. We examine the effects of such war exposures on labor market careers, showing that they often become apparent only at certain life stages. While war injuries reduced employment in old age, former prisoners of war prolonged their time in the workforce before retiring. Many displaced workers, especially women, never returned to employment. These responses align with standard life-cycle theory and thus likely hold relevance for other conflicts.
[19]
arXiv:2306.07619
(replaced)
[pdf, html, other]
Title:
Kernel Choice Matters for Local Polynomial Density Estimators at Boundaries
Shunsuke Imai, Yuta Okamoto
Subjects:
Econometrics (econ.EM)
This paper examines kernel selection for local polynomial density (LPD) estimators at boundary points. Contrary to conventional wisdom, we demonstrate that the choice of kernel has a substantial impact on the efficiency of LPD estimators. In particular, we provide theoretical results and present simulation and empirical evidence showing that commonly used kernels, such as the triangular kernel, suffer from several efficiency issues: They yield a larger mean squared error than our preferred Laplace kernel. For inference, the efficiency loss is even more pronounced, with confidence intervals based on popular kernels being wide, whereas those based on the Laplace kernel are markedly tighter. Furthermore, the variance of the LPD estimator with such popular kernels explodes as the sample size decreases, reflecting the fact -- formally proven here -- that its finite-sample variance is infinite. This small-sample problem, however, can be avoided by employing kernels with unbounded support. Taken together, both asymptotic and finite-sample analyses justify the use of the Laplace kernel: Simply changing the kernel function improves the reliability of LPD estimation and inference, and its effect is numerically significant.
[20]
arXiv:2309.06753
(replaced)
[pdf, other]
Title:
A Reexamination of Proof Approaches for the Impossibility Theorem
Kazuya Yamamoto
Subjects:
Theoretical Economics (econ.TH)
Revised proofs of Kenneth Arrow's impossibility theorem have been presented in prose form, incorporating novel ideas such as decisive sets and pivotal voters. This study develops another approach to proving the theorem. Using a proof calculus in formal logic, we construct a proof with a full mathematical representation. While previous proofs emphasize intuitive accessibility, this one focuses on meticulous derivation and reveals the global structure of the social welfare function central to the theorem.
[21]
arXiv:2310.18563
(replaced)
[pdf, html, other]
Title:
Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects
Tymon Słoczyński, S. Derya Uysal, Jeffrey M. Wooldridge
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
How should researchers adjust for covariates? We show that if the propensity score is estimated using a specific covariate balancing approach, inverse probability weighting (IPW), augmented inverse probability weighting (AIPW), and inverse probability weighted regression adjustment (IPWRA) estimators are numerically equivalent for the average treatment effect (ATE), and likewise for the average treatment effect on the treated (ATT). The resulting weights are inherently normalized, making normalized and unnormalized IPW and AIPW identical. We discuss implications for instrumental variables and difference-in-differences estimators and illustrate with two applications how these numerical equivalences simplify analysis and interpretation.
[22]
arXiv:2401.08064
(replaced)
[pdf, other]
Title:
A mechanistic model of trust based on neural information processing
Scott E. Allen, René F. Kizilcec, A. David Redish
Subjects:
General Economics (econ.GN); Human-Computer Interaction (cs.HC); Neurons and Cognition (q-bio.NC)
Trust is central to human social interactions, manifesting in actions that make one vulnerable to another. We argue that trust will thus depend on the decision-making processes that arise in neural systems. Building on advances in the cognitive neuroscience of decision making, we propose a mechanistic model of trust arising from multiple parallel systems that perform distinct, complementary information processing. Because each system learns via different mechanisms, trust can be created (or destroyed) in multiple ways. This systems-level taxonomy of information representations provides a principled basis for differentiating forms of trust, linking them to specific learning processes, and generating testable predictions about their expression in behavior. By situating trust within a broader theory of neural decision systems, our account unifies diverse findings across psychology, neuroscience, and the social sciences, and offers a foundation for explaining how humans develop, maintain, and repair trust in a complex social world.
[23]
arXiv:2406.13122
(replaced)
[pdf, html, other]
Title:
Testing for Underpowered Literatures
Stefan Faridani
Subjects:
Econometrics (econ.EM)
How many experimental studies would have come to different conclusions had they been run on larger samples? I show how to estimate the expected number of statistically significant results that a set of experiments would have reported had their sample sizes all been counterfactually increased. The proposed deconvolution estimator is asymptotically normal and adjusts for publication bias. Unlike related methods, this approach requires no assumptions of any kind about the distribution of true intervention treatment effects and allows for point masses. Simulations find good coverage even when the t-score is only approximately normal. An application to randomized trials (RCTs) published in economics journals finds that doubling every sample would increase the power of t-tests by 7.2 percentage points on average. This effect is smaller than for non-RCTs and comparable to systematic replications in laboratory psychology where previous studies enabled more accurate power calculations. This suggests that RCTs are on average relatively insensitive to sample size increases. Research funders who wish to raise power should generally consider sponsoring better-measured and higher quality experiments -- rather than only larger ones.
[24]
arXiv:2407.12924
(replaced)
[pdf, html, other]
Title:
Concentration-Based Inference for Evaluating Horizontal Mergers
Paul S. Koh
Subjects:
General Economics (econ.GN)
Antitrust authorities routinely rely on market concentration measures to assess the potential adverse effects of mergers on consumer welfare. Using a first-order approximation argument with logit and CES demand, I derive the relationship between the welfare effect of a merger on consumer surplus and the change in the Herfindahl-Hirschman Index (HHI). My results suggest that merger harm is correlated with the merger-induced change in HHI, and the proportionality coefficient depends on the price responsiveness parameter, market size, and the distribution of market shares within and across the merging firms. I present numerical validation of my formula along with an empirical illustration.
[25]
arXiv:2410.17154
(replaced)
[pdf, other]
Title:
Estimating Spillovers from Sampled Connections
Kieran Marray
Subjects:
General Economics (econ.GN)
Empirical researchers often estimate spillover effects by fitting linear or non-linear regression models to sampled network data. We show that common sampling schemes bias these estimates, potentially upwards, and derive biased-corrected estimators that researchers can construct from aggregate network statistics. Our results apply under different assumptions on the relationship between observed and unobserved links, allow researchers to bound true effect sizes, and to determine robustness to mismeasured links. As an application, we estimate the propagation of climate shocks between U.S. public firms from self-reported supply links, building a new dataset of county-level incidence of large climate shocks.
[26]
arXiv:2410.18272
(replaced)
[pdf, html, other]
Title:
Partially Identified Rankings from Pairwise Interactions
Federico Crippa, Danil Fedchenko
Subjects:
Econometrics (econ.EM)
This paper considers the problem of ranking objects based on their latent merits using data from pairwise interactions. We allow for incomplete observation of these interactions and study what can be inferred about rankings in such settings. First, we show that identification of the ranking depends on a trade-off between the tournament graph and the interaction function: in parametric models, such as the Bradley-Terry-Luce, rankings are point identified even with sparse graphs, whereas nonparametric models require dense graphs. Second, moving beyond point identification, we characterize the identified set in the nonparametric model under any tournament structure and represent it through moment inequalities. Finally, we propose a likelihood-based statistic to test whether a ranking belongs to the identified set. We study two testing procedures: one is finite-sample valid but computationally intensive; the other is easy to implement and valid asymptotically. We illustrate our results using Brazilian employer-employee data to study how workers rank firms when moving across jobs.
[27]
arXiv:2501.17455
(replaced)
[pdf, html, other]
Title:
Uniform Confidence Band for Marginal Treatment Effect Function
Toshiki Tsuda, Yanchun Jin, Ryo Okui
Subjects:
Econometrics (econ.EM)
This paper presents a method for constructing uniform confidence bands for the marginal treatment effect (MTE) function. The shape of the MTE function offers insight into how the unobserved propensity to receive treatment is related to the treatment effect. Our approach visualizes the statistical uncertainty of an estimated function, facilitating inferences about the function's shape. The proposed method is computationally inexpensive and requires only minimal information: sample size, standard errors, kernel function, and bandwidth. This minimal data requirement enables applications to both new analyses and published results without access to original data. We derive a Gaussian approximation for a local quadratic estimator and consider the approximation of the distribution of its supremum in polynomial order. Monte Carlo simulations demonstrate that our bands provide the desired coverage and are less conservative than those based on the Gumbel approximation. An empirical illustration regarding the returns to education is included.
[28]
arXiv:2503.15443
(replaced)
[pdf, other]
Title:
Are Elites Meritocratic and Efficiency-Seeking? Evidence from MBA Students
Marcel Preuss, Germán Reyes, Jason Somerville, Joy Wu
Comments:
JEL codes: D63, C91, H23
Subjects:
General Economics (econ.GN)
Elites disproportionately influence policymaking, yet little is known about their fairness and efficiency preferences -- key determinants of support for redistributive policies. We investigate these preferences in an incentivized lab experiment with a group of future elites -- Ivy League MBA students. We find that MBA students implement substantially more unequal earnings distributions than the average American, regardless of whether inequality stems from luck or merit. Their redistributive choices are also highly responsive to efficiency costs, with an effect that is an order of magnitude larger than that found in representative U.S. samples. Analyzing fairness ideals, we find that MBA students are less likely to be strict meritocrats than the broader population. These findings provide novel insights into how elites' redistributive preferences may shape high levels of inequality in the U.S.
[29]
arXiv:2506.12976
(replaced)
[pdf, other]
Title:
Does Medicaid Expansion Lead to Income Adjustment? Evidence from the Survey of Income Program Participation
Mingjian Li
Comments:
43 pages
Subjects:
General Economics (econ.GN)
Using monthly Survey of Income Program Participation (SIPP) data and a regression discontinuity (RD) design at the 138 percent Federal Poverty Line (FPL) threshold, this paper shows that childless adults in Medicaid expansion states lowered their lowest monthly earnings by about 39 FPL percentage points, equal to 700 dollars in 2025 for a two person household, to qualify for Medicaid. The response was largest when the individual mandate penalty was in effect and declined after the penalty was reduced to zero in 2019. Evidence from zero earnings months and hours worked indicates adjustments along both extensive and intensive margins. These results validate lowest monthly earnings as a practical eligibility measure and provide new evidence of substantial labor supply responses to the Affordable Care Act Medicaid notch.
[30]
arXiv:2508.10585
(replaced)
[pdf, html, other]
Title:
Racial bias, colorism, and overcorrection
Kenneth Colombe, Alex Krumer, Rosa Lavelle-Hill, Tim Pawlowski
Subjects:
General Economics (econ.GN)
This paper examines whether increased awareness can affect racial bias and colorism. We exploit a natural experiment from the widespread publicity of Price and Wolfers (2010), which intensified scrutiny of racial bias in men's basketball officiating. We investigate refereeing decisions in the Women's National Basketball Association (WNBA), an organization with a long-standing commitment to diversity, equity, and inclusion (DEI). We apply machine learning techniques to predict player race and to measure skin tone. Our empirical strategy exploits the quasi-random assignment of referees to games, combined with high-dimensional fixed effects, to estimate the relationship between referee-player racial and skin tone compositions and foul-calling behavior. We find no racial bias before the intense media coverage. However, we find evidence of overcorrection, whereby a player receives fewer fouls when facing more referees from the opposite race and skin tone. This overcorrection wears off over time, returning to zero-bias levels. We highlight the need to consider baseline levels of bias before applying any prescription with direct relevance to policymakers and organizations given the recent discourse on DEI.
[31]
arXiv:2509.05760
(replaced)
[pdf, html, other]
Title:
Rethinking Beta: A Causal Take on CAPM
Naftali Cohen
Subjects:
Theoretical Economics (econ.TH); Pricing of Securities (q-fin.PR); Statistical Finance (q-fin.ST); Applications (stat.AP)
The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.
[32]
arXiv:2509.08472
(replaced)
[pdf, html, other]
Title:
On the Identification of Diagnostic Expectations: Econometric Insights from DSGE Models
Jinting Guo
Subjects:
Econometrics (econ.EM); Theoretical Economics (econ.TH)
This paper provides the first econometric evidence for diagnostic expectations (DE) in DSGE models. Using the identification framework of Qu and Tkachenko (2017), I show that DE generate dynamics unreplicable under rational expectations (RE), with no RE parameterization capable of matching the autocovariance implied by DE. Consequently, DE are not observationally equivalent to RE and constitute an endogenous source of macroeconomic fluctuations, distinct from both structural frictions and exogenous shocks. From an econometric perspective, DE preserve overall model identification but weaken the identification of shock variances. To ensure robust conclusions across estimation methods and equilibrium conditions, I extend Bayesian estimation with Sequential Monte Carlo sampling to the indeterminacy domain. These findings advance the econometric study of expectations and highlight the macroeconomic relevance of diagnostic beliefs.
[33]
arXiv:2509.14057
(replaced)
[pdf, html, other]
Title:
Machines are more productive than humans until they aren't, and vice versa
Riccardo Zanardelli
Comments:
Results enriched by experiment focusing on machine skill achieving high performance across all task difficulties; results of the primary experiment unchanged; data analysis section expanded; conclusions enriched and re-organized; abstract perfected; example in section A.4.1 enhanced; corrections to Table 17 (now Table 21); minor typos corrected
Subjects:
General Economics (econ.GN); Artificial Intelligence (cs.AI)
With the growth of artificial skills, organizations are increasingly confronting with the problem of optimizing skill policy decisions guided by economic principles. This paper addresses the underlying complexity of this challenge by developing an in-silico framework based on Monte Carlo simulations grounded in empirical realism to analyze the economic impact of human and machine skills, individually or jointly deployed in the execution of tasks presenting varying levels of complexity. Our results provide quantitative support for the established notions that automation tends to be the most economically-effective strategy for tasks characterized by low-to-medium generalization difficulty, while automation may struggle to match the economic utility of human skills in more complex scenarios. Critically, our simulations highlight that, when high level of generalization is required and the cost of errors is high, combining human and machine skills can be the most effective strategy, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine policy is severely penalized by the inherent costs of its dual skill structure, causing it to destroy value and becoming the worst choice from an economic perspective. The takeaway for decision-makers is unambiguous: in complex and critical contexts, simply allocating human and machine skills to a task may be insufficient, and a human-machine skill policy is neither a silver-bullet solution nor a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
[34]
arXiv:2501.09483
(replaced)
[pdf, html, other]
Title:
Semiparametrics via parametrics and contiguity
Adam Lee, Emil A. Stoltenberg, Per A. Mykland
Subjects:
Statistics Theory (math.ST); Econometrics (econ.EM); Methodology (stat.ME)
Inference on the parametric part of a semiparametric model is no trivial task. If one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model and inference may proceed by the method of maximum likelihood. Under regularity conditions, the ensuing maximum likelihood estimator is asymptotically normal and efficient in the approximating parametric model. Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting 'semiparametric' estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous: we move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the conventional sieve literature by being more specific about the approximating parametric models, by working not only with but also under these when treating the parametric models, and by taking full advantage of the mutual contiguity that we require between the parametric and semiparametric models. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.
[35]
arXiv:2504.09716
(replaced)
[pdf, html, other]
Title:
Dominated Actions in Imperfect-Information Games
Sam Ganzfried
Subjects:
Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Theoretical Economics (econ.TH)
Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.
[36]
arXiv:2507.12891
(replaced)
[pdf, html, other]
Title:
Refining the Notion of No Anticipation in Difference-in-Differences Studies
Marco Piccininni, Eric J. Tchetgen Tchetgen, Mats J. Stensrud
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
We address an ambiguity in identification strategies using difference-in-differences, which are widely applied in empirical research, particularly in economics. The assumption commonly referred to as the "no-anticipation assumption" states that treatment has no effect on outcomes before its implementation. However, because standard causal models rely on a temporal structure in which causes precede effects, such an assumption seems to be inherently satisfied. This raises the question of whether the assumption is repeatedly stated out of redundancy or because the formal statements fail to capture the intended subject-matter interpretation. We argue that confusion surrounding the no-anticipation assumption arises from ambiguity in the intervention considered and that current formulations of the assumption are ambiguous. Therefore, new definitions and identification results are proposed.
Total of 36 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack