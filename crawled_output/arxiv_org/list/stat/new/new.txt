Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Thursday, 25 September 2025
Total of 91 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 32 of 32 entries)
[1]
arXiv:2509.19404
[pdf, other]
Title:
Particle Filtering for Non-Deterministic Electrocardiographic Imaging
Emma Lagracie (UB), Luc de Montella
Subjects:
Methodology (stat.ME); Probability (math.PR); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)
Electrocardiographic imaging (ECGI) aims to non-invasively reconstruct activation maps of the heart from temporal body surface potentials. While most existing approaches rely on inverse and optimization techniques that may yield satisfactory reconstructions, they typically provide a single deterministic solution, overlooking the inherent uncertainty of the problem stemming from its very ill-posed nature, the poor knowledge of biophysical features and the unavoidable presence of noise in the measurements. The Bayesian framework, which naturally incorporates uncertainty while also accounting for temporal correlations across time steps, can be used to address this limitation. In this work, we propose a low-dimensional representation of the activation sequence that enables the use of particle filtering, a Bayesian filtering method that does not rely on predefined assumptions regarding the shape of the posterior distribution, in contrast to approaches like the Kalman filter. This allows to produce not only activation maps but also probabilistic maps indicating the likelihood of activation at each point on the heart over time, as well as pseudo-probability maps reflecting the likelihood of a point being part of an earliest activation site. Additionally, we introduce a method to estimate the probability of the presence of a conduction lines of block on the heart surface. Combined with classical reconstruction techniques, this could help discriminate artificial from true lines of block in activation maps. We support our approach with a numerical study based on simulated data, demonstrating the potential of our method.
[2]
arXiv:2509.19418
[pdf, html, other]
Title:
Forecasting High Dimensional Time Series with Dynamic Dimension Reduction
Daniel Peña, Victor J. Yohai
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Many dimension reduction techniques have been developed for independent data, and most have also been extended to time series. However, these methods often fail to account for the dynamic dependencies both within and across series. In this work, we propose a general framework for forecasting high-dimensional time series that integrates dynamic dimension reduction with regularization techniques. The effectiveness of the proposed approach is illustrated through a simulated example and a forecasting application using an economic dataset. We show that several specific methods are encompassed within this framework, including Dynamic Principal Components and Reduced Rank Autoregressive Models. Furthermore, time-domain formulations of Dynamic Canonical Correlation and Dynamic Redundancy Analysis are introduced here for the first time as particular instances of the proposed methodology. All of these techniques are analyzed as special cases of a unified procedure, enabling a coherent derivation and interpretation across methods.
[3]
arXiv:2509.19455
[pdf, html, other]
Title:
Anchored Langevin Algorithms
Mert Gurbuzbalaban, Hoang M. Nguyen, Xicheng Zhang, Lingjiong Zhu
Comments:
49 pages, 8 figures, 1 table
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR)
Standard first-order Langevin algorithms such as the unadjusted Langevin algorithm (ULA) are obtained by discretizing the Langevin diffusion and are widely used for sampling in machine learning because they scale to high dimensions and large datasets. However, they face two key limitations: (i) they require differentiable log-densities, excluding targets with non-differentiable components; and (ii) they generally fail to sample heavy-tailed targets. We propose anchored Langevin dynamics, a unified approach that accommodates non-differentiable targets and certain classes of heavy-tailed distributions. The method replaces the original potential with a smooth reference potential and modifies the Langevin diffusion via multiplicative scaling. We establish non-asymptotic guarantees in the 2-Wasserstein distance to the target distribution and provide an equivalent formulation derived via a random time change of the Langevin diffusion. We provide numerical experiments to illustrate the theory and practical performance of our proposed approach.
[4]
arXiv:2509.19461
[pdf, other]
Title:
Some Simplifications for the Expectation-Maximization (EM) Algorithm: The Linear Regression Model Case
Daniel A. Griffith
Comments:
23 pages, 7 tables, 1 figure, former Interstat (now defunct) publication
Subjects:
Methodology (stat.ME)
The EM algorithm is a generic tool that offers maximum likelihood solutions when datasets are incomplete with data values missing at random or completely at random. At least for its simplest form, the algorithm can be rewritten in terms of an ANCOVA regression specification. This formulation allows several analytical results to be derived that permit the EM algorithm solution to be expressed in terms of new observation predictions and their variances. Implementations can be made with a linear regression or a nonlinear regression model routine, allowing missing value imputations, even when they must satisfy constraints. Fourteen example datasets gleaned from the EM algorithm literature are reanalyzed. Imputation results have been verified with SAS PROC MI. Six theorems are proved that broadly contextualize imputation findings in terms of the theory, methodology, and practice of statistical science.
[5]
arXiv:2509.19490
[pdf, other]
Title:
Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning
Nathan Cheng, Asher Spector, Lucas Janson
Comments:
26+7+97 pages (main text, references, appendix), 6+15 figures (main text, appendix)
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
In regression and causal inference, controlled subgroup selection aims to identify, with inferential guarantees, a subgroup (defined as a subset of the covariate space) on which the average response or treatment effect is above a given threshold. E.g., in a clinical trial, it may be of interest to find a subgroup with a positive average treatment effect. However, existing methods either lack inferential guarantees, heavily restrict the search for the subgroup, or sacrifice efficiency by naive data splitting. We propose a novel framework called chiseling that allows the analyst to interactively refine and test a candidate subgroup by iteratively shrinking it. The sole restriction is that the shrinkage direction only depends on the points outside the current subgroup, but otherwise the analyst may leverage any prior information or machine learning algorithm. Despite this flexibility, chiseling controls the probability that the discovered subgroup is null (e.g., has a non-positive average treatment effect) under minimal assumptions: for example, in randomized experiments, this inferential validity guarantee holds under only bounded moment conditions. When applied to a variety of simulated datasets and a real survey experiment, chiseling identifies substantially better subgroups than existing methods with inferential guarantees.
[6]
arXiv:2509.19500
[pdf, html, other]
Title:
One Person, How Many Votes? Demographic Distortions in United States Elections
Lee Kennedy-Shaffer
Comments:
28 pages, 3 figures, 1 table
Subjects:
Applications (stat.AP)
Representative democracy in the United States relies on election systems that transmit votes into representatives in three key bodies: the two chambers of the federal legislature (House of Representatives and Senate) and the Electoral College, which selects the President and Vice-President. This happens through a process of re-weighting based on geographic units (congressional districts and states) that can introduce substantial distortion. In this paper, I propose quantitative measures of this distortion that can be applied to demographic groups, using Census data, to assess and visualize these distortive effects. These include the absolute weight of votes under these systems and the excess population represented in the bodies through the distortions. Visualizing these metrics from 2000 -- 2020 shows persistent malapportionment in key demographic categories. White (non-Hispanic) residents, residents of rural areas, and owner-occupied households are overrepresented in the Senate and Electoral College; Black and Hispanic people, urban dwellers, and renter-occupied households are underrepresented. For urban residents, this underrepresentation is the equivalent of 25 million fewer residents in the Senate and nearly 5 million in the Electoral College. I discuss implications for further research on the effects of these distortions and their interactions with other features of the electoral system.
[7]
arXiv:2509.19511
[pdf, html, other]
Title:
A direct approach for full-field state-parameter estimation from fusion of noncollocated multi-rate sensor data using UKF-based algorithms
Dhiraj Ghosh, Adrita Kundu, Suparno Mukhopadhyay
Comments:
14 pages, 11 figures
Subjects:
Other Statistics (stat.OT)
Heterogeneous sensor setups may entail measurements recorded at varying sampling frequencies, commonly known as multi-rate data. For system identification and state estimation with such data, existing studies mostly focus on data fusion algorithms that utilize acceleration measurements, with collocated measurements of other types at lower sampling frequencies, to estimate the displacement at the collocated location with the sampling frequency of the acceleration measurements. The obtained displacements, along with the available acceleration measurements, are then utilized for system identification. This paper introduces a direct and straightforward methodology aimed at estimating the states (i.e., displacements and velocities) along with the unknown structural parameters from fused multi-rate data through Unscented Kalman Filter (UKF) based algorithms with a modification during measurement update. By utilizing all available measurements at any time instant, which can differ due to the multi-rate nature, and by modifying the non-linear measurement equation of the system accordingly at the considered time instant, the UKF framework is suitably tailored for direct applications with multi-rate measurements. The approach is demonstrated with a variety of numerical and laboratory-scale experiments, including fusion of higher sampling frequency acceleration data with lower sampling frequency displacement, axial strain, or bending strain data. The results show that the approach is successful in accurately estimating full-field states and parameters. The state estimates compare well with those obtained using existing data fusion algorithms. The advantages of the approach lie in not requiring collocated sensing, in its generalizability for different types of measurements, in its simplicity and ease of implementation, and in achieving both the state and parameter estimates simultaneously.
[8]
arXiv:2509.19559
[pdf, html, other]
Title:
Stochastic Path Planning in Correlated Obstacle Fields
Li Zhou, Elvan Ceyhan
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
We introduce the Stochastic Correlated Obstacle Scene (SCOS) problem, a navigation setting with spatially correlated obstacles of uncertain blockage status, realistically constrained sensors that provide noisy readings and costly disambiguation. Modeling the spatial correlation with Gaussian Random Field (GRF), we develop Bayesian belief updates that refine blockage probabilities, and use the posteriors to reduce search space for efficiency. To find the optimal traversal policy, we propose a novel two-stage learning framework. An offline phase learns a robust base policy via optimistic policy iteration augmented with information bonus to encourage exploration in informative regions, followed by an online rollout policy with periodic base updates via a Bayesian mechanism for information adaptation. This framework supports both Monte Carlo point estimation and distributional reinforcement learning (RL) to learn full cost distributions, leading to stronger uncertainty quantification. We establish theoretical benefits of correlation-aware updating and convergence property under posterior sampling. Comprehensive empirical evaluations across varying obstacle densities, sensor capabilities demonstrate consistent performance gains over baselines. This framework addresses navigation challenges in environments with adversarial interruptions or clustered natural hazards.
[9]
arXiv:2509.19577
[pdf, other]
Title:
MAGIC: Multi-task Gaussian process for joint imputation and classification in healthcare time series
Dohyun Ku, Catherine D. Chong, Visar Berisha, Todd J. Schwedt, Jing Li
Comments:
36 pages, 4 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Time series analysis has emerged as an important tool for improving patient diagnosis and management in healthcare applications. However, these applications commonly face two critical challenges: time misalignment and data sparsity. Traditional approaches address these issues through a two-step process of imputation followed by prediction. We propose MAGIC (Multi-tAsk Gaussian Process for Imputation and Classification), a novel unified framework that simultaneously performs class-informed missing value imputation and label prediction within a hierarchical multi-task Gaussian process coupled with functional logistic regression. To handle intractable likelihood components, MAGIC employs Taylor expansion approximations with bounded error analysis, and parameter estimation is performed using EM algorithm with block coordinate optimization supported by convergence analysis. We validate MAGIC through two healthcare applications: prediction of post-traumatic headache improvement following mild traumatic brain injury and prediction of in-hospital mortality within 48 hours after ICU admission. In both applications, MAGIC achieves superior predictive accuracy compared to existing methods. The ability to generate real-time and accurate predictions with limited samples facilitates early clinical assessment and treatment planning, enabling healthcare providers to make more informed treatment decisions.
[10]
arXiv:2509.19652
[pdf, html, other]
Title:
Quality-Ensured In-Situ Process Monitoring with Deep Canonical Correlation Analysis
Xiaoyang Song, Wenbo Sun, Metin Kayitmazbatir, Jionghua (Judy)Jin
Subjects:
Applications (stat.AP)
This paper proposes a deep learning-based approach for in-situ process monitoring that captures nonlinear relationships between in-control high-dimensional process signature signals and offline product quality data. Specifically, we introduce a Deep Canonical Correlation Analysis (DCCA)-based framework that enables the joint feature extraction and correlation analysis of multi-modal data sources, such as optical emission spectra and CT scan images, which are collected in advanced manufacturing processes. This unified framework facilitates online quality monitoring by learning quality-oriented representations without requiring labeled defective samples and avoids the non-normality issues that often degrade traditional control chart-based monitoring techniques. We provide theoretical guarantees for the method's stability and convergence and validate its effectiveness and practical applicability through simulation experiments and a real-world case study on Direct Metal Deposition (DMD) additive manufacturing.
[11]
arXiv:2509.19707
[pdf, other]
Title:
Diffusion and Flow-based Copulas: Forgetting and Remembering Dependencies
David Huk, Theodoros Damoulas
Comments:
Preprint
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)
Copulas are a fundamental tool for modelling multivariate dependencies in data, forming the method of choice in diverse fields and applications. However, the adoption of existing models for multimodal and high-dimensional dependencies is hindered by restrictive assumptions and poor scaling. In this work, we present methods for modelling copulas based on the principles of diffusions and flows. We design two processes that progressively forget inter-variable dependencies while leaving dimension-wise distributions unaffected, provably defining valid copulas at all times. We show how to obtain copula models by learning to remember the forgotten dependencies from each process, theoretically recovering the true copula at optimality. The first instantiation of our framework focuses on direct density estimation, while the second specialises in expedient sampling. Empirically, we demonstrate the superior performance of our proposed methods over state-of-the-art copula approaches in modelling complex and high-dimensional dependencies from scientific datasets and images. Our work enhances the representational power of copula models, empowering applications and paving the way for their adoption on larger scales and more challenging domains.
[12]
arXiv:2509.19710
[pdf, html, other]
Title:
Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions
Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick
Comments:
61 pages, 10 figures, 15 tables
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
The advent of Scientific Machine Learning has heralded a transformative era in scientific discovery, driving progress across diverse domains. Central to this progress is uncovering scientific laws from experimental data through symbolic regression. However, existing approaches are dominated by heuristic algorithms or data-hungry black-box methods, which often demand low-noise settings and lack principled uncertainty quantification. Motivated by interpretable Statistical Artificial Intelligence, we develop a hierarchical Bayesian framework for symbolic regression that represents scientific laws as ensembles of tree-structured symbolic expressions endowed with a regularized tree prior. This coherent probabilistic formulation enables full posterior inference via an efficient Markov chain Monte Carlo algorithm, yielding a balance between predictive accuracy and structural parsimony. To guide symbolic model selection, we develop a marginal posterior-based criterion adhering to the Occam's window principle and further quantify structural fidelity to ground truth through a tailored expression-distance metric. On the theoretical front, we establish near-minimax rate of Bayesian posterior concentration, providing the first rigorous guarantee in context of symbolic regression. Empirical evaluation demonstrates robust performance of our proposed methodology against state-of-the-art competing modules on a simulated example, a suite of canonical Feynman equations, and single-atom catalysis dataset.
[13]
arXiv:2509.19748
[pdf, html, other]
Title:
Generalized Bayesian Inference for Dynamic Random Dot Product Graphs
Joshua Daniel Loyal
Comments:
50 pages, 12 figures, and 2 tables
Subjects:
Methodology (stat.ME); Computation (stat.CO)
The random dot product graph is a popular model for network data with extensions that accommodate dynamic (time-varying) networks. However, two significant deficiencies exist in the dynamic random dot product graph literature: (1) no coherent Bayesian way to update one's prior beliefs about the latent positions in dynamic random dot product graphs due to their complicated constraints, and (2) no approach to forecast future networks with meaningful uncertainty quantification. This work proposes a generalized Bayesian framework that addresses these needs using a Gibbs posterior that represents a coherent updating of Bayesian beliefs based on a least-squares loss function. We establish the consistency and contraction rate of this Gibbs posterior under commonly adopted Gaussian random walk priors. For estimation, we develop a fast Gibbs sampler with a time complexity for sampling the latent positions that is linear in the observed edges in the dynamic network, which is substantially faster than existing exact samplers. Simulations and an application to forecasting international conflicts show that the proposed method's in-sample and forecasting performance outperforms competitors.
[14]
arXiv:2509.19788
[pdf, html, other]
Title:
Convex Regression with a Penalty
Eunji Lim
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
A common way to estimate an unknown convex regression function $f_0: \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$ from a set of $n$ noisy observations is to fit a convex function that minimizes the sum of squared errors. However, this estimator is known for its tendency to overfit near the boundary of $\Omega$, posing significant challenges in real-world applications. In this paper, we introduce a new estimator of $f_0$ that avoids this overfitting by minimizing a penalty on the subgradient while enforcing an upper bound $s_n$ on the sum of squared errors. The key advantage of this method is that $s_n$ can be directly estimated from the data. We establish the uniform almost sure consistency of the proposed estimator and its subgradient over $\Omega$ as $n \rightarrow \infty$ and derive convergence rates. The effectiveness of our estimator is illustrated through its application to estimating waiting times in a single-server queue.
[15]
arXiv:2509.19814
[pdf, html, other]
Title:
Causal Inference under Threshold Manipulation: Bayesian Mixture Modeling and Heterogeneous Treatment Effects
Kohsuke Kubota, Shonosuke Sugasawa
Comments:
Submitted to AAAI 2026
Subjects:
Methodology (stat.ME); Artificial Intelligence (cs.AI)
Many marketing applications, including credit card incentive programs, offer rewards to customers who exceed specific spending thresholds to encourage increased consumption. Quantifying the causal effect of these thresholds on customers is crucial for effective marketing strategy design. Although regression discontinuity design is a standard method for such causal inference tasks, its assumptions can be violated when customers, aware of the thresholds, strategically manipulate their spending to qualify for the rewards. To address this issue, we propose a novel framework for estimating the causal effect under threshold manipulation. The main idea is to model the observed spending distribution as a mixture of two distributions: one representing customers strategically affected by the threshold, and the other representing those unaffected. To fit the mixture model, we adopt a two-step Bayesian approach consisting of modeling non-bunching customers and fitting a mixture model to a sample around the threshold. We show posterior contraction of the resulting posterior distribution of the causal effect under large samples. Furthermore, we extend this framework to a hierarchical Bayesian setting to estimate heterogeneous causal effects across customer subgroups, allowing for stable inference even with small subgroup sample sizes. We demonstrate the effectiveness of our proposed methods through simulation studies and illustrate their practical implications using a real-world marketing dataset.
[16]
arXiv:2509.19820
[pdf, html, other]
Title:
High-Dimensional Statistical Process Control via Manifold Fitting and Learning
Burak I. Tas, Enrique del Castillo
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)
We address the Statistical Process Control (SPC) of high-dimensional, dynamic industrial processes from two complementary perspectives: manifold fitting and manifold learning, both of which assume data lies on an underlying nonlinear, lower dimensional space. We propose two distinct monitoring frameworks for online or 'phase II' Statistical Process Control (SPC). The first method leverages state-of-the-art techniques in manifold fitting to accurately approximate the manifold where the data resides within the ambient high-dimensional space. It then monitors deviations from this manifold using a novel scalar distribution-free control chart. In contrast, the second method adopts a more traditional approach, akin to those used in linear dimensionality reduction SPC techniques, by first embedding the data into a lower-dimensional space before monitoring the embedded observations. We prove how both methods provide a controllable Type I error probability, after which they are contrasted for their corresponding fault detection ability. Extensive numerical experiments on a synthetic process and on a replicated Tennessee Eastman Process show that the conceptually simpler manifold-fitting approach achieves performance competitive with, and sometimes superior to, the more classical lower-dimensional manifold monitoring methods. In addition, we demonstrate the practical applicability of the proposed manifold-fitting approach by successfully detecting surface anomalies in a real image dataset of electrical commutators.
[17]
arXiv:2509.19889
[pdf, html, other]
Title:
Improving Disease Risk Estimation in Small Areas by Accounting for Spatiotemporal Local Discontinuities
Santafé, G., Adin, A., Ugarte, M.L
Subjects:
Methodology (stat.ME)
This work proposes a two-step method to enhance disease risk estimation in small areas by integrating spatiotemporal cluster detection within a Bayesian hierarchical spatiotemporal model. First, we introduce an efficient scan-statistic-based clustering algorithm that employs a greedy search within the scan window, enabling flexible cluster detection across large spatial domains. We then integrate these detected clusters into a Bayesian spatiotemporal model to estimate relative risks, explicitly accounting for identified risk discontinuities. We apply this methodology to large-scale cancer mortality data at the municipality level across continental Spain. Our results show our method offers superior cluster detection accuracy compared to SaTScan. Furthermore, integrating cluster information into a Bayesian spatiotemporal model significantly improves model fit and risk estimate performance, as evidenced by better DIC, WAIC, and logarithmic scores than SaTScan-based or standard BYM2 models. This methodology provides a powerful tool for epidemiological analysis, offering a more precise identification of high- and low-risk areas and enhancing the accuracy of risk estimation models.
[18]
arXiv:2509.19920
[pdf, html, other]
Title:
Extending finite mixture models with skew-normal distributions and hidden Markov models for time series
Andrea Nigri, Marco Forti, Han Lin Shang
Comments:
37 pages, 10 figures, 6 tables
Subjects:
Methodology (stat.ME); Applications (stat.AP)
We introduce an extension of finite mixture models by incorporating skew-normal distributions within a Hidden Markov Model framework. By assuming a constant transition probability matrix and allowing emission distributions to vary according to hidden states, the proposed model effectively captures dynamic dependencies between variables. Through the estimation of state-specific parameters, including location, scale, and skewness, the proposed model enables the detection of structural changes, such as shifts in the observed data distribution, while addressing challenges such as overfitting and computational inefficiencies inherent in Gaussian mixtures. Both simulation studies and real data analysis demonstrate the robustness and flexibility of the approach, highlighting its ability to accurately model asymmetric data and detect regime transitions. This methodological advancement broadens the applicability of a finite mixture of hidden Markov models across various fields, including demography, economics, finance, and environmental studies, offering a powerful tool for understanding complex temporal dynamics.
[19]
arXiv:2509.19929
[pdf, html, other]
Title:
Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later
Arnaud Vadeboncoeur, Gregory Duthé, Mark Girolami, Eleni Chatzi
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an)
Uncertainty Quantification (UQ) is paramount for inference in engineering applications. A common inference task is to recover full-field information of physical systems from a small number of noisy observations, a usually highly ill-posed problem. Critically, engineering systems often have complicated and variable geometries prohibiting the use of standard Bayesian UQ. In this work, we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework for learning geometry-aware generative models of physical responses that serve as highly informative geometry-conditioned priors for Bayesian inversion. Following a ''learn first, observe later'' paradigm, GABI distills information from large datasets of systems with varying geometries, without requiring knowledge of governing PDEs, boundary conditions, or observation processes, into a rich latent prior. At inference time, this prior is seamlessly combined with the likelihood of the specific observation process, yielding a geometry-adapted posterior distribution. Our proposed framework is architecture agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling yields an efficient implementation that utilizes modern GPU hardware. We test our method on: steady-state heat over rectangular domains; Reynold-Averaged Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source localization on 3D car bodies; RANS airflow over terrain. We find: the predictive accuracy to be comparable to deterministic supervised learning approaches in the restricted setting where supervised learning is applicable; UQ to be well calibrated and robust on challenging problems with complex geometries. The method provides a flexible geometry-aware train-once-use-anywhere foundation model which is independent of any particular observation process.
[20]
arXiv:2509.19956
[pdf, html, other]
Title:
Multi-state Models For Modeling Disease Histories Based On Longitudinal Data
Simon Wiegrebe, Johannes Piller, Mathias Gorski, Merle Behr, Helmut Küchenhoff, Iris M. Heid, Andreas Bender
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Multi-stage disease histories derived from longitudinal data are becoming increasingly available as registry data and biobanks expand. Multi-state models are suitable to investigate transitions between different disease stages in presence of competing risks. In this context, however their estimation is complicated by dependent left-truncation, multiple time scales, index event bias, and interval-censoring. In this work, we investigate the extension of piecewise exponential additive models (PAMs) to this setting and their applicability given the above challenges. In simulation studies we show that PAMs can handle dependent left-truncation and accommodate multiple time scales. Compared to a stratified single time scale model, a multiple time scales model is found to be less robust to the data generating process. We also quantify the extent of index event bias in multiple settings, demonstrating its dependence on the completeness of covariate adjustment. In general, PAMs recover baseline and fixed effects well in most settings, except for baseline hazards in interval-censored data. Finally, we apply our framework to estimate multi-state transition hazards and probabilities of chronic kidney disease (CKD) onset and progression in a UK Biobank dataset (n=$142,667$). We observe CKD progression risk to be highest for individuals with early CKD onset and to further increase over age. In addition, the well-known genetic variant rs77924615 in the UMOD locus is found to be associated with CKD onset hazards, but not with risk of further CKD progression.
[21]
arXiv:2509.19988
[pdf, html, other]
Title:
BioBO: Biology-informed Bayesian Optimization for Perturbation Design
Yanke Li, Tianyu Cui, Tommaso Mansi, Mangal Prakash, Rui Liao
Comments:
NeurIPS: Structured Probabilistic Inference & Generative Modeling, 2025
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)
Efficient design of genomic perturbation experiments is crucial for accelerating drug discovery and therapeutic target identification, yet exhaustive perturbation of the human genome remains infeasible due to the vast search space of potential genetic interactions and experimental constraints. Bayesian optimization (BO) has emerged as a powerful framework for selecting informative interventions, but existing approaches often fail to exploit domain-specific biological prior knowledge. We propose Biology-Informed Bayesian Optimization (BioBO), a method that integrates Bayesian optimization with multimodal gene embeddings and enrichment analysis, a widely used tool for gene prioritization in biology, to enhance surrogate modeling and acquisition strategies. BioBO combines biologically grounded priors with acquisition functions in a principled framework, which biases the search toward promising genes while maintaining the ability to explore uncertain regions. Through experiments on established public benchmarks and datasets, we demonstrate that BioBO improves labeling efficiency by 25-40%, and consistently outperforms conventional BO by identifying top-performing perturbations more effectively. Moreover, by incorporating enrichment analysis, BioBO yields pathway-level explanations for selected perturbations, offering mechanistic interpretability that links designs to biologically coherent regulatory circuits.
[22]
arXiv:2509.20013
[pdf, html, other]
Title:
A decision-theoretic framework for uncertainty quantification in epidemiological modelling
Nicholas Steyn, Freddie Bickford Smith, Cathal Mills, Vik Shirvaikar, Christl A Donnelly, Kris V Parag
Comments:
16 pages, 6 figures
Subjects:
Methodology (stat.ME); Quantitative Methods (q-bio.QM)
Estimating, understanding, and communicating uncertainty is fundamental to statistical epidemiology, where model-based estimates regularly inform real-world decisions. However, sources of uncertainty are rarely formalised, and existing classifications are often defined inconsistently. This lack of structure hampers interpretation, model comparison, and targeted data collection. Connecting ideas from machine learning, information theory, experimental design, and health economics, we present a first-principles decision-theoretic framework that defines uncertainty as the expected loss incurred by making an estimate based on incomplete information, arguing that this is a highly useful and practically relevant definition for epidemiology. We show how reasoning about future data leads to a notion of expected uncertainty reduction, which induces formal definitions of reducible and irreducible uncertainty. We demonstrate our approach using a case study of SARS-CoV-2 wastewater surveillance in Aotearoa New Zealand, estimating the uncertainty reduction if wastewater surveillance were expanded to the full population. We then connect our framework to relevant literature from adjacent fields, showing how it unifies and extends many of these ideas and how it allows these ideas to be applied to a wider range of models. Altogether, our framework provides a foundation for more reliable, consistent, and policy-relevant uncertainty quantification in infectious disease epidemiology.
[23]
arXiv:2509.20083
[pdf, html, other]
Title:
Rethinking player evaluation in sports: Goals above expectation and beyond
Robert Bajons, Lucas Kook
Subjects:
Applications (stat.AP)
A popular quantitative approach to evaluating player performance in sports involves comparing an observed outcome to the expected outcome ignoring player involvement, which is estimated using statistical or machine learning methods. In soccer, for instance, goals above expectation (GAX) of a player measure how often shots of this player led to a goal compared to the model-derived expected outcome of the shots. Typically, sports data analysts rely on flexible machine learning models, which are capable of handling complex nonlinear effects and feature interactions, but fail to provide valid statistical inference due to finite-sample bias and slow convergence rates. In this paper, we close this gap by presenting a framework for player evaluation with metrics derived from differences in actual and expected outcomes using flexible machine learning algorithms, which nonetheless allows for valid frequentist inference. We first show that the commonly used metrics are directly related to Rao's score test in parametric regression models for the expected outcome. Motivated by this finding and recent developments in double machine learning, we then propose the use of residualized versions of the original metrics. For GAX, the residualization step corresponds to an additional regression predicting whether a given player would take the shot under the circumstances described by the features. We further relate metrics in the proposed framework to player-specific effect estimates in interpretable semiparametric regression models, allowing us to infer directional effects, e.g., to determine players that have a positive impact on the outcome. Our primary use case are GAX in soccer. We further apply our framework to evaluate goal-stopping ability of goalkeepers, shooting skill in basketball, quarterback passing skill in American football, and injury-proneness of soccer players.
[24]
arXiv:2509.20101
[pdf, html, other]
Title:
First-Extinction Law for Resampling Processes
Matteo Benati, Alessandro Londei, Denise Lanzieri, Vittorio Loreto
Subjects:
Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST); Data Analysis, Statistics and Probability (physics.data-an); Populations and Evolution (q-bio.PE)
Extinction times in resampling processes are fundamental yet often intractable, as previous formulas scale as $2^M$ with the number of states $M$ present in the initial probability distribution. We solve this by treating multinomial updates as independent square-root diffusions of zero drift, yielding a closed-form law for the first-extinction time. We prove that the mean coincides exactly with the Wright-Fisher result of Baxter et al., thereby replacing exponential-cost evaluations with a linear-cost expression, and we validate this result through extensive simulations. Finally, we demonstrate predictive power for model collapse in a simple self-training setup: the onset of collapse coincides with the resampling-driven first-extinction time computed from the model's initial stationary distribution. These results hint to a unified view of resampling extinction dynamics.
[25]
arXiv:2509.20194
[pdf, other]
Title:
Identification and Semiparametric Estimation of Conditional Means from Aggregate Data
Cory McCartan, Shiro Kuriwaki
Comments:
24 pages, plus references and appendices
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
We introduce a new method for estimating the mean of an outcome variable within groups when researchers only observe the average of the outcome and group indicators across a set of aggregation units, such as geographical areas. Existing methods for this problem, also known as ecological inference, implicitly make strong assumptions about the aggregation process. We first formalize weaker conditions for identification, which motivates estimators that can efficiently control for many covariates. We propose a debiased machine learning estimator that is based on nuisance functions restricted to a partially linear form. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying assumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under additional assumptions. Simulations and validation on real-world data where ground truth is available demonstrate the advantages of our approach over existing methods. Open-source software is available which implements the proposed methods.
[26]
arXiv:2509.20206
[pdf, other]
Title:
Non-overlap Average Treatment Effect Bounds
Herbert P. Susmann, Alec McClean, Iván Díaz
Comments:
34 pages, 3 figures
Subjects:
Methodology (stat.ME)
The average treatment effect (ATE), the mean difference in potential outcomes under treatment and control, is a canonical causal effect. Overlap, which says that all subjects have non-zero probability of either treatment status, is necessary to identify and estimate the ATE. When overlap fails, the standard solution is to change the estimand, and target a trimmed effect in a subpopulation satisfying overlap; however, this no longer addresses the original goal of estimating the ATE. When the outcome is bounded, we demonstrate that this compromise is unnecessary. We derive non-overlap bounds: partial identification bounds on the ATE that do not require overlap. They are the sum of a trimmed effect within the overlap subpopulation and worst-case bounds on the ATE in the non-overlap subpopulation. Non-overlap bounds have width proportional to the size of the non-overlap subpopulation, making them informative when overlap violations are limited -- a common scenario in practice. Since the bounds are non-smooth functionals, we derive smooth approximations of them that contain the ATE but can be estimated using debiased estimators leveraging semiparametric efficiency theory. Specifically, we propose a Targeted Minimum Loss-Based estimator that is $\sqrt{n}$-consistent and asymptotically normal under nonparametric assumptions on the propensity score and outcome regression. We then show how to obtain a uniformly valid confidence set across all trimming and smoothing parameters with the multiplier bootstrap. This allows researchers to consider many parameters, choose the tightest confidence interval, and still attain valid coverage. We demonstrate via simulations that non-overlap bound estimators can detect non-zero ATEs with higher power than traditional doubly-robust point estimators. We illustrate our method by estimating the ATE of right heart catheterization on mortality.
[27]
arXiv:2509.20221
[pdf, html, other]
Title:
Measuring Partial Exchangeability with Reproducing Kernel Hilbert Spaces
Marta Catalano, Hugo Lavenant, Francesco Mascari
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
In Bayesian multilevel models, the data are structured in interconnected groups, and their posteriors borrow information from one another due to prior dependence between latent parameters. However, little is known about the behaviour of the dependence a posteriori. In this work, we develop a general framework for measuring partial exchangeability for parametric and nonparametric models, both a priori and a posteriori. We define an index that detects exchangeability for common models, is invariant by reparametrization, can be estimated through samples, and, crucially, is well-suited for posteriors. We achieve these properties through the use of Reproducing Kernel Hilbert Spaces, which map any random probability to a random object on a Hilbert space. This leads to many convenient properties and tractable expressions, especially a priori and under mixing. We apply our general framework to i) investigate the dependence a posteriori for the hierarchical Dirichlet process, retrieving a parametric convergence rate under very mild assumptions on the data; ii) eliciting the dependence structure of a parametric model for a principled comparison with a nonparametric alternative.
[28]
arXiv:2509.20239
[pdf, html, other]
Title:
Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing
Andrea Della Vecchia, Damir Filipović
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computational Finance (q-fin.CP); Pricing of Securities (q-fin.PR); Applications (stat.AP)
This paper investigates theoretical and methodological foundations for stochastic optimal control (SOC) in discrete time. We start formulating the control problem in a general dynamic programming framework, introducing the mathematical structure needed for a detailed convergence analysis. The associate value function is estimated through a sequence of approximations combining nonparametric regression methods and Monte Carlo subsampling. The regression step is performed within reproducing kernel Hilbert spaces (RKHSs), exploiting the classical KRR algorithm, while Monte Carlo sampling methods are introduced to estimate the continuation value. To assess the accuracy of our value function estimator, we propose a natural error decomposition and rigorously control the resulting error terms at each time step. We then analyze how this error propagates backward in time-from maturity to the initial stage-a relatively underexplored aspect of the SOC literature. Finally, we illustrate how our analysis naturally applies to a key financial application: the pricing of American options.
[29]
arXiv:2509.20249
[pdf, html, other]
Title:
Indirect Statistical Inference with Guaranteed Necessity and Sufficiency
Z Zhang, X Hu, C Lu, T Liu
Subjects:
Statistics Theory (math.ST)
This paper develops a new framework for indirect statistical inference with guaranteed necessity and sufficiency, applicable to continuous random variables. We prove that when comparing exponentially transformed order statistics from an assumed distribution with those from simulated unit exponential samples, the ranked quotients exhibit distinct asymptotics: the left segment converges to a non-degenerate distribution, while the middle and right segments degenerate to one. This yields a necessary and sufficient condition in probability for two sequences of continuous random variables to follow the same distribution. Building on this, we propose an optimization criterion based on relative errors between ordered samples. The criterion achieves its minimum if and only if the assumed and true distributions coincide, providing a second necessary and sufficient condition in optimization. These dual NS properties, rare in the literature, establish a fundamentally stronger inference framework than existing methods. Unlike classical approaches based on absolute errors (e.g., Kolmogorov-Smirnov), NSE exploits relative errors to ensure faster convergence, requires only mild approximability of the cumulative distribution function, and provides both point and interval estimates. Simulations and real-data applications confirm NSE's superior performance in preserving distributional assumptions where traditional methods fail.
[30]
arXiv:2509.20272
[pdf, html, other]
Title:
Transfer Learning in Regression with Influential Points
Bingbing Wang, Jiaqi Wang, Yu Tang
Subjects:
Methodology (stat.ME)
Regression prediction plays a crucial role in practical applications and strongly relies on data annotation. However, due to prohibitive annotation costs or domain-specific constraints, labeled data in the target domain is often scarce, making transfer learning a critical solution by leveraging knowledge from resource-rich source domains. In the practical target scenario, although transfer learning has been widely applied, influential points can significantly distort parameter estimation for the target domain model. This issue is further compounded when influential points are also present in source domains, leading to aggravated performance degradation and posing critical robustness challenges for existing transfer learning frameworks. In this study, we innovatively introduce a transfer learning collaborative optimization (Trans-CO) framework for influential point detection and regression model fitting. Extensive simulation experiments demonstrate that the proposed Trans-CO algorithm outperforms competing methods in terms of model fitting performance and influential point identification accuracy. Furthermore, it achieves superior predictive accuracy on real-world datasets, providing a novel solution for transfer learning in regression with influential points
[31]
arXiv:2509.20332
[pdf, other]
Title:
Scale two-sample testing with arbitrarily missing data
Yijin Zeng, Niall M. Adams, Dean A. Bodenham
Comments:
82 pages, 14 figures
Subjects:
Methodology (stat.ME)
This work proposes a novel rank-based scale two-sample testing method for univariate, distinct data when a subset of the data may be missing. Our approach is based on mathematically tight bounds of the Ansari-Bradley test statistic in the presence of missing data, and rejects the null hypothesis if the test statistic is significant regardless of the missing values. This proposed scale testing method is then combined with the location testing method proposed by Zeng et al. (2024) using the Holm-Bonferroni correction for location-scale testing. We show that our methods control the Type I error regardless of the values of the missing data. Simulation results demonstrate that our methods have good statistical power, typically when less than 10% of the data are missing, while other missing data methods, such as case deletion or imputation methods, fail to control the Type I error when the data are missing not at random. We illustrate the proposed location-scale testing method on hepatitis C virus dataset where a subset of values is unobserved.
[32]
arXiv:2509.20345
[pdf, html, other]
Title:
Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees
Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)
The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.
Cross submissions (showing 23 of 23 entries)
[33]
arXiv:2509.19329
(cross-list from cs.CL)
[pdf, html, other]
Title:
How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment
Julie Jung, Max Lu, Sina Chole Benker, Dogus Darici
Comments:
9 pages, 4 figures, accepted at NCME AIME 2025
Subjects:
Computation and Language (cs.CL); Methodology (stat.ME)
We examined how model size, temperature, and prompt style affect Large Language Models' (LLMs) alignment within itself, between models, and with human in assessing clinical reasoning skills. Model size emerged as a key factor in LLM-human score alignment. Study highlights the importance of checking alignments across multiple levels.
[34]
arXiv:2509.19367
(cross-list from eess.SP)
[pdf, other]
Title:
Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods
Borhan Uddin Chowdhury, Damian Valles, Md Raf E Ul Shougat
Comments:
Copyright 2025 IEEE. This is the author's version of the work accepted for publication in FMLDS 2025. The final version will be published by IEEE and available via DOI (to be inserted when available). Accepted at FMLDS 2025, to appear in IEEE Xplore. 8 pages, 17 figures, 3 tables
Subjects:
Signal Processing (eess.SP); Machine Learning (cs.LG); Machine Learning (stat.ML)
We present a sensor-fusion framework for rapid, non-destructive classification and quality control of organic substances, built on a standard Arduino Mega 2560 microcontroller platform equipped with three commercial environmental and gas sensors. All data used in this study were generated in-house: sensor outputs for ten distinct classes - including fresh and expired samples of apple juice, onion, garlic, and ginger, as well as cinnamon and cardamom - were systematically collected and labeled using this hardware setup, resulting in a unique, application-specific dataset. Correlation analysis was employed as part of the preprocessing pipeline for feature selection. After preprocessing and dimensionality reduction (PCA/LDA), multiple supervised learning models - including Support Vector Machine (SVM), Decision Tree (DT), and Random Forest (RF), each with hyperparameter tuning, as well as an Artificial Neural Network (ANN) and an ensemble voting classifier - were trained and cross-validated on the collected dataset. The best-performing models, including tuned Random Forest, ensemble, and ANN, achieved test accuracies in the 93 to 94 percent range. These results demonstrate that low-cost, multisensory platforms based on the Arduino Mega 2560, combined with advanced machine learning and correlation-driven feature engineering, enable reliable identification and quality control of organic compounds.
[35]
arXiv:2509.19375
(cross-list from cs.LG)
[pdf, html, other]
Title:
Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation
Mridul Sharma (1), Adeetya Patel (1), Zaneta D' Souza (1), Samira Abbasgholizadeh Rahimi (1 and 3), Siva Reddy (2 and 3), Sreenath Madathil (1) ((1) Faculty of Dental Medicine and Oral Health Sciences, McGill University, Montreal, Canada (2) School of Computer Science, McGill University, Montreal, Canada (3) Mila-Quebec Artificial Intelligence Institute, Montreal, Canada)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Despite their widespread applications, Large Language Models (LLMs) often struggle to express uncertainty, posing a challenge for reliable deployment in high stakes and safety critical domains like clinical diagnostics. Existing standard baseline methods such as model logits and elicited probabilities produce overconfident and poorly calibrated estimates. In this work, we propose Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference, based approach that treats LLMs as a stochastic simulator to infer posterior distributions over predictive probabilities. We evaluate our ABC approach on two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to standard baselines, our approach improves accuracy by up to 46.9\%, reduces Brier scores by 74.4\%, and enhances calibration as measured by Expected Calibration Error (ECE) and predictive entropy.
[36]
arXiv:2509.19379
(cross-list from cs.LG)
[pdf, html, other]
Title:
Learning from Observation: A Survey of Recent Advances
Returaj Burnwal, Hriday Mehta, Nirav Pravinbhai Bhatt, Balaraman Ravindran
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)
Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.
[37]
arXiv:2509.19387
(cross-list from eess.SP)
[pdf, html, other]
Title:
Hybrid Pipeline SWD Detection in Long-Term EEG Signals
Antonio Quintero Rincon, Nicolas Masino, Veronica Marsico, Hadj Batatia
Comments:
11 pages, 8 figures, 4 tables, SABI 2025 CLIC 2025
Subjects:
Signal Processing (eess.SP); Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of absence epilepsy, yet their manual identification in multi-day recordings remains labour-intensive and error-prone. We present a lightweight hybrid pipeline that couples analytical features with a shallow artificial neural network (ANN) for accurate, patient-specific SWD detection in long-term, monopolar EEG. A two-sided moving-average (MA) filter first suppresses the high-frequency components of normal background activity. The residual signal is then summarised by the mean and the standard deviation of its normally distributed samples, yielding a compact, two-dimensional feature vector for every 20s window. These features are fed to a single-hidden-layer ANN trained via back-propagation to classify each window as SWD or non-SWD. The method was evaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392 annotated SWD events. It correctly detected 384 events (sensitivity: 98%) while achieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because feature extraction is analytic, and the classifier is small, the pipeline runs in real-time and requires no manual threshold tuning. These results indicate that normal-distribution descriptors combined with a modest ANN provide an effective and computationally inexpensive solution for automated SWD screening in extended EEG recordings.
[38]
arXiv:2509.19408
(cross-list from cs.LG)
[pdf, html, other]
Title:
Enhancing Credit Default Prediction Using Boruta Feature Selection and DBSCAN Algorithm with Different Resampling Techniques
Obu-Amoah Ampomah, Edmund Agyemang, Kofi Acheampong, Louis Agyekum
Comments:
16 pages, 8 figures and 5 tables
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
This study examines credit default prediction by comparing three techniques, namely SMOTE, SMOTE-Tomek, and ADASYN, that are commonly used to address the class imbalance problem in credit default situations. Recognizing that credit default datasets are typically skewed, with defaulters comprising a much smaller proportion than non-defaulters, we began our analysis by evaluating machine learning (ML) models on the imbalanced data without any resampling to establish baseline performance. These baseline results provide a reference point for understanding the impact of subsequent balancing methods. In addition to traditional classifiers such as Naive Bayes and K-Nearest Neighbors (KNN), our study also explores the suitability of advanced ensemble boosting algorithms, including Extreme Gradient Boosting (XGBoost), AdaBoost, Gradient Boosting Machines (GBM), and Light GBM for credit default prediction using Boruta feature selection and DBSCAN-based outlier detection, both before and after resampling. A real-world credit default data set sourced from the University of Cleveland ML Repository was used to build ML classifiers, and their performances were tested. The criteria chosen to measure model performance are the area under the receiver operating characteristic curve (ROC-AUC), area under the precision-recall curve (PR-AUC), G-mean, and F1-scores. The results from this empirical study indicate that the Boruta+DBSCAN+SMOTE-Tomek+GBM classifier outperformed the other ML models (F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%) in a credit default context. The findings establish a foundation for future progress in creating more resilient and adaptive credit default systems, which will be essential as credit-based transactions continue to rise worldwide.
[39]
arXiv:2509.19417
(cross-list from cs.LG)
[pdf, html, other]
Title:
Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting
Andreas Lebedev, Abhinav Das, Sven Pappert, Stephan Schlüter
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST)
Precise probabilistic forecasts are fundamental for energy risk management, and there is a wide range of both statistical and machine learning models for this purpose. Inherent to these probabilistic models is some form of uncertainty quantification. However, most models do not capture the full extent of uncertainty, which arises not only from the data itself but also from model and distributional choices. In this study, we examine uncertainty quantification in state-of-the-art statistical and deep learning probabilistic forecasting models for electricity price forecasting in the German market. In particular, we consider deep distributional neural networks (DDNNs) and augment them with an ensemble approach, Monte Carlo (MC) dropout, and conformal prediction to account for model uncertainty. Additionally, we consider the LASSO-estimated autoregressive (LEAR) approach combined with quantile regression averaging (QRA), generalized autoregressive conditional heteroskedasticity (GARCH), and conformal prediction. Across a range of performance metrics, we find that the LEAR-based models perform well in terms of probabilistic forecasting, irrespective of the uncertainty quantification method. Furthermore, we find that DDNNs benefit from incorporating both data and model uncertainty, improving both point and probabilistic forecasting. Uncertainty itself appears to be best captured by the models using conformal prediction. Overall, our extensive study shows that all models under consideration perform competitively. However, their relative performance depends on the choice of metrics for point and probabilistic forecasting.
[40]
arXiv:2509.19465
(cross-list from cs.LG)
[pdf, html, other]
Title:
A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models
Kin G. Olivares, Malcolm Wolff, Tatiana Konstantinova, Shankar Ramasubramanian, Andrew Gordon Wilson, Andres Potapczynski, Willa Potosnak, Mengfei Cao, Boris Oreshkin, Dmitry Efimov
Comments:
Thirty-Ninth Annual Conference on Neural Information Processing Systems {NeurIPS 2025}. Recent Advances in Time Series Foundation Models Have We Reached the 'BERT Moment'?
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Applications (stat.AP)
Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.
[41]
arXiv:2509.19494
(cross-list from astro-ph.CO)
[pdf, html, other]
Title:
Testing the Constancy of Type Ia Supernova Luminosities with Gaussian Process
Akshay Rana
Comments:
14 Pages, 04 Figures, Under review
Subjects:
Cosmology and Nongalactic Astrophysics (astro-ph.CO); Astrophysics of Galaxies (astro-ph.GA); Data Analysis, Statistics and Probability (physics.data-an); Methodology (stat.ME)
Type Ia supernovae (SNe~Ia) are central to studies of cosmic expansion, under the assumption that their absolute magnitude $M_B$ does not evolve with redshift. Even small drifts in brightness can bias cosmological parameters such as $H_0$ and $w$. Here we test this assumption using a non-parametric Gaussian Process (GP) reconstruction of the expansion history from cosmic chronometer $H(z)$ data, which provides a model-independent baseline distance modulus, $\mu_{\rm GP}(z)$. To propagate uncertainties, we draw Monte Carlo realizations of $H(z)$ from the GP posterior and evaluate them on a Chebyshev grid, which improves numerical stability and quadrature accuracy. Supernova observations are then compared to this baseline through residuals, $\Delta M_B(z)$, and their derivatives. Applying this method to Pantheon+ (1701 SNe~Ia) and DES 5YR (435 SNe~Ia), we find that SNe~Ia are consistent with being standard candles within $1\sigma$, though both datasets exhibit localized departures: near $z \sim 1$ in Pantheon+ and at $z \sim 0.3$--$0.5$ in DES. The presence of similar features in two independent surveys suggests they are not purely statistical. Our results point toward a possible non-monotonic luminosity evolution, likely reflecting different physical drivers at different epochs, and highlight the need for a deeper astrophysical understanding of SN~Ia populations.
[42]
arXiv:2509.19633
(cross-list from cs.LG)
[pdf, html, other]
Title:
Mamba Modulation: On the Length Generalization of Mamba
Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui
Comments:
Accepted to The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025. First two authors contributed equally
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.
[43]
arXiv:2509.19705
(cross-list from cs.LG)
[pdf, html, other]
Title:
Causal Machine Learning for Surgical Interventions
J. Ben Tamo, Nishant S. Chouhan, Micky C. Nnamdi, Yining Yuan, Shreya S. Chivilkar, Wenqi Shi, Steven W. Hwang, B. Randall Brenn, May D. Wang
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Applications (stat.AP); Methodology (stat.ME)
Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task meta-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private AIS dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$ (0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs in AIS, X-MultiTask consistently shows superior performance across all domains, with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at this https URL.
[44]
arXiv:2509.19830
(cross-list from cs.LG)
[pdf, html, other]
Title:
On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators
Wei Liu, Eleni Chatzi, Zhilu Lai
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable framework for multivariate function approximation by composing univariate transformations through additive or multiplicative aggregation. This paper establishes theoretical convergence guarantees for KANs when the univariate components are represented by B-splines. We prove that both additive and hybrid additive-multiplicative KANs attain the minimax-optimal convergence rate $O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We further derive guidelines for selecting the optimal number of knots in the B-splines. The theory is supported by simulation studies that confirm the predicted convergence rates. These results provide a theoretical foundation for using KANs in nonparametric regression and highlight their potential as a structured alternative to existing methods.
[45]
arXiv:2509.19900
(cross-list from eess.SP)
[pdf, html, other]
Title:
Generalized Nonnegative Structured Kruskal Tensor Regression
Xinjue Wang, Esa Ollila, Sergiy A. Vorobyov, Ammar Mian
Subjects:
Signal Processing (eess.SP); Applications (stat.AP); Machine Learning (stat.ML)
This paper introduces Generalized Nonnegative Structured Kruskal Tensor Regression (NS-KTR), a novel tensor regression framework that enhances interpretability and performance through mode-specific hybrid regularization and nonnegativity constraints. Our approach accommodates both linear and logistic regression formulations for diverse response variables while addressing the structural heterogeneity inherent in multidimensional tensor data. We integrate fused LASSO, total variation, and ridge regularizers, each tailored to specific tensor modes, and develop an efficient alternating direction method of multipliers (ADMM) based algorithm for parameter estimation. Comprehensive experiments on synthetic signals and real hyperspectral datasets demonstrate that NS-KTR consistently outperforms conventional tensor regression methods. The framework's ability to preserve distinct structural characteristics across tensor dimensions while ensuring physical interpretability makes it especially suitable for applications in signal processing and hyperspectral image analysis.
[46]
arXiv:2509.19901
(cross-list from cs.LG)
[pdf, html, other]
Title:
Pure Exploration via Frank-Wolfe Self-Play
Xinyu Liu, Chao Qin, Wei You
Subjects:
Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Statistics Theory (math.ST); Machine Learning (stat.ML)
We study pure exploration in structured stochastic multi-armed bandits, aiming to efficiently identify the correct hypothesis from a finite set of alternatives. For a broad class of tasks, asymptotic analyses reduce to a maximin optimization that admits a two-player zero-sum game interpretation between an experimenter and a skeptic: the experimenter allocates measurements to rule out alternatives while the skeptic proposes alternatives. We reformulate the game by allowing the skeptic to adopt a mixed strategy, yielding a concave-convex saddle-point problem. This viewpoint leads to Frank-Wolfe Self-Play (FWSP): a projection-free, regularization-free, tuning-free method whose one-hot updates on both sides match the bandit sampling paradigm. However, structural constraints introduce sharp pathologies that complicate algorithm design and analysis: our linear-bandit case study exhibits nonunique optima, optimal designs with zero mass on the best arm, bilinear objectives, and nonsmoothness at the boundary. We address these challenges via a differential-inclusion argument, proving convergence of the game value for best-arm identification in linear bandits. Our analysis proceeds through a continuous-time limit: a differential inclusion with a Lyapunov function that decays exponentially, implying a vanishing duality gap and convergence to the optimal value. Although Lyapunov analysis requires differentiability of the objective, which is not guaranteed on the boundary, we show that along continuous trajectories the algorithm steers away from pathological nonsmooth points and achieves uniform global convergence to the optimal game value. We then embed the discrete-time updates into a perturbed flow and show that the discrete game value also converges. Building on FWSP, we further propose a learning algorithm based on posterior sampling. Numerical experiments demonstrate a vanishing duality gap.
[47]
arXiv:2509.19910
(cross-list from cs.IT)
[pdf, other]
Title:
Understanding the ratio of the partition sum to its Bethe approximation via double covers
Pascal O. Vontobel
Comments:
Extended version (including appendices) of a paper appearing in Proc. 2025 IEEE Information Theory Workshop, Sydney, Australia, Sept./Oct., 2025
Subjects:
Information Theory (cs.IT); Combinatorics (math.CO); Statistics Theory (math.ST)
For various classes of graphical models it has been observed that the ratio of the partition sum to its Bethe approximation is often close to being the square of the ratio of the partition sum to its degree-2 Bethe approximation. This is of relevance because the latter ratio can often better be analyzed and/or quantified than the former ratio. In this paper, we give some justifications for the observed relationship between these two ratios and then analyze these ratios for two classes of log-supermodular graphical models.
[48]
arXiv:2509.19930
(cross-list from cs.LG)
[pdf, html, other]
Title:
How deep is your network? Deep vs. shallow learning of transfer operators
Mohammad Tabish, Benedict Leimkuhler, Stefan Klus
Subjects:
Machine Learning (cs.LG); Dynamical Systems (math.DS); Machine Learning (stat.ML)
We propose a randomized neural network approach called RaNNDy for learning transfer operators and their spectral decompositions from data. The weights of the hidden layers of the neural network are randomly selected and only the output layer is trained. The main advantage is that without a noticeable reduction in accuracy, this approach significantly reduces the training time and resources while avoiding common problems associated with deep learning such as sensitivity to hyperparameters and slow convergence. Additionally, the proposed framework allows us to compute a closed-form solution for the output layer which directly represents the eigenfunctions of the operator. Moreover, it is possible to estimate uncertainties associated with the computed spectral properties via ensemble learning. We present results for different dynamical operators, including Koopman and Perron-Frobenius operators, which have important applications in analyzing the behavior of complex dynamical systems, and the Schrödinger operator. The numerical examples, which highlight the strengths but also weaknesses of the proposed framework, include several stochastic dynamical systems, protein folding processes, and the quantum harmonic oscillator.
[49]
arXiv:2509.19962
(cross-list from cs.LG)
[pdf, html, other]
Title:
Learnable Sampler Distillation for Discrete Diffusion Models
Feiyang Fu, Tongxian Guo, Zhaoqiang Liu
Comments:
NeurIPS 2025
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{this https URL}{this https URL}.
[50]
arXiv:2509.20015
(cross-list from q-fin.MF)
[pdf, html, other]
Title:
Roughness Analysis of Realized Volatility and VIX through Randomized Kolmogorov-Smirnov Distribution
Sergio Bianchi, Daniele Angelini
Comments:
21 pages, 2 figures
Subjects:
Mathematical Finance (q-fin.MF); Computational Finance (q-fin.CP); Methodology (stat.ME)
We introduce a novel distribution-based estimator for the Hurst parameter of log-volatility, leveraging the Kolmogorov-Smirnov statistic to assess the scaling behavior of entire distributions rather than individual moments. To address the temporal dependence of financial volatility, we propose a random permutation procedure that effectively removes serial correlation while preserving marginal distributions, enabling the rigorous application of the KS framework to dependent data. We establish the asymptotic variance of the estimator, useful for inference and confidence interval construction. From a computational standpoint, we show that derivative-free optimization methods, particularly Brent's method and the Nelder-Mead simplex, achieve substantial efficiency gains relative to grid search while maintaining estimation accuracy. Empirical analysis of the CBOE VIX index and the 5-minute realized volatility of the S&P 500 reveals a statistically significant hierarchy of roughness, with implied volatility smoother than realized volatility. Both measures, however, exhibit Hurst exponents well below one-half, reinforcing the rough volatility paradigm and highlighting the open challenge of disentangling local roughness from long-memory effects in fractional modeling.
[51]
arXiv:2509.20034
(cross-list from eess.SP)
[pdf, html, other]
Title:
Reproduction Number and Spatial Connectivity Structure Estimation via Graph Sparsity-Promoting Penalized Functional
Etienne Lasalle, Barbara Pascal
Comments:
11 pages, 3 figures
Subjects:
Signal Processing (eess.SP); Optimization and Control (math.OC); Applications (stat.AP)
During an epidemic outbreak, decision makers crucially need accurate and robust tools to monitor the pathogen propagation. The effective reproduction number, defined as the expected number of secondary infections stemming from one contaminated individual, is a state-of-the-art indicator quantifying the epidemic intensity. Numerous estimators have been developed to precisely track the reproduction number temporal evolution. Yet, COVID-19 pandemic surveillance raised unprecedented challenges due to the poor quality of worldwide reported infection counts. When monitoring the epidemic in different territories simultaneously, leveraging the spatial structure of data significantly enhances both the accuracy and robustness of reproduction number estimates. However, this requires a good estimate of the spatial structure. To tackle this major limitation, the present work proposes a joint estimator of the reproduction number and connectivity structure. The procedure is assessed through intensive numerical simulations on carefully designed synthetic data and illustrated on real COVID-19 spatiotemporal infection counts.
[52]
arXiv:2509.20201
(cross-list from cs.LG)
[pdf, html, other]
Title:
Staying on the Manifold: Geometry-Aware Noise Injection
Albert Kjøller Jacobsen, Johanna Marie Gegenfurtner, Georgios Arvanitidis
Subjects:
Machine Learning (cs.LG); Differential Geometry (math.DG); Machine Learning (stat.ML)
It has been shown that perturbing the input during training implicitly regularises the gradient of the learnt function, leading to smoother models and enhancing generalisation. However, previous research mostly considered the addition of ambient noise in the input space, without considering the underlying structure of the data. In this work, we propose several methods of adding geometry-aware input noise that accounts for the lower dimensional manifold the input space inhabits. We start by projecting ambient Gaussian noise onto the tangent space of the manifold. In a second step, the noise sample is mapped on the manifold via the associated geodesic curve. We also consider Brownian motion noise, which moves in random steps along the manifold. We show that geometry-aware noise leads to improved generalization and robustness to hyperparameter selection on highly curved manifolds, while performing at least as well as training without noise on simpler manifolds. Our proposed framework extends to learned data manifolds.
[53]
arXiv:2509.20283
(cross-list from cs.CR)
[pdf, html, other]
Title:
Monitoring Violations of Differential Privacy over Time
Önder Askin, Tim Kutta, Holger Dette
Subjects:
Cryptography and Security (cs.CR); Statistics Theory (math.ST); Methodology (stat.ME)
Auditing differential privacy has emerged as an important area of research that supports the design of privacy-preserving mechanisms. Privacy audits help to obtain empirical estimates of the privacy parameter, to expose flawed implementations of algorithms and to compare practical with theoretical privacy guarantees. In this work, we investigate an unexplored facet of privacy auditing: the sustained auditing of a mechanism that can go through changes during its development or deployment. Monitoring the privacy of algorithms over time comes with specific challenges. Running state-of-the-art (static) auditors repeatedly requires excessive sampling efforts, while the reliability of such methods deteriorates over time without proper adjustments. To overcome these obstacles, we present a new monitoring procedure that extracts information from the entire deployment history of the algorithm. This allows us to reduce sampling efforts, while sustaining reliable outcomes of our auditor. We derive formal guarantees with regard to the soundness of our methods and evaluate their performance for important mechanisms from the literature. Our theoretical findings and experiments demonstrate the efficacy of our approach.
[54]
arXiv:2509.20294
(cross-list from cs.LG)
[pdf, html, other]
Title:
Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels
Dongming Huang, Zhifan Li, Yicheng Li, Qian Lin
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST)
We study spectral algorithms in the setting where kernels are learned from data. We introduce the effective span dimension (ESD), an alignment-sensitive complexity measure that depends jointly on the signal, spectrum, and noise level $\sigma^2$. The ESD is well-defined for arbitrary kernels and signals without requiring eigen-decay conditions or source conditions. We prove that for sequence models whose ESD is at most $K$, the minimax excess risk scales as $\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and prove that it can reduce the ESD. This finding establishes a connection between adaptive feature learning and provable improvements in generalization of spectral algorithms. We demonstrate the generality of the ESD framework by extending it to linear models and RKHS regression, and we support the theory with numerical experiments. This framework provides a novel perspective on generalization beyond traditional fixed-kernel theories.
[55]
arXiv:2509.20323
(cross-list from cs.LG)
[pdf, other]
Title:
A Recovery Guarantee for Sparse Neural Networks
Sara Fridovich-Keil, Mert Pilanci
Comments:
Code is available at this https URL
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
We prove the first guarantees of sparse recovery for ReLU neural networks, where the sparse network weights constitute the signal to be recovered. Specifically, we study structural properties of the sparse network weights for two-layer, scalar-output networks under which a simple iterative hard thresholding algorithm recovers these weights exactly, using memory that grows linearly in the number of nonzero weights. We validate this theoretical result with simple experiments on recovery of sparse planted MLPs, MNIST classification, and implicit neural representations. Experimentally, we find performance that is competitive with, and often exceeds, a high-performing but memory-inefficient baseline based on iterative magnitude pruning.
Replacement submissions (showing 36 of 36 entries)
[56]
arXiv:2109.05227
(replaced)
[pdf, html, other]
Title:
Factor and Idiosyncratic VAR Volatility Matrix Models for Heavy-Tailed High-Frequency Financial Observations
Minseok Shin, Donggyu Kim, Yazhen Wang, Jianqing Fan
Comments:
62 pages, 7 figures
Subjects:
Methodology (stat.ME)
This paper introduces a novel process for both factor and idiosyncratic volatility matrices whose eigenvalues follow the vector auto-regressive (VAR) model. We call it the factor and idiosyncratic VAR (FIVAR) model. The FIVAR model accounts for the dynamics of the factor and idiosyncratic volatilities and includes many parameters. In addition, many empirical studies have shown that high-frequency stock returns and volatilities often exhibit heavy tails. To handle these two problems simultaneously, we propose a penalized optimization procedure with a truncation scheme for parameter estimation. We apply the proposed parameter estimation procedure to predicting large volatility matrices and establish its asymptotic properties.
[57]
arXiv:2202.08419
(replaced)
[pdf, other]
Title:
High-Dimensional Time-Varying Coefficient Estimation in Diffusion Models
Donggyu Kim, Minseog Oh, Minseok Shin
Comments:
66 pages, 9 figures
Subjects:
Methodology (stat.ME)
In this paper, we develop a novel high-dimensional time-varying coefficient estimation method, based on high-dimensional Ito diffusion processes. To account for high-dimensional time-varying coefficients, we first estimate local (or instantaneous) coefficients using a time-localized Dantzig selection scheme under a sparsity condition, which results in biased local coefficient estimators due to the regularization. To handle the bias, we propose a debiasing scheme, which provides well-performing unbiased local coefficient estimators. With the unbiased local coefficient estimators, we estimate the integrated coefficient, and to further account for the sparsity of the coefficient process, we apply thresholding schemes. We call this Thresholding dEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED estimator. In the empirical analysis, we apply the TED procedure to analyzing high-dimensional factor models using high-frequency data.
[58]
arXiv:2210.06140
(replaced)
[pdf, html, other]
Title:
Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies
Zhanyu Wang, Guang Cheng, Jordan Awan
Comments:
22 pages before appendices and references. 50 pages total
Subjects:
Machine Learning (stat.ML); Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)
Differentially private (DP) mechanisms protect individual-level information by introducing randomness into the statistical analysis procedure. Despite the availability of numerous DP tools, there remains a lack of general techniques for conducting statistical inference under DP. We examine a DP bootstrap procedure that releases multiple private bootstrap estimates to infer the sampling distribution and construct confidence intervals (CIs). Our privacy analysis presents new results on the privacy cost of a single DP bootstrap estimate, applicable to any DP mechanism, and identifies some misapplications of the bootstrap in the existing literature. For the composition of the DP bootstrap, we present a numerical method to compute the exact privacy cost of releasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP) framework (Dong et al., 2022), we show that the release of $B$ DP bootstrap estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Then, we perform private statistical inference by post-processing the DP bootstrap estimates. We prove that our point estimates are consistent, our standard CIs are asymptotically valid, and both enjoy optimal convergence rates. To further improve the finite performance, we use deconvolution with DP bootstrap estimates to accurately infer the sampling distribution. We derive CIs for tasks such as population mean estimation, logistic regression, and quantile regression, and we compare them to existing methods using simulations and real-world experiments on 2016 Canada Census data. Our private CIs achieve the nominal coverage level and offer the first approach to private inference for quantile regression.
[59]
arXiv:2302.13658
(replaced)
[pdf, html, other]
Title:
Robust High-Dimensional Time-Varying Coefficient Estimation
Minseok Shin, Donggyu Kim
Comments:
61 pages, 7 figures
Subjects:
Methodology (stat.ME)
In this paper, we develop a novel high-dimensional coefficient estimation procedure based on high-frequency data. Unlike usual high-dimensional regression procedures such as LASSO, we additionally handle the heavy-tailedness of high-frequency observations as well as time variations of coefficient processes. Specifically, we employ the Huber loss and a truncation scheme to handle heavy-tailed observations, while $\ell_{1}$-regularization is adopted to overcome the curse of dimensionality. To account for the time-varying coefficient, we estimate local coefficients which are biased due to the $\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we propose a debiasing scheme to enjoy the law of large numbers property and employ a thresholding scheme to further accommodate the sparsity of the coefficients. We call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show that the RED-LASSO estimator can achieve a near-optimal convergence rate. In the empirical study, we apply the RED-LASSO procedure to the high-dimensional integrated coefficient estimation using high-frequency trading data.
[60]
arXiv:2304.12414
(replaced)
[pdf, html, other]
Title:
Bayesian Geostatistics Using Predictive Stacking
Lu Zhang, Wenpin Tang, Sudipto Banerjee
Comments:
64 pages, 30 figures. To be published in Journal of the American Statistical Association (Theory and Methods)
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Computation (stat.CO)
We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.
[61]
arXiv:2305.15671
(replaced)
[pdf, html, other]
Title:
Matrix Autoregressive Model with Vector Time Series Covariates for Spatio-Temporal Data
Hu Sun, Zuofeng Shang, Yang Chen
Subjects:
Methodology (stat.ME)
We develop a new methodology for forecasting matrix-valued time series with historical matrix data and auxiliary vector time series data. We focus on a time series of matrices defined on a static 2-D spatial grid and an auxiliary time series of non-spatial vectors. The proposed model, Matrix AutoRegression with Auxiliary Covariates (MARAC), contains an autoregressive component for the historical matrix predictors and an additive component that maps the auxiliary vector predictors to a matrix response via tensor-vector product. The autoregressive component adopts a bi-linear transformation framework following Chen et al. (2021), significantly reducing the number of parameters. The auxiliary component posits that the tensor coefficient, which maps non-spatial predictors to a spatial response, contains slices of spatially smooth matrix coefficients that are discrete evaluations of smooth functions on a spatial grid from a Reproducing Kernel Hilbert Space (RKHS). We propose to estimate the model parameters under a penalized maximum likelihood estimation framework coupled with an alternating minimization algorithm. We establish the joint asymptotics of the autoregressive and tensor parameters under fixed and high-dimensional regimes. Extensive simulations and a geophysical application for forecasting the global Total Electron Content (TEC) are conducted to validate the performance of MARAC.
[62]
arXiv:2309.08808
(replaced)
[pdf, other]
Title:
Adaptive Neyman Allocation
Jinglong Zhao
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. We demonstrate the effectiveness of our adaptive Neyman allocation algorithm using both online A/B testing data from a social media site and synthetic data.
[63]
arXiv:2405.05597
(replaced)
[pdf, other]
Title:
The empirical copula process in high dimensions: Stute's representation and applications
Axel Bücher, Cambyse Pakzad
Comments:
35 pages, including an appendix of 7 pages
Subjects:
Statistics Theory (math.ST)
The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size. Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins. The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence. Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes.
[64]
arXiv:2406.16523
(replaced)
[pdf, html, other]
Title:
YEAST: Yet Another Sequential Test
Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo
Comments:
18 pages, 1 figure, 7 tables
Subjects:
Methodology (stat.ME)
Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.
[65]
arXiv:2407.13446
(replaced)
[pdf, html, other]
Title:
Subsampled One-Step Estimation for Fast Statistical Inference
Miaomiao Su, Ruoyu Wang
Subjects:
Methodology (stat.ME)
Subsampling is an effective approach to alleviate the computational burden associated with large-scale datasets. Nevertheless, existing subsampling estimators incur a substantial loss in estimation efficiency compared to estimators based on the full dataset. Specifically, the convergence rate of existing subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$, where $n$ and $N$ denote the subsample and full data sizes, respectively. This paper proposes a subsampled one-step (SOS) method to mitigate the estimation efficiency loss utilizing the asymptotic expansions of the subsampling and full-data estimators. The resulting SOS estimator is computationally efficient and achieves a fast convergence rate of $\max\{n^{-1}, N^{-1/2}\}$ rather than $n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator, which can be non-normal in general, and construct confidence intervals on top of the asymptotic distribution. Furthermore, we prove that the SOS estimator is asymptotically normal and equivalent to the full data-based estimator when $n / \sqrt{N} \to \infty$.Simulation studies and real data analyses were conducted to demonstrate the finite sample performance of the SOS estimator. Numerical results suggest that the SOS estimator is almost as computationally efficient as the uniform subsampling estimator while achieving similar estimation efficiency to the full data-based estimator.
[66]
arXiv:2408.14710
(replaced)
[pdf, html, other]
Title:
The role of assignment in defining and identifying causal effects in randomized trials
Issa J. Dahabreh, Lawson Ung, Miguel A. Hernán, Yu-Han Chiu
Subjects:
Methodology (stat.ME)
In randomized trials, the per-protocol effect, that is, the effect of being assigned a treatment strategy and receiving treatment according to the assigned strategy, is sometimes thought to reflect the effect of the treatment strategy itself, without intervention on assignment. Here, we argue by example that this is not necessarily the case. We examine a causal structure for a randomized trial where these two causal estimands -- the per-protocol effect and the effect of the treatment strategy -- are not equal, and where their corresponding identifying observed data functionals are not the same, but both require information on assignment for identification. Our example highlights the conceptual difference between the per-protocol effect and the effect of the treatment strategy, the conditions under which these causal estimands are equal, and suggests that in some cases their identification requires information on assignment, even when assignment is randomized. Furthermore, both per-protocol effects and effects of treatment may be unidentifiable without information on treatment assignment, unless one makes additional assumptions -- informally, that assignment does not affect the outcome except through treatment (i.e., an exclusion-restriction assumption), and that assignment is not a confounder of the treatment-outcome association conditional on other variables in the analysis. Our analyses suggest a need to more clearly define the role of assignment when specifying causal effects of interest in randomized trials, which has implications for identification, analysis methods, and the interpretation of trial results.
[67]
arXiv:2411.02225
(replaced)
[pdf, html, other]
Title:
Sparse Max-Affine Regression
Haitham Kanj, Seonho Kim, Kiryung Lee
Subjects:
Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)
This paper presents Sparse Gradient Descent as a solution for variable selection in convex piecewise linear regression, where the model is given as the maximum of $k$-affine functions $ x \mapsto \max_{j \in [k]} \langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$. Here, $\{ a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth weight vectors and intercepts. A non-asymptotic local convergence analysis is provided for Sp-GD under sub-Gaussian noise when the covariate distribution satisfies the sub-Gaussianity and anti-concentration properties. When the model order and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given $\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where $\sigma_z^2$ denotes the noise variance. This also implies the exact parameter recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. The proposed initialization scheme uses sparse principal component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$, then applies an $r$-covering search to estimate the model parameters. A non-asymptotic analysis is presented for this initialization scheme when the covariates and noise samples follow Gaussian distributions. When the model order and parameters are fixed, this initialization scheme provides an $\epsilon$-accurate estimate given $\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$ observations. A new transformation named Real Maslov Dequantization (RMD) is proposed to transform sparse generalized polynomials into sparse max-affine models. The error decay rate of RMD is shown to be exponentially small in its temperature parameter. Furthermore, theoretical guarantees for Sp-GD are extended to the bounded noise model induced by RMD. Numerical Monte Carlo results corroborate theoretical findings for Sp-GD and the initialization scheme.
[68]
arXiv:2502.01512
(replaced)
[pdf, html, other]
Title:
Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices
Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.
[69]
arXiv:2502.03969
(replaced)
[pdf, html, other]
Title:
Spectrally Deconfounded Random Forests
Markus Ulmer, Cyrill Scheidegger, Peter Bühlmann
Subjects:
Computation (stat.CO)
We introduce a modification of Random Forests to estimate functions when unobserved confounding variables are present. The technique is tailored for high-dimensional settings with many observed covariates. We use spectral deconfounding techniques to minimize a deconfounded version of the least squares objective, resulting in the Spectrally Deconfounded Random Forests (SDForests). We show how the omitted variable bias gets small given some assumptions. We compare the performance of SDForests to classical Random Forests in a simulation study and a semi-synthetic setting using single-cell gene expression data. Empirical results suggest that SDForests outperform classical Random Forests in estimating the direct regression function, even if the theoretical assumptions, requiring linear and dense confounding, are not perfectly met, and that SDForests have comparable performance in the non-confounded case.
[70]
arXiv:2502.13570
(replaced)
[pdf, other]
Title:
A Scalable Nyström-Based Kernel Two-Sample Test with Permutations
Antoine Chatalic, Marco Letizia, Nicolas Schreuder, Lorenzo Rosasco
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nyström approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing applicability to realistic scientific data.
[71]
arXiv:2502.20123
(replaced)
[pdf, other]
Title:
Stein's unbiased risk estimate and Hyvärinen's score matching
Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)
Given a collection of observed signals corrupted with Gaussian noise, how can we learn to optimally denoise them? This fundamental problem arises in both empirical Bayes and generative modeling. In empirical Bayes, the predominant approach is via nonparametric maximum likelihood estimation (NPMLE), while in generative modeling, score matching (SM) methods have proven very successful. In our setting, Hyvärinen's implicit SM is equivalent to another classical idea from statistics -- Stein's Unbiased Risk Estimate (SURE). Revisiting SURE minimization, we establish, for the first time, that SURE achieves nearly parametric rates of convergence of the regret in the classical empirical Bayes setting with homoscedastic noise. We also prove that SURE-training can achieve fast rates of convergence to the oracle denoiser in a commonly studied misspecified model. In contrast, the NPMLE may not even converge to the oracle denoiser under misspecification of the class of signal distributions. We show how to practically implement our method in settings involving heteroscedasticity and side-information, such as in an application to the estimation of economic mobility in the Opportunity Atlas. Our empirical results demonstrate the superior performance of SURE-training over NPMLE under misspecification. Collectively, our findings advance SURE/SM as a strong alternative to the NPMLE for empirical Bayes problems in both theory and practice.
[72]
arXiv:2503.20940
(replaced)
[pdf, html, other]
Title:
A Restricted Latent Class Hidden Markov Model for Polytomous Responses, Polytomous Attributes, and Covariates: Identifiability and Application
Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers
Comments:
57 pages, 3 figures, 31 tables; literature review expanded, notation and identifiability proof revised, additional simulation study and data application added
Subjects:
Methodology (stat.ME)
We introduce a restricted latent class exploratory model for longitudinal data with ordinal attributes and respondent-specific covariates. Responses follow a time inhomogeneous hidden Markov model where the probability of a particular latent state at a time point is conditional on values at the previous time point of the respondent's covariates and latent state. We prove that the model is identifiable, state a Bayesian formulation, and demonstrate its efficacy in a variety of scenarios through two simulation studies. We apply the model to response data from a mathematics examination, comparing the results to a previously published confirmatory analysis, and also apply it to emotional state response data which was measured over a several-day period.
[73]
arXiv:2504.13110
(replaced)
[pdf, html, other]
Title:
Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time
Margalit Glasgow, Denny Wu, Joan Bruna
Comments:
72 pages
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We study the approximation gap between the dynamics of a polynomial-width neural network and its infinite-width counterpart, both trained using projected gradient descent in the mean-field scaling regime. We demonstrate how to tightly bound this approximation gap through a differential equation governed by the mean-field dynamics. A key factor influencing the growth of this ODE is the local Hessian of each particle, defined as the derivative of the particle's velocity in the mean-field dynamics with respect to its position. We apply our results to the canonical feature learning problem of estimating a well-specified single-index model; we permit the information exponent to be arbitrarily large, leading to convergence times that grow polynomially in the ambient dimension $d$. We show that, due to a certain ``self-concordance'' property in these problems -- where the local Hessian of a particle is bounded by a constant times the particle's velocity -- polynomially many neurons are sufficient to closely approximate the mean-field dynamics throughout training.
[74]
arXiv:2504.19018
(replaced)
[pdf, html, other]
Title:
Finite-Sample Properties of Generalized Ridge Estimators in Nonlinear Models
Masamune Iwasawa
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
This paper addresses the longstanding challenge of analyzing the mean squared error (MSE) of ridge-type estimators in nonlinear models, including duration, Poisson, and multinomial choice models, where theoretical results have been scarce. Using a finite-sample approximation technique from the econometrics literature, we derive new results showing that the generalized ridge maximum likelihood estimator (MLE) with a sufficiently small penalty achieves lower finite-sample MSE for both estimation and prediction than the conventional MLE, regardless of whether the hypotheses incorporated in the penalty are valid. A key theoretical contribution is to demonstrate that generalized ridge estimators generate a variance-bias trade-off in the first-order MSE of nonlinear likelihood-based models -- a feature absent for the conventional MLE -- which enables ridge-type estimators to attain smaller MSE when the penalty is properly selected. Extensive simulations and an empirical application to the estimation of marginal mean and quantile treatment effects further confirm the superior performance and practical relevance of the proposed method.
[75]
arXiv:2505.04773
(replaced)
[pdf, html, other]
Title:
Estimating the Heritability of Longitudinal Rate-of-Change: Genetic Insights into PSA Velocity in Prostate Cancer-Free Individuals
Pei Zhang, Xiaoyu Wang, Jianxin Shi, Paul S. Albert
Subjects:
Applications (stat.AP); Computation (stat.CO)
Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.
[76]
arXiv:2505.13364
(replaced)
[pdf, html, other]
Title:
Modeling Innovation Ecosystem Dynamics through Interacting Reinforced Bernoulli Processes
Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti, Federico Nutarelli
Subjects:
Applications (stat.AP); Statistics Theory (math.ST)
Understanding how capabilities evolve into core capabilities-and how core capabilities may ossify into rigidities-is central to innovation strategy (Leonard-Barton 1992, Teece 2009). A major challenge in formalizing this process lies in the interactive nature of innovation: successes in one domain often reshape others, endogenizing specialization and complicating isolated modeling. This is especially true in ecosystems where firm capabilities and innovation outcomes hinge on managing interdependencies and complementarities (Jacobides, Cennamo and Gawer 2018, 2024).
To address this, we propose a novel formal model based on interacting reinforced Bernoulli processes. This framework captures how patent successes propagate across technological categories and how these categories co-evolve. The model is able to jointly account for several stylized facts in the empirical innovation literature, including sublinear success growth (successprobability decay), convergence of success shares across fields, and diminishing cross-category correlations over time.
Empirical validation using GLOBAL PATSTAT (1980-2018) supports the theoretical predictions. We estimate the structural parameters of the interaction matrix and we also propose a statistical procedure to make inference on the intensity of cross-category interactions under the mean-field assumption.
By endogenizing technological specialization, our model provides a strategic tool for policymakers and managers, supporting decision-making in complex, co-evolving innovation ecosystems-where targeted interventions can produce systemic effects, influencing competitive trajectories and shaping long-term patterns of specialization.
[77]
arXiv:2507.02240
(replaced)
[pdf, html, other]
Title:
A Variance Decomposition Approach to Inconclusives in Forensic Black Box Studies
Amanda Luby, Joseph B. Kadane
Subjects:
Applications (stat.AP)
In the US, `black box' studies are increasingly being used to estimate the error rate of forensic disciplines. A sample of forensic examiner participants are asked to evaluate a set of items whose source is known to the researchers but not to the participants. Participants are asked to make a source determination (typically an identification, exclusion, or some kind of inconclusive). We study inconclusives in two black box studies, one on fingerprints and one on bullets. Rather than treating all inconclusive responses as functionally correct (as is the practice in reported error rates in the two studies we address), irrelevant to reported error rates (as some would do), or treating them all as potential errors (as others would do), we propose that the overall pattern of inconclusives in a particular black box study can shed light on the proportion of inconclusives that are due to examiner variability. Raw item and examiner variances are computed, and compared with the results of a logistic regression model that takes account of which items were addressed by which examiner. The error rates reported in black box studies are substantially smaller than ``failure rate" analyses that take inconclusives into account. The magnitude of this difference is highly dependent on the particular study at hand.
[78]
arXiv:2509.02871
(replaced)
[pdf, html, other]
Title:
Learning from geometry-aware near-misses to real-time COR: A spatiotemporal grouped random GEV framework
Mohammad Anis, Yang Zhou, Dominique Lord
Comments:
13 figures, 8 Tables
Subjects:
Applications (stat.AP); Computation (stat.CO)
Real-time prediction of corridor-level crash occurrence risk (COR) remains challenging, as existing near-miss based extreme value models oversimplify collision geometry, exclude vehicle-infrastructure (V-I) interactions, and inadequately capture spatial heterogeneity in vehicle dynamics. This study introduces a geometry-aware two-dimensional time-to-collision (2D-TTC) indicator within a Hierarchical Bayesian spatiotemporal grouped random parameter (HBSGRP) framework using a non-stationary univariate generalized extreme value (UGEV) model to estimate short-term COR in urban corridors. High-resolution trajectories from the Argoverse-2 dataset, covering 28 locations along Miami's Biscayne Boulevard, were analyzed to extract extreme V-V and V-I near misses. The model incorporates dynamic variables and roadway features as covariates, with partial pooling across locations to address unobserved heterogeneity. Results show that the HBSGRP-UGEV framework outperforms fixed-parameter alternatives, reducing DIC by up to 7.5% for V-V and 3.1% for V-I near-misses. Predictive validation using ROC-AUC confirms strong performance: 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections. Model interpretation reveals that relative speed and distance dominate V-V risks at intersections and segments, with deceleration critical in segments, while V-I risks are driven by speed, boundary proximity, and steering/heading adjustments. These findings highlight the value of a statistically rigorous, geometry-sensitive, and spatially adaptive modeling approach for proactive corridor-level safety management, supporting real-time interventions and long-term design strategies aligned with Vision Zero.
[79]
arXiv:2509.08647
(replaced)
[pdf, other]
Title:
Assessing Bias in the Variable Bandpass Periodic Block Bootstrap Method
Yanan Sun, Eric Rose, Kai Zhang, Edward Valachovic
Comments:
27 pages, 8 figures, 1 table
Subjects:
Methodology (stat.ME)
The Variable Bandpass Periodic Block Bootstrap(VBPBB) is an innovative method for time series with periodically correlated(PC) components. This method applies bandpass filters to extract specific PC components from datasets, effectively eliminating unwanted interference such as noise. It then bootstraps the PC components, maintaining their correlation structure while resampling and enabling a clearer analysis of the estimation of the statistical properties of periodic patterns in time series data. While its efficiency has been demonstrated in environmental and epidemiological research, the theoretical properties of VBPBB, particularly regarding its bias of the estimated sampling distributions, remain unexamined. This study investigates issues regarding biases in VBPBB, including overall mean bias and pointwise mean bias, across a range of time series models of varying complexity, all of which exhibit periodic components. Using the R programming language, we simulate various PC time series and apply VBPBB to assess its bias under different conditions. Our findings provide key insights into the validity of VBPBB for periodic time series analysis and offer practical recommendations for its implementation, as well as directions for future theoretical advancements.
[80]
arXiv:2509.13886
(replaced)
[pdf, html, other]
Title:
Three Distributional Approaches for PM10 Assessment in Northern Italy
Marco F. De Sanctis, Andrea Gilardi, Giacomo Milan, Laura M. Sangalli, Francesca Ieva, Piercesare Secchi
Subjects:
Applications (stat.AP); Methodology (stat.ME)
We propose three spatial methods for estimating the full probability distribution of PM10 concentrations, with the ultimate goal of assessing air quality in Northern Italy. Moving beyond spatial averages and simple indicators, we adopt a distributional perspective to capture the complex variability of pollutant concentrations across space. The first proposed approach predicts class-based compositions via Fixed Rank Kriging; the second estimates multiple, non-crossing quantiles through a spatial regression with differential regularization; the third directly reconstructs full probability densities leveraging on both Fixed Rank Kriging and multiple quantiles spatial regression within a Simplicial Principal Component Analysis framework. These approaches are applied to daily PM10 measurements, collected from 2018 to 2022 in Northern Italy, to estimate spatially continuous distributions and to identify regions at risk of regulatory exceedance. The three approaches exhibit localized differences, revealing how modeling assumptions may influence the prediction of fine-scale pollutant concentration patterns. Nevertheless, they consistently agree on the broader spatial patterns of pollution. This general agreement supports the robustness of a distributional approach, which offers a comprehensive and policy-relevant framework for assessing air quality and regulatory exceedance risks.
[81]
arXiv:2006.14061
(replaced)
[pdf, html, other]
Title:
Beyond Grids: Multi-objective Bayesian Optimization With Adaptive Discretization
Andi Nika, Sepehr Elahi, Çağın Ararat, Cem Tekin
Subjects:
Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
We consider the problem of optimizing a vector-valued objective function $\boldsymbol{f}$ sampled from a Gaussian Process (GP) whose index set is a well-behaved, compact metric space $({\cal X},d)$ of designs. We assume that $\boldsymbol{f}$ is not known beforehand and that evaluating $\boldsymbol{f}$ at design $x$ results in a noisy observation of $\boldsymbol{f}(x)$. Since identifying the Pareto optimal designs via exhaustive search is infeasible when the cardinality of ${\cal X}$ is large, we propose an algorithm, called Adaptive $\boldsymbol{\epsilon}$-PAL, that exploits the smoothness of the GP-sampled function and the structure of $({\cal X},d)$ to learn fast. In essence, Adaptive $\boldsymbol{\epsilon}$-PAL employs a tree-based adaptive discretization technique to identify an $\boldsymbol{\epsilon}$-accurate Pareto set of designs in as few evaluations as possible. We provide both information-type and metric dimension-type bounds on the sample complexity of $\boldsymbol{\epsilon}$-accurate Pareto set identification. We also experimentally show that our algorithm outperforms other Pareto set identification methods.
[82]
arXiv:2401.07876
(replaced)
[pdf, other]
Title:
Characterization of the asymptotic behavior of $U$-statistics on row-column exchangeable matrices
Tâm Le Minh
Subjects:
Probability (math.PR); Statistics Theory (math.ST)
We consider $U$-statistics on row-column exchangeable matrices, arrays invariant to separate permutations of rows and columns and common in bipartite data. Under the standard dissociation assumption, we develop a graph-indexed analogue of the Hoeffding decomposition tailored to RCE dependence. We present a new decomposition based on orthogonal projections onto probability spaces generated by sets of Aldous-Hoover-Kallenberg variables. These sets are indexed by bipartite graphs, enabling the application of graph-theoretic concepts to describe the decomposition. This framework provides new insights into the characterization of $U$-statistics on row-column exchangeable matrices, particularly their asymptotic behavior, including in degenerate cases. Notably, the limit distribution depends only on specific terms in the decomposition, corresponding to non-zero components indexed by the smallest graphs, namely the principal support graphs. We show that the asymptotic behavior of a $U$-statistic is characterized by the properties of its principal support graphs. The number of nodes in these graphs (the principal degree) dictates the convergence rate to the limit distribution, with degeneracy occurring if and only if this number is strictly greater than 1. Furthermore, when the principal support graphs are connected, the limit distribution is Gaussian, even in degenerate cases. Applications to network analysis illustrate these findings.
[83]
arXiv:2403.15220
(replaced)
[pdf, html, other]
Title:
Modelling with Sensitive Variables
Felix Chan, Laszlo Matyas, Agoston Reguly
Comments:
31 pages, 2 tables, 2 figures
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.
[84]
arXiv:2410.09296
(replaced)
[pdf, html, other]
Title:
The 2020 United States Decennial Census Is More Private Than You (Might) Think
Buxin Su, Weijie J. Su, Chendi Wang
Subjects:
Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Applications (stat.AP); Machine Learning (stat.ML)
The U.S. Decennial Census serves as the foundation for many high-profile policy decision-making processes, including federal funding allocation and redistricting. In 2020, the Census Bureau adopted differential privacy to protect the confidentiality of individual responses through a disclosure avoidance system that injects noise into census data tabulations. The Bureau subsequently posed an open question: Could stronger privacy guarantees be obtained for the 2020 U.S. Census compared to their published guarantees, or equivalently, had the privacy budgets been fully utilized?
In this paper, we address this question affirmatively by demonstrating that the 2020 U.S. Census provides significantly stronger privacy protections than its nominal guarantees suggest at each of the eight geographical levels, from the national level down to the block level. This finding is enabled by our precise tracking of privacy losses using $f$-differential privacy, applied to the composition of private queries across these geographical levels. Our analysis reveals that the Census Bureau introduced unnecessarily high levels of noise to meet the specified privacy guarantees for the 2020 Census. Consequently, we show that noise variances could be reduced by $15.08\%$ to $24.82\%$ while maintaining nearly the same level of privacy protection for each geographical level, thereby improving the accuracy of privatized census statistics. We empirically demonstrate that reducing noise injection into census statistics mitigates distortion caused by privacy constraints in downstream applications of private census data, illustrated through a study examining the relationship between earnings and education.
[85]
arXiv:2503.06464
(replaced)
[pdf, other]
Title:
Detecting Correlation Efficiently in Stochastic Block Models: Breaking Otter's Threshold in the Entire Supercritical Regime
Guanyi Chen, Jian Ding, Shuyang Gong, Zhangsong Li
Comments:
This substantially improves the result in an earlier version. 98 pages, 13 figures
Subjects:
Data Structures and Algorithms (cs.DS); Probability (math.PR); Statistics Theory (math.ST)
Consider a pair of sparse correlated stochastic block models $\mathcal S(n,\tfrac{\lambda}{n},\epsilon;s)$ subsampled from a common parent stochastic block model with two symmetric communities, average degree $\lambda=O(1)$, divergence parameter $\epsilon\in (0,1)$ and subsampling probability $s$. For all $\epsilon\in(0,1)$ and $\Delta>0$, we construct a statistic based on the combination of two low-degree polynomials and show that there exists a sufficiently small constant $\delta=\delta(\epsilon,\lambda,\Delta)>0$ such that if $\epsilon^2 \lambda s>1+\Delta$ and $s>\sqrt{\alpha}-\delta$ where $\alpha\approx 0.338$ is Otter's constant, this statistic can distinguish this model and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n},\epsilon)$ with probability $1-o(1)$. We also provide an efficient algorithm that approximates this statistic in polynomial time.
The crux of our statistic's construction lies in a carefully curated family of multigraphs called \emph{decorated trees}, which enables effective aggregation of the community signal and graph correlation by leveraging the counts of the same decorated tree while suppressing the undesirable correlations among counts of different decorated trees. We believe such construction may be of independent interest.
[86]
arXiv:2504.21156
(replaced)
[pdf, html, other]
Title:
Publication Design with Incentives in Mind
Ravi Jagadeesan, Davide Viviano
Subjects:
Econometrics (econ.EM); Theoretical Economics (econ.TH); Statistics Theory (math.ST)
The publication process both determines which research receives the most attention, and influences the supply of research through its impact on researchers' private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs. We illustrate the implications of our model in an application to medical studies.
[87]
arXiv:2505.22182
(replaced)
[pdf, html, other]
Title:
Post-processing of wind gusts from COSMO-REA6 with a spatial Bayesian hierarchical extreme value model
Philipp Ertz, Petra Friederichs
Comments:
43 Pages, 17 figures. This manuscript has been submitted to Advances in Statistical Climatology, Meteorology and Oceanography (ASCMO) and is currently under review
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP)
The aim of this study is to provide a probabilistic gust analysis for the region of Germany that is calibrated with station observations and with an interpolation to unobserved locations. To this end, we develop a spatial Bayesian hierarchical model (BHM) for the post-processing of surface maximum wind gusts from the COSMO-REA6 reanalysis. Our approach uses a non-stationary extreme value distribution for the gust observations, with parameters that vary according to a linear model using COSMO-REA6 predictor variables. To capture spatial patterns in surface wind gust behavior, the regression coefficients are modeled as 2-dimensional Gaussian random fields with a constant mean and an isotropic covariance function that depends on the distance between locations. In addition, we include an elevation offset in the distance metric for the covariance function to account for the topography. This allows us to include data from mountaintop stations in the training process. The training of the BHM is carried out with an independent data set from which the data at the station to be predicted are excluded. We evaluate the spatial prediction performance at the withheld station using Brier score and quantile score, including their decomposition, and compare the performance of our BHM to climatological forecasts and a non-hierarchical, spatially constant baseline model. This is done for 109 weather stations in Germany. Compared to the spatially constant baseline model, the spatial BHM significantly improves the estimation of local gust parameters. It shows up to 5 % higher skill for prediction quantiles and provides a particularly improved skill for extreme wind gusts. In addition, the BHM improves the prediction of threshold levels at most of the stations. Although a spatially constant approach already provides high skill, our BHM further improves predictions and improves spatial consistency.
[88]
arXiv:2507.15809
(replaced)
[pdf, html, other]
Title:
Diffusion models for multivariate subsurface generation and efficient probabilistic inversion
Roberto Miele, Niklas Linde
Comments:
35 pages, 16 figures, reviewed version. Changes include the following. General revision of the text to improve the language clarity and correct minor errors; Ip values are now reported in SI (Pa.s/m); addition of three bibliographic references and correction of one reference wrongly reported (Yismaw et al., 2024); standardization of the figures' labels
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Geophysics (physics.geo-ph); Applications (stat.AP)
Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.
[89]
arXiv:2509.05760
(replaced)
[pdf, html, other]
Title:
Rethinking Beta: A Causal Take on CAPM
Naftali Cohen
Subjects:
Theoretical Economics (econ.TH); Pricing of Securities (q-fin.PR); Statistical Finance (q-fin.ST); Applications (stat.AP)
The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.
[90]
arXiv:2509.17729
(replaced)
[pdf, other]
Title:
A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis
Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.
[91]
arXiv:2509.19189
(replaced)
[pdf, html, other]
Title:
Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws
Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu
Comments:
52 pages, accepted by NeurIPS 2025 as a spotlight paper
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.
Total of 91 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack