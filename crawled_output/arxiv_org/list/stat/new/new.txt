Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Monday, 22 September 2025
Total of 73 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 29 of 29 entries)
[1]
arXiv:2509.15244
[pdf, html, other]
Title:
Kernel Model Validation: How To Do It, And Why You Should Care
Carlo Graziani, Marieme Ngom
Comments:
12 pages, 6 figures. To appear in ITEA Journal of Test and Evaluation, Vol. 46, Issue 3, September 2025
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)
Gaussian Process (GP) models are popular tools in uncertainty quantification (UQ) because they purport to furnish functional uncertainty estimates that can be used to represent model uncertainty. It is often difficult to state with precision what probabilistic interpretation attaches to such an uncertainty, and in what way is it calibrated. Without such a calibration statement, the value of such uncertainty estimates is quite limited and qualitative. We motivate the importance of proper probabilistic calibration of GP predictions by describing how GP predictive calibration failures can cause degraded convergence properties in a target optimization algorithm called Targeted Adaptive Design (TAD). We discuss the interpretation of GP-generated uncertainty intervals in UQ, and how one may learn to trust them, through a formal procedure for covariance kernel validation that exploits the multivariate normal nature of GP predictions. We give simple examples of GP regression misspecified 1-dimensional models, and discuss the situation with respect to higher-dimensional models.
[2]
arXiv:2509.15280
[pdf, html, other]
Title:
A Latent Principal Stratification Method to Address One-Sided Cluster and Individual Noncompliance in Cluster RCTs
Anthony Sisti, Ellen McCreedy, Roee Gutman
Subjects:
Applications (stat.AP); Methodology (stat.ME)
In pragmatic cluster randomized controlled trials (PCRCTs), the unit of randomization may be the healthcare provider. In these studies, noncompliance can occur at both the patient and cluster levels. Some studies measure cluster-level implementation using multiple continuous metrics while documenting individual binary compliance. The complier average causal effect estimates the intervention effects among individuals that comply with the assigned intervention. However, it does not account for compliance metrics at the cluster level. When compliance with the intervention is influenced by both providers and individuals, it can be scientifically beneficial to describe the effects of the intervention between all levels of compliance. We propose a Bayesian method for PCRCTs with one-sided binary noncompliance at the individual level and one-sided partial compliance at the cluster level. Our Bayesian model classifies clusters into latent compliance strata based on pretreatment characteristics, partial compliance status, and individual outcomes. Because compliance is only observed in the treatment arm, the method imputes unobserved compliance for control clusters and the individuals within them. This approach estimates finite and super-population estimands within strata defined by both cluster- and individual-level compliance. We apply this method to the METRIcAL trial, a multi-part, pragmatic cluster randomized trial evaluating the effects of a personalized music intervention on agitation in nursing home residents with dementia.
[3]
arXiv:2509.15353
[pdf, html, other]
Title:
On the convergence rate in the central limit theorem for linearly extended negative quadrant dependent random variables and its applications
Mohamed Kaber El Alem, Zohra Guessoum, Abdelkader Tatachak, Ourida Sadki
Comments:
This manuscript has been submitted to the Electronic Journal of Probability
Subjects:
Statistics Theory (math.ST)
In this paper, we establish the convergence rate in central limit theorem (CLT) for linearly extended negative quadrant dependent (LENQD) random variables (rv's). Under some weak conditions, the rate of normal approximation is shown as $O(n^{-1/9})$. As an application, the convergence rate in CLT of the wavelet estimator for the nonparametric regression model with LENQD errors is presented as $O(n^{-1/9})$. The performance of the main results is illustrated through a simulation study based on a real dataset.
[4]
arXiv:2509.15359
[pdf, html, other]
Title:
Bayesian Mixture Models for Heterogeneous Extremes
Viviana Carcaiso, Miguel de Carvalho, Ilaria Prosdocimi, Isadora Antoniano-Villalobos
Subjects:
Methodology (stat.ME)
The conventional use of the Generalized Extreme Value (GEV) distribution to model block maxima may be inappropriate when extremes are actually structured into multiple heterogeneous groups. This can result in inaccurate risk estimation of extreme events based on return levels. In this work, we propose a novel approach for describing the behavior of extreme values in the presence of such heterogeneity. Rather than defaulting to the GEV distribution simply because it arises as a theoretical limit, we show that alternative block-maxima-based models can also align with the extremal types theorem while providing improved robustness and flexibility in practice. Our formulation leads us to a mixture model that has a Bayesian nonparametric interpretation as a Dirichlet process mixture of GEV distributions. The use of an infinite number of components enables the characterization of every possible block behavior, while at the same time defining similarities between observations based on their extremal behavior. By employing a Dirichlet process prior on the mixing measure, we can capture the complex structure of the data without the need to pre-specify the number of mixture components. The application of the proposed model is illustrated using both simulated and real-world data.
[5]
arXiv:2509.15371
[pdf, html, other]
Title:
KoMbine: Propagating Statistical and Systematic Errors to Kaplan--Meier Curves
Jeffrey Roskes
Comments:
submitted to JSS
Subjects:
Methodology (stat.ME); Quantitative Methods (q-bio.QM)
Kaplan--Meier curves are widely used in medical research to evaluate the performance of biomarkers and predict patient outcomes. These curves are often shown without error bands, and even when error bands are provided, they typically only account for the statistical uncertainty resulting from the finite number of patients in the study. In reality, other sources of uncertainty affect the measurements as well. As datasets grow, the statistical uncertainty on the number of patients no longer dominates the overall uncertainty, and other uncertainties are increasingly important to model. The KoMbine package, developed based on procedures used in particle physics, provides the first method to propagate both statistical and systematic uncertainties through the Kaplan--Meier curve estimation processes.
[6]
arXiv:2509.15379
[pdf, html, other]
Title:
A Single Index Approach to Integrated Species Distribution Modeling for Fisheries Abundance Data
Quan Vu, Francis K. C. Hui, A. H. Welsh, Samuel Muller, Eva Cantoni, Christopher R. Haak
Subjects:
Applications (stat.AP)
In fisheries ecology, species abundance data are often collected by multiple surveys, each with unique characteristics. This article focuses on Atlantic sea scallop abundance data along the northeast coast of the United States, collected from two bottom trawl surveys which cover a larger spatial domain but have low catch efficiency, and a dredge survey which is more efficient but limited to domains where the species are believed to be present. To model such data, integrated species distribution models (ISDMs) have been proposed to incorporate information from multiple surveys, by including common environmental effects along with correlated survey-specific spatial fields. However, while flexible, these ISDMs can be susceptible to overfitting, which can complicate interpretability of the shared environmental effects and potentially lead to poor predictive performance. To overcome these drawbacks, we introduce a novel single index ISDM, built from a single index (with spatial random effects) that represents a latent measure of the true species distribution, and survey-specific catch efficiency functions which map the single index to the survey-specific expected catch. Our results show that the single index ISDM offers more meaningful interpretations of the environmental effects and survey catch efficiency differences, while potentially achieving better predictive performance than existing ISDMs.
[7]
arXiv:2509.15402
[pdf, html, other]
Title:
Joint Learning of Panel VAR models with Low Rank and Sparse Structure
Yuchen Xu, George Michailidis
Comments:
46 pages, 12 figures, 4 tables
Subjects:
Methodology (stat.ME)
Panel vector auto-regressive (VAR) models are widely used to capture the dynamics of multivariate time series across different subpopulations, where each subpopulation shares a common set of variables. In this work, we propose a panel VAR model with a shared low-rank structure, modulated by subpopulation-specific weights, and complemented by idiosyncratic sparse components. To ensure parameter identifiability, we impose structural constraints that lead to a nonsmooth, nonconvex optimization problem. We develop a multi-block Alternating Direction Method of Multipliers (ADMM) algorithm for parameter estimation and establish its convergence under mild regularity conditions. Furthermore, we derive consistency guarantees for the proposed estimators under high-dimensional scaling. The effectiveness of the proposed modeling framework and estimators is demonstrated through experiments on both synthetic data and a real-world neuroscience data set.
[8]
arXiv:2509.15444
[pdf, html, other]
Title:
Leveraging the group structure of hypotheses for more powerful multiple testing with FDR control for the filtered rejection set
Marina Bogomolov, Shinjini Nandi
Comments:
51 pages, 15 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Modern biological studies often involve testing many hypotheses organized in a group or a hierarchical structure, such as a directed acyclic graph (DAG). In these studies, researchers often wish to control the false discovery rate (FDR) after filtering the discoveries to obtain interpretable results. For addressing this goal, Katsevich, Sabatti, and Bogomolov (2023, Journal of the American Statistical Association, 118(541), 165-176) developed a general method, Focused BH, that guarantees FDR control for the filtered rejection set for a pre-specified filter, under certain assumptions. We propose improving the power of Focused BH by adapting it to group or hierarchical structures of hypotheses using data-dependent weights. The general method incorporating such weights is referred to as Weighted Focused BH (WFBH). For DAG-structured hypotheses, we propose a variant of WFBH, which can gain power by being adaptive to the DAG structure, and by exploiting the logical relationships among the hypotheses. We prove that WFBH with weights that were proposed to adapt the Benjamini-Hochberg procedure to different group structures, as well as its proposed variant for testing DAG-structured hypotheses, control the post-filtering FDR under certain assumptions. Through simulations, we demonstrate that the latter variant is robust to deviations from these assumptions and can be considerably more powerful than comparable methods. Finally, we elucidate its practical use by applying it to real datasets from microbiome and gene expression studies.
[9]
arXiv:2509.15480
[pdf, html, other]
Title:
A tree-based kernel for densities and its applications in clustering DNase-seq profiles
Yuliang Xu, Kaixuan Luo, Li Ma
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Modeling multiple sampling densities within a hierarchical framework enables borrowing of information across samples. These density random effects can act as kernels in latent variable models to represent exchangeable subgroups or clusters. A key feature of these kernels is the (functional) covariance they induce, which determines how densities are grouped in mixture models. Our motivating problem is clustering chromatin accessibility profiles from high-throughput DNase-seq experiments to detect transcription factor (TF) binding. TF binding typically produces footprint profiles with spatial patterns, creating long-range dependency across genomic locations. Existing nonparametric hierarchical models impose restrictive covariance assumptions and cannot accommodate such dependencies, often leading to biologically uninformative clusters. We propose a nonparametric density kernel flexible enough to capture diverse covariance structures and adaptive to various spatial patterns of TF footprints. The kernel specifies dyadic tree splitting probabilities via a multivariate logit-normal model with a sparse precision matrix. Bayesian inference for latent variable models using this kernel is implemented through Gibbs sampling with Polya-Gamma augmentation. Extensive simulations show that our kernel substantially improves clustering accuracy. We apply the proposed mixture model to DNase-seq data from the ENCODE project, which results in biologically meaningful clusters corresponding to binding events of two common TFs.
[10]
arXiv:2509.15500
[pdf, html, other]
Title:
Efficient Estimation of Unfactorizable Systematic Uncertainties
Alexis Romero, Kyle Cranmer, Daniel Whiteson
Subjects:
Methodology (stat.ME); High Energy Physics - Experiment (hep-ex)
Accurate assessment of systematic uncertainties is an increasingly vital task in physics studies, where large, high-dimensional datasets, like those collected at the Large Hadron Collider, hold the key to new discoveries. Common approaches to assessing systematic uncertainties rely on simplifications, such as assuming that the impact of the various sources of uncertainty factorizes. In this paper, we provide realistic example scenarios in which this assumption fails. We introduce an algorithm that uses Gaussian process regression to estimate the impact of systematic uncertainties \textit{without} assuming factorization. The Gaussian process models are enhanced with derivative information, which increases the accuracy of the regression without increasing the number of samples. In addition, we present a novel sampling strategy based on Bayesian experimental design, which is shown to be more efficient than random and grid sampling in our example scenarios.
[11]
arXiv:2509.15508
[pdf, html, other]
Title:
Modelling time series of counts with hysteresis
Xintong Ma, Dong Li, Howell Tong
Subjects:
Methodology (stat.ME)
In this article, we propose a novel model for time series of counts called the hysteretic Poisson autoregressive (HPART) model with thresholds by extending the linear Poisson autoregressive model into a nonlinear model. Unlike other approaches that bear the adjective ``hysteretic", our model incorporates a scientifically relevant controlling factor that produces genuine hysteresis. Further, we re-analyse the buffered Poisson autoregressive (BPART) model with thresholds. Although the two models share the convenient piecewise linear structure, the HPART model probes deeper into the intricate dynamics that governs regime switching. We study the maximum likelihood estimation of the parameters of both models and their asymptotic properties in a unified manner, establish tests of separate families of hypotheses for the non-nested case involving a BPART model and a HPART model, and demonstrate the finite-sample efficacy of parameter estimation and tests with Monte Carlo simulation. We showcase advantages of the HPART model with two real time series, including plausible interpretations and improved out-of-sample predictions.
[12]
arXiv:2509.15554
[pdf, html, other]
Title:
Direct Estimation of Eigenvalues of Large Dimensional Precision Matrix
Jie Zhou, Junhao Xie, Jiaqi Chen
Subjects:
Statistics Theory (math.ST); Signal Processing (eess.SP); Applications (stat.AP)
In this paper, we consider directly estimating the eigenvalues of precision matrix, without inverting the corresponding estimator for the eigenvalues of covariance matrix. We focus on a general asymptotic regime, i.e., the large dimensional regime, where both the dimension $N$ and the sample size $K$ tend to infinity whereas their quotient $N/K$ converges to a positive constant. By utilizing tools from random matrix theory, we construct an improved estimator for eigenvalues of precision matrix. We prove the consistency of the new estimator under large dimensional regime. In order to obtain the asymptotic bias term of the proposed estimator, we provide a theoretical result that characterizes the convergence rate of the expected Stieltjes transform (with its derivative) of the spectra of the sample covariance matrix. Using this result, we prove that the asymptotic bias term of the proposed estimator is of order $O(1/K^2)$. Additionally, we establish a central limiting theorem (CLT) to describe the fluctuations of the new estimator. Finally, some numerical examples are presented to validate the excellent performance of the new estimator and to verify the accuracy of the CLT.
[13]
arXiv:2509.15576
[pdf, html, other]
Title:
Subset Selection for Stratified Sampling in Online Controlled Experiments
Haru Momozu, Yuki Uehara, Naoki Nishimura, Koya Ohashi, Deddy Jobson, Yilin Li, Phuong Dinh, Noriyoshi Sukegawa, Yuichi Takano
Comments:
14 pages, 15 figures, The 22nd Pacific Rim International Conference on Artificial Intelligence 2025 (PRICAI 2025)
Subjects:
Computation (stat.CO); Machine Learning (stat.ML)
Online controlled experiments, also known as A/B testing, are the digital equivalent of randomized controlled trials for estimating the impact of marketing campaigns on website visitors. Stratified sampling is a traditional technique for variance reduction to improve the sensitivity (or statistical power) of controlled experiments; this technique first divides the population into strata (homogeneous subgroups) based on stratification variables and then draws samples from each stratum to avoid sampling bias. To enhance the estimation accuracy of stratified sampling, we focus on the problem of selecting a subset of stratification variables that are effective in variance reduction. We design an efficient algorithm that selects stratification variables one by one by simulating a series of stratified sampling processes. We also estimate the computational complexity of our subset selection algorithm. Computational experiments using synthetic and real-world datasets demonstrate that our method can outperform other variance reduction techniques especially when multiple variables have a certain correlation with the outcome variable. Our subset selection method for stratified sampling can improve the sensitivity of online controlled experiments, thus enabling more reliable marketing decisions.
[14]
arXiv:2509.15593
[pdf, html, other]
Title:
SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant
Chunna Li, Yiwei Song, Yuanhai Shao
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
In transfer learning, a source domain often carries diverse knowledge, and different domains usually emphasize different types of knowledge. Different from handling only a single type of knowledge from all domains in traditional transfer learning methods, we introduce an ensemble learning framework with a weak mode of convergence in the form of Statistical Invariant (SI) for multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI extracts and integrates various types of knowledge from both source and target domains, which not only effectively utilizes diverse knowledge but also accelerates the convergence process. Further, SETrLUSI incorporates stochastic SI selection, proportional source domain sampling, and target domain bootstrapping, which improves training efficiency while enhancing model stability. Experiments show that SETrLUSI has good convergence and outperforms related methods with a lower time cost.
[15]
arXiv:2509.15594
[pdf, html, other]
Title:
Beyond the Average: Distributional Causal Inference under Imperfect Compliance
Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui
Comments:
arXiv admin note: text overlap with arXiv:2506.05945
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST); Applications (stat.AP); Machine Learning (stat.ML)
We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect-the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment.
[16]
arXiv:2509.15611
[pdf, html, other]
Title:
Interpretable Network-assisted Random Forest+
Tiffany M. Tang, Elizaveta Levina, Ji Zhu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Machine learning algorithms often assume that training samples are independent. When data points are connected by a network, the induced dependency between samples is both a challenge, reducing effective sample size, and an opportunity to improve prediction by leveraging information from network neighbors. Multiple methods taking advantage of this opportunity are now available, but many, including graph neural networks, are not easily interpretable, limiting their usefulness for understanding how a model makes its predictions. Others, such as network-assisted linear regression, are interpretable but often yield substantially worse prediction performance. We bridge this gap by proposing a family of flexible network-assisted models built upon a generalization of random forests (RF+), which achieves highly-competitive prediction accuracy and can be interpreted through feature importance measures. In particular, we develop a suite of interpretation tools that enable practitioners to not only identify important features that drive model predictions, but also quantify the importance of the network contribution to prediction. Importantly, we provide both global and local importance measures as well as sample influence measures to assess the impact of a given observation. This suite of tools broadens the scope and applicability of network-assisted machine learning for high-impact problems where interpretability and transparency are essential.
[17]
arXiv:2509.15734
[pdf, html, other]
Title:
Strong uniform consistency of nonparametric estimation for quantile-based entropy function under length-biased sampling
Vaishnavi Pavithradas, Rajesh G
Subjects:
Methodology (stat.ME)
For studies in reliability, biometry, and survival analysis, the length-biased distribution is often well-suited for certain natural sampling plans. In this paper, we study the strong uniform consistency of two nonparametric estimators for the quantile-based Shannon entropy in the context of length-biased data. A simulation study is conducted to examine the behavior of the estimators in finite samples, followed by a comparative analysis with existing estimators. Furthermore, the usefulness of the proposed estimators is evaluated using a real dataset.
[18]
arXiv:2509.15797
[pdf, html, other]
Title:
Transfer learning under latent space model
Kuangnan Fang, Ruixuan Qin, Xinyan Fan
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
Latent space model plays a crucial role in network analysis, and accurate estimation of latent variables is essential for downstream tasks such as link prediction. However, the large number of parameters to be estimated presents a challenge, especially when the latent space dimension is not exceptionally small. In this paper, we propose a transfer learning method that leverages information from networks with latent variables similar to those in the target network, thereby improving the estimation accuracy for the target. Given transferable source networks, we introduce a two-stage transfer learning algorithm that accommodates differences in node numbers between source and target networks. In each stage, we derive sufficient identification conditions and design tailored projected gradient descent algorithms for estimation. Theoretical properties of the resulting estimators are established. When the transferable networks are unknown, a detection algorithm is introduced to identify suitable source networks. Simulation studies and analyses of two real datasets demonstrate the effectiveness of the proposed methods.
[19]
arXiv:2509.15798
[pdf, html, other]
Title:
Deep learning based doubly robust test for Granger causality
Yongchang Hui, Chijin Liu, Xiaojun Song
Subjects:
Methodology (stat.ME)
Granger causality is popular for analyzing time series data in many applications from natural science to social science including genomics, neuroscience, economics, and finance. Consequently, the Granger causality test has become one of the main concerns of the econometrician for decades. Taking advantage of the theoretical breakthroughs in deep learning in recent years, we propose a doubly robust Granger causality test (DRGCT). Our method offers several key advantages. The first and most direct benefit is for the users, DRGCT allows them to handle large lag orders while alleviating the curse of dimensionality that traditional nonlinear Granger causality tests usually face. Second, introducing a doubly robust test statistic for time series based on neural networks that achieves a parametric convergence rate not only suggests a new paradigm for nonparametric inference in econometrics, but also broadens the application scope of deep learning. Third, a multiplier bootstrap method, combined with the doubly robust approach, provides an efficient way to obtain critical values, effectively reducing computational time and avoiding redundant calculations. We prove that the test asymptotically controls the type I error, while achieving power approaches one, and validate the effectiveness of our test through numerical simulations. In real data analysis, we apply DRGCT to revisit the price-volume relationship problem in the stock markets of America, China, and Japan.
[20]
arXiv:2509.15822
[pdf, html, other]
Title:
Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities
Alexandra Carpentier, Christophe Giraud, Nicolas Verzelen
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)
Predictions from statistical physics postulate that recovery of the communities in Stochastic Block Model (SBM) is possible in polynomial time above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has given rise to a rich literature, proving that non-trivial community recovery is indeed possible in SBM above the KS threshold, as long as the number $K$ of communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes in the observed graph. Failure of low-degree polynomials below the KS threshold was also proven when $K=o(\sqrt{n})$.
When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse regime, community recovery in polynomial time is possible below the KS threshold by counting non-backtracking paths. This breakthrough result lead them to postulate a new threshold for the many communities regime $K\geq \sqrt{n}$. In this work, we provide evidences that confirm their conjecture for $K\geq \sqrt{n}$:
1- We prove that, for any density of the graph, low-degree polynomials fail to recover communities below the threshold postulated by Chin et al.(2025);
2- We prove that community recovery is possible in polynomial time above the postulated threshold, not only in the sparse regime of~Chin et al., but also in some (but not all) moderately sparse regimes by essentially counting clique occurence in the observed graph.
[21]
arXiv:2509.15846
[pdf, html, other]
Title:
Doubly Robust Estimation of Continuous Outcomes under Multiple Treatment Levels via GPS, CBPS, and Penalized Empirical Likelihood
Byeonghee Lee, Joonsung Kang
Subjects:
Methodology (stat.ME)
This paper develops a unified framework for estimating continuous outcomes under multiple treatment levels in observational studies. We integrate the Generalized Propensity Score (GPS), Covariate Balancing Propensity Score (CBPS), and outcome regression into a Penalized Empirical Likelihood (PEL) formulation. The GPS is parameterized by $\boldsymbol{\beta}$ and denoted $\pi_{\boldsymbol{\beta}}(\mathbf{X})$, while CBPS imposes moment conditions to ensure covariate balance. Outcome regression flexibly models the continuous response $Y$, and doubly robust estimation ensures consistency under either correct model specification. PEL allows simultaneous estimation and variable selection using general estimating equations. Simulation results and comparisons with state-of-the-art meta-learners confirm the effectiveness of our method.
[22]
arXiv:2509.15939
[pdf, html, other]
Title:
Bi-dendrograms for clustering the categories of a multivariate categorical data set
Michael Greenacre, Maurizio Vichi
Comments:
15 pages, 5 figures, 3 tables
Subjects:
Methodology (stat.ME)
The clustering of categories in a multivariate categorical data set is investigated, where the problem separates into that of merging categories of the same variables (i.e., within-variable categories), and combining categories of different variables (i.e., between-variable categories). For the within-variable problem, the objective is to arrive at fewer categories (and, consequently, lower data dimensionality) without affecting the essential features of the data set, thereby simplifying the interpretation of any analysis using the categorical variables. The categories can be of an ordinal or nominal nature, and this property is respected in the clustering, where only adjacent categories of ordinal variables can be combined. For the between-variable problem, the objective is to arrive at asmall number of category clusters that typify the observations in the data set. In this latter problem there is no restriction on which categories can combine, as long as they do not combine within the same variable. In each of these problems, results are given in the form of a pair of dendrograms stacked one on top of the other, called a bi-dendrogram. For the within-variable problem, once all categories within each variable have been merged, the second stage is to cluster the variables themselves. For the between-variable problem, the second stage is to cluster groups of respondents that fall into the response sets arrived at in the first stage of clustering. The approach is illustrated using a sociological survey data set from the International Social Survey Program.
[23]
arXiv:2509.15989
[pdf, html, other]
Title:
Model-free algorithms for fast node clustering in SBM type graphs and application to social role inference in animals
Bertrand Cloez, Adrien Cotil, Jean-Baptiste Menassol, Nicolas Verzelen
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We propose a novel family of model-free algorithms for node clustering and parameter inference in graphs generated from the Stochastic Block Model (SBM), a fundamental framework in community detection. Drawing inspiration from the Lloyd algorithm for the $k$-means problem, our approach extends to SBMs with general edge weight distributions. We establish the consistency of our estimator under a natural identifiability condition. Through extensive numerical experiments, we benchmark our methods against state-of-the-art techniques, demonstrating significantly faster computation times with the lower order of estimation error. Finally, we validate the practical relevance of our algorithms by applying them to empirical network data from behavioral ecology.
[24]
arXiv:2509.16007
[pdf, html, other]
Title:
Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation
James E. Warner, Geoffrey F. Bomarito, Gianluca Geraci, Michael S. Eldred
Comments:
26 pages with 15 figures in main text
Subjects:
Computation (stat.CO)
Multifidelity uncertainty propagation combines the efficiency of low-fidelity models with the accuracy of a high-fidelity model to construct statistical estimators of quantities of interest. It is well known that the effectiveness of such methods depends crucially on the relative correlations and computational costs of the available computational models. However, the question of how to automatically tune low-fidelity models to maximize performance remains an open area of research. This work investigates automated model tuning, which optimizes model hyperparameters to minimize estimator variance within a target computational budget. Focusing on multifidelity trajectory simulation estimators, the cost-versus-precision tradeoff enabled by this approach is demonstrated in a practical, online setting where upfront tuning costs cannot be amortized. Using a real-world entry, descent, and landing example, it is shown that automated model tuning largely outperforms hand-tuned models even when the overall computational budget is relatively low. Furthermore, for scenarios where the computational budget is large, model tuning solutions can approach the best-case multifidelity estimator performance where optimal model hyperparameters are known a priori. Recommendations for applying model tuning in practice are provided and avenues for enabling adoption of such approaches for budget-constrained problems are highlighted.
[25]
arXiv:2509.16027
[pdf, html, other]
Title:
What is a good matching of probability measures? A counterfactual lens on transport maps
Lucas De Lara, Luca Ganassali
Comments:
37 pages; comments most welcome
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
Coupling probability measures lies at the core of many problems in statistics and machine learning, from domain adaptation to transfer learning and causal inference. Yet, even when restricted to deterministic transports, such couplings are not identifiable: two atomless marginals admit infinitely many transport maps. The common recourse to optimal transport, motivated by cost minimization and cyclical monotonicity, obscures the fact that several distinct notions of multivariate monotone matchings coexist. In this work, we first carry a comparative analysis of three constructions of transport maps: cyclically monotone, quantile-preserving and triangular monotone maps. We establish necessary and sufficient conditions for their equivalence, thereby clarifying their respective structural properties. In parallel, we formulate counterfactual reasoning within the framework of structural causal models as a problem of selecting transport maps between fixed marginals, which makes explicit the role of untestable assumptions in counterfactual reasoning. Then, we are able to connect these two perspectives by identifying conditions on causal graphs and structural equations under which counterfactual maps coincide with classical statistical transports. In this way, we delineate the circumstances in which causal assumptions support the use of a specific structure of transport map. Taken together, our results aim to enrich the theoretical understanding of families of transport maps and to clarify their possible causal interpretations. We hope this work contributes to establishing new bridges between statistical transport and causal inference.
[26]
arXiv:2509.16062
[pdf, html, other]
Title:
Transient regime of piecewise deterministic Monte Carlo algorithms
Sanket Agrawal, Joris Bierkens, Kengo Kamatani, Gareth O. Roberts
Comments:
39 pages, 6 figures
Subjects:
Computation (stat.CO)
Piecewise Deterministic Markov Processes (PDMPs) such as the Bouncy Particle Sampler and the Zig-Zag Sampler, have gained attention as continuous-time counterparts of classical Markov chain Monte Carlo. We study their transient regime under convex potentials, namely how trajectories that start in low-probability regions move toward higher-probability sets. Using fluid-limit arguments with a decomposition of the generator into fast and slow parts, we obtain deterministic ordinary differential equation descriptions of early-stage behaviour. The fast dynamics alone are non-ergodic because once the event rate reaches zero it does not restart. The slow component reactivates the dynamics, so averaging remains valid when taken over short micro-cycles rather than with respect to an invariant law.
Using the expected number of jump events as a cost proxy for gradient evaluations, we find that for Gaussian targets the transient cost of PDMP methods is comparable to that of random-walk Metropolis. For convex heavy-tailed families with subquadratic growth, PDMP methods can be more efficient when event simulation is implemented well. Forward Event-Chain and Coordinate Samplers can, under the same assumptions, reach the typical set with an order-one expected number of jumps. For the Zig-Zag Sampler we show that, under a diagonal-dominance condition, the transient choice of direction coincides with the solution of a box-constrained quadratic program; outside that regime we give a formal derivation and a piecewise-smooth update rule that clarifies the roles of the gradient and the Hessian. These results provide theoretical insight and practical guidance for the use of PDMP samplers in large-scale inference.
[27]
arXiv:2509.16076
[pdf, html, other]
Title:
Studying Optimal Designs for Multivariate Crossover Trials
Shubham Niphadkar, Siuli Mukhopadhyay
Subjects:
Methodology (stat.ME)
This article discusses $A$-, $D$- and $E$-optimality results for multivariate crossover designs, where more than one response is measured from every period for each subject. The motivation for these multivariate designs comes from a $3 \times 3$ crossover trial that investigates how an oral drug affects biomarkers of mucosal inflammation, by analyzing the various gene profiles from each participant. A multivariate response crossover model with fixed effects including direct and carryover effects, and with heteroscedastic error terms is considered to fit the multiple responses measured. It is assumed all throughout the article that there is no correlation between responses but there is presence of correlation within responses. Corresponding to the direct effects, we obtain the information matrix in a multiple response setup. Various results regarding this information matrix are studied. For $p$ periods and $t$ treatments, orthogonal array design of type $I$ and strength $2$ is proved as $A$-, $D$- and $E$-optimal, when $p=t \geq 3$.
[28]
arXiv:2509.16085
[pdf, html, other]
Title:
A more efficient method for large-sample model-free feature screening via multi-armed bandits
Xiaxue Ouyang, Xinlai Kang, Mengyu Li, Zhenxing Dou, Jun Yu, Cheng Meng
Comments:
26 pages,5 figures
Subjects:
Machine Learning (stat.ML); Computation (stat.CO)
We consider the model-free feature screening in large-scale ultrahigh-dimensional data analysis. Existing feature screening methods often face substantial computational challenges when dealing with large sample sizes. To alleviate the computational burden, we propose a rank-based model-free sure independence screening method (CR-SIS) and its efficient variant, BanditCR-SIS. The CR-SIS method, based on Chatterjee's rank correlation, is as straightforward to implement as the sure independence screening (SIS) method based on Pearson correlation introduced by Fan and Lv(2008), but it is significantly more powerful in detecting nonlinear relationships between variables. Motivated by the multi-armed bandit (MAB) problem, we reformulate the feature screening procedure to significantly reduce the computational complexity of CR-SIS. For a predictor matrix of size n \times p, the computational cost of CR-SIS is O(nlog(n)p), while BanditCR-SIS reduces this to O(\sqrt(n)log(n)p + nlog(n)). Theoretically, we establish the sure screening property for both CR-SIS and BanditCR-SIS under mild regularity conditions. Furthermore, we demonstrate the effectiveness of our methods through extensive experimental studies on both synthetic and real-world datasets. The results highlight their superior performance compared to classical screening methods, requiring significantly less computational time.
[29]
arXiv:2509.16116
[pdf, html, other]
Title:
Estimating systematic errors in Bayesian inversion using transport maps
Maren Casfor, Philipp Trunschke, Sebastian Heidenreich, Nando Hegemann
Comments:
25 pages, 7 figures
Subjects:
Methodology (stat.ME); Numerical Analysis (math.NA); Probability (math.PR)
In indirect measurements, the measurand is determined by solving an inverse problem which requires a model of the measurement process. Such models are often approximations and introduce systematic errors leading to a bias of the posterior distribution in Bayesian inversion. We propose a unified framework that combines transport maps from a reference distribution to the posterior distribution with the model error approach. This leads to an adaptive algorithm that jointly estimates the posterior distribution of the measurand and the model error. The efficiency and accuracy of the method are demonstrated on two model problems, showing that the approach effectively corrects biases while enabling fast sampling.
Cross submissions (showing 16 of 16 entries)
[30]
arXiv:2509.15401
(cross-list from econ.EM)
[pdf, html, other]
Title:
Inference on the Distribution of Individual Treatment Effects in Nonseparable Triangular Models
Jun Ma, Vadim Marmer, Zhengfei Yu
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
In this paper, we develop inference methods for the distribution of heterogeneous individual treatment effects (ITEs) in the nonseparable triangular model with a binary endogenous treatment and a binary instrument of Vuong and Xu (2017) and Feng, Vuong, and Xu (2019). We focus on the estimation of the cumulative distribution function (CDF) of the ITE, which can be used to address a wide range of practically important questions such as inference on the proportion of individuals with positive ITEs, the quantiles of the distribution of ITEs, and the interquartile range as a measure of the spread of the ITEs, as well as comparison of the ITE distributions across sub-populations. Moreover, our CDF-based approach can deliver more precise results than density-based approach previously considered in the literature. We establish weak convergence to tight Gaussian processes for the empirical CDF and quantile function computed from nonparametric ITE estimates of Feng, Vuong, and Xu (2019). Using those results, we develop bootstrap-based nonparametric inferential methods, including uniform confidence bands for the CDF and quantile function of the ITE distribution.
[31]
arXiv:2509.15420
(cross-list from cs.LG)
[pdf, html, other]
Title:
Top-$k$ Feature Importance Ranking
Yuxi Chen, Tiffany Tang, Genevera Allen
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Accurate ranking of important features is a fundamental challenge in interpretable machine learning with critical applications in scientific discovery and decision-making. Unlike feature selection and feature importance, the specific problem of ranking important features has received considerably less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And Recursive Trimming), a framework that utilizes any existing feature importance measure in a novel algorithm specifically tailored for ranking the top-$k$ features. Our approach combines an adaptive sequential halving strategy that progressively focuses computational resources on promising features with an efficient ensembling technique using both observation and feature subsampling. Unlike existing methods that convert importance scores to ranks as post-processing, our framework explicitly optimizes for ranking accuracy. We provide theoretical guarantees showing that RAMPART achieves the correct top-$k$ ranking with high probability under mild conditions, and demonstrate through extensive simulation studies that RAMPART consistently outperforms popular feature importance methods, concluding with a high-dimensional genomics case study.
[32]
arXiv:2509.15448
(cross-list from cs.LG)
[pdf, html, other]
Title:
Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems
Saeed Amizadeh, Sara Abdali, Yinheng Li, Kazuhito Koishida
Comments:
In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)
Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.
[33]
arXiv:2509.15475
(cross-list from eess.SP)
[pdf, html, other]
Title:
(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation
Lioz Berman, Sharon Gannot, Tom Tirer
Comments:
Code can be found at this https URL
Subjects:
Signal Processing (eess.SP); Machine Learning (cs.LG); Machine Learning (stat.ML)
We consider the problem of estimating the directions of arrival (DOAs) of multiple sources from a single snapshot of an antenna array, a task with many practical applications. In such settings, the classical Bartlett beamformer is commonly used, as maximum likelihood estimation becomes impractical when the number of sources is unknown or large, and spectral methods based on the sample covariance are not applicable due to the lack of multiple snapshots. However, the accuracy and resolution of the Bartlett beamformer are fundamentally limited by the array aperture. In this paper, we propose a deep learning technique, comprising a novel architecture and training strategy, for generating a high-resolution spatial spectrum from a single snapshot. Specifically, we train a deep neural network that takes the measurements and a hypothesis angle as input and learns to output a score consistent with the capabilities of a much wider array. At inference time, a heatmap can be produced by scanning an arbitrary set of angles. We demonstrate the advantages of our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and sparsity-based DOA estimation methods.
[34]
arXiv:2509.15517
(cross-list from cs.LG)
[pdf, html, other]
Title:
Manifold Dimension Estimation: An Empirical Study
Zelong Bi, Pierre Lafaye de Micheaux
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
The manifold hypothesis suggests that high-dimensional data often lie on or near a low-dimensional manifold. Estimating the dimension of this manifold is essential for leveraging its structure, yet existing work on dimension estimation is fragmented and lacks systematic evaluation. This article provides a comprehensive survey for both researchers and practitioners. We review often-overlooked theoretical foundations and present eight representative estimators. Through controlled experiments, we analyze how individual factors such as noise, curvature, and sample size affect performance. We also compare the estimators on diverse synthetic and real-world datasets, introducing a principled approach to dataset-specific hyperparameter tuning. Our results offer practical guidance and suggest that, for a problem of this generality, simpler methods often perform better.
[35]
arXiv:2509.15538
(cross-list from cs.GR)
[pdf, html, other]
Title:
Geometric Integration for Neural Control Variates
Daniel Meister, Takahiro Harada
Subjects:
Graphics (cs.GR); Machine Learning (cs.LG); Machine Learning (stat.ML)
Control variates are a variance-reduction technique for Monte Carlo integration. The principle involves approximating the integrand by a function that can be analytically integrated, and integrating using the Monte Carlo method only the residual difference between the integrand and the approximation, to obtain an unbiased estimate. Neural networks are universal approximators that could potentially be used as a control variate. However, the challenge lies in the analytic integration, which is not possible in general. In this manuscript, we study one of the simplest neural network models, the multilayered perceptron (MLP) with continuous piecewise linear activation functions, and its possible analytic integration. We propose an integration method based on integration domain subdivision, employing techniques from computational geometry to solve this problem in 2D. We demonstrate that an MLP can be used as a control variate in combination with our integration method, showing applications in the light transport simulation.
[36]
arXiv:2509.15553
(cross-list from cs.CV)
[pdf, html, other]
Title:
Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification
Tian Lan, Yiming Zheng, Jianxin Yin
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Applications (stat.AP)
Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious "Layer $12$" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at this https URL.
[37]
arXiv:2509.15591
(cross-list from cs.LG)
[pdf, html, other]
Title:
Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification
Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin
Comments:
Published in NeurIPS 2025
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at this https URL. The project website is at this https URL.
[38]
arXiv:2509.15641
(cross-list from cs.LG)
[pdf, html, other]
Title:
Information Geometry of Variational Bayes
Mohammad Emtiyaz Khan
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
We highlight a fundamental connection between information geometry and variational Bayes (VB) and discuss its consequences for machine learning. Under certain conditions, a VB solution always requires estimation or computation of natural gradients. We show several consequences of this fact by using the natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian Learning Rule (BLR). These include (i) a simplification of Bayes' rule as addition of natural gradients, (ii) a generalization of quadratic surrogates used in gradient-based methods, and (iii) a large-scale implementation of VB algorithms for large language models. Neither the connection nor its consequences are new but we further emphasize the common origins of the two fields of information geometry and Bayes with a hope to facilitate more work at the intersection of the two fields.
[39]
arXiv:2509.15776
(cross-list from cs.LG)
[pdf, html, other]
Title:
Generalization and Optimization of SGD with Lookahead
Kangcheng Li, Yunwen Lei
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
The Lookahead optimizer enhances deep learning models by employing a dual-weight update mechanism, which has been shown to improve the performance of underlying optimizers such as SGD. However, most theoretical studies focus on its convergence on training data, leaving its generalization capabilities less understood. Existing generalization analyses are often limited by restrictive assumptions, such as requiring the loss function to be globally Lipschitz continuous, and their bounds do not fully capture the relationship between optimization and generalization. In this paper, we address these issues by conducting a rigorous stability and generalization analysis of the Lookahead optimizer with minibatch SGD. We leverage on-average model stability to derive generalization bounds for both convex and strongly convex problems without the restrictive Lipschitzness assumption. Our analysis demonstrates a linear speedup with respect to the batch size in the convex setting.
[40]
arXiv:2509.15932
(cross-list from cs.LG)
[pdf, html, other]
Title:
The Alignment Bottleneck
Wenjun Cao
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (stat.ML)
Large language models improve with scale, yet feedback-based alignment still exhibits systematic deviations from intended behavior. Motivated by bounded rationality in economics and cognitive science, we view judgment as resource-limited and feedback as a constrained channel. On this basis, we model the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive capacity $C_{\text{cog}|S}$ and average total capacity $\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment Performance Interval. It pairs a data size-independent Fano lower bound proved on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes bound becomes an upper bound on the same true risk when the canonical observable loss is used and the dataset is drawn from the same mixture. Under these matched conditions, both limits are governed by a single capacity. Consequences include that, with value complexity and capacity fixed, adding labels alone cannot cross the bound; attaining lower risk on more complex targets requires capacity that grows with $\log M$; and once useful signal saturates capacity, further optimization tends to fit channel regularities, consistent with reports of sycophancy and reward hacking. The analysis views alignment as interface engineering: measure and allocate limited capacity, manage task complexity, and decide where information is spent.
[41]
arXiv:2509.15981
(cross-list from cs.LG)
[pdf, html, other]
Title:
Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations
Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)
In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at this https URL.
[42]
arXiv:2509.16115
(cross-list from econ.EM)
[pdf, other]
Title:
KRED: Korea Research Economic Database for Macroeconomic Research
Changryong Baek, Seunghyun Moon, Seunghyeon Lee
Subjects:
Econometrics (econ.EM); Applications (stat.AP)
We introduce KRED (Korea Research Economic Database), a new FRED MD style macroeconomic dataset for South Korea. KRED is constructed by aggregating 88 key monthly time series from multiple official sources (e.g., Bank of Korea ECOS, Statistics Korea KOSIS) into a unified, publicly available database. The dataset is aligned with the FRED MD format, enabling standardized transformations and direct comparability; an Appendix maps each Korean series to its FRED MD counterpart. Using a balanced panel of 80 series from 2009 to 2024, we extract four principal components via PCA that explain approximately 40% of the total variance. These four factors have intuitive economic interpretations, capturing monetary conditions, labor market activity, real output, and housing demand, analogous to diffusion indexes summarizing broad economic movements. Notably, the factor based diffusion indexes derived from KRED clearly trace major macroeconomic fluctuations over the sample period such as the 2020 COVID 19 recession. Our results demonstrate that KRED's factor structure can effectively condense complex economic information into a few informative indexes, yielding new insights into South Korea's business cycles and co movements.
[43]
arXiv:2509.16118
(cross-list from math.PR)
[pdf, other]
Title:
Mixing properties of some Markov chains models in random environments
Attila Lovas, Lionel Truquet
Subjects:
Probability (math.PR); Statistics Theory (math.ST)
Markov chains in random environments (MCREs) have recently attracted renewed interest, as these processes naturally arise in many applications, such as econometrics and machine learning. Although specific asymptotic results, such as the law of the large numbers and central limit theorems, have been obtained for some of these models, their annealed dependence properties, such as strong mixing properties, are not well understood in general. We derive strong mixing properties for a wide range of MCREs that satisfy some drift/small set conditions, with general assumptions on the corresponding stochastic parameters and the mixing properties of the environments. We then demonstrate the wide range of applications of our results in time series analysis and stochastic gradient Langevin dynamics, with fewer restrictions than those found in existing literature.
[44]
arXiv:2509.16180
(cross-list from cs.DS)
[pdf, html, other]
Title:
Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph
Gautam Kamath, Alireza F. Pour, Matthew Regehr, David P. Woodruff
Subjects:
Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)
We propose an algorithm with improved query-complexity for the problem of hypothesis selection under local differential privacy constraints. Given a set of $k$ probability distributions $Q$, we describe an algorithm that satisfies local differential privacy, performs $\tilde{O}(k^{3/2})$ non-adaptive queries to individuals who each have samples from a probability distribution $p$, and outputs a probability distribution from the set $Q$ which is nearly the closest to $p$. Previous algorithms required either $\Omega(k^2)$ queries or many rounds of interactive queries.
Technically, we introduce a new object we dub the Scheffé graph, which captures structure of the differences between distributions in $Q$, and may be of more broad interest for hypothesis selection tasks.
[45]
arXiv:2509.16186
(cross-list from quant-ph)
[pdf, other]
Title:
Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation
Naipunnya Raj, Rajiv Sangle, Avinash Singh, Krishna Kumar Sabapathy
Comments:
27 pages, 28 figures, 4 tables, 1 algorithm
Subjects:
Quantum Physics (quant-ph); Machine Learning (cs.LG); Machine Learning (stat.ML)
In this work, we introduce the Quantum Generative Adversarial Autoencoder (QGAA), a quantum model for generation of quantum data. The QGAA consists of two components: (a) Quantum Autoencoder (QAE) to compress quantum states, and (b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of the trained QAE. This approach imparts the QAE with generative capabilities. The utility of QGAA is demonstrated in two representative scenarios: (a) generation of pure entangled states, and (b) generation of parameterized molecular ground states for H$_2$ and LiH. The average errors in the energies estimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in simulations upto 6 qubits. These results illustrate the potential of QGAA for quantum state generation, quantum chemistry, and near-term quantum machine learning applications.
Replacement submissions (showing 28 of 28 entries)
[46]
arXiv:2303.12502
(replaced)
[pdf, other]
Title:
Measuring agreement among several raters classifying subjects into one or more (hierarchical) categories: A generalization of Fleiss' kappa
Filip Moons, Ellen Vandervieren
Comments:
19 pages, 2 figures. Behavior Research Methods (2025)
Journal-ref:
Behavior Research Methods, 57, 287 (2025)
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Cohen's and Fleiss' kappa are well-known measures of inter-rater agreement, but they restrict each rater to selecting only one category per subject. This limitation is consequential in contexts where subjects may belong to multiple categories, such as psychiatric diagnoses involving multiple disorders or classifying interview snippets into multiple codes of a codebook. We propose a generalized version of Fleiss' kappa, which accommodates multiple raters assigning subjects to one or more nominal categories. Our proposed $\kappa$ statistic can incorporate category weights based on their importance and account for hierarchical category structures, such as primary disorders with sub-disorders. The new $\kappa$ statistic can also manage missing data and variations in the number of raters per subject or category. We review existing methods that allow for multiple category assignments and detail the derivation of our measure, proving its equivalence to Fleiss' kappa when raters select a single category per subject. The paper discusses the assumptions, premises, and potential paradoxes of the new measure, as well as the range of possible values and guidelines for interpretation. The measure was developed to investigate the reliability of a new mathematics assessment method, of which an example is elaborated. The paper concludes with a worked-out example of psychiatrists diagnosing patients with multiple disorders. All calculations are provided as R script and an Excel sheet to facilitate access to the new $\kappa$ tatistic.
[47]
arXiv:2306.16571
(replaced)
[pdf, html, other]
Title:
Causal inference for the expected number of recurrent events in the presence of a terminal event
Benjamin R. Baer, Trang Bui, Daniel Mork, Robert L. Strawderman, Ashkan Ertefaie
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
While recurrent event analyses have been extensively studied, limited attention has been given to causal inference within the framework of recurrent event analysis. We develop a multiply robust estimation framework for causal inference in recurrent event data with a terminal failure event. We define our estimand as the vector comprising both the expected number of recurrent events and the failure survival function evaluated along a sequence of landmark times. We show that the estimand can be identified under a weaker condition than conditionally independent censoring and derive the associated class of influence functions under general censoring and failure distributions (i.e., without assuming absolute continuity). We propose a particular estimator within this class for further study, conduct comprehensive simulation studies to evaluate the small-sample performance of our estimator, and illustrate the proposed estimator using a large Medicare dataset to assess the causal effect of PM$_{2.5}$ on recurrent cardiovascular hospitalization.
[48]
arXiv:2308.14335
(replaced)
[pdf, html, other]
Title:
Improved learning theory for kernel distribution regression with two-stage sampling
François Bachoc, Louis Béthune, Alberto González-Sanz, Jean-Michel Loubes
Journal-ref:
Annals of Statistics 2025, Vol. 53, No. 4, 1753-1782
Subjects:
Statistics Theory (math.ST); Machine Learning (stat.ML)
The distribution regression problem encompasses many important statistics and machine learning tasks, and arises in a large range of applications. Among various existing approaches to tackle this problem, kernel methods have become a method of choice. Indeed, kernel distribution regression is both computationally favorable, and supported by a recent learning theory. This theory also tackles the two-stage sampling setting, where only samples from the input distributions are available. In this paper, we improve the learning theory of kernel distribution regression. We address kernels based on Hilbertian embeddings, that encompass most, if not all, of the existing approaches. We introduce the novel near-unbiased condition on the Hilbertian embeddings, that enables us to provide new error bounds on the effect of the two-stage sampling, thanks to a new analysis. We show that this near-unbiased condition holds for three important classes of kernels, based on optimal transport and mean embedding. As a consequence, we strictly improve the existing convergence rates for these kernels. Our setting and results are illustrated by numerical experiments.
[49]
arXiv:2402.18105
(replaced)
[pdf, html, other]
Title:
JEL ratio test for independence between a continuous and a categorical random variable
Saparya Suresh, Sudheesh K. Kattumannil
Comments:
This is the first test developed in this direction
Subjects:
Methodology (stat.ME)
The categorical Gini covariance is a dependence measure between a numerical variable and a categorical variable. The Gini covariance measures dependence by quantifying the difference between the conditional and unconditional distributional functions. The categorical Gini covariance equals zero if and only if the numerical variable and the categorical variable are independent. We propose a non-parametric test for testing the independence between a numerical and categorical variable using a modified categorical Gini covariance. We used the theory of U-statistics to find the test statistics and study the properties. The test has an asymptotic normal distribution. Since the implementation of a normal-based test is difficult, we develop a jackknife empirical likelihood (JEL) ratio test for testing independence. Extensive Monte Carlo simulation studies are carried out to validate the performance of the proposed JEL ratio test. We illustrate the test procedure using Iris flower data set.
[50]
arXiv:2404.07440
(replaced)
[pdf, other]
Title:
Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions
Johannes Brachem, Paul F. V. Wiemann, Thomas Kneib
Subjects:
Methodology (stat.ME)
Penalized transformation models (PTMs) are a semiparametric location-scale regression family that estimate a response's conditional distribution directly from the data, and model the location and scale through structured additive predictors. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference. PTMs can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach and includes comparisons to many alternative methods. Applications to the Fourth Dutch Growth Study and the Framingham Heart Study illustrate the usage and practical utility. A full-featured implementation is available as a Python library. Supplementary material for this article is available online.
[51]
arXiv:2405.18499
(replaced)
[pdf, html, other]
Title:
Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection
Hai-Vy Nguyen, Fabrice Gamboa, Sixin Zhang, Reda Chhaibi, Serge Gratton, Thierry Giaccone
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Robustness of deep neural networks to input noise remains a critical challenge, as naive noise injection often degrades accuracy on clean (uncorrupted) data. We propose a novel training framework that addresses this trade-off through two complementary objectives. First, we introduce a loss function applied at the penultimate layer that explicitly enforces intra-class compactness and increases the margin to analytically defined decision boundaries. This enhances feature discriminativeness and class separability for clean data. Second, we propose a class-wise feature alignment mechanism that brings noisy data clusters closer to their clean counterparts. Furthermore, we provide a theoretical analysis demonstrating that improving feature stability under additive Gaussian noise implicitly reduces the curvature of the softmax loss landscape in input space, as measured by Hessian this http URL thus naturally enhances robustness without explicit curvature penalties. Conversely, we also theoretically show that lower curvatures lead to more robust models. We validate the effectiveness of our method on standard benchmarks and our custom dataset. Our approach significantly reinforces model robustness to various perturbations while maintaining high accuracy on clean data, advancing the understanding and practice of noise-robust deep learning.
[52]
arXiv:2407.17848
(replaced)
[pdf, html, other]
Title:
Bayesian Benchmarking Small Area Estimation via Entropic Tilting
Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo
Comments:
31 pages
Subjects:
Methodology (stat.ME)
Benchmarking estimation and its risk evaluation is a practically important issue in small area estimation. While Bayesian methods have been widely adopted in small area estimation, existing benchmarking approaches are often ad-hoc, such as projecting each MCMC draw to satisfy the constraint. In contrast, our work provides a unified Bayesian formulation based on entropic tilting, which offers a more principled way to define the benchmarked posterior distribution. This approach yields benchmarked point estimates together with coherent uncertainty quantification. We first introduce general Monte Carlo methods for obtaining a benchmarked posterior under hierarchical Bayesian approaches and then show that the benchmarked posterior under empirical Bayesian frameworks can be obtained in an analytical form for some small area models. We demonstrate the usefulness of the proposed method through simulation and empirical studies.
[53]
arXiv:2501.17514
(replaced)
[pdf, html, other]
Title:
Semiparametric principal stratification analysis beyond monotonicity
Jiaqi Tong, Brennan Kahan, Michael O. Harhay, Fan Li
Subjects:
Methodology (stat.ME)
Intercurrent events, common in clinical trials and observational studies, affect the existence or interpretation of final outcomes. Principal stratification addresses this challenge by defining local average treatment effect estimands within subpopulations, but often relies on restrictive assumptions such as monotonicity and counterfactual intermediate independence. To overcome these limitations, we propose a semiparametric framework for principal stratification analysis leveraging a margin-free, conditional odds ratio sensitivity parameter. Under principal ignorability, we derive nonparametric identification formulas and efficient estimation methods, including a conditionally doubly robust parametric estimator and a debiased machine learning estimator with data-adaptive nuisance learners. Our simulations show that incorrectly assuming monotonicity can frequently lead to biased inference, but incorrectly assuming non-monotonicity when monotonicity holds may maintain approximately valid inference. We demonstrate our methods in the context of a critical care trial, where monotonicity is unlikely to be valid.
[54]
arXiv:2502.14566
(replaced)
[pdf, html, other]
Title:
Addressing Positivity Violations in Continuous Interventions through Data-Adaptive Strategies
Han Bao, Michael Schomaker
Comments:
30 pages (22 without appendix), 8 figures
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Positivity violations pose a key challenge in the estimation of causal effects, particularly for continuous interventions. Current approaches for addressing this issue include the use of weights or modified treatment policies. While effective in many contexts, these methods can result in estimands that do not always align well with the original research question, thereby compromising interpretability. In this paper, we introduce a novel diagnostic tool-the non-overlap ratio-to detect positivity violations. To address these violations while maintaining interpretability, we propose a data-adaptive solution, specifically the "most feasible" intervention strategy. Our strategy operates on a unit-specific basis. For a given intervention of interest, we first assess whether the intervention value is feasible for each unit. For units with sufficient support-conditional on confounders-we adhere to the intervention of interest. However, for units lacking sufficient support, we do not assign the actual intervention value of interest. Instead, we assign the closest feasible value within the support region. The non-overlap ratio provides a diagnostic summary of such support across the population. We propose an estimator using g-computation coupled with flexible conditional density estimation to identify high- and low-support regions and to estimate this new estimand. Through simulations, we demonstrate that our method effectively reduces bias across various scenarios by addressing positivity violations. Moreover, when positivity violations are absent, the method successfully recovers the standard estimand. We further validate its practical utility using real-world data from the CHAPAS-3 trial, which enrolled HIV-positive children in Zambia and Uganda.
[55]
arXiv:2504.13320
(replaced)
[pdf, html, other]
Title:
Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems
Robert Gruhlke, Matei Hanu, Claudia Schillings, Philipp Wacker
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Numerical Analysis (math.NA); Computation (stat.CO)
We introduce a gradient-free framework for Bayesian Optimal Experimental Design (BOED) in sequential settings, aimed at complex systems where gradient information is unavailable. Our method combines Ensemble Kalman Inversion (EKI) for design optimization with the Affine-Invariant Langevin Dynamics (ALDI) sampler for efficient posterior sampling-both of which are derivative-free and ensemble-based. To address the computational challenges posed by nested expectations in BOED, we propose variational Gaussian and parametrized Laplace approximations that provide tractable upper and lower bounds on the Expected Information Gain (EIG). These approximations enable scalable utility estimation in high-dimensional spaces and PDE-constrained inverse problems. We demonstrate the performance of our framework through numerical experiments ranging from linear Gaussian models to PDE-based inference tasks, highlighting the method's robustness, accuracy, and efficiency in information-driven experimental design.
[56]
arXiv:2506.02260
(replaced)
[pdf, html, other]
Title:
MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements
Howon Ryu, Yuliang Chen, Yacun Wang, Andrea Z. LaCroix, Chongzhi Di, Loki Natarajan, Yu Wang, Jingjing Zou
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)
Wearable devices enable continuous multi-modal physiological and behavioral monitoring, yet analysis of these data streams faces fundamental challenges including the lack of gold-standard labels and incomplete sensor data. While self-supervised learning approaches have shown promise for addressing these issues, existing multi-modal extensions present opportunities to better leverage the rich temporal and cross-modal correlations inherent in simultaneously recorded wearable sensor data. We propose the Multi-modal Cross-masked Autoencoder (MoCA), a self-supervised learning framework that combines transformer architecture with masked autoencoder (MAE) methodology, using a principled cross-modality masking scheme that explicitly leverages correlation structures between sensor modalities. MoCA demonstrates strong performance boosts across reconstruction and downstream classification tasks on diverse benchmark datasets. We further establish theoretical guarantees by establishing a fundamental connection between multi-modal MAE loss and kernelized canonical correlation analysis through a Reproducing Kernel Hilbert Space framework, providing principled guidance for correlation-aware masking strategy design. Our approach offers a novel solution for leveraging unlabeled multi-modal wearable data while handling missing modalities, with broad applications across digital health domains.
[57]
arXiv:2507.16734
(replaced)
[pdf, html, other]
Title:
Gaussian Sequence Model: Sample Complexities of Testing, Estimation and LFHT
Zeyu Jia, Yury Polyanskiy
Subjects:
Statistics Theory (math.ST); Information Theory (cs.IT)
We study the Gaussian sequence model, i.e. $X \sim N(\mathbf{\theta}, I_\infty)$, where $\mathbf{\theta} \in \Gamma \subset \ell_2$ is assumed to be convex and compact. We show that goodness-of-fit testing sample complexity is lower bounded by the square-root of the estimation complexity, whenever $\Gamma$ is orthosymmetric. We show that the lower bound is tight when $\Gamma$ is also quadratically convex, thus significantly extending validity of the testing-estimation relationship from [GP24]. Using similar methods, we also completely characterize likelihood-free hypothesis testing (LFHT) complexity for $\ell_p$-bodies, discovering new types of tradeoff between the numbers of simulation and observation samples.
[58]
arXiv:2509.12666
(replaced)
[pdf, html, other]
Title:
PBPK-iPINNs: Inverse Physics-Informed Neural Networks for Physiologically Based Pharmacokinetic Brain Models
Charuka D. Wickramasinghe, Krishanthi C. Weerasinghe, Pradeep K. Ranaweera
Comments:
24 pages, 11 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Numerical Analysis (math.NA)
Physics-Informed Neural Networks (PINNs) leverage machine learning with differential equations to solve direct and inverse problems, ensuring predictions follow physical laws. Physiologically based pharmacokinetic (PBPK) modeling advances beyond classical compartmental approaches by using a mechanistic, physiology focused framework. A PBPK model is based on a system of ODEs, with each equation representing the mass balance of a drug in a compartment, such as an organ or tissue. These ODEs include parameters that reflect physiological, biochemical, and drug-specific characteristics to simulate how the drug moves through the body. In this paper, we introduce PBPK-iPINN, a method to estimate drug-specific or patient-specific parameters and drug concentration profiles in PBPK brain compartment models using inverse PINNs. We demonstrate that, for the inverse problem to converge to the correct solution, the loss function components (data loss, initial conditions loss, and residual loss) must be appropriately weighted, and parameters (including number of layers, number of neurons, activation functions, learning rate, optimizer, and collocation points) must be carefully tuned. The performance of the PBPK-iPINN approach is then compared with established traditional numerical and statistical methods.
[59]
arXiv:2203.11820
(replaced)
[pdf, html, other]
Title:
Dealing with Logs and Zeros in Regression Models
David Benatia, Christophe Bellégo, Louis Pape
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
The log transformation is widely used in linear regression, mainly because coefficients are interpretable as proportional effects. Yet this practice has fundamental limitations, most notably that the log is undefined at zero, creating an identification problem. We propose a new estimator, iterated OLS (iOLS), which targets the normalized average treatment effect, preserving the percentage-change interpretation while addressing these limitations. Our procedure is the theoretically justified analogue of the ad-hoc log(1+Y) transformation and delivers a consistent and asymptotically normal estimator of the parameters of the exponential conditional mean model. iOLS is computationally efficient, globally convergent, and free of the incidental-parameter bias, while extending naturally to endogenous regressors through iterated 2SLS. We illustrate the methods with simulations and revisit three influential publications.
[60]
arXiv:2403.15220
(replaced)
[pdf, html, other]
Title:
Modelling with Sensitive Variables
Felix Chan, Laszlo Matyas, Agoston Reguly
Comments:
31 pages, 2 tables, 2 figures
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.
[61]
arXiv:2405.17764
(replaced)
[pdf, html, other]
Title:
BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic Representation
Tianhao Zhang, Zhecheng Sheng, Zhexiao Lin, Chen Jiang, Dongyeop Kang
Journal-ref:
The 2025 Conference on Empirical Methods in Natural Language Processing
Subjects:
Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Statistics Theory (math.ST)
Autoregressive generative models play a key role in various language tasks, especially for modeling and evaluating long text sequences. While recent methods leverage stochastic representations to better capture sequence dynamics, encoding both temporal and structural dependencies and utilizing such information for evaluation remains challenging. In this work, we observe that fitting transformer-based model embeddings into a stochastic process yields ordered latent representations from originally unordered model outputs. Building on this insight and prior work, we theoretically introduce a novel likelihood-based evaluation metric BBScoreV2. Empirically, we demonstrate that the stochastic latent space induces a "clustered-to-temporal ordered" mapping of language model representations in high-dimensional space, offering both intuitive and quantitative support for the effectiveness of BBScoreV2. Furthermore, this structure aligns with intrinsic properties of natural language and enhances performance on tasks such as temporal consistency evaluation (e.g., Shuffle tasks) and AI-generated content detection.
[62]
arXiv:2410.05837
(replaced)
[pdf, html, other]
Title:
A noise-corrected Langevin algorithm and sampling by half-denoising
Aapo Hyvärinen
Comments:
Final version published at TMLR
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy-data score function", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm only needs to know the noisy score function for one single noise level. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.
[63]
arXiv:2410.15555
(replaced)
[pdf, html, other]
Title:
Bayesian Concept Bottleneck Models with LLM Priors
Jean Feng, Avni Kothari, Luke Zier, Chandan Singh, Yan Shuo Tan
Comments:
2025 Conference on Neural Information Processing Systems
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set of human-interpretable concepts, extract their values from the training data, and identify a sparse subset as inputs to a transparent prediction model. However, such approaches are often hampered by the tradeoff between exploring a sufficiently large set of concepts versus controlling the cost of obtaining concept extractions, resulting in a large interpretability-accuracy tradeoff. This work investigates a novel approach that sidesteps these challenges: BC-LLM iteratively searches over a potentially infinite set of concepts within a Bayesian framework, in which Large Language Models (LLMs) serve as both a concept extraction mechanism and prior. Even though LLMs can be miscalibrated and hallucinate, we prove that BC-LLM can provide rigorous statistical inference and uncertainty quantification. Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings, converges more rapidly towards relevant concepts, and is more robust to out-of-distribution samples.
[64]
arXiv:2410.22069
(replaced)
[pdf, html, other]
Title:
Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks
Nikolaos Tsilivis, Eitan Gronich, Gal Vardi, Julia Kempe
Comments:
The earlier conference version (ICLR 2025) of this paper showed a bias towards KKT points of the max-margin problem only in the case of 'smooth' norms. The current version (submitted to JMLR) proves that this holds true for any norm. It also includes new experiments on the implicit bias of the Shampoo algorithm. v3 corrected a mistake in the proof of v2
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).
[65]
arXiv:2412.10069
(replaced)
[pdf, html, other]
Title:
Spatio-temporal Dynamical Indices for Complex Systems
Chenyu Dong, Gabriele Messori, Davide Faranda, Adriano Gualandi, Valerio Lucarini, Gianmarco Mengaldo
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Dynamical Systems (math.DS); Methodology (stat.ME)
Complex systems span multiple spatial and temporal scales, making their dynamics challenging to understand and predict. This challenge is especially daunting when one wants to study localized and/or rare events. Advances in dynamical systems theory, including the development of state-dependent dynamical indices, namely local dimension and persistence, have provided powerful tools for studying these phenomena. However, existing applications of such indices rely on a predefined and fixed spatial domain, that provides a single scalar quantity for the entire region of interest. This aspect prevents understanding the spatially localized dynamical behavior of the system. In this work, we introduce Spatio-temporal Dynamical Indices (SDIs), that leverage the existing framework of state-dependent local dimension and persistence. SDIs are obtained via a sliding window approach, enabling the exploration of space-dependent properties in spatio-temporal data. As an example, we show that, through this framework, we are able to reconcile previously different perspectives on European summertime heatwaves. This result showcases the importance of accounting for spatial scales when performing scale-dependent dynamical analyses.
[66]
arXiv:2412.14650
(replaced)
[pdf, html, other]
Title:
Permutation recovery of spikes in noisy high-dimensional tensor estimation
Gérard Ben Arous, Cédric Gerbelot, Vanessa Piccolo
Comments:
35 pages, 2 figures. Version 2: minor revisions. To appear in Stochastic Analysis and Applications, Springer Proceedings in Mathematics & Statistics (2025). arXiv admin note: substantial text overlap with arXiv:2408.06401
Subjects:
Probability (math.PR); Machine Learning (cs.LG); Machine Learning (stat.ML)
We study the dynamics of gradient flow in high dimensions for the multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal vectors (spikes) from noisy Gaussian tensor observations. Specifically, we analyze the maximum likelihood estimation procedure, which involves optimizing a highly nonconvex random function. We determine the sample complexity required for gradient flow to efficiently recover all spikes, without imposing any assumptions on the separation of the signal-to-noise ratios (SNRs). More precisely, our results provide the sample complexity required to guarantee recovery of the spikes up to a permutation. Our work builds on our companion paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and determines the sample complexity and separation conditions for the SNRs necessary for ensuring exact recovery of the spikes (where the recovered permutation matches the identity). During the recovery process, the correlations between the estimators and the hidden vectors increase in a sequential manner. The order in which these correlations become significant depends on their initial values and the corresponding SNRs, which ultimately determines the permutation of the recovered spikes.
[67]
arXiv:2501.10077
(replaced)
[pdf, html, other]
Title:
Double descent in quantum kernel methods
Marie Kempkes, Aroosa Ijaz, Elies Gil-Fuster, Carlos Bravo-Prieto, Jakob Spiegelberg, Evert van Nieuwenburg, Vedran Dunjko
Subjects:
Quantum Physics (quant-ph); Machine Learning (cs.LG); Machine Learning (stat.ML)
The double descent phenomenon challenges traditional statistical learning theory by revealing scenarios where larger models do not necessarily lead to reduced performance on unseen data. While this counterintuitive behavior has been observed in a variety of classical machine learning models, particularly modern neural network architectures, it remains elusive within the context of quantum machine learning. In this work, we analytically demonstrate that linear regression models in quantum feature spaces can exhibit double descent behavior by drawing on insights from classical linear regression and random matrix theory. Additionally, our numerical experiments on quantum kernel methods across different real-world datasets and system sizes further confirm the existence of a test error peak, a characteristic feature of double descent. Our findings provide evidence that quantum models can operate in the modern, overparameterized regime without experiencing overfitting, potentially opening pathways to improved learning performance beyond traditional statistical learning theory.
[68]
arXiv:2501.16120
(replaced)
[pdf, html, other]
Title:
Copyright and Competition: Estimating Supply and Demand with Unstructured Data
Sukjin Han, Kyungho Lee
Subjects:
Econometrics (econ.EM); Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
We study the competitive and welfare effects of copyright in creative industries in the face of cost-reducing technologies such as generative artificial intelligence. Creative products often feature unstructured attributes (e.g., images and text) that are complex and high-dimensional. To address this challenge, we study a stylized design product -- fonts -- using data from the world's largest font marketplace. We construct neural network embeddings to quantify unstructured attributes and measure visual similarity in a manner consistent with human perception. Spatial regression and event-study analyses demonstrate that competition is local in the visual characteristics space. Building on this evidence, we develop a structural model of supply and demand that incorporates embeddings and captures product positioning under copyright-based similarity constraints. Our estimates reveal consumers' heterogeneous design preferences and producers' cost-effective mimicry advantages. Counterfactual analyses show that copyright protection can raise consumer welfare by encouraging product relocation, and that the optimal policy depends on the interaction between copyright and cost-reducing technologies.
[69]
arXiv:2501.18164
(replaced)
[pdf, html, other]
Title:
Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size
Kanata Oowada, Hideaki Iiduka
Comments:
Accepted at ACML2025
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
We theoretically analyzed the convergence behavior of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster convergence than using a constant batch size, not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate improves from $O(T^{-1}+C)$ with a constant batch size to $O(T^{-1})$ with an increasing batch size, where $T$ denotes the total number of iterations and $C$ is a constant. Using principal component analysis and low-rank matrix completion, we investigated, both theoretically and numerically, how an increasing batch size affects computational time as quantified by stochastic first-order oracle (SFO) complexity. An increasing batch size was found to reduce the SFO complexity of RSGD. Furthermore, an increasing batch size was found to offer the advantages of both small and large constant batch sizes.
[70]
arXiv:2502.08440
(replaced)
[pdf, html, other]
Title:
Scenario Analysis with Multivariate Bayesian Machine Learning Models
Michael Pfarrhofer, Anna Stelzer
Comments:
Keywords: conditional forecast, generalized impulse response function, Bayesian additive regression trees, nonlinearities, structural inference; JEL: C32, C53, E44
Subjects:
Econometrics (econ.EM); Applications (stat.AP)
We present an econometric framework that adapts tools for scenario analysis, such as variants of conditional forecasts and generalized impulse responses, for use with dynamic nonparametric models. The proposed algorithms are based on predictive simulation and sequential Monte Carlo methods. Their utility is demonstrated with three applications: (1) conditional forecasts based on stress test scenarios, measuring (2) macroeconomic risk under varying financial stress, and estimating the (3) asymmetric effects of financial shocks in the US and their international spillovers. Our empirical results indicate the importance of nonlinearities and asymmetries in relationships between macroeconomic and financial variables.
[71]
arXiv:2503.02536
(replaced)
[pdf, html, other]
Title:
The Likelihood Correspondence
Thomas Kahle, Hal Schenck, Bernd Sturmfels, Maximilian Wiesmann
Comments:
18 pages. Comments are welcome!
Subjects:
Commutative Algebra (math.AC); Algebraic Geometry (math.AG); Combinatorics (math.CO); Statistics Theory (math.ST)
An arrangement of hypersurfaces in projective space is strict normal crossing (SNC) if and only if its Euler discriminant is nonzero. We study the critical loci of arbitrary Laurent monomials in the equations of the smooth hypersurfaces. The family of these loci forms an irreducible variety in the product of two projective spaces, known in algebraic statistics as the likelihood correspondence and in particle physics as the scattering correspondence. We establish an explicit determinantal representation for the minimal generators of the bihomogeneous prime ideal that defines this variety.
[72]
arXiv:2507.09087
(replaced)
[pdf, other]
Title:
Deep Reinforcement Learning with Gradient Eligibility Traces
Esraa Elelimy, Brett Daley, Andrew Patterson, Marlos C. Machado, Adam White, Martha White
Journal-ref:
Reinforcement Learning Journal, 2025
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the generalized Projected Bellman Error ($\overline{\text{PBE}}$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the generalized $\overline{\text{PBE}}$ objective to support multistep credit assignment based on the $\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at this https URL\_algos
[73]
arXiv:2508.03593
(replaced)
[pdf, other]
Title:
On the (In)Significance of Feature Selection in High-Dimensional Datasets
Bhavesh Neekhra, Debayan Gupta, Partha Pratim Chakrabarti
Comments:
(review in progress). supplementary material included in pdf; anonymized code at: this https URL
Subjects:
Machine Learning (cs.LG); Genomics (q-bio.GN); Machine Learning (stat.ML)
Feature selection (FS) is assumed to improve predictive performance and identify meaningful features in high-dimensional datasets. Surprisingly, small random subsets of features (0.02-1%) match or outperform the predictive performance of both full feature sets and FS across 28 out of 30 diverse datasets (microarray, bulk and single-cell RNA-Seq, mass spectrometry, imaging, etc.). In short, any arbitrary set of features is as good as any other (with surprisingly low variance in results) - so how can a particular set of selected features be "important" if they perform no better than an arbitrary set? These results challenge the assumption that computationally selected features reliably capture meaningful signals, emphasizing the importance of rigorous validation before interpreting selected features as actionable, particularly in computational genomics.
Total of 73 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack