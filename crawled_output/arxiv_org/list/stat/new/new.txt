Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Wednesday, 24 September 2025
Total of 83 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 27 of 27 entries)
[1]
arXiv:2509.18148
[pdf, html, other]
Title:
Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method
Kairong Han, Weidong Huang, Taiyang Zhou, Peng Zhen, Kun Kuang
Comments:
Accepted by CIKM 2025
Subjects:
Methodology (stat.ME); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)
In the online ride-hailing pricing context, companies often conduct randomized controlled trials (RCTs) and utilize uplift models to assess the effect of discounts on customer orders, which substantially influences competitive market outcomes. However, due to the high cost of RCTs, the proportion of trial data relative to observational data is small, which only accounts for 0.65\% of total traffic in our context, resulting in significant bias when generalizing to the broader user base. Additionally, the complexity of industrial processes reduces the quality of RCT data, which is often subject to heterogeneity from potential interference and selection bias, making it difficult to correct. Moreover, existing data fusion methods are challenging to implement effectively in complex industrial settings due to the high dimensionality of features and the strict assumptions that are hard to verify with real-world data. To address these issues, we propose an empirical data fusion method called pseudo-sample matching. By generating pseudo-samples from biased, low-quality RCT data and matching them with the most similar samples from large-scale observational data, the method expands the RCT dataset while mitigating its heterogeneity. We validated the method through simulation experiments, conducted offline and online tests using real-world data. In a week-long online experiment, we achieved a 0.41\% improvement in profit, which is a considerable gain when scaled to industrial scenarios with hundreds of millions in revenue. In addition, we discuss the harm to model training, offline evaluation, and online economic benefits when the RCT data quality is not high, and emphasize the importance of improving RCT data quality in industrial scenarios. Further details of the simulation experiments can be found in the GitHub repository this https URL.
[2]
arXiv:2509.18155
[pdf, html, other]
Title:
Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification
Aaron Pim, Tristan Pryer
Comments:
21 pages, 23 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)
Accurate proton dose calculation using Monte Carlo (MC) is computationally demanding in workflows like robust optimisation, adaptive replanning, and probabilistic inference, which require repeated evaluations. To address this, we develop a neural surrogate that integrates Monte Carlo dropout to provide fast, differentiable dose predictions along with voxelwise predictive uncertainty. The method is validated through a series of experiments, starting with a one-dimensional analytic benchmark that establishes accuracy, convergence, and variance decomposition. Two-dimensional bone-water phantoms, generated using TOPAS Geant4, demonstrate the method's behavior under domain heterogeneity and beam uncertainty, while a three-dimensional water phantom confirms scalability for volumetric dose prediction. Across these settings, we separate epistemic (model) from parametric (input) contributions, showing that epistemic variance increases under distribution shift, while parametric variance dominates at material boundaries. The approach achieves significant speedups over MC while retaining uncertainty information, making it suitable for integration into robust planning, adaptive workflows, and uncertainty-aware optimisation in proton therapy.
[3]
arXiv:2509.18319
[pdf, html, other]
Title:
Sequential Design for the Efficient Estimation of Offshore Structure Failure Probability
Matthew Speers, Jonathan Angus Tawn, Philip Jonathan
Subjects:
Applications (stat.AP)
Estimation of the failure probability of offshore structures exposed to extreme ocean environments is critical to their safe design and operation. The conditional density of the environment (CDE) quantifies regions of the space of long term environment responsible for extreme structural response. Moreover, the probability of structural failure is obtained by simply integrating the CDE over the environment space. In this work, two methodologies for estimation of the CDE and failure probability are considered. The first (IS-PT) combines parallel tempering MCMC (for CDE estimation) with important sampling (for eventual estimation of failure probability). The second (AGE) combines adaptive Gaussian emulation with Bayesian quadrature. We evaluate IS-PT and two variants of the AGE procedure in application to a simple synthetic structure with multimodal CDE, and a monopile structure exhibiting non-linear resonant response. IS-PT provides reliable results for both applications for lesser compute cost than naive integration. The AGE procedures require balancing exploration and exploitation of the environment space, using a typically-unknown weight parameter, lambda. When lambda is known, perhaps from prior engineering knowledge, AGE provides a further reduction in computational cost over IS-PT. However, when unknown, IS-PT is more reliable.
[4]
arXiv:2509.18349
[pdf, html, other]
Title:
Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity
Saptati Datta, Nicolas W. Hengartner, Yulia Pimonova, Natalie E. Klein, Nicholas Lubbers
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Meta-learning has emerged as a powerful paradigm for leveraging information across related tasks to improve predictive performance on new tasks. In this paper, we propose a statistical framework for analyzing meta-learning through the lens of predictor subspace characterization and quantification of task diversity. Specifically, we model the shared structure across tasks using a latent subspace and introduce a measure of diversity that captures heterogeneity across task-specific predictors. We provide both simulation-based and theoretical evidence indicating that achieving the desired prediction accuracy in meta-learning depends on the proportion of predictor variance aligned with the shared subspace, as well as on the accuracy of subspace estimation.
[5]
arXiv:2509.18406
[pdf, other]
Title:
Estimation and inference in generalised linear models with constrained iteratively-reweighted least squares
Pierre Masselot, Devon Nenon, Jacopo Vanoli, Zaid Chalabi, Antonio Gasparrini
Comments:
Submitted for peer reviewed publication
Subjects:
Methodology (stat.ME)
We propose a simple and flexible framework for generalised linear models (GLM) with linear constraints on the coefficients. Linear constraints are useful in a wide range of applications, allowing the fitting of model with high-dimensional or highly collinear predictors, as well as encoding assumptions on the association between some or all predictors and the response. We propose the constrained iteratively-reweighted least squares (CIRLS) to fit the model, iterating quadratic programs to ensure the coefficient vector remains feasible according to the constraints. Inference for constrained coefficients can be obtained by simulating from a truncated multivariate normal distribution and computing empirical confidence intervals or variance-covariance matrix from the simulated coefficient vectors. We additionally discuss the complexity of a constrained GLM, proposing a measure of expected degrees of freedom which accounts for the stringency of constraints in the reduction of the model degrees of freedom. An extensive simulations study shows that constraining the coefficients introduces some bias to the estimation, but also decreases the estimator variance. This trade-off results in an improved estimator when constraints are chosen appropriately. The simulations also show that our proposed inference results in error in variance estimation and coverage. The proposed framework is illustrated on two case studies, showing its usefulness as well as some of its weaknesses.
[6]
arXiv:2509.18414
[pdf, html, other]
Title:
Hierarchical Semi-Markov Models with Duration-Aware Dynamics for Activity Sequences
Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan
Subjects:
Applications (stat.AP); Machine Learning (stat.ML)
Residential electricity demand at granular scales is driven by what people do and for how long. Accurately forecasting this demand for applications like microgrid management and demand response therefore requires generative models that can produce realistic daily activity sequences, capturing both the timing and duration of human behavior. This paper develops a generative model of human activity sequences using nationally representative time-use diaries at a 10-minute resolution. We use this model to quantify which demographic factors are most critical for improving predictive performance.
We propose a hierarchical semi-Markov framework that addresses two key modeling challenges. First, a time-inhomogeneous Markov \emph{router} learns the patterns of ``which activity comes next." Second, a semi-Markov \emph{hazard} component explicitly models activity durations, capturing ``how long" activities realistically last. To ensure statistical stability when data are sparse, the model pools information across related demographic groups and time blocks. The entire framework is trained and evaluated using survey design weights to ensure our findings are representative of the U.S. population.
On a held-out test set, we demonstrate that explicitly modeling durations with the hazard component provides a substantial and statistically significant improvement over purely Markovian models. Furthermore, our analysis reveals a clear hierarchy of demographic factors: Sex, Day-Type, and Household Size provide the largest predictive gains, while Region and Season, though important for energy calculations, contribute little to predicting the activity sequence itself. The result is an interpretable and robust generator of synthetic activity traces, providing a high-fidelity foundation for downstream energy systems modeling.
[7]
arXiv:2509.18459
[pdf, html, other]
Title:
Evaluating Bias Reduction Methods in Binary Emax Model for Reliable Dose-Response Estimation
Jiangshan Zhang, Vivek Pradhan, Yuxi Zhao
Subjects:
Applications (stat.AP); Methodology (stat.ME)
The Binary Emax model is widely employed in dose-response analysis during Phase II clinical studies to identify the optimal dose for subsequence confirmatory trials. The parameter estimation and inference heavily rely on the asymptotic properties of Maximum Likelihood (ML) estimators; however, this approach may be questionable under small or moderate sample sizes and is not robust to violation of model assumptions. To provide a reliable solution, this paper examines three bias-reduction methods: the Cox-Snell bias correction, Firth-score modification, and a maximum penalized likelihood estimator (MPLE) using Jeffreys prior. Through comprehensive simulation studies, we evaluate the performance of these methods in reducing bias and controlling variance, especially when model assumptions are violated. The results demonstrate that both Firth and MPLE methods provide robust estimates, with MPLE outperforming in terms of stability and lower variance. We further illustrate the practical application of these methods using data from the TURANDOT study, a Phase II clinical trial. Our findings suggest that MPLE with Jeffreys prior offers an effective and reliable alternative to the Firth method, particularly for dose-response relationships that deviate from monotonicity, making it valuable for robust parameter estimation in dose-ranging studies.
[8]
arXiv:2509.18477
[pdf, html, other]
Title:
End-Cut Preference in Survival Trees
Xiaogang Su
Comments:
24 pages, 2 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The end-cut preference (ECP) problem, referring to the tendency to favor split points near the boundaries of a feature's range, is a well-known issue in CART (Breiman et al., 1984). ECP may induce highly imbalanced and biased splits, obscure weak signals, and lead to tree structures that are both unstable and difficult to interpret. For survival trees, we show that ECP also arises when using greedy search to select the optimal cutoff point by maximizing the log-rank test statistic. To address this issue, we propose a smooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator function is replaced by a smooth sigmoid function. We further demonstrate, both theoretically and through numerical illustrations, that SSS provides an effective remedy for mitigating or avoiding ECP.
[9]
arXiv:2509.18484
[pdf, html, other]
Title:
Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning
Yuanchen Wu, Yubai Yuan
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Estimating causal effects on networks is important for both scientific research and practical applications. Unlike traditional settings that assume the Stable Unit Treatment Value Assumption (SUTVA), interference allows an intervention/treatment on one unit to affect the outcomes of others. Understanding both direct and spillover effects is critical in fields such as epidemiology, political science, and economics. Causal inference on networks faces two main challenges. First, causal effects are typically heterogeneous, varying with unit features and local network structure. Second, connected units often exhibit dependence due to network homophily, creating confounding between structural correlations and causal effects. In this paper, we propose a two-stage method to estimate heterogeneous direct and spillover effects on networks. The first stage uses graph neural networks to estimate nuisance components that depend on the complex network topology. In the second stage, we adjust for network confounding using these estimates and infer causal effects through a novel attention-based interference model. Our approach balances expressiveness and interpretability, enabling downstream tasks such as identifying influential neighborhoods and recovering the sign of spillover effects. We integrate the two stages using Neyman orthogonalization and cross-fitting, which ensures that errors from nuisance estimation contribute only at higher order. As a result, our causal effect estimates are robust to bias and misspecification in modeling causal effects under network dependencies.
[10]
arXiv:2509.18489
[pdf, html, other]
Title:
Latent class multivariate probit and latent trait models for evaluating test accuracy without a gold standard: A simulation study
Enzo Cerullo, Sean Pinkney, Alex J. Sutton, Tim Lucas, Nicola J. Cooper, Hayley E. Jones
Comments:
61 pages
Subjects:
Methodology (stat.ME); Applications (stat.AP)
In the context of an imperfect gold standard, latent class modelling can be used to estimate accuracy of multiple medical tests. However, the conditional independence (CI) assumption is rarely thought to be clinically valid. Two models accommodating conditional dependence are the latent class multivariate probit (LC-MVP) and latent trait models. Despite LC-MVP's greater flexibility - modelling full correlation matrices versus the latent trait's restricted structure - the latent trait has been more widely used. No simulation studies have directly compared these two models.
We conducted a comprehensive simulation study comparing both models across five data generating mechanisms: CI, low-heterogeneity (latent trait-generated), and high-heterogeneity (LC-MVP-generated) correlation structures. We evaluated multiple priors, including novel constrained correlation priors using Pinkney's method that preserves prior interpretability. Models were fit using our BayesMVP R package, which achieves GPU-like speed-ups on these inherently serial models.
The LC-MVP model demonstrated superior overall performance. Whilst the latent trait model performed acceptably on its own generated data, it failed for high-heterogeneity structures, sometimes performing worse than the CI model. The CI model did badly for most dependent structures. We also found ceiling effects: high sensitivities reduced the importance of correlation recovery, explaining paradoxes where models achieved good performance despite poor correlation recovery.
Our results strongly favour LC-MVP for practical applications. The latent trait model's severe consequences under realistic correlation structures make it a more risky choice. However, LC-MVP with custom correlation constraints and priors provides a safer, more flexible framework for test accuracy evaluation without a perfect gold standard.
[11]
arXiv:2509.18491
[pdf, html, other]
Title:
Functional Mixed effects Model for Joint Analysis of Longitudinal and Cross-Sectional Growth Data
Long Chen, Ji Chen, Yingchun Zhou
Subjects:
Methodology (stat.ME)
A new method is proposed to perform joint analysis of longitudinal and cross-sectional growth data. Clustering is first performed to group similar subjects in cross-sectional data to form a pseudo longitudinal data set, then the pseudo longitudinal data and real longitudinal data are combined and analyzed by using a functional mixed effects model. To account for the variational difference between pseudo and real longitudinal growth data, it is assumed that the covariance functions of the random effects and the variance functions of the measurement errors for pseudo and real longitudinal data can be different. Various simulation studies and real data analysis demonstrate the good performance of the method.
[12]
arXiv:2509.18494
[pdf, html, other]
Title:
Enhanced Survival Trees
Ruiwen Zhou, Ke Xie, Lei Liu, Zhichen Xu, Jimin Ding, Xiaogang Su
Comments:
34 pages plus a 7-page supplement
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
We introduce a new survival tree method for censored failure time data that incorporates three key advancements over traditional approaches. First, we develop a more computationally efficient splitting procedure that effectively mitigates the end-cut preference problem, and we propose an intersected validation strategy to reduce the variable selection bias inherent in greedy searches. Second, we present a novel framework for determining tree structures through fused regularization. In combination with conventional pruning, this approach enables the merging of non-adjacent terminal nodes, producing more parsimonious and interpretable models. Third, we address inference by constructing valid confidence intervals for median survival times within the subgroups identified by the final tree. To achieve this, we apply bootstrap-based bias correction to standard errors. The proposed method is assessed through extensive simulation studies and illustrated with data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.
[13]
arXiv:2509.18580
[pdf, html, other]
Title:
Adaptive Bayesian Joint Latent Space Modeling via Cumulative Shrinkage
Bin Lv, Yincai Tang, Siliang Zhang
Subjects:
Methodology (stat.ME)
Network models are increasingly vital in psychometrics for analyzing relational data, which are often accompanied by high-dimensional node attributes. Joint latent space models (JLSM) provide an elegant framework for integrating these data sources by assuming a shared underlying latent representation; however, a persistent methodological challenge is determining the dimension of the latent space, as existing methods typically require pre-specification or rely on computationally intensive post-hoc procedures. We develop a novel Bayesian joint latent space model that incorporates a cumulative ordered spike-and-slab (COSS) prior. This approach enables the latent dimension to be inferred automatically and simultaneously with all model parameters. We develop an efficient Markov Chain Monte Carlo (MCMC) algorithm for posterior computation. Theoretically, we establish that the posterior distribution concentrates on the true latent dimension and that parameter estimates achieve Hellinger consistency at a near-optimal rate that adapts to the unknown dimensionality. Through extensive simulations and two real-data applications, we demonstrate the method's superior performance in both dimension recovery and parameter estimation. Our work offers a principled, computationally efficient, and theoretically grounded solution for adaptive dimension selection in psychometric network models.
[14]
arXiv:2509.18708
[pdf, other]
Title:
Optimization-centric cutting feedback for semiparametric models
Linda S. L. Tan, David J. Nott, David T. Frazier
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Computation (stat.CO)
Modern statistics deals with complex models from which the joint model used for inference is built by coupling submodels, called modules. We consider modular inference where the modules may depend on parametric and nonparametric components. In such cases, a joint Bayesian inference is highly susceptible to misspecification across any module, and inappropriate priors for nonparametric components may deliver subpar inferences for parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. The proposed generalized cut posteriors are defined through a variational optimization problem for generalized posteriors where regularization is based on Rényi divergence, rather than Kullback-Leibler divergence (KLD), and variational computational methods are developed. We show empirically that using Rényi divergence to define the cut posterior delivers more robust inferences than KLD. We derive novel posterior concentration results that accommodate the Rényi divergence and allow for semiparametric components, greatly extending existing results for cut posteriors that were derived for parametric models and KLD. We demonstrate these new methods in a benchmark toy example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.
[15]
arXiv:2509.18739
[pdf, html, other]
Title:
Consistency of Selection Strategies for Fraud Detection
Christos Revelas, Otilia Boldea, Bas J.M. Werker
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper studies how insurers can chose which claims to investigate for fraud. Given a prediction model, typically only claims with the highest predicted propability of being fraudulent are investigated. We argue that this can lead to inconsistent learning and propose a randomized alternative. More generally, we draw a parallel with the multi-arm bandit literature and argue that, in the presence of selection, the obtained observations are not iid. Hence, dependence on past observations should be accounted for when updating parameter estimates. We formalize selection in a binary regression framework and show that model updating and maximum-likelihood estimation can be implemented as if claims were investigated at random. Then, we define consistency of selection strategies and conjecture sufficient conditions for consistency. Our simulations suggest that the often-used selection strategy can be inconsistent while the proposed randomized alternative is consistent. Finally, we compare our randomized selection strategy with Thompson sampling, a standard multi-arm bandit heuristic. Our simulations suggest that the latter can be inefficient in learning low fraud probabilities.
[16]
arXiv:2509.18780
[pdf, html, other]
Title:
Tractable Approximation of Labeled Multi-Object Posterior Densities
Thi Hong Thai Nguyen, Ba-Ngu Vo, Ba-Tuong Vo
Subjects:
Methodology (stat.ME); Signal Processing (eess.SP)
Multi-object estimation in state-space models (SSMs) wherein the system state is represented as a finite set has attracted significant interest in recent years. In Bayesian inference, the posterior density captures all information on the system trajectory since it considers the past history of states. In most multi-object SSM applications, closed-form multi-object posteriors are not available for non-standard multi-object models. Thus, functional approximation is necessary because these posteriors are very high-dimensional. This work provides a tractable multi-scan Generalized Labeled Multi-Bernoulli (GLMB) approximation that matches the trajectory cardinality distribution of the labeled multi-object posterior density. The proposed approximation is also proven to minimize the Kullback-Leibler divergence over a special class of multi-scan GLMB model. Additionally, we develop a tractable algorithm for computing the approximate multi-object posteriors over finite windows. Numerical experiments, including simulation results on a multi-object SSM with social force model and uninformative observations, are presented to validate the applicability of the approximation method.
[17]
arXiv:2509.18875
[pdf, html, other]
Title:
Dynamic Prediction in Mixture Cure Models: A Model-Based Landmarking Approach
Marta Cipriani, Marco Alfò, Mirko Signorelli
Subjects:
Methodology (stat.ME)
Mixture cure models are widely used in survival analysis when a portion of patients is considered cured and is no longer at risk for the event of interest. In clinical settings, dynamic survival prediction is particularly important to refine prognosis by incorporating updated patient information over time. Landmarking methods have emerged as a flexible approach for this purpose, as they allow to summarize longitudinal covariates up to a given landmark time and to use these summaries in subsequent prediction. For mixture cure models, the only landmarking strategy available in the literature relies on the last observation carried forward (LOCF) method to summarize longitudinal dynamics up to the landmark time. However, LOCF discards most of the longitudinal information, does not correct for measurement error, and may rely on outdated values if observation times are far apart. To overcome these limitations, we propose a sequential approach that integrates model-based landmarking within a mixture cure model. Initially, longitudinal covariates are modeled using (generalized) linear mixed models, from which individual-specific random effects are predicted. The predicted random effects are then incorporated as covariates into a Cox proportional hazards cure model. We investigated the performance of the proposed approach under different cure fractions, sample sizes, and longitudinal data structures through an extensive simulation study. The results show that the model-based strategy provides more refined predictions compared to LOCF, even when the model is misspecified in favour of the LOCF approach. Finally, we illustrate our method using a real-world dataset on renal transplant patients.
[18]
arXiv:2509.18978
[pdf, html, other]
Title:
Improving Cramér-Rao Bound With Multivariate Parameters: An Extrinsic Geometry Perspective
Sunder Ram Krishnan
Comments:
Vector parameter extension of work done in arXiv:2509.17886
Subjects:
Statistics Theory (math.ST); Differential Geometry (math.DG); Probability (math.PR)
We derive a vector generalization of the square root embedding-based curvature-corrected Cramér--Rao bound (CRB) previously considered by the same author in \cite{srk} with scalar parameters. A \emph{directional} curvature correction is established first, and sufficient conditions for a conservative matrix-level CRB refinement are formulated using a simple semidefinite program. The directional correction theorem is rigorously illustrated with a Gaussian example.
[19]
arXiv:2509.18983
[pdf, html, other]
Title:
Markov Combinations of Discrete Statistical Models
Orlando Marigliano, Eva Riccomagno
Comments:
23 pages, 3 figures
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
Markov combination is an operation that takes two statistical models and produces a third whose marginal distributions include those of the original models. Building upon and extending existing work in the Gaussian case, we develop Markov combinations for categorical variables and their statistical models. We present several variants of this operation, both algorithmically and from a sampling perspective, and discuss relevant examples and theoretical properties. We describe Markov combinations for special models such as regular exponential families, discrete copulas, and staged trees. Finally, we offer results about model invariance and the maximum likelihood estimation of Markov combinations.
[20]
arXiv:2509.19007
[pdf, html, other]
Title:
Causal tail coefficient for compound extremes in multivariate time series
Cathy Yin, Adam M. Sykulski, Almut E.D. Veraart
Subjects:
Methodology (stat.ME)
Extreme events are often multivariate in nature. A compound extreme occurs when a combination of variables jointly produces a significant impact, even if individual components are not necessarily marginally extreme. Compound extremes have been observed across a wide range of domains, including space weather, climate, and environmental science. For example, heavy rainfall sustained over consecutive days can impose cumulative stress on urban drainage systems, potentially resulting in flooding. However, most existing methods for detecting extremal causality focus primarily on individual extreme values and lack the flexibility to capture causal relationships between compound extremes. This work introduces a novel framework for detecting causal dependencies between extreme events, including compound extremes. We introduce the compound causal tail coefficient that captures the extremal dependance of compound events between pairs of stationary time series. Based on a consistent estimator of this coefficient, we develop a bootstrap hypothesis test to evaluate the presence and direction of causal relationships. Our method can accommodate nonlinearity and latent confounding variables. We demonstrate the effectiveness of our method by establishing theoretic properties and through simulation studies and an application to space-weather data.
[21]
arXiv:2509.19040
[pdf, html, other]
Title:
Nonparametric efficient estimation of the longitudinal front-door functional
Marie S. Breum, Helene C. W. Rytgaard, Torben Martinussen, Erin E. Gabriel
Subjects:
Methodology (stat.ME)
The front-door criterion is an identification strategy for the intervention-specific mean outcome in settings where the standard back-door criterion fails due to unmeasured exposure-outcome confounders, but an intermediate variable exists that completely mediates the effect of exposure on the outcome and is not affected by unmeasured confounding. The front-door criterion has been extended to the longitudinal setting, where exposure and mediator are measured repeatedly over time. However, to the best of our knowledge, applications of the longitudinal front-door criterion remain unexplored. This may reflect both limited awareness of the method and the absence of suitable estimation techniques. In this report, we propose nonparametric efficient estimators of the longitudinal front-door functional. The estimators are multiply robust and allow for the use of data-adaptive (machine learning) methods for nuisance estimation while providing valid inference. The theoretical properties of the estimators are showcased in a simulation study.
[22]
arXiv:2509.19123
[pdf, html, other]
Title:
George Udny Yule and the Interpretation of Regression Betas
Francesco Corielli
Subjects:
Other Statistics (stat.OT)
Initially applied in astronomy and geodesy, the linear regression model aimed to find the best estimates for parameters with predefined meanings. E.g., orbital elements, geodetic constants. As its use expanded to other disciplines, often to summarize data without an underlying theoretical model, the need for a general interpretation of regression betas arose. Early attempts by Galton and Karl Pearson met with mixed success. G. U. Yule was the first to develop a general statistical interpretation, the culmination of efforts begun in 1896. Yule interpretation is based on the partial regression theorem, which he proved in 1907.
[23]
arXiv:2509.19126
[pdf, html, other]
Title:
Modified Lepage-type test statistics for the weak null hypothesis
Abid Hussain, Michail Tsagris
Subjects:
Methodology (stat.ME)
Detecting simultaneous shifts in the location and scale of two populations is a challenging problem in statistical research. A common way to address this issue is by combining location and scale test statistics. A well-known example is the Lepage test, which combines the Wilcoxon-Mann-Whitney test for location with the Ansari-Bradley test for scale. However, the Wilcoxon-Mann-Whitney test assumes that the population variances are equal, while the Ansari-Bradley test assumes the population medians are equal. This study introduces new approaches that combine recent methodological advances to relax these assumptions. We incorporate the Fligner-Policello test, a distribution-free alternative to the Wilcoxon-Mann-Whitney test that does not require the assumption of equal variances. The Fligner-Policello test is further enhanced by the Fong-Huang method, which provides an improved variance estimation. Additionally, we propose a new variance estimator for the Ansari-Bradley test, thereby eliminating the need for the equal medians assumption. These methodological modifications are integrated into the Lepage framework to operate under a weak null hypothesis. Simulation results suggest that these new tests are promising candidates for location-scale testing. The practical utility of the proposed tests is then demonstrated through an analysis of four real-world biomedical datasets. These empirical applications confirm the robustness and reliability of the modified tests for the two-sample independent location-scale problem.
[24]
arXiv:2509.19226
[pdf, html, other]
Title:
Neighbor Embeddings Using Unbalanced Optimal Transport Metrics
Muhammad Rana, Keaton Hamm
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper proposes the use of the Hellinger--Kantorovich metric from unbalanced optimal transport (UOT) in a dimensionality reduction and learning (supervised and unsupervised) pipeline. The performance of UOT is compared to that of regular OT and Euclidean-based dimensionality reduction methods on several benchmark datasets including MedMNIST. The experimental results demonstrate that, on average, UOT shows improvement over both Euclidean and OT-based methods as verified by statistical hypothesis tests. In particular, on the MedMNIST datasets, UOT outperforms OT in classification 81\% of the time. For clustering MedMNIST, UOT outperforms OT 83\% of the time and outperforms both other metrics 58\% of the time.
[25]
arXiv:2509.19250
[pdf, html, other]
Title:
Recovering Wasserstein Distance Matrices from Few Measurements
Muhammad Rana, Abiy Tasissa, HanQin Cai, Yakov Gavriyelov, Keaton Hamm
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper proposes two algorithms for estimating square Wasserstein distance matrices from a small number of entries. These matrices are used to compute manifold learning embeddings like multidimensional scaling (MDS) or Isomap, but contrary to Euclidean distance matrices, are extremely costly to compute. We analyze matrix completion from upper triangular samples and Nyström completion in which $\mathcal{O}(d\log(d))$ columns of the distance matrices are computed where $d$ is the desired embedding dimension, prove stability of MDS under Nyström completion, and show that it can outperform matrix completion for a fixed budget of sample distances. Finally, we show that classification of the OrganCMNIST dataset from the MedMNIST benchmark is stable on data embedded from the Nyström estimation of the distance matrix even when only 10\% of the columns are computed.
[26]
arXiv:2509.19276
[pdf, html, other]
Title:
A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models
Tim Y. J. Wang, O. Deniz Akyildiz
Comments:
Accepted at the 2nd Workshop on Frontiers in Probabilistic Inference: Sampling Meets Learning, 39th Conference on Neural Information Processing Systems (NeurIPS 2025)
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
Solving ill-posed inverse problems requires powerful and flexible priors. We propose leveraging pretrained latent diffusion models for this task through a new training-free approach, termed Diffusion-regularized Wasserstein Gradient Flow (DWGF). Specifically, we formulate the posterior sampling problem as a regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the latent space. We demonstrate the performance of our method on standard benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.
[27]
arXiv:2509.19285
[pdf, html, other]
Title:
The information flow among Green Bonds exchange traded funds
Wenderson Gomes Barbosa, Kerolly Kedma Felix do Nascimento, Fabio Sandro dos Santos, Tiago A. E. Ferreira
Comments:
27 pages,13 figures
Subjects:
Applications (stat.AP)
This article investigates the information flow between 13 Green Bond ETFs (Exchange Traded Funds) from three global markets: the USA, Canada,and Europe, between 2021 and 2022. We used the transfer entropy and effective transfer entropy methods to model and investigate the Green Bond price information flow between these global markets. The American market demonstrated market dominance among the other two markets (Canadian and European). The FLMB Green Bond of the American ETF presented the greatest flow of information transfer among the ETFs analyzed, being considered the dominant ETF among the three Green Bond ETF markets investigated. The HGGB ETF has emerged as a major information transmitter in Europe and in the Canadian market, but it has had a strong influence from the American ETF FLMB. In the European market, the FLRG and this http URL bonds played a major role in the flow of information sent to other ETFs in Europe. The KLMH.F in Europe is highlighted as the largest receiver of information. Thus, through this article, it was possible to understand the direction of the flow of information between the Green Bond ETF markets and their dimensionality.
Cross submissions (showing 24 of 24 entries)
[28]
arXiv:2108.00490
(cross-list from cs.LG)
[pdf, html, other]
Title:
A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning and ABC
F. Llorente, L. Martino, J. Read, D. Delgado
Journal-ref:
International Statistical Review. 2024
Subjects:
Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME); Machine Learning (stat.ML)
This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.
[29]
arXiv:2509.18124
(cross-list from cs.LG)
[pdf, html, other]
Title:
Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters
Edmund Agyemang, Lawrence Agbota, Vincent Agbenyeavu, Peggy Akabuah, Bismark Bimpong, Christopher Attafuah
Comments:
13 pages, 6 figures and 4 tables
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
This study explores the application of supervised machine learning algorithms to predict coffee ratings based on a combination of influential textual and numerical attributes extracted from user reviews. Through careful data preprocessing including text cleaning, feature extraction using TF-IDF, and selection with SelectKBest, the study identifies key factors contributing to coffee quality assessments. Six models (Decision Tree, KNearest Neighbors, Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained and evaluated using optimized hyperparameters. Model performance was assessed primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as Multi-layer Perceptron, consistently outperform simpler classifiers (Decision Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1 scores, G-mean and AUC. The findings highlight the essence of rigorous feature selection and hyperparameter tuning in building robust predictive systems for sensory product evaluation, offering a data driven approach to complement traditional coffee cupping by expertise of trained professionals.
[30]
arXiv:2509.18141
(cross-list from cs.LG)
[pdf, html, other]
Title:
KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots
Yao Zhao, Haoyue Sun, Yantian Ding, Yanxun Xu
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Applications (stat.AP); Machine Learning (stat.ML)
Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots provides valuable insights for evidence synthesis in clinical research. However, existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD without manual input or intervention. Its hybrid reasoning architecture automates the conversion of unstructured information into structured data flows and validates data extraction from complex KM plots. To improve accessibility, KM-GPT is equipped with a user-friendly web interface and an integrated AI assistant, enabling researchers to reconstruct IPD without requiring programming expertise. KM-GPT was rigorously evaluated on synthetic and real-world datasets, consistently demonstrating superior accuracy. To illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and biomarker-based subgroup analyses. By automating traditionally manual processes and providing a scalable, web-based solution, KM-GPT transforms clinical research by leveraging reconstructed IPD to enable more informed downstream analyses, supporting evidence-based decision-making.
[31]
arXiv:2509.18149
(cross-list from math.NA)
[pdf, other]
Title:
Tensor Train Completion from Fiberwise Observations Along a Single Mode
Shakir Showkat Sofi, Lieven De Lathauwer
Comments:
Submitted to Numerical Algorithms (28 pages)
Subjects:
Numerical Analysis (math.NA); Machine Learning (cs.LG); Signal Processing (eess.SP); Optimization and Control (math.OC); Computation (stat.CO); Machine Learning (stat.ML)
Tensor completion is an extension of matrix completion aimed at recovering a multiway data tensor by leveraging a given subset of its entries (observations) and the pattern of observation. The low-rank assumption is key in establishing a relationship between the observed and unobserved entries of the tensor. The low-rank tensor completion problem is typically solved using numerical optimization techniques, where the rank information is used either implicitly (in the rank minimization approach) or explicitly (in the error minimization approach). Current theories concerning these techniques often study probabilistic recovery guarantees under conditions such as random uniform observations and incoherence requirements. However, if an observation pattern exhibits some low-rank structure that can be exploited, more efficient algorithms with deterministic recovery guarantees can be designed by leveraging this structure. This work shows how to use only standard linear algebra operations to compute the tensor train decomposition of a specific type of ``fiber-wise" observed tensor, where some of the fibers of a tensor (along a single specific mode) are either fully observed or entirely missing, unlike the usual entry-wise observations. From an application viewpoint, this setting is relevant when it is easier to sample or collect a multiway data tensor along a specific mode (e.g., temporal). The proposed completion method is fast and is guaranteed to work under reasonable deterministic conditions on the observation pattern. Through numerical experiments, we showcase interesting applications and use cases that illustrate the effectiveness of the proposed approach.
[32]
arXiv:2509.18228
(cross-list from q-bio.QM)
[pdf, other]
Title:
Forest tree species classification and entropy-derived uncertainty mapping using extreme gradient boosting and Sentinel-1/2 data
Abdulhakim M. Abdi, Fan Wang
Comments:
24 pages, 6 figures, 2 tables
Subjects:
Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)
We present a new 10-meter map of dominant tree species in Swedish forests accompanied by pixel-level uncertainty estimates. The tree species classification is based on spatiotemporal metrics derived from Sentinel-1 and Sentinel-2 satellite data, combined with field observations from the Swedish National Forest Inventory. We apply an extreme gradient boosting model with Bayesian optimization to relate field observations to satellite-derived features and generate the final species map. Classification uncertainty is quantified using Shannon's entropy of the predicted class probabilities, which provide a spatially explicit measure of model confidence. The final model achieved an overall accuracy of 85% (F1 score = 0.82, Matthews correlation coefficient = 0.81), and mapped species distributions showed strong agreement with official forest statistics (r = 0.96).
[33]
arXiv:2509.18310
(cross-list from eess.SP)
[pdf, html, other]
Title:
On Multi-entity, Multivariate Quickest Change Point Detection
Bahar Kor, Bipin Gaikwad, Abani Patra, Eric L. Miller
Subjects:
Signal Processing (eess.SP); Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)
We propose a framework for online Change Point Detection (CPD) from multi-entity, multivariate time series data, motivated by applications in crowd monitoring where traditional sensing methods (e.g., video surveillance) may be infeasible. Our approach addresses the challenge of detecting system-wide behavioral shifts in complex, dynamic environments where the number and behavior of individual entities may be uncertain or evolve. We introduce the concept of Individual Deviation from Normality (IDfN), computed via a reconstruction-error-based autoencoder trained on normal behavior. We aggregate these individual deviations using mean, variance, and Kernel Density Estimates (KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or abrupt changes, we apply statistical deviation metrics and the Cumulative Sum (CUSUM) technique to these scores. Our unsupervised approach eliminates the need for labeled data or feature extraction, enabling real-time operation on streaming input. Evaluations on both synthetic datasets and crowd simulations, explicitly designed for anomaly detection in group behaviors, demonstrate that our method accurately detects significant system-level changes, offering a scalable and privacy-preserving solution for monitoring complex multi-agent systems. In addition to this methodological contribution, we introduce new, challenging multi-entity multivariate time series datasets generated from crowd simulations in Unity and coupled nonlinear oscillators. To the best of our knowledge, there is currently no publicly available dataset of this type designed explicitly to evaluate CPD in complex collective and interactive systems, highlighting an essential gap that our work addresses.
[34]
arXiv:2509.18452
(cross-list from cs.LG)
[pdf, html, other]
Title:
Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion
Anton Lebedev, Won Kyung Lee, Soumyadip Ghosh, Olha I. Yaman, Vassilis Kalantzis, Yingdong Lu, Tomasz Nowicki, Shashanka Ubaru, Lior Horesh, Vassil Alexandrov
Comments:
8 pages, 3 figures, 1 algorithm, 1 table of experiment cases
Subjects:
Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)
Large, sparse linear systems are pervasive in modern science and engineering, and Krylov subspace solvers are an established means of solving them. Yet convergence can be slow for ill-conditioned matrices, so practical deployments usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix inversion can generate such preconditioners and accelerate Krylov iterations, but its effectiveness depends on parameters whose optima vary across matrices; manual or grid search is costly. We present an AI-driven framework recommending MCMC parameters for a given linear system. A graph neural surrogate predicts preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition function then chooses the parameter sets most likely to minimise iterations. On a previously unseen ill-conditioned system, the framework achieves better preconditioning with 50\% of the search budget of conventional methods, yielding about a 10\% reduction in iterations to convergence. These results suggest a route for incorporating MCMC-based preconditioners into large-scale systems.
[35]
arXiv:2509.18469
(cross-list from cs.LG)
[pdf, html, other]
Title:
Probabilistic Geometric Principal Component Analysis with application to neural data
Han-Lin Hsieh, Maryam M. Shanechi
Comments:
Published at the International Conference on Learning Representations (ICLR) 2025. Code is available at GitHub this https URL
Journal-ref:
ICLR 2025
Subjects:
Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)
Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold.
[36]
arXiv:2509.18504
(cross-list from cs.CV)
[pdf, html, other]
Title:
Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning
Jiaxin Dai, Xiang Xiang
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)
In the field of machine learning, hyperbolic space demonstrates superior representation capabilities for hierarchical data compared to conventional Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe approach, which contrastively learns coarse class labels and subsequently normalizes and freezes the classifier weights of learned fine classes in the embedding space. To better interpret the "coarse-to-fine" paradigm, we propose embedding the feature extractor into hyperbolic space. Specifically, we employ the Poincaré ball model of hyperbolic space, enabling the feature extractor to transform input images into feature vectors within the Poincaré ball instead of Euclidean space. We further introduce hyperbolic contrastive loss and hyperbolic fully-connected layers to facilitate model optimization and classification in hyperbolic space. Additionally, to enhance performance under few-shot conditions, we implement maximum entropy distribution in hyperbolic space to estimate the probability distribution of fine-class feature vectors. This allows generation of augmented features from the distribution to mitigate overfitting during training with limited samples. Experiments on C2FSCIL benchmarks show that our method effectively improves both coarse and fine class accuracies.
[37]
arXiv:2509.18614
(cross-list from quant-ph)
[pdf, html, other]
Title:
Connecting Quantum Computing with Classical Stochastic Simulation
Jose Blanchet, Mark S. Squillante, Mario Szegedy, Guanyang Wang
Comments:
15 pages, tutorial paper prepared for the 2025 Winter Simulation Conference
Subjects:
Quantum Physics (quant-ph); Numerical Analysis (math.NA); Computational Finance (q-fin.CP); Computation (stat.CO)
This tutorial paper introduces quantum approaches to Monte Carlo computation with applications in computational finance. We outline the basics of quantum computing using Grover's algorithm for unstructured search to build intuition. We then move slowly to amplitude estimation problems and applications to counting and Monte Carlo integration, again using Grover-type iterations. A hands-on Python/Qiskit implementation illustrates these concepts applied to finance. The paper concludes with a discussion on current challenges in scaling quantum simulation techniques.
[38]
arXiv:2509.18695
(cross-list from physics.ao-ph)
[pdf, html, other]
Title:
Quantifying the Effect of a Parallax Correcting Algorithm for Passive Microwave Satellite Precipitation Retrievals across the Continental United States
Andres F. Monsalve, Hernan A. Moreno, Eric Goldenstern, Christian Kummerow
Comments:
This Work has been accepted to Journal of Hydrometeorology. The AMS does not guarantee that the copy provided here is an accurate copy of the Version of Record (VoR)
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP)
Satellite precipitation retrieval algorithms whose measurement instruments are tilted to the zenith line are subject to a spatial mismatch between the theoretical ground coordinates and the coordinate pair corresponding to the cloud layers sending spectral signals to the satellite. This is the case of the precipitation retrievals of the GPM Passive Microwave Imagery (GMI) on board the core satellite of the Global Precipitation Mission (GPM) that uses the Goddard Profiling Algorithm (GPROF). Currently, no geometrical correction is applied to GMI retrievals of surface precipitation, creating a horizontal displacement (or parallax mismatching) between the reported surface and the corrected coordinates corresponding to the cloud structures intersecting the field of view.
GPROF precipitation retrievals over the Continental United States are analyzed using the ground-validated Multi-Resolution Multi-Sensor (GV-MRMS) system data and the European Centre for Medium-Range Weather Forecasts Reanalysis version 5 (ERA5) temperature profiles. Results applying this parallax correction scheme show improvements in the overall retrieval accuracy of GPROF, mainly during the summer months, for every precipitation type, when the freezing level (FL) is relatively high. The development of this new parallax-correction algorithm for passive microwave radiometers will significantly improve the accuracy of remote sensing data by minimizing spatial distortions in atmospheric measurements, leading to more precise weather forecasting, climate monitoring, and environmental assessments.
[39]
arXiv:2509.18766
(cross-list from cs.LG)
[pdf, html, other]
Title:
Diagonal Linear Networks and the Lasso Regularization Path
Raphaël Berthier
Comments:
29 pages, 1 figure
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
Diagonal linear networks are neural networks with linear activation and diagonal weight matrices. Their theoretical interest is that their implicit regularization can be rigorously analyzed: from a small initialization, the training of diagonal linear networks converges to the linear predictor with minimal 1-norm among minimizers of the training loss. In this paper, we deepen this analysis showing that the full training trajectory of diagonal linear networks is closely related to the lasso regularization path. In this connection, the training time plays the role of an inverse regularization parameter. Both rigorous results and simulations are provided to illustrate this conclusion. Under a monotonicity assumption on the lasso regularization path, the connection is exact while in the general case, we show an approximate connection.
[40]
arXiv:2509.18820
(cross-list from q-fin.ST)
[pdf, html, other]
Title:
Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market
Marcin Wątorek, Marija Bezbradica, Martin Crane, Jarosław Kwapień, Stanisław Drożdż
Subjects:
Statistical Finance (q-fin.ST); Computational Engineering, Finance, and Science (cs.CE); Econometrics (econ.EM); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)
Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
[41]
arXiv:2509.18857
(cross-list from econ.EM)
[pdf, html, other]
Title:
Optimal estimation for regression discontinuity design with binary outcomes
Takuya Ishihara, Masayuki Sawada, Kohei Yata
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST); Methodology (stat.ME)
We develop a finite-sample optimal estimator for regression discontinuity designs when the outcomes are bounded, including binary outcomes as the leading case. Our finite-sample optimal estimator achieves the exact minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function of a bounded outcome lies in a Lipschitz class. Although the original minimax problem involves an iterating (n+1)-dimensional non-convex optimization problem where n is the sample size, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of our estimator is that the Lipschitz constant is the only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than conventional large-sample techniques which may be unreliable when the effective sample size is small. We apply our method to an empirical multi-cutoff design where the sample size for each cutoff is small. In the application, our method yields informative confidence intervals, in contrast to the leading large-sample approach.
[42]
arXiv:2509.18887
(cross-list from econ.EM)
[pdf, html, other]
Title:
Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting
Yousef Adeli Sadabad, Mohammad Reza Hesamzadeh, Gyorgy Dan, Matin Bagherpour, Darryl R. Biggar
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST)
The System Price (SP) of the Nordic electricity market serves as a key reference for financial hedge contracts such as Electricity Price Area Differentials (EPADs) and other risk management instruments. Therefore, the identification of drivers and the accurate forecasting of SP are essential for market participants to design effective hedging strategies. This paper develops a systematic framework that combines interpretable drivers analysis with robust forecasting methods. It proposes an interpretable feature engineering algorithm to identify the main drivers of the Nordic SP based on a novel combination of K-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal Autoregressive Integrated Moving Average (SARIMA) model. Then, it applies principal component analysis (PCA) to the identified data matrix, which is adapted to the downstream task of price forecasting to mitigate the issue of imperfect multicollinearity in the data. Finally, we propose a multi-forecast selection-shrinkage algorithm for Nordic SP forecasting, which selects a subset of complementary forecast models based on their bias-variance tradeoff at the ensemble level and then computes the optimal weights for the retained forecast models to minimize the error variance of the combined forecast. Using historical data from the Nordic electricity market, we demonstrate that the proposed approach outperforms individual input models uniformly, robustly, and significantly, while maintaining a comparable computational cost. Notably, our systematic framework produces superior results using simple input models, outperforming the state-of-the-art Temporal Fusion Transformer (TFT). Furthermore, we show that our approach also exceeds the performance of several well-established practical forecast combination methods.
[43]
arXiv:2509.18964
(cross-list from cs.LG)
[pdf, html, other]
Title:
Central Limit Theorems for Asynchronous Averaged Q-Learning
Xingtu Liu
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
This paper establishes central limit theorems for Polyak-Ruppert averaged Q-learning under asynchronous updates. We present a non-asymptotic central limit theorem, where the convergence rate in Wasserstein distance explicitly reflects the dependence on the number of iterations, state-action space size, the discount factor, and the quality of exploration. In addition, we derive a functional central limit theorem, showing that the partial-sum process converges weakly to a Brownian motion.
[44]
arXiv:2509.19029
(cross-list from math.OC)
[pdf, other]
Title:
Clapping: Removing Per-sample Storage for Pipeline Parallel Distributed Optimization with Communication Compression
Boao Kong, Xu Huang, Yuqi Xu, Yixuan Liang, Bin Wang, Kun Yuan
Comments:
60 pages
Subjects:
Optimization and Control (math.OC); Machine Learning (stat.ML)
Pipeline-parallel distributed optimization is essential for large-scale machine learning but is challenged by significant communication overhead from transmitting high-dimensional activations and gradients between workers. Existing approaches often depend on impractical unbiased gradient assumptions or incur sample-size memory overhead. This paper introduces Clapping, a Communication compression algorithm with LAzy samPling for Pipeline-parallel learnING. Clapping adopts a lazy sampling strategy that reuses data samples across steps, breaking sample-wise memory barrier and supporting convergence in few-epoch or online training regimes. Clapping comprises two variants including Clapping-FC and Clapping-FU, both of which achieve convergence without unbiased gradient assumption, effectively addressing compression error propagation in multi-worker settings. Numerical experiments validate the performance of Clapping across different learning tasks.
[45]
arXiv:2509.19088
(cross-list from cs.CY)
[pdf, other]
Title:
A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement
Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia
Subjects:
Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Applications (stat.AP)
Do "digital twins" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines.
[46]
arXiv:2509.19098
(cross-list from cs.LG)
[pdf, html, other]
Title:
Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning
Adrien Prevost, Timothee Mathieu, Odalric-Ambrym Maillard
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST)
We study the non-contextual multi-armed bandit problem in a transfer learning setting: before any pulls, the learner is given N'_k i.i.d. samples from each source distribution nu'_k, and the true target distributions nu_k lie within a known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first derive a problem-dependent asymptotic lower bound on cumulative regret that extends the classical Lai-Robbins result to incorporate the transfer parameters (d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that matches this new bound in the Gaussian case. Finally, we validate our approach via simulations, showing that KL-UCB-Transfer significantly outperforms the no-prior baseline when source and target distributions are sufficiently close.
[47]
arXiv:2509.19104
(cross-list from cs.LG)
[pdf, html, other]
Title:
DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment
Sharan Sahu, Martin T. Wells
Comments:
70 pages, 9 figures, 3 tables
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.
[48]
arXiv:2509.19151
(cross-list from math.PR)
[pdf, html, other]
Title:
Sharp Large Deviations and Gibbs Conditioning for Threshold Models in Portfolio Credit Risk
Fengnan Deng, Anand N. Vidyashankar, Jeffrey F. Collamore
Subjects:
Probability (math.PR); Statistics Theory (math.ST); Mathematical Finance (q-fin.MF); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM)
We obtain sharp large deviation estimates for exceedance probabilities in dependent triangular array threshold models with a diverging number of latent factors. The prefactors quantify how latent-factor dependence and tail geometry enter at leading order, yielding three regimes: Gaussian or exponential-power tails produce polylogarithmic refinements of the Bahadur-Rao $n^{-1/2}$ law; regularly varying tails yield index-driven polynomial scaling; and bounded-support (endpoint) cases lead to an $n^{-3/2}$ prefactor. We derive these results through Laplace-Olver asymptotics for exponential integrals and conditional Bahadur-Rao estimates for the triangular arrays. Using these estimates, we establish a Gibbs conditioning principle in total variation: conditioned on a large exceedance event, the default indicators become asymptotically i.i.d., and the loss-given-default distribution is exponentially tilted (with the boundary case handled by an endpoint analysis). As illustrations, we obtain second-order approximations for Value-at-Risk and Expected Shortfall, clarifying when portfolios operate in the genuine large-deviation regime. The results provide a transferable set of techniques-localization, curvature, and tilt identification-for sharp rare-event analysis in dependent threshold systems.
[49]
arXiv:2509.19189
(cross-list from cs.LG)
[pdf, html, other]
Title:
Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws
Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu
Comments:
52 pages, accepted by NeurIPS 2025 as a spotlight paper
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.
[50]
arXiv:2509.19235
(cross-list from eess.SP)
[pdf, html, other]
Title:
On the Performance of THz Wireless Systems over $α$-$\mathcal{F}$ Channels with Beam Misalignment and Mobility
Wamberto J. L. Queiroz, Hugerles S. Silva, Higo T. P. Silva, Alexandros-Apostolos A. Boulogeorgos
Subjects:
Signal Processing (eess.SP); Statistics Theory (math.ST)
This paper investigates the performance of terahertz~(THz) wireless systems over the $\alpha$-$\mathcal{F}$ fading channels with beam misalignment and mobility. New expressions are derived for the probability density, cumulative distribution, and moment generating functions, as well as higher-order moments of the instantaneous signal-to-noise ratio. Building upon the aforementioned expressions, we extract novel formulas for the outage probability, symbol error probability, and average channel capacity. Asymptotic metrics are also deduced, which provide useful insights. Monte Carlo simulations results are presented to support the derived analytical framework.
[51]
arXiv:2509.19242
(cross-list from cs.DS)
[pdf, html, other]
Title:
Linear Regression under Missing or Corrupted Coordinates
Ilias Diakonikolas, Jelena Diakonikolas, Daniel M. Kane, Jasper C.H. Lee, Thanasis Pittas
Subjects:
Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
We study multivariate linear regression under Gaussian covariates in two settings, where data may be erased or corrupted by an adversary under a coordinate-wise budget. In the incomplete data setting, an adversary may inspect the dataset and delete entries in up to an $\eta$-fraction of samples per coordinate; a strong form of the Missing Not At Random model. In the corrupted data setting, the adversary instead replaces values arbitrarily, and the corruption locations are unknown to the learner. Despite substantial work on missing data, linear regression under such adversarial missingness remains poorly understood, even information-theoretically. Unlike the clean setting, where estimation error vanishes with more samples, here the optimal error remains a positive function of the problem parameters. Our main contribution is to characterize this error up to constant factors across essentially the entire parameter range. Specifically, we establish novel information-theoretic lower bounds on the achievable error that match the error of (computationally efficient) algorithms. A key implication is that, perhaps surprisingly, the optimal error in the missing data setting matches that in the corruption setting-so knowing the corruption locations offers no general advantage.
Replacement submissions (showing 32 of 32 entries)
[52]
arXiv:2403.13488
(replaced)
[pdf, html, other]
Title:
The DeepJoint algorithm: An innovative approach for studying the longitudinal evolution of quantitative mammographic density and its association with screen-detected breast cancer risk
Manel Rakez, Julien Guillaumin, Aurelien Chick, Gaelle Coureau, Foucauld Chamming's, Pierre Fillard, Brice Amadeo, Virginie Rondeau
Subjects:
Applications (stat.AP)
Mammographic density is a dynamic risk factor for breast cancer and affects the sensitivity of mammography-based screening. While automated machine and deep learning-based methods provide more consistent and precise measurements compared to subjective BI-RADS assessments, they often fail to account for the longitudinal evolution of density. Many of these methods assess mammographic density in a cross-sectional manner, overlooking correlations in repeated measures, irregular visit intervals, missing data, and informative dropouts. Joint models, however, are well-suited for capturing the longitudinal relationship between biomarkers and survival outcomes. We present the DeepJoint algorithm, an open-source solution that integrates deep learning for quantitative mammographic density estimation with joint modeling to assess the longitudinal relationship between mammographic density and breast cancer risk. Our method efficiently analyzes processed mammograms from various manufacturers, estimating both dense area and percent density--established risk factors for breast cancer. We utilize a joint model to explore their association with breast cancer risk and provide individualized risk predictions. Bayesian inference and the Monte Carlo consensus algorithm make the approach reliable for large screening datasets. Our method allows for accurate analysis of processed mammograms from multiple manufacturers, offering a comprehensive view of breast cancer risk based on individual longitudinal density profiles. The complete pipeline is publicly available, promoting broader application and comparison with other methods.
[53]
arXiv:2403.16673
(replaced)
[pdf, html, other]
Title:
Quasi-randomization tests for network interference
Supriya Tiwari, Pallavi Basu
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
Network interference amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity and are assumption-lean. In this paper, we incorporate the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. To build the (non-sharp) null distribution of no spillover effects, we use random graph null models. We show that our method is exactly valid in finite samples under mild assumptions. Our method displays enhanced power over state-of-the-art methods, with a substantial improvement in cluster randomized trials. We illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China.
[54]
arXiv:2406.16523
(replaced)
[pdf, html, other]
Title:
YEAST: Yet Another Sequential Test
Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo
Comments:
18 pages, 1 figure, 7 tables
Subjects:
Methodology (stat.ME)
Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of online experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.
[55]
arXiv:2408.13430
(replaced)
[pdf, html, other]
Title:
The ICML 2023 Ranking Experiment: Examining Author Self-Assessment in ML/AI Peer Review
Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie Su
Comments:
Minor revision of Section 4; Published in Journal of the American Statistical Association (JASA) as a Discussion Paper
Subjects:
Applications (stat.AP); Digital Libraries (cs.DL); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Machine Learning (stat.ML)
We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML), asking authors with multiple submissions to rank their papers based on perceived quality. In total, we received 1,342 rankings, each from a different author, covering 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using the author-provided rankings. Our analysis shows that these ranking-calibrated scores outperform the raw review scores in estimating the ground truth ``expected review scores'' in terms of both squared and absolute error metrics. Furthermore, we propose several cautious, low-risk applications of the Isotonic Mechanism and author-provided rankings in peer review, including supporting senior area chairs in overseeing area chairs' recommendations, assisting in the selection of paper awards, and guiding the recruitment of emergency reviewers.
[56]
arXiv:2408.13901
(replaced)
[pdf, html, other]
Title:
On the minimum strength of (unobserved) covariates to overturn an insignificant result
Danielle Tsao, Ronan Perry, Carlos Cinelli
Subjects:
Statistics Theory (math.ST); Applications (stat.AP)
We study conditions under which the addition of variables to a regression equation can turn a previously statistically insignificant result into a significant one. Specifically, we characterize the minimum strength of association required for these variables--both with the dependent and independent variables, or with the dependent variable alone--to elevate the observed t-statistic above a specified significance threshold. Interestingly, we show that it is considerably difficult to overturn a statistically insignificant result solely by reducing the standard error. Instead, included variables must also alter the point estimate to achieve such reversals in practice. Our results can be used to conduct sensitivity analyses against unobserved variables and to bound the maximum t-value one can obtain given different subsets of observed covariates, and may also offer algebraic explanations for patterns of reversals seen in empirical research, such as those documented by Lenz and Sahn (2021).
[57]
arXiv:2410.04020
(replaced)
[pdf, html, other]
Title:
"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring
Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang
Comments:
21 pages, 1 table, 1 figure
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. Therefore, two approaches have been recently proposed for the pre-specified analysis of OS as a safety endpoint (Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.
[58]
arXiv:2410.04359
(replaced)
[pdf, html, other]
Title:
Semiparametric Spatial Point Processes
Xindi Lin, Bumjun Park, Christopher Zahasky, Hyunseung Kang
Subjects:
Methodology (stat.ME)
We introduce a broad class of models called semiparametric spatial point process for making inference between spatial point patterns and spatial covariates. These models feature an intensity function with both parametric and nonparametric components. For the parametric component, we derive the semiparametric efficiency lower bound under Poisson point patterns and propose a point process double machine learning estimator that can achieve this lower bound. The proposed estimator for the parametric component is also shown to be consistent and asymptotically normal for non-Poisson point patterns. For the nonparametric component, we propose a kernel-based estimator and characterize its rates of convergence. Computationally, we introduce a fast, numerical approximation that transforms the proposed estimator into an estimator derived from weighted generalized partial linear models. We conclude with a simulation study and two real data analyses from ecology and hydrogeology.
[59]
arXiv:2502.03848
(replaced)
[pdf, other]
Title:
Consistent model selection in a collection of stochastic block models
Lucie Arts (LPSM (UMR\_8001))
Subjects:
Statistics Theory (math.ST)
We introduce the penalized Krichevsky-Trofimov (KT) estimator as a convergent method for estimating the number of nodes clusters when observing multiple networks within both multi-layer and dynamic Stochastic Block Models. We establish the consistency of the KT estimator, showing that it converges to the correct number of clusters in both types of models when the number of nodes in the networks increases. Our estimator does not require a known upper bound on this number to be consistent. Furthermore, we show that these consistency results hold in both dense and sparse regimes, making the penalized KT estimator robust across various network configurations. We illustrate its performance on synthetic datasets.
[60]
arXiv:2502.13085
(replaced)
[pdf, html, other]
Title:
A Neural Difference-of-Entropies Estimator for Mutual Information
Haoran Ni, Martin Lotz
Comments:
23 pages, 17 figures
Subjects:
Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG)
Estimating Mutual Information (MI), a key measure of dependence of random quantities without specific modelling assumptions, is a challenging problem in high dimensions. We propose a novel mutual information estimator based on parametrizing conditional densities using normalizing flows, a deep generative model that has gained popularity in recent years. This estimator leverages a block autoregressive structure to achieve improved bias-variance trade-offs on standard benchmark tasks.
[61]
arXiv:2502.18475
(replaced)
[pdf, html, other]
Title:
Least squares variational inference
Yvann Le Fay, Nicolas Chopin, Simon Barthelmé
Comments:
NeurIPS 2025, 41 pages, 8 figures
Subjects:
Computation (stat.CO)
Variational inference consists in finding the best approximation of a target distribution within a certain family, where `best' means (typically) smallest Kullback-Leiber divergence. We show that, when the approximation family is exponential, the best approximation is the solution of a fixed-point equation. We introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant of the corresponding fixed-point recursion, where each iteration boils down to ordinary least squares regression and does not require computing gradients. We show that LSVI is equivalent to stochastic mirror descent; we use this insight to derive convergence guarantees. We introduce various ideas to improve LSVI further when the approximation family is Gaussian, leading to a $O(d^3)$ complexity in the dimension $d$ of the target in the full-covariance case, and a $O(d)$ complexity in the mean-field case. We show that LSVI outperforms state-of-the-art methods in a range of examples, while remaining gradient-free, that is, it does not require computing gradients.
[62]
arXiv:2504.08396
(replaced)
[pdf, html, other]
Title:
Fairness is in the details: Face Dataset Auditing
Valentin Lafargue, Emmanuelle Claeys, Jean-Michel Loubes
Subjects:
Applications (stat.AP)
Auditing involves verifying the proper implementation of a given policy. As such, auditing is essential for ensuring compliance with the principles of fairness, equity, and transparency mandated by the European Union's AI Act. Moreover, biases present during the training phase of a learning system can persist in the modeling process and result in discrimination against certain subgroups of individuals when the model is deployed in production. Assessing bias in image datasets is a particularly complex task, as it first requires a feature extraction step, then to consider the extraction's quality in the statistical tests. This paper proposes a robust methodology for auditing image datasets based on so-called "sensitive" features, such as gender, age, and ethnicity. The proposed methodology consists of both a feature extraction phase and a statistical analysis phase. The first phase introduces a novel convolutional neural network (CNN) architecture specifically designed for extracting sensitive features with a limited number of manual annotations. The second phase compares the distributions of sensitive features across subgroups using a novel statistical test that accounts for the imprecision of the feature extraction model. Our pipeline constitutes a comprehensive and fully automated methodology for dataset auditing. We illustrate our approach using two manually annotated datasets. The code and datasets are available at this http URL.
[63]
arXiv:2504.12617
(replaced)
[pdf, html, other]
Title:
Bayesian Multivariate Density-Density Regression
Khai Nguyen, Yang Ni, Peter Mueller
Comments:
56 pages, 25 figures, 1 table
Subjects:
Methodology (stat.ME); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)
We introduce a novel and scalable Bayesian framework for multivariate-density-density regression (DDR), designed to model relationships between multivariate distributions. Our approach addresses the critical issue of distributions residing in spaces of differing dimensions. We utilize a generalized Bayes framework, circumventing the need for a fully specified likelihood by employing the sliced Wasserstein distance to measure the discrepancy between fitted and observed distributions. This choice not only handles high-dimensional data and varying sample sizes efficiently but also facilitates a Metropolis-adjusted Langevin algorithm (MALA) for posterior inference. Furthermore, we establish the posterior consistency of our generalized Bayesian approach, ensuring that the posterior distribution concentrates around the true parameters as the sample size increases. Through simulations and application to a population-scale single-cell dataset, we show that Bayesian DDR provides robust fits, superior predictive performance compared to traditional methods, and valuable insights into complex biological interactions.
[64]
arXiv:2504.13110
(replaced)
[pdf, html, other]
Title:
Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time
Margalit Glasgow, Denny Wu, Joan Bruna
Comments:
72 pages
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We study the approximation gap between the dynamics of a polynomial-width neural network and its infinite-width counterpart, both trained using projected gradient descent in the mean-field scaling regime. We demonstrate how to tightly bound this approximation gap through a differential equation governed by the mean-field dynamics. A key factor influencing the growth of this ODE is the local Hessian of each particle, defined as the derivative of the particle's velocity in the mean-field dynamics with respect to its position. We apply our results to the canonical feature learning problem of estimating a well-specified single-index model; we permit the information exponent to be arbitrarily large, leading to convergence times that grow polynomially in the ambient dimension $d$. We show that, due to a certain ``self-concordance'' property in these problems -- where the local Hessian of a particle is bounded by a constant times the particle's velocity -- polynomially many neurons are sufficient to closely approximate the mean-field dynamics throughout training.
[65]
arXiv:2506.10899
(replaced)
[pdf, html, other]
Title:
Demystifying Spectral Feature Learning for Instrumental Variable Regression
Dimitri Meunier, Antoine Moulin, Jakub Wornbard, Vladimir R. Kostic, Arthur Gretton
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
We address the problem of causal effect estimation in the presence of hidden confounders, using nonparametric instrumental variable (IV) regression. A leading strategy employs spectral features - that is, learned features spanning the top eigensubspaces of the operator linking treatments to instruments. We derive a generalization error bound for a two-stage least squares estimator based on spectral features, and gain insights into the method's performance and failure modes. We show that performance depends on two key factors, leading to a clear taxonomy of outcomes. In a good scenario, the approach is optimal. This occurs with strong spectral alignment, meaning the structural function is well-represented by the top eigenfunctions of the conditional operator, coupled with this operator's slow eigenvalue decay, indicating a strong instrument. Performance degrades in a bad scenario: spectral alignment remains strong, but rapid eigenvalue decay (indicating a weaker instrument) demands significantly more samples for effective feature learning. Finally, in the ugly scenario, weak spectral alignment causes the method to fail, regardless of the eigenvalues' characteristics. Our synthetic experiments empirically validate this taxonomy.
[66]
arXiv:2506.13731
(replaced)
[pdf, html, other]
Title:
Probabilistic patient risk profiling with pair-copula constructions
Özge Şahin
Subjects:
Methodology (stat.ME); Applications (stat.AP)
We propose vine copula-based classifiers for probabilistic risk prediction in perioperative settings. We obtain full joint probability models for mixed continuous-ordinal variables by fitting a separate vine copula to each outcome class, capturing nonlinear and tail-asymmetric dependence. In a cohort of 767 elective bowel surgeries (81 serious vs. 686 non-serious complications), posterior probabilities from the fitted vine classification models are used to allocate patients into low-, moderate-, and high-risk groups. Compared to weighted logistic regression and random forests with stratified sampling, the vine copula-based classifiers achieve up to 10% lower class-specific Brier scores and negative log-likelihoods on the out-of-sample. The vine copula-based classifier identifies a large cohort of true low-risk patients potentially eligible for early discharge. Scenario analyses based on the fitted vine copula models provide interpretable risk profiles, including nonlinear relationships between body mass index, surgery duration, and blood loss, which might remain undetected under linear models. These results demonstrate that vine copula-based classifiers offer a reliable and interpretable framework for individualized, probability-based patient risk profiling. As such, they represent a new, promising tool for data-driven decision-making in perioperative care.
[67]
arXiv:2507.05470
(replaced)
[pdf, html, other]
Title:
Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine Learning Framework for Adaptive Risk Forecasting
Agnideep Aich, Ashit Baran Aich, Dipak C. Jain
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We propose Temporal Conformal Prediction (TCP), a distribution-free framework for constructing well-calibrated prediction intervals in nonstationary time series. TCP combines a quantile forecaster with split-conformal calibration on a rolling window and, in its TCP-RM variant, augments the conformal threshold with a Robbins-Monro (RM) offset to steer coverage toward a target level in real time. We benchmark TCP against GARCH, Historical Simulation, and a rolling Quantile Regression (QR) baseline across equities (S&P500), cryptocurrency (Bitcoin), and commodities (Gold). Three consistent findings emerge. First, rolling QR produces the sharpest intervals but is materially under-calibrated (e.g., S&P500: 86.3% vs. 95% target). Second, TCP and TCP-RM achieve near-nominal coverage while delivering substantially narrower intervals than Historical Simulation (e.g., S&P500: 29% reduction in width). Third, the RM update improves calibration with negligible width cost. Crisis-window visualizations around March 2020 show TCP/TCP-RM expanding and contracting intervals promptly as volatility spikes and recedes, with red dots marking days of miscoverage. A sensitivity study confirms robustness to window size and step-size choices. Overall, TCP provides a practical, theoretically grounded solution for calibrated uncertainty quantification under distribution shift, bridging statistical inference and machine learning for risk forecasting.
[68]
arXiv:2507.21434
(replaced)
[pdf, html, other]
Title:
Measuring Sample Quality with Copula Discrepancies
Agnideep Aich, Ashit Baran Aich, Bruce Wade
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern Bayesian machine learning, such as Stochastic Gradient Langevin Dynamics (SGLD), sacrifice asymptotic exactness for computational speed, creating a critical diagnostic gap: traditional sample quality measures fail catastrophically when applied to biased samplers. While powerful Stein-based diagnostics can detect distributional mismatches, they provide no direct assessment of dependence structure, often the primary inferential target in multivariate problems. We introduce the Copula Discrepancy (CD), a principled and computationally efficient diagnostic that leverages Sklar's theorem to isolate and quantify the fidelity of a sample's dependence structure independent of its marginals. Our theoretical framework provides the first structure-aware diagnostic specifically designed for the era of approximate inference. Empirically, we demonstrate that a moment-based CD dramatically outperforms standard diagnostics like effective sample size for hyperparameter selection in biased MCMC, correctly identifying optimal configurations where traditional methods fail. Furthermore, our robust MLE-based variant can detect subtle but critical mismatches in tail dependence that remain invisible to rank correlation-based approaches, distinguishing between samples with identical Kendall's tau but fundamentally different extreme-event behavior. With computational overhead orders of magnitude lower than existing Stein discrepancies, the CD provides both immediate practical value for MCMC practitioners and a theoretical foundation for the next generation of structure-aware sample quality assessment.
[69]
arXiv:2507.22218
(replaced)
[pdf, html, other]
Title:
Attenuation Bias with Latent Predictors
Connor T. Jerzak, Stephen A. Jessee
Comments:
45 pages, 4 figures, 2 tables
Subjects:
Applications (stat.AP)
Many core concepts in political science are latent and therefore can only be measured with error. Measurement error in a predictor attenuates slope coefficient estimates in regression, biasing them toward zero. We show that widely used strategies for correcting attenuation bias -- including instrumental variables and the method of composition -- are themselves biased, sometimes even more than simple regression ignoring the measurement error altogether. We derive appropriate bias correction methods using split-sample measurement strategies. Our approach is modular and can be easily deployed with additive score, factor, or machine learning models, requiring no joint estimation while yielding consistent slopes under standard assumptions. Simulations and applications -- political knowledge, democracy indices, and text-based sentiment -- show stronger relationships after correction, sometimes by 50 percent. Open-source software implements the procedure. Results underscore that latent predictors demand tailored error correction; otherwise, conventional practice can exacerbate bias.
[70]
arXiv:2508.00770
(replaced)
[pdf, html, other]
Title:
On admissibility in post-hoc hypothesis testing
Ben Chugg, Tyron Lardy, Aaditya Ramdas, Peter Grünwald
Comments:
56 pages
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
The validity of classical hypothesis testing requires the significance level $\alpha$ be fixed before any statistical analysis takes place. This is a stringent requirement. For instance, it prohibits updating $\alpha$ during (or after) an experiment due to changing concern about the cost of false positives, or to reflect unexpectedly strong evidence against the null. Perhaps most disturbingly, witnessing a p-value $p\ll\alpha$ vs $p= \alpha- \epsilon$ for tiny $\epsilon > 0$ has no (statistical) relevance for any downstream decision-making. Following recent work of Grünwald (2024), we develop a theory of post-hoc hypothesis testing, enabling $\alpha$ to be chosen after seeing and analyzing the data. To study "good" post-hoc tests we introduce $\Gamma$-admissibility, where $\Gamma$ is a set of adversaries which map the data to a significance level. We classify the set of $\Gamma$-admissible rules for various sets $\Gamma$, showing they must be based on e-values, and recover the Neyman-Pearson lemma when $\Gamma$ is the constant map. We also give a Rao-Blackwellization result, proving that the expected utility of an e-value can be improved (for any concave utility) by conditioning on a sufficient statistic.
[71]
arXiv:2509.08162
(replaced)
[pdf, html, other]
Title:
Survival Analysis with Discrete Biomarkers Under a Semiparametric Bayesian Conditional Poisson Model
Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo
Comments:
Paper has 11 pages, 4 figures and 1 table; supplementary has 12 pages, 8 figures and 13 tables
Subjects:
Methodology (stat.ME)
Discrete biomarkers derived as cell densities or counts from tissue microarrays and immunostaining are widely used to study immune signatures in relation to survival outcomes in cancer. Although routinely collected, these signatures are not measured with exact precision because the sampling mechanism involves examination of small tissue cores from a larger section of interest. We model these error-prone biomarkers as Poisson processes with latent rates, inducing heteroscedasticity in their conditional variance. While critical for tumor histology, such measurement error frameworks remain understudied for conditionally Poisson-distributed covariates. To address this, we propose a Bayesian joint model that incorporates a Dirichlet process (DP) mixture to flexibly characterize the latent covariate distribution. The proposed approach is evaluated using simulation studies which demonstrate a superior bias reduction and robustness to the underlying model in realistic settings when compared to existing methods. We further incorporate Bayes factors for hypothesis testing in the Bayesian semiparametric joint model. The methodology is applied to a survival study of high-grade serous carcinoma where comparisons are made between the proposed and existing approaches. Accompanying R software is available at the GitHub repository listed in the Web Appendices.
[72]
arXiv:2509.17543
(replaced)
[pdf, other]
Title:
Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality
Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett
Comments:
43 pages, 20 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.
[73]
arXiv:2210.09184
(replaced)
[pdf, html, other]
Title:
Packed-Ensembles for Efficient Uncertainty Estimation
Olivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel, Jean-Marc Martinez, Andrei Bursuc, Gianni Franchi
Comments:
Published as a conference paper at ICLR 2023 (notable 25%)
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at this https URL.
[74]
arXiv:2211.09604
(replaced)
[pdf, html, other]
Title:
Cointegration with Occasionally Binding Constraints
James A. Duffy, Sophocles Mavroeidis, Sam Wycherley
Comments:
ii + 58 pp., 4 figures; author accepted manuscript
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST)
In the literature on nonlinear cointegration, a long-standing open problem relates to how a (nonlinear) vector autoregression, which provides a unified description of the short- and long-run dynamics of a vector of time series, can generate 'nonlinear cointegration' in the profound sense of those series sharing common nonlinear stochastic trends. We consider this problem in the setting of the censored and kinked structural VAR (CKSVAR), which provides a flexible yet tractable framework within which to model time series that are subject to threshold-type nonlinearities, such as those arising due to occasionally binding constraints, of which the zero lower bound (ZLB) on short-term nominal interest rates provides a leading example. We provide a complete characterisation of how common linear and nonlinear stochastic trends may be generated in this model, via unit roots and appropriate generalisations of the usual rank conditions, providing the first extension to date of the Granger-Johansen representation theorem to a nonlinearly cointegrated setting, and thereby giving the first successful treatment of the open problem. The limiting common trend processes include regulated, censored and kinked Brownian motions, none of which have previously appeared in the literature on cointegrated VARs. Our results and running examples illustrate that the CKSVAR is capable of supporting a far richer variety of long-run behaviour than is a linear VAR, in ways that may be particularly useful for the identification of structural parameters.
[75]
arXiv:2407.00706
(replaced)
[pdf, html, other]
Title:
Sum-of-norms regularized Nonnegative Matrix Factorization
Andersen Ang, Waqas Bin Hamed, Hans De Sterck
Comments:
27 pages, 8 figures
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
When applying nonnegative matrix factorization (NMF), the rank parameter is generally unknown. This rank, called the nonnegative rank, is usually estimated heuristically since computing its exact value is NP-hard. In this work, we propose an approximation method to estimate the rank on-the-fly while solving NMF. We use the sum-of-norm (SON), a group-lasso structure that encourages pairwise sim- ilarity, to reduce the rank of a factor matrix when the initial rank is overestimated. On various datasets, SON-NMF can reveal the correct nonnegative rank of the data without prior knowledge or parameter tuning. SON-NMF is a nonconvex, nonsmooth, non-separable, and non-proximable problem, making it nontrivial to solve. First, since rank estimation in NMF is NP-hard, the proposed approach does not benefit from lower computational com- plexity. Using a graph-theoretic argument, we prove that the complexity of SON- NMF is essentially irreducible. Second, the per-iteration cost of algorithms for SON-NMF can be high. This motivates us to propose a first-order BCD algorithm that approximately solves SON-NMF with low per-iteration cost via the proximal average operator. SON-NMF exhibits favorable features for applications. Besides the ability to automatically estimate the rank from data, SON-NMF can handle rank-deficient data matrices and detect weak components with small energy. Furthermore, in hyperspectral imaging, SON-NMF naturally addresses the issue of spectral variability.
[76]
arXiv:2410.08226
(replaced)
[pdf, html, other]
Title:
EarthquakeNPP: A Benchmark for Earthquake Forecasting with Neural Point Processes
Samuel Stockman, Daniel Lawson, Maximilian Werner
Subjects:
Geophysics (physics.geo-ph); Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
For decades, classical point process models, such as the epidemic-type aftershock sequence (ETAS) model, have been widely used for forecasting the event times and locations of earthquakes. Recent advances have led to Neural Point Processes (NPPs), which promise greater flexibility and improvements over such classical models. However, the currently-used benchmark for NPPs does not represent an up-to-date challenge in the seismological community, since it contains data leakage and omits the largest earthquake sequence from the region. Additionally, initial earthquake forecasting benchmarks fail to compare NPPs with state-of-the-art forecasting models commonly used in seismology. To address these gaps, we introduce EarthquakeNPP: a collection of benchmark datasets to facilitate testing of NPPs on earthquake data, accompanied by an implementation of the state-of-the-art forecasting model: ETAS. The datasets cover a range of small to large target regions within California, dating from 1971 to 2021, and include different methodologies for dataset generation. Benchmarking experiments, using both log-likelihood and generative evaluation metrics widely recognised in seismology, show that none of the five NPPs tested outperform ETAS. These findings suggest that current NPP implementations are not yet suitable for practical earthquake forecasting. Nonetheless, EarthquakeNPP provides a platform to foster future collaboration between the seismology and machine learning communities.
[77]
arXiv:2411.08019
(replaced)
[pdf, html, other]
Title:
Language Models as Causal Effect Generators
Lucius E.J. Bynum, Kyunghyun Cho
Subjects:
Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME); Machine Learning (stat.ML)
In this work, we present sequence-driven structural causal models (SD-SCMs), a framework for specifying causal models with user-defined structure and language-model-defined mechanisms. We characterize how an SD-SCM enables sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data to test treatment effect estimation. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods for average, conditional average, and individual treatment effect estimation. We find under this benchmark that (1) causal methods outperform non-causal methods and that (2) even state-of-the-art methods struggle with individualized effect estimation, suggesting this benchmark captures some inherent difficulties in causal estimation. Apart from generating data, this same technique can underpin the auditing of language models for (un)desirable causal effects, such as misinformation or discrimination. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.
[78]
arXiv:2501.11869
(replaced)
[pdf, html, other]
Title:
Saturation-Aware Snapshot Compressive Imaging: Theory and Algorithm
Mengyu Zhao, Shirin Jalali
Comments:
13 pages
Subjects:
Image and Video Processing (eess.IV); Information Theory (cs.IT); Applications (stat.AP)
Snapshot Compressive Imaging (SCI) uses coded masks to compress a 3D data cube into a single 2D snapshot. In practice, multiplexing can push intensities beyond the sensor's dynamic range, producing saturation that violates the linear SCI model and degrades reconstruction. This paper provides the first theoretical characterization of SCI recovery under saturation. We model clipping as an element-wise nonlinearity and derive a finite-sample recovery bound for compression-based SCI that links reconstruction error to mask density and the extent of saturation. The analysis yields a clear design rule: optimal Bernoulli masks use densities below one-half, decreasing further as saturation strengthens. Guided by this principle, we optimize mask patterns and introduce a novel reconstruction framework, Saturation-Aware PnP Net (SAPnet), which explicitly enforces consistency with saturated measurements. Experiments on standard video-SCI benchmarks confirm our theory and demonstrate that SAPnet significantly outperforms existing PnP-based methods.
[79]
arXiv:2503.16187
(replaced)
[pdf, html, other]
Title:
Manifold learning in metric spaces
Liane Xu, Amit Singer
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Laplacian-based methods are popular for the dimensionality reduction of data lying in $\mathbb{R}^N$. Several theoretical results for these algorithms depend on the fact that the Euclidean distance locally approximates the geodesic distance on the underlying submanifold which the data are assumed to lie on. However, for some applications, other metrics, such as the Wasserstein distance, may provide a more appropriate notion of distance than the Euclidean distance. We provide a framework that generalizes the problem of manifold learning to metric spaces and study when a metric satisfies sufficient conditions for the pointwise convergence of the graph Laplacian.
[80]
arXiv:2505.18269
(replaced)
[pdf, html, other]
Title:
Representative Action Selection for Large Action Space Meta-Bandits
Quan Zhou, Mark Kozdoba, Shie Mannor
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)
We study the problem of selecting a subset from a large action space shared by a family of bandits, with the goal of achieving performance nearly matching that of using the full action space. We assume that similar actions tend to have related payoffs, modeled by a Gaussian process. To exploit this structure, we propose a simple epsilon-net algorithm to select a representative subset. We provide theoretical guarantees for its performance and compare it empirically to Thompson Sampling and Upper Confidence Bound.
[81]
arXiv:2506.03159
(replaced)
[pdf, html, other]
Title:
Bayes Error Rate Estimation in Difficult Situations
Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi
Comments:
23 pages, 13 figures, 20 tables
Subjects:
Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)
The Bayes Error Rate (BER) is the fundamental limit on the achievable generalizable classification accuracy of any machine learning model due to inherent uncertainty within the data. BER estimators offer insight into the difficulty of any classification problem and set expectations for optimal classification performance. In order to be useful, the estimators must also be accurate with a limited number of samples on multivariate problems with unknown class distributions. To determine which estimators meet the minimum requirements for "usefulness", an in-depth examination of their accuracy is conducted using Monte Carlo simulations with synthetic data in order to obtain their confidence bounds for binary classification. To examine the usability of the estimators for real-world applications, new non-linear multi-modal test scenarios are introduced. In each scenario, 2500 Monte Carlo simulations per scenario are run over a wide range of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques, results show that kNN is overwhelmingly the more accurate non-parametric estimator. In order to reach the target of an under 5% range for the 95% confidence bounds, the minimum number of required samples per class is 1000. As more features are added, more samples are needed, so that 2500 samples per class are required at only 4 features. Other estimators do become more accurate than kNN as more features are added, but continuously fail to meet the target range.
[82]
arXiv:2508.19356
(replaced)
[pdf, html, other]
Title:
Graph Data Modeling: Molecules, Proteins, & Chemical Processes
José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Adrian Jinich, Radhakrishnan Mahadevan, Benjamin Sanchez-Lengeling
Comments:
3 to 4 hours read time. 73 pages. 35 figures
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine. This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes, introduces graphs as mathematical objects in chemistry and shows how learning algorithms (particularly graph neural networks) can operate on them. We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling. Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.
[83]
arXiv:2509.13232
(replaced)
[pdf, html, other]
Title:
Single-stream Policy Optimization
Zhongwen Xu, Zihan Ding
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.
Total of 83 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack