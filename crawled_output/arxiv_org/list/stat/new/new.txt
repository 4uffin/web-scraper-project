Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 23 September 2025
Total of 113 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 35 of 35 entries)
[1]
arXiv:2509.16337
[pdf, html, other]
Title:
Learning Centre Partitions from Summaries
Zinsou Max Debaly, Jean-Francois Ethier, Michael H. Neumann, Félix Camirand Lemyre
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Applications (stat.AP); Machine Learning (stat.ML)
Multi-centre studies increasingly rely on distributed inference, where sites share only centre-level summaries. Homogeneity of parameters across centres is often violated, motivating methods that both \emph{test} for equality and \emph{learn} centre groupings before estimation. We develop multivariate Cochran-type tests that operate on summary statistics and embed them in a sequential, test-driven \emph{Clusters-of-Centres (CoC)} algorithm that merges centres (or blocks) only when equality is not rejected. We derive the asymptotic $\chi^2$-mixture distributions of the test statistics and provide plug-in estimators for implementation. To improve finite-sample integration, we introduce a multi-round bootstrap CoC that re-evaluates merges across independently resampled summary sets; under mild regularity and a separation condition, we prove a \emph{golden-partition recovery} result: as the number of rounds grows with $n$, the true partition is recovered with probability tending to one. We also give simple numerical guidelines, including a plateau-based stopping rule, to make the multi-round procedure reproducible. Simulations and a real-data analysis of U.S.\ airline on-time performance (2007) show accurate heterogeneity detection and partitions that change little with the choice of resampling scheme.
[2]
arXiv:2509.16395
[pdf, html, other]
Title:
Low-Rank Adaptation of Evolutionary Deep Neural Networks for Efficient Learning of Time-Dependent PDEs
Jiahao Zhang, Shiheng Zhang, Guang Lin
Comments:
17 pages
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We study the Evolutionary Deep Neural Network (EDNN) framework for accelerating numerical solvers of time-dependent partial differential equations (PDEs). We introduce a Low-Rank Evolutionary Deep Neural Network (LR-EDNN), which constrains parameter evolution to a low-rank subspace, thereby reducing the effective dimensionality of training while preserving solution accuracy. The low-rank tangent subspace is defined layer-wise by the singular value decomposition (SVD) of the current network weights, and the resulting update is obtained by solving a well-posed, tractable linear system within this subspace. This design augments the underlying numerical solver with a parameter efficient EDNN component without requiring full fine-tuning of all network weights. We evaluate LR-EDNN on representative PDE problems and compare it against corresponding baselines. Across cases, LR-EDNN achieves comparable accuracy with substantially fewer trainable parameters and reduced computational cost. These results indicate that low-rank constraints on parameter velocities, rather than full-space updates, provide a practical path toward scalable, efficient, and reproducible scientific machine learning for PDEs.
[3]
arXiv:2509.16419
[pdf, html, other]
Title:
An Error Model for Evaluating the Accuracy of Satellite-Based XCO$_2$ Products
Vineet Yadav, Jonathan Hobbs, Hai M. Nguyen, Susan S. Kulawik, Junjie Liu, David F. Baker, Isamu Morino, Hirofumi Ohyama, Voltaire A. Velazco, Mihalis Vrekoussis, Manvendra K. Dubey
Subjects:
Applications (stat.AP)
Several satellites (e.g., OCO-2 & 3) and their derived products now provide spatially extensive coverage of the abundance of carbon dioxide in the atmospheric column (XCO$_2$). However, the accuracy of the XCO$_2$ reported in these products needs to be carefully assessed for any downstream scientific analysis; this involves comparison with reference datasets, such as those from the Total Carbon Column Observing Network (TCCON). Previously, systematic and random errors have been used to quantify differences between satellite-based XCO$_2$ measurements and TCCON data. The spatiotemporal density of satellite observations enables the decomposition of the error variability into these components. This study aims to unify the definitions of these error components through a hierarchical statistical model with explicit mathematical terms, which enables a formal definition of the underlying assumptions and estimation of each component. Specifically, we focus on defining model elements, like global bias and systematic and random error, as part of this framework. We use it to compare OCO-2 XCO$_2$ v11.1 data (both original scenes from the `Lite' files and 10-sec averages) and gridded Making Earth System Data Records for Use in Research Environments (MEaSUREs) products to TCCON data. The MEaSUREs products exhibit comparable systematic errors to other OCO-2 products, with larger errors over land versus ocean. We describe the methodology for creating the MEaSUREs products, including their prior and posterior error covariances, with information on spatial correlation for efficient incorporation into scientific analysis.
[4]
arXiv:2509.16466
[pdf, html, other]
Title:
SynthIPD: assumption-lean synthetic individual patient data generation
Zixuan Zhao, Zexin Ren, Guannan Zhai, Feifang Hu, Will Ma, En Xie, Qian Shi
Subjects:
Applications (stat.AP)
Individual patient data (IPD) are essential for statistical inference in clinical research. However, privacy concerns, high data-sharing costs, and restrictive access often make IPD unavailable. Conventional synthetic data generation usually relies on black box models such as generative adversial networks. These methods, however, requires a large piece of IPD for model training, may be ungeneralizable and lacks interpretability. This paper introduces an assumption-lean, three-step methodology for generating synthetic IPD with survival endpoints only based on published clinical trial articles. The method mainly leverages Kaplan-Meier (KM) curves with at-risk/censoring information and subgroup-level summary statistics. It digitizes the KM curve using Scalable Vector Graphics (SVG) beyond pixel accuracy and then generates synthetic covariates based on the statistics. We illustrate the method's potential through $2$ detailed case studies and simulation studies. The method offers important implications, enabling high-fidelity IPD generation to support evidence-based medical decisions.
[5]
arXiv:2509.16627
[pdf, other]
Title:
Conditional Multidimensional Scaling with Incomplete Conditioning Data
Anh Tuan Bui
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Conditional multidimensional scaling seeks for a low-dimensional configuration from pairwise dissimilarities, in the presence of other known features. By taking advantage of available data of the known features, conditional multidimensional scaling improves the estimation quality of the low-dimensional configuration and simplifies knowledge discovery tasks. However, existing conditional multidimensional scaling methods require full data of the known features, which may not be always attainable due to time, cost, and other constraints. This paper proposes a conditional multidimensional scaling method that can learn the low-dimensional configuration when there are missing values in the known features. The method can also impute the missing values, which provides additional insights of the problem. Computer codes of this method are maintained in the cml R package on CRAN.
[6]
arXiv:2509.16636
[pdf, html, other]
Title:
Optimal and Efficient Sample Size Re-estimation: A Dynamic Cost Framework
Rui Jin, Cai Wu, Qiqi Deng
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Adaptive sample size re-estimation (SSR) is a well-established strategy for improving the efficiency and flexibility of clinical trials. Its central challenge is determining whether, and by how much, to increase the sample size at an interim analysis. This decision requires a rational framework for balancing the potential gain in statistical power against the risk and cost of further investment. Prevailing optimization approaches, such as the Jennison and Turnbull (JT) method, address this by maximizing power for a fixed cost per additional participant. While statistically efficient, this paradigm assumes the cost of enrolling another patient is constant, regardless of whether the interim evidence is promising or weak. This can lead to impractical recommendations and inefficient resource allocation, particularly in weak-signal scenarios.
We reframe SSR as a decision problem under dynamic costs, where the effective cost of additional enrollment reflects the interim strength of evidence. Within this framework, we derive two novel rules: (i) a likelihood-ratio based rule, shown to be Pareto optimal in achieving smaller average sample size under the null without loss of power under the alternative; and (ii) a return-on-investment (ROI) rule that directly incorporates economic considerations by linking SSR decisions to expected net benefit. To unify existing methods, we further establish a representation theorem demonstrating that a broad class of SSR rules can be expressed through implicit dynamic cost functions, providing a common analytical foundation for their comparison. Simulation studies calibrated to Phase III trial settings confirm that dynamic-cost approaches improve resource allocation relative to fixed-cost methods.
[7]
arXiv:2509.16663
[pdf, html, other]
Title:
System-Level Uncertainty Quantification with Multiple Machine Learning Models: A Theoretical Framework
Xiaoping Du
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
ML models have errors when used for predictions. The errors are unknown but can be quantified by model uncertainty. When multiple ML models are trained using the same training points, their model uncertainties may be statistically dependent. In reality, model inputs are also random with input uncertainty. The effects of these types of uncertainty must be considered in decision-making and design. This study develops a theoretical framework that generates the joint distribution of multiple ML predictions given the joint distribution of model uncertainties and the joint distribution of model inputs. The strategy is to decouple the coupling between the two types of uncertainty and transform them as independent random variables. The framework lays a foundation for numerical algorithm development for various specific applications.
[8]
arXiv:2509.16803
[pdf, html, other]
Title:
Efficient Brain Network Estimation with Sparse ICA in Non-Human Primate Neuroimaging
Qiang Li, Liang Ma, Masoud Seraji, Shujian Yu, Yun Wang, Jingyu Liu, Vince D. Calhoun
Comments:
Submitted to ICASSP 2026
Subjects:
Applications (stat.AP); Neurons and Cognition (q-bio.NC)
Independent component analysis (ICA) is widely used to separate mixed signals and recover statistically independent components. However, in non-human primate neuroimaging studies, most ICA-recovered spatial maps are often dense. To extract the most relevant brain activation patterns, post-hoc thresholding is typically applied-though this approach is often imprecise and arbitrary. To address this limitation, we employed the Sparse ICA method, which enforces both sparsity and statistical independence, allowing it to extract the most relevant activation maps without requiring additional post-processing. Simulation experiments demonstrate that Sparse ICA performs competitively against 11 classical linear ICA methods. We further applied Sparse ICA to real non-human primate neuroimaging data, identifying several independent component networks spanning different brain networks. These spatial maps revealed clearly defined activation areas, providing further evidence that Sparse ICA is effective and reliable in practical applications.
[9]
arXiv:2509.16842
[pdf, html, other]
Title:
DoubleGen: Debiased Generative Modeling of Counterfactuals
Alex Luedtke, Kenji Fukumizu
Comments:
Keywords: generative modeling, counterfactual, doubly robust, debiased machine learning
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Generative models for counterfactual outcomes face two key sources of bias. Confounding bias arises when approaches fail to account for systematic differences between those who receive the intervention and those who do not. Misspecification bias arises when methods attempt to address confounding through estimation of an auxiliary model, but specify it incorrectly. We introduce DoubleGen, a doubly robust framework that modifies generative modeling training objectives to mitigate these biases. The new objectives rely on two auxiliaries -- a propensity and outcome model -- and successfully address confounding bias even if only one of them is correct. We provide finite-sample guarantees for this robustness property. We further establish conditions under which DoubleGen achieves oracle optimality -- matching the convergence rates standard approaches would enjoy if interventional data were available -- and minimax rate optimality. We illustrate DoubleGen with three examples: diffusion models, flow matching, and autoregressive language models.
[10]
arXiv:2509.17025
[pdf, other]
Title:
Monte Carlo on a single sample
Nils Detering, Paul Eisenberg, Nicole Hufnagel
Comments:
32 pages, 8 figures, 2 tables
Subjects:
Statistics Theory (math.ST); Numerical Analysis (math.NA); Probability (math.PR); Computation (stat.CO)
In this paper, we consider a Monte Carlo simulation method (MinMC) that approximates prices and risk measures for a range $\Gamma$ of model parameters at once. The simulation method that we study has recently gained popularity [HS20, FPP22, BDG24], and we provide a theoretical framework and convergence rates for it. In particular, we show that sample-based approximations to $\mathbb{E}_{\theta}[X]$, where $\theta$ denotes the model and $\mathbb{E}_{\theta}$ the expectation with respect to the distribution $P_\theta$ of the model $\theta$, can be obtained across all $\theta \in \Gamma$ by minimizing a map $V:H\rightarrow \mathbb{R}$ with $H$ a suitable function space. The minimization can be achieved easily by fitting a standard feedforward neural network with stochastic gradient descent. We show that MinMC, which uses only one sample for each model, significantly outperforms a traditional Monte Carlo method performed for multiple values of $\theta$, which are subsequently interpolated. Our case study suggests that MinMC might serve as a new benchmark for parameter-dependent Monte Carlo simulations, which appear not only in quantitative finance but also in many other areas of scientific computing.
[11]
arXiv:2509.17122
[pdf, html, other]
Title:
Insensitivity-induced potential non-uniqueness in system identification of Bouc-Wen models
Adrita Kundu, Suparno Mukhopadhyay
Comments:
23 pages, 18 figures
Subjects:
Other Statistics (stat.OT)
During system identification of a structural system with Bouc-Wen (BW) restoring force mechanisms, the estimated BW parameters may be different for different sets of input-output measurements, indicating potential non-uniqueness in the parameter estimates. Nonetheless, the non-unique and incorrectly estimated BW parameters may result in dynamic responses and hysteretic behaviours which are very similar to those obtained for the correct system. In this work, the existence of alternate sets of BW parameters, which result in hysteretic restoring force behaviour similar to the true system, is studied analytically. Approximate expressions for the rate of change of the hysteretic force with deformation are derived and analyzed in detail. It is shown that alternate sets of BW parameters with significant deviations from a set of "true" BW parameters may exist, which result in the rate of change of the hysteretic force, and consequently, the restoring force behaviour itself, to remain very similar to that obtained with the "true" BW parameters. The existence of these alternate parameters results in potential non-uniqueness of the BW parameter estimates, despite satisfying analytical identifiability requirements. Furthermore, the deviations of the alternate BW parameters depend on the magnitudes of the "true" BW parameters as well as the extent of the hysteretic action being developed by the input excitation. The results are illustrated using different inputs: sinusoidal, El Centro motion, and a suite of ground motions compatible with the Kanai-Tajimi spectrum. The results of this work help in a better understanding of the potential non-uniqueness issues associated with the estimation of the BW parameters from measured responses using any system identification technique, which is caused by the insensitivity of these parameters towards the dynamic responses of the structure.
[12]
arXiv:2509.17128
[pdf, html, other]
Title:
Large Scale Partial Correlation Screening with Uncertainty Quantification
Emily Neo, Peter Radchenko, Bala Rajaratnam
Subjects:
Methodology (stat.ME)
Identifying multivariate dependencies in high-dimensional data is an important problem in large-scale inference. This problem has motivated recent advances in mining (partial) correlations, which focus on the challenging ultra-high dimensional setting where the sample size, n, is fixed, while the number of features, p, grows without bound. The state-of-the-art method for partial correlation screening can lead to undesirable results. This paper introduces a novel principled framework for partial correlation screening with error control (PARSEC), which leverages the connection between partial correlations and regression coefficients. We establish the inferential properties of PARSEC when n is fixed and p grows super-exponentially. First, we provide "fixed-n-large-p" asymptotic expressions for the familywise error rate (FWER) and k-FWER. Equally importantly, our analysis leads to a novel discovery which permits the calculation of exact marginal p-values for controlling the false discovery rate (FDR), and also the positive FDR (pFDR). To our knowledge, no other competing approach in the "fixed-n large-p" setting allows for error control across the spectrum of multiple hypothesis testing metrics. We establish the computational complexity of PARSEC and rigorously demonstrate its scalability to the large p setting. The theory and methods are successfully validated on simulated and real data, and PARSEC is shown to outperform the current state-of-the-art.
[13]
arXiv:2509.17140
[pdf, other]
Title:
An Italian Gender Equality Index
Lorenzo Panebianco
Subjects:
Applications (stat.AP); Methodology (stat.ME)
Following the works on the Gender Equality Index (GEI), we propose a composite indicator to measure the gender gap across Italian regions. Our approach differs from the original GEI in both the selection of indicators and the aggregation methodology. Specifically, the choice of indicators is inspired by the both the GEI and the WeWorld Index Italia, while the aggregation relies on an original variation of the Mazziotta-Pareto Index. Finally, we apply our results drawing 2023 open data.
[14]
arXiv:2509.17155
[pdf, html, other]
Title:
Self-Tuned Rejection Sampling within Gibbs and a Case Study in Small Area Estimation
Andrew M. Raim, Kyle M. Irimata, James A. Livsey
Subjects:
Methodology (stat.ME); Applications (stat.AP); Computation (stat.CO)
When preparing a Gibbs sampler, some conditionals may be unfamiliar distributions without well-known variate generation routines. Rejection sampling may be used to draw from such distributions exactly; however, it can be challenging to obtain practical proposal distributions. A practical proposal is one where accepted draws are not extremely rare occurrences and which is not too computationally intensive to use repeatedly within the Gibbs sampler. Consequently, approximate methods such as Metropolis-Hastings steps tend to be used in this setting. This work revisits the vertical weighted strips (VWS) method of proposal construction from arXiv:2401.09696 for univariate conditionals within Gibbs. VWS constructs a finite mixture based on the form of the target density and provides an upper bound on the rejection probability. The rejection probability can be reduced by refining terms in the finite mixture. Naïvely constructing a new proposal for each target encountered in a Gibbs sampler can be computationally impractical. Instead, we consider proposal distributions which persist over the Gibbs sampler and tune themselves gradually to avoid very high rejection probabilities while discarding mixture terms with low contribution. We explore a motivating application in small area estimation, applied to the estimation of county-level population counts of school-aged children in poverty. Here, a Gibbs sampler for a Bayesian model of interest includes a family of unfamiliar densities to be drawn for each observation in the data. Self-tuned VWS is applied to obtain exact draws within Gibbs while keeping the computational workload of proposal maintenance under control.
[15]
arXiv:2509.17161
[pdf, other]
Title:
A Bayesian dawn in linguistics: Trends, benefits and good practices
Natalia Levshina
Subjects:
Applications (stat.AP)
In recent years, Bayesian statistics has gained traction across a wide range of scientific disciplines. This paper explores the growing application of Bayesian methods within the field of linguistics and considers their future potential. A survey of articles from different linguistics journals indicates that Bayesian regression has transitioned from fringe to fairly mainstream over the past five years. This paper discusses the main drivers of this shift, including the increased availability of user-friendly software and the replicability crisis in adjacent disciplines, which exposed the shortcomings of the traditional statistical paradigm. It outlines the fundamental conceptual distinctions between frequentist and Bayesian approaches, emphasizing how Bayesian methods can help address the problems. Additionally, the paper highlights the methodological benefits of Bayesian regression for a diverse array of research questions and data types. It also identifies key theoretical and practical challenges associated with Bayesian analysis and offers a set of good practices and recommendations for researchers considering the adoption of Bayesian methods.
[16]
arXiv:2509.17251
[pdf, html, other]
Title:
Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization
Jingfeng Wu, Peter L. Bartlett, Jason D. Lee, Sham M. Kakade, Bin Yu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Existing theory suggests that for linear regression problems categorized by capacity and source conditions, gradient descent (GD) is always minimax optimal, while both ridge regression and online stochastic gradient descent (SGD) are polynomially suboptimal for certain categories of such problems. Moving beyond minimax theory, this work provides instance-wise comparisons of the finite-sample risks for these algorithms on any well-specified linear regression problem.
Our analysis yields three key findings. First, GD dominates ridge regression: with comparable regularization, the excess risk of GD is always within a constant factor of ridge, but ridge can be polynomially worse even when tuned optimally. Second, GD is incomparable with SGD. While it is known that for certain problems GD can be polynomially better than SGD, the reverse is also true: we construct problems, inspired by benign overfitting theory, where optimally stopped GD is polynomially worse. Finally, GD dominates SGD for a significant subclass of problems -- those with fast and continuously decaying covariance spectra -- which includes all problems satisfying the standard capacity condition.
[17]
arXiv:2509.17301
[pdf, html, other]
Title:
On Quantification of Borrowing of Information in Hierarchical Bayesian Models
Prasenjit Ghosh, Anirban Bhattacharya, Debdeep Pati
Comments:
36 pages, 2 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
In this work, we offer a thorough analytical investigation into the role of shared hyperparameters in a hierarchical Bayesian model, examining their impact on information borrowing and posterior inference. Our approach is rooted in a non-asymptotic framework, where observations are drawn from a mixed-effects model, and a Gaussian distribution is assumed for the true effect generator. We consider a nested hierarchical prior distribution model to capture these effects and use the posterior means for Bayesian estimation. To quantify the effect of information borrowing, we propose an integrated risk measure relative to the true data-generating distribution. Our analysis reveals that the Bayes estimator for the model with a deeper hierarchy performs better, provided that the unknown random effects are correlated through a compound symmetric structure. Our work also identifies necessary and sufficient conditions for this model to outperform the one nested within it. We further obtain sufficient conditions when the correlation is perturbed. Our study suggests that the model with a deeper hierarchy tends to outperform the nested model unless the true data-generating distribution favors sufficiently independent groups. These findings have significant implications for Bayesian modeling, and we believe they will be of interest to researchers across a wide range of fields.
[18]
arXiv:2509.17382
[pdf, html, other]
Title:
Bias-variance Tradeoff in Tensor Estimation
Shivam Kumar, Haotian Xu, Carlos Misael Madrid Padilla, Yuehaw Khoo, Oscar Hernan Madrid Padilla, Daren Wang
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
We study denoising of a third-order tensor when the ground-truth tensor is not necessarily Tucker low-rank. Specifically, we observe $$ Y=X^\ast+Z\in \mathbb{R}^{p_{1} \times p_{2} \times p_{3}}, $$ where $X^\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$, $$ \| \widetilde{X} - X^* \|_{ \mathrm{F}}^2 = O \Big( \kappa^2 \Big\{ r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k} \Big\} \; + \; \xi_{(r_{1},r_{2},r_{3})}^2\Big) \quad \text{ with high probability.} $$ Here, the bias term $\xi_{(r_1,r_2,r_3)}$ corresponds to the best achievable approximation error of $X^\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$; $\kappa^2$ quantifies the noise level; and the variance term $\kappa^2 \{r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k}\}$ scales with the effective number of free parameters in the estimator $\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias--variance tradeoff: as we increase the ranks of estimator $\widetilde{X}$, the bias $\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.
[19]
arXiv:2509.17385
[pdf, html, other]
Title:
Bayesian Semi-supervised Inference via a Debiased Modeling Approach
Gözde Sert, Abhishek Chakrabortty, Anirban Bhattacharya
Comments:
60 pages (including supplement); to appear in Econometrics and Statistics
Journal-ref:
Econometrics and Statistics (2025)
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)
Inference in semi-supervised (SS) settings has gained substantial attention in recent years due to increased relevance in modern big-data problems. In a typical SS setting, there is a much larger-sized unlabeled data, containing only observations of predictors, and a moderately sized labeled data containing observations for both an outcome and the set of predictors. Such data naturally arises when the outcome, unlike the predictors, is costly or difficult to obtain. One of the primary statistical objectives in SS settings is to explore whether parameter estimation can be improved by exploiting the unlabeled data. We propose a novel Bayesian method for estimating the population mean in SS settings. The approach yields estimators that are both efficient and optimal for estimation and inference. The method itself has several interesting artifacts. The central idea behind the method is to model certain summary statistics of the data in a targeted manner, rather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying appropriate summary statistics crucially relies on a debiased representation of the population mean that incorporates unlabeled data through a flexible nuisance function while also learning its estimation bias. Combined with careful usage of sample splitting, this debiasing approach mitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from the posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete theoretical results, via Bernstein--von Mises theorems, are established, validating all claims, and are further supported through extensive numerical studies. To our knowledge, this is possibly the first work on Bayesian inference in SS settings, and its central ideas also apply more broadly to other Bayesian semi-parametric inference problems.
[20]
arXiv:2509.17411
[pdf, html, other]
Title:
Robust Mixture Models for Algorithmic Fairness Under Latent Heterogeneity
Siqi Li, Molei Liu, Ziye Tian, Chuan Hong, Nan Liu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Standard machine learning models optimized for average performance often fail on minority subgroups and lack robustness to distribution shifts. This challenge worsens when subgroups are latent and affected by complex interactions among continuous and discrete features. We introduce ROME (RObust Mixture Ensemble), a framework that learns latent group structure from data while optimizing for worst-group performance. ROME employs two approaches: an Expectation-Maximization algorithm for linear models and a neural Mixture-of-Experts for nonlinear settings. Through simulations and experiments on real-world datasets, we demonstrate that ROME significantly improves algorithmic fairness compared to standard methods while maintaining competitive average performance. Importantly, our method requires no predefined group labels, making it practical when sources of disparities are unknown or evolving.
[21]
arXiv:2509.17463
[pdf, other]
Title:
A Canonical Variate Analysis Biplot based on the Generalized Singular Value Decomposition
Raeesa Ganey, Sugnet Lubbe
Subjects:
Computation (stat.CO); Methodology (stat.ME)
Canonical Variate Analysis (CVA) is a multivariate statistical technique and a direct application of Linear Discriminant Analysis (LDA) that aims to find linear combinations of variables that best differentiate between groups in a dataset. The data is partitioned into groups based on some predetermined criteria, and then linear combinations of the original variables are derived such that they maximize the separation between the groups. However, a common limitation of this optimization in CVA is that the within cluster scatter matrix must be nonsingular, which restricts the use of datasets when the number of variables is larger than the number of observations. By applying the generalized singular value decomposition (GSVD), the same goal of CVA can be achieved regardless on the number of variables. In this paper we use this approach to show that CVA can be applied and graphical representations to such data can be constructed. Specifically, we will be looking at the construction of a CVA biplot for such data that will display observations as points and variables as axes in a reduced dimension. Finally, we present experimental results that confirm the effectiveness of our approach.
[22]
arXiv:2509.17499
[pdf, other]
Title:
ToMATo: an efficient and robust clustering algorithm for high dimensional datasets. An illustration with spike sorting
Louise Martineau (IRMA), Christophe Pouzat (IRMA), Ségolen Geffray (IRMA)
Subjects:
Applications (stat.AP)
Clustering algorithms became an essential part of the neurophysiological data analysis toolbox in the last twenty five years. Many problems, from the definition of cell types/groups based on morphological, molecular and physiological data to the identification of sub-networks in fMRI data, are now routinely tackled with clustering analysis. Since the datasets to which this type of analysis is applied tend to be defined in larger and larger dimensional spaces, there is a need for efficient and robust clustering methods in high dimension. There is also a need for methods that assume as little as possible about the clusters shape and size. We report here our experience with the ToMATo (Topological Mode Analysis Tool) algorithm. It is based on a definitely deep mathematical theory (algebraic topology), but its Python based open-source implementation is easily accessible to practitioners. We applied ToMATo to a problem we know well, spike sorting. Its capability to work in the ''native'' space of the data (no dimension reduction is required) is remarkable, as well as its robustness with respect to outliers (superposed spikes).
[23]
arXiv:2509.17504
[pdf, html, other]
Title:
A new perspective on dominating the James-Stein estimator
Yuzo Maruyama, Akimichi Takemura
Comments:
11 pages
Subjects:
Statistics Theory (math.ST)
This paper presents a novel approach to constructing estimators that dominate the classical James-Stein estimator under the quadratic loss for multivariate normal means. Building on Stein's risk representation, we introduce a new sufficient condition involving a monotonicity property of a transformed shrinkage function. We derive a general class of shrinkage estimators that satisfy minimaxity and dominance over the James-Stein estimator, including cases with polynomial or logarithmic convergence to the optimal shrinkage factor. We also provide conditions for uniform dominance across dimensions and for improved asymptotic risk performance. We present several examples and numerical validations to illustrate the theoretical results.
[24]
arXiv:2509.17543
[pdf, html, other]
Title:
Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality
Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett
Comments:
43 pages, 20 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.
[25]
arXiv:2509.17557
[pdf, html, other]
Title:
A Bayesian approach to aggregated chemical exposure assessment
Sophie Van Den Neucker, Alexander Grigoriev, Heidi Demaegdt, Jan Mast, Karlien Cheyns, Sofie De Broe, Roberto Cerina
Subjects:
Applications (stat.AP)
Human exposure to chemicals commonly arises from multiple sources, yet traditional assessments often treat these sources in isolation, overlooking their combined impact. We introduce a Bayesian framework for aggregated chemical exposure assessment that explicitly accounts for these intertwined pathways. By integrating diverse datasets - such as consumption surveys, demographics, chemical measurements, and market presence - our approach addresses typical data challenges, including missing values, limited sample sizes, and inconsistencies, while incorporating relevant prior knowledge. Through a simulation-based strategy that reflects the full spectrum of individual exposure scenarios, we derive robust, population-level estimates of aggregated exposure. We demonstrate the value of this method using titanium dioxide, a chemical found in foods, dietary supplements, medicines, and personal care products. By capturing the complexity of real-world exposures, this comprehensive Bayesian approach provides decision-makers with more reliable probabilistic estimates to inform public health policies.
[26]
arXiv:2509.17636
[pdf, html, other]
Title:
Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime
Mohammed Racim Moussa Boudjemaa, Alper Kalle, Xiaoyi Mai, José Henrique de Morais Goulart, Cédric Févotte
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Whitening is a classical technique in unsupervised learning that can facilitate estimation tasks by standardizing data. An important application is the estimation of latent variable models via the decomposition of tensors built from high-order moments. In particular, whitening orthogonalizes the means of a spherical Gaussian mixture model (GMM), thereby making the corresponding moment tensor orthogonally decomposable, hence easier to decompose. However, in the large-dimensional regime (LDR) where data are high-dimensional and scarce, the standard whitening matrix built from the sample covariance becomes ineffective because the latter is spectrally distorted. Consequently, whitened means of a spherical GMM are no longer orthogonal. Using random matrix theory, we derive exact limits for their dot products, which are generally nonzero in the LDR. As our main contribution, we then construct a corrected whitening matrix that restores asymptotic orthogonality, allowing for performance gains in spherical GMM estimation.
[27]
arXiv:2509.17806
[pdf, html, other]
Title:
Bayesian Nonhomogeneous hidden Markov models to leverage routine in physical activity monitoring with informative wear time
Beatrice Cantoni, Savannah V. Rauschendorfer, Michael E. Roth, J. Andrew Livingston, Eugenie S. Kleinerman, Corwin M. Zigler
Subjects:
Applications (stat.AP)
Missing data is among the most prominent challenges in the analysis of physical activity (PA) data collected from wearable devices, with the threat of nonignorabile missingness arising when patterns of device wear relate to underlying activity patterns. We offer a rigorous consideration of assumptions about missing data mechanisms in the context of the common modeling paradigm of state space models with a finite, meaningful, set of underlying PA states. Focusing in particular on hidden Markov models, we identify inherent limitations in the presence of missing data when covariates are required to satisfy common missing data assumptions. In response to this limitation, we propose a Bayesian non-homogeneous state space model that can accommodate covariate dependence in the transitions between latent activity states, which in this case relates to whether patients' routine behavior can inform how they transition between PA states and thus support imputation of missing PA data. We show the benefits of the proposed model for missing data imputation and inference for relevant PA summaries. Our development advances analytic capacity to confront the ubiquitous challenge of missing data when analyzing PA studies using wearables. We illustrate with the analysis of a cohort of adolescent and young adult (AYA) cancer patients who wore commercial Fitbit devices for varying durations during the course of treatment.
[28]
arXiv:2509.17886
[pdf, html, other]
Title:
Improving Cramér-Rao Bound And Its Variants: An Extrinsic Geometry Perspective
Sunder Ram Krishnan
Comments:
27 pages, version 1
Subjects:
Statistics Theory (math.ST); Differential Geometry (math.DG); Probability (math.PR)
This work presents a geometric refinement of the classical Cramér-Rao bound (CRB) by incorporating curvature-aware corrections based on the second fundamental form associated with the statistical model manifold. That is, our formulation shows that relying on the extrinsic geometry of the square root embedding of the manifold in the ambient Hilbert space comprising square integrable functions with respect to a fixed base measure offers a rigorous (and intuitive) way to improve upon the CRB and some of its variants, such as the Bhattacharyya-type bounds, that use higher-order derivatives of the log-likelihood. Precisely, the improved bounds in the latter case make explicit use of the elegant framework offered by employing the Faà di Bruno formula and exponential Bell polynomials in expressing the jets associated with the square root embedding in terms of the raw scores. The interplay between the geometry of the statistical embedding and the behavior of the estimator variance is quantitatively analyzed in concrete examples, showing that our corrections can meaningfully tighten the lower bound, suggesting further exploration into connections with estimator efficiency in more general situations.
[29]
arXiv:2509.17960
[pdf, html, other]
Title:
Everything all at once: On choosing an estimand for multi-component environmental exposures
Kara E. Rudolph, Shodai Inose, Nicholas Williams, Ivan Diaz, Lucia Calderon, Jacqueline M. Torres, Marianthi-Anna Kioumourtzoglou
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Many research questions -- particularly those in environmental health -- do not involve binary exposures. In environmental epidemiology, this includes multivariate exposure mixtures with nondiscrete components. Causal inference estimands and estimators to quantify the relationship between an exposure mixture and an outcome are relatively few. We propose an approach to quantify a relationship between a shift in the exposure mixture and the outcome -- either in the single timepoint or longitudinal setting. The shift in the exposure mixture can be defined flexibly in terms of shifting one or more components, including examining interaction between mixture components, and in terms of shifting the same or different amounts across components. The estimand we discuss has a similar interpretation as a main effect regression coefficient. First, we focus on choosing a shift in the exposure mixture supported by observed data. We demonstrate how to assess extrapolation and modify the shift to minimize reliance on extrapolation. Second, we propose estimating the relationship between the exposure mixture shift and outcome completely nonparametrically, using machine learning in model-fitting. This is in contrast to other current approaches, which employ parametric modeling for at least some relationships, which we would like to avoid because parametric modeling assumptions in complex, nonrandomized settings are tenuous at best. We are motivated by longitudinal data on pesticide exposures among participants in the CHAMACOS Maternal Cognition cohort. We examine the relationship between longitudinal exposure to agricultural pesticides and risk of hypertension. We provide step-by-step code to facilitate the easy replication and adaptation of the approaches we use.
[30]
arXiv:2509.17980
[pdf, html, other]
Title:
Covariance-Corrected WAIC for Bayesian Sequential Data Models
Safaa K. Kadhem
Comments:
20 pages, 2 tables, 1 figure, Preprint
Subjects:
Methodology (stat.ME); Computation (stat.CO)
This paper introduces and develops a theoretical extension of the widely applicable information criterion (WAIC), called the Covariance-Corrected WAIC (CC-WAIC), that applied for Bayesian sequential data models. The CC-WAIC accounts for temporal or structural dependence by incorporating the full posterior covariance structure of the log-likelihood contributions, in contrast to the classical WAIC that assumes conditional independence among data. We exploit the limitations of classical WAIC in the sequential data contexts and derive the CC-WAIC criterion under a theoretical framework. In addition, we propose a bias correction based on effective sample size to improve estimation from Markov Chain Monte Carlo (MCMC) simulations. Furthermore, we highlight the advantages of CC-WAIC in terms of stability and appropriateness for dependent data. This new criterion is supported by formal mathematical derivations, illustrative examples, and discussion of implications for model selection in both classical and modern Bayesian applications. To evaluate the reliability of CC-WAIC under varying data regimes, we conduct simulation experiments across multiple time series lengths (small, medium, and large) and different levels of temporal dependence, enabling a comprehensive performance assessment.
[31]
arXiv:2509.18011
[pdf, html, other]
Title:
Robust, Online, and Adaptive Decentralized Gaussian Processes
Fernando Llorente, Daniel Waxman, Sanket Jantre, Nathan M. Urban, Susan E. Minkoff
Comments:
Submitted to Icassp 2026 Special Session on "Bridging Signal Processing and Machine Learning with Gaussian Processes."
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Signal Processing (eess.SP)
Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling.
[32]
arXiv:2509.18013
[pdf, html, other]
Title:
Fréchet Geodesic Boosting
Yidong Zhou, Su I Iao, Hans-Georg Müller
Comments:
23 pages, 4 figures, 10 tables
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Gradient boosting has become a cornerstone of machine learning, enabling base learners such as decision trees to achieve exceptional predictive performance. While existing algorithms primarily handle scalar or Euclidean outputs, increasingly prevalent complex-structured data, such as distributions, networks, and manifold-valued outputs, present challenges for traditional methods. Such non-Euclidean data lack algebraic structures such as addition, subtraction, or scalar multiplication required by standard gradient boosting frameworks. To address these challenges, we introduce Fréchet geodesic boosting (FGBoost), a novel approach tailored for outputs residing in geodesic metric spaces. FGBoost leverages geodesics as proxies for residuals and constructs ensembles in a way that respects the intrinsic geometry of the output space. Through theoretical analysis, extensive simulations, and real-world applications, we demonstrate the strong performance and adaptability of FGBoost, showcasing its potential for modeling complex data.
[33]
arXiv:2509.18024
[pdf, html, other]
Title:
Core-elements Subsampling for Alternating Least Squares
Dunyao Xue, Mengyu Li, Cheng Meng, Jingyi Zhang
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)
In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.
[34]
arXiv:2509.18037
[pdf, html, other]
Title:
Kernel K-means clustering of distributional data
Amparo Baíllo, Jose R. Berrendero, Martín Sánchez-Signorini
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
We consider the problem of clustering a sample of probability distributions from a random distribution on $\mathbb R^p$. Our proposed partitioning method makes use of a symmetric, positive-definite kernel $k$ and its associated reproducing kernel Hilbert space (RKHS) $\mathcal H$. By mapping each distribution to its corresponding kernel mean embedding in $\mathcal H$, we obtain a sample in this RKHS where we carry out the $K$-means clustering procedure, which provides an unsupervised classification of the original sample. The procedure is simple and computationally feasible even for dimension $p>1$. The simulation studies provide insight into the choice of the kernel and its tuning parameter. The performance of the proposed clustering procedure is illustrated on a collection of Synthetic Aperture Radar (SAR) images.
[35]
arXiv:2509.18047
[pdf, html, other]
Title:
Functional effects models: Accounting for preference heterogeneity in panel data with machine learning
Nicolas Salvadé, Tim Hillel
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)
In this paper, we present a general specification for Functional Effects Models, which use Machine Learning (ML) methodologies to learn individual-specific preference parameters from socio-demographic characteristics, therefore accounting for inter-individual heterogeneity in panel choice data. We identify three specific advantages of the Functional Effects Model over traditional fixed, and random/mixed effects models: (i) by mapping individual-specific effects as a function of socio-demographic variables, we can account for these effects when forecasting choices of previously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of functional effects avoids the incidental parameters problem of the fixed effects model, even when the number of observed choices per individual is small; and (iii) we do not rely on the strong distributional assumptions of the random effects model, which may not match reality. We learn functional intercept and functional slopes with powerful non-linear machine learning regressors for tabular data, namely gradient boosting decision trees and deep neural networks. We validate our proposed methodology on a synthetic experiment and three real-world panel case studies, demonstrating that the Functional Effects Model: (i) can identify the true values of individual-specific effects when the data generation process is known; (ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual heterogeneity in terms of predictive performance, as well as traditional static panel choice models in terms of learning inter-individual heterogeneity. The results indicate that the FI-RUMBoost model, which combines the individual-specific constants of the Functional Effects Model with the complex, non-linear utilities of RUMBoost, performs marginally best on large-scale revealed preference panel data.
Cross submissions (showing 22 of 22 entries)
[36]
arXiv:2509.16224
(cross-list from cs.CY)
[pdf, other]
Title:
Predicting First Year Dropout from Pre Enrolment Motivation Statements Using Text Mining
K.F.B. Soppe, A. Bagheri, S. Nadi, I.G. Klugkist, T. Wubbels, L.D.N.V. Wijngaards-De Meij
Subjects:
Computers and Society (cs.CY); Computation and Language (cs.CL); Machine Learning (cs.LG); Applications (stat.AP)
Preventing student dropout is a major challenge in higher education and it is difficult to predict prior to enrolment which students are likely to drop out and which students are likely to succeed. High School GPA is a strong predictor of dropout, but much variance in dropout remains to be explained. This study focused on predicting university dropout by using text mining techniques with the aim of exhuming information contained in motivation statements written by students. By combining text data with classic predictors of dropout in the form of student characteristics, we attempt to enhance the available set of predictive student characteristics. Our dataset consisted of 7,060 motivation statements of students enrolling in a non-selective bachelor at a Dutch university in 2014 and 2015. Support Vector Machines were trained on 75 percent of the data and several models were estimated on the test data. We used various combinations of student characteristics and text, such as TFiDF, topic modelling, LIWC dictionary. Results showed that, although the combination of text and student characteristics did not improve the prediction of dropout, text analysis alone predicted dropout similarly well as a set of student characteristics. Suggestions for future research are provided.
[37]
arXiv:2509.16233
(cross-list from cs.LG)
[pdf, other]
Title:
Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing
Dipayan Sanpui, Anirban Chandra, Henry Chan, Sukriti Manna, Subramanian KRS Sankaranarayanan
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We present a probabilistic framework to accurately estimate dimensions of additively manufactured components. Using a dataset of 405 parts from nine production runs involving two machines, three polymer materials, and two-part configurations, we examine five key design features. To capture both design information and manufacturing variability, we employ models integrating continuous and categorical factors. For predicting Difference from Target (DFT) values, we test deterministic and probabilistic machine learning methods. Deterministic models, trained on 80% of the dataset, provide precise point estimates, with Support Vector Regression (SVR) achieving accuracy close to process repeatability. To address systematic deviations, we adopt Gaussian Process Regression (GPR) and Bayesian Neural Networks (BNNs). GPR delivers strong predictive performance and interpretability, while BNNs capture both aleatoric and epistemic uncertainties. We investigate two BNN approaches: one balancing accuracy and uncertainty capture, and another offering richer uncertainty decomposition but with lower dimensional accuracy. Our results underscore the importance of quantifying epistemic uncertainty for robust decision-making, risk assessment, and model improvement. We discuss trade-offs between GPR and BNNs in terms of predictive power, interpretability, and computational efficiency, noting that model choice depends on analytical needs. By combining deterministic precision with probabilistic uncertainty quantification, our study provides a rigorous foundation for uncertainty-aware predictive modeling in AM. This approach not only enhances dimensional accuracy but also supports reliable, risk-informed design strategies, thereby advancing data-driven manufacturing methodologies.
[38]
arXiv:2509.16379
(cross-list from cs.LG)
[pdf, html, other]
Title:
EMPEROR: Efficient Moment-Preserving Representation of Distributions
Xinran Liu, Shansita D. Sharma, Soheil Kolouri
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We introduce EMPEROR (Efficient Moment-Preserving Representation of Distributions), a mathematically rigorous and computationally efficient framework for representing high-dimensional probability measures arising in neural network representations. Unlike heuristic global pooling operations, EMPEROR encodes a feature distribution through its statistical moments. Our approach leverages the theory of sliced moments: features are projected onto multiple directions, lightweight univariate Gaussian mixture models (GMMs) are fit to each projection, and the resulting slice parameters are aggregated into a compact descriptor. We establish determinacy guarantees via Carleman's condition and the Cramér-Wold theorem, ensuring that the GMM is uniquely determined by its sliced moments, and we derive finite-sample error bounds that scale optimally with the number of slices and samples. Empirically, EMPEROR captures richer distributional information than common pooling schemes across various data modalities, while remaining computationally efficient and broadly applicable.
[39]
arXiv:2509.16385
(cross-list from q-bio.PE)
[pdf, html, other]
Title:
Parameter variability can produce heavy tails in a model for the spatial distribution of settling organisms
Luis F. Gordillo, Priscilla E. Greenwood
Subjects:
Populations and Evolution (q-bio.PE); Applications (stat.AP)
We show that a simple mechanistic model of spatial dispersal for settling organisms, subject to parameter variability, can generate heavy-tailed radial probability density functions. The movement of organisms in the model consists of a two-dimensional diffusion that ceases after a random time, where the parameters that characterize each of these stages have been randomized. Our findings show that these minimal assumptions can yield heavy-tailed dispersal patterns, providing a simplified framework that increases the understanding of long-distance dispersal events in movement ecology.
[40]
arXiv:2509.16393
(cross-list from cs.LG)
[pdf, html, other]
Title:
Federated Learning for Financial Forecasting
Manuel Noseda, Alberto De Luca, Lukas Von Briel, Nathan Lacour
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
This paper studies Federated Learning (FL) for binary classification of volatile financial market trends. Using a shared Long Short-Term Memory (LSTM) classifier, we compare three scenarios: (i) a centralized model trained on the union of all data, (ii) a single-agent model trained on an individual data subset, and (iii) a privacy-preserving FL collaboration in which agents exchange only model updates, never raw data. We then extend the study with additional market features, deliberately introducing not independent and identically distributed data (non-IID) across agents, personalized FL and employing differential privacy. Our numerical experiments show that FL achieves accuracy and generalization on par with the centralized baseline, while significantly outperforming the single-agent model. The results show that collaborative, privacy-preserving learning provides collective tangible value in finance, even under realistic data heterogeneity and personalization requirements.
[41]
arXiv:2509.16411
(cross-list from cs.IR)
[pdf, html, other]
Title:
Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe
Chong You, Rajesh Jayaram, Ananda Theertha Suresh, Robin Nittka, Felix Yu, Sanjiv Kumar
Comments:
NeurIPS 2025
Subjects:
Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)
Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their simplicity and scalability. However, the Euclidean geometry of the embedding space limits the expressive power of DEs, which may compromise their quality. This paper investigates such limitations in the context of hierarchical retrieval (HR), where the document set has a hierarchical structure and the matching documents for a query are all of its ancestors. We first prove that DEs are feasible for HR as long as the embedding dimension is linear in the depth of the hierarchy and logarithmic in the number of documents. Then we study the problem of learning such embeddings in a standard retrieval setup where DEs are trained on samples of matching query and document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune recipe that significantly improves long-distance retrieval without sacrificing performance on closer documents. We experiment on a realistic hierarchy from WordNet for retrieving documents at various levels of abstraction, and show that pretrain-finetune boosts the recall on long-distance pairs from 19% to 76%. Finally, we demonstrate that our method improves retrieval of relevant products on a shopping queries dataset.
[42]
arXiv:2509.16451
(cross-list from math.OC)
[pdf, html, other]
Title:
Overfitting in Adaptive Robust Optimization
Karl Zhu, Dimitris Bertsimas
Comments:
4 pages, 1 figure, NeuroIPS 2025 ML x OR workshop submission
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
Adaptive robust optimization (ARO) extends static robust optimization by allowing decisions to depend on the realized uncertainty - weakly dominating static solutions within the modeled uncertainty set. However, ARO makes previous constraints that were independent of uncertainty now dependent, making it vulnerable to additional infeasibilities when realizations fall outside the uncertainty set. This phenomenon of adaptive policies being brittle is analogous to overfitting in machine learning. To mitigate against this, we propose assigning constraint-specific uncertainty set sizes, with harder constraints given stronger probabilistic guarantees. Interpreted through the overfitting lens, this acts as regularization: tighter guarantees shrink adaptive coefficients to ensure stability, while looser ones preserve useful flexibility. This view motivates a principled approach to designing uncertainty sets that balances robustness and adaptivity.
[43]
arXiv:2509.16586
(cross-list from cs.LG)
[pdf, html, other]
Title:
Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs
Yukuan Wei, Xudong Li, Lin F. Yang
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Recent advances have significantly improved our understanding of the sample complexity of learning in average-reward Markov decision processes (AMDPs) under the generative model. However, much less is known about the constrained average-reward MDP (CAMDP), where policies must satisfy long-run average constraints. In this work, we address this gap by studying the sample complexity of learning an $\epsilon$-optimal policy in CAMDPs under a generative model. We propose a model-based algorithm that operates under two settings: (i) relaxed feasibility, which allows small constraint violations, and (ii) strict feasibility, where the output policy satisfies the constraint. We show that our algorithm achieves sample complexities of $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ and $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ under the relaxed and strict feasibility settings, respectively. Here, $\zeta$ is the Slater constant indicating the size of the feasible region, $H$ is the span bound of the bias function, and $B$ is the transient time bound. Moreover, a matching lower bound of $\tilde{\Omega}\left(\frac{S A (B+H)}{ \epsilon^2\zeta^2}\right)$ for the strict feasibility case is established, thus providing the first minimax-optimal bounds for CAMDPs. Our results close the theoretical gap in understanding the complexity of constrained average-reward MDPs.
[44]
arXiv:2509.16599
(cross-list from cs.CL)
[pdf, other]
Title:
Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence
Sandro Tsang
Comments:
11 pages, 7 figures and 4 tables. This work describes an information retrieval-driven workflow for medical evidence synthesis, with an application to endometriosis recurrence. The method can be generalized to other systematic reviews. The preregistered protocol is available: this https URL
Subjects:
Computation and Language (cs.CL); Information Retrieval (cs.IR); Applications (stat.AP); Methodology (stat.ME)
Background: Evidence synthesis facilitates evidence-based medicine. Without information retrieval techniques, this task is impossible due to the vast and expanding literature. Objective: Building on prior work, this study evaluates an information retrieval-driven workflow to enhance the efficiency, transparency, and reproducibility of systematic reviews. We use endometriosis recurrence as an ideal case due to its complex and ambiguous literature. Methods: Our hybrid approach integrates PRISMA guidelines with computational techniques. We applied semi-automated deduplication to efficiently filter records before manual screening. This workflow synthesized evidence from randomised controlled trials on the efficacy of a subclass of gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow efficiently reduced the screening workload. It took only 11 days to fetch and filter 812 records. Seven RCTs were eligible, providing evidence from 841 patients in 4 countries. The pooled random-effects model yielded a Risk Ratio (RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity ($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence. Sensitivity analyses and bias assessments supported the robustness of our findings. Conclusion: This study demonstrates an information-retrieval-driven workflow for medical evidence synthesis. Our approach yields valuable clinical results while providing a framework for accelerating the systematic review process. It bridges the gap between clinical research and computer science and can be generalized to other complex systematic reviews.
[45]
arXiv:2509.16959
(cross-list from cs.LG)
[pdf, html, other]
Title:
Gradient Interference-Aware Graph Coloring for Multitask Learning
Santosh Patapati, Trisanth Srinivasan
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)
When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby reducing the final model's performance. To address this, we introduce a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated. The grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, model performance will be improved rather than impeded. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers.
[46]
arXiv:2509.16974
(cross-list from math.OC)
[pdf, other]
Title:
Hessian-guided Perturbed Wasserstein Gradient Flows for Escaping Saddle Points
Naoya Yamamoto, Juno Kim, Taiji Suzuki
Comments:
45 pages, 2 figures. Accepted for publication at NeurIPS 2025
Subjects:
Optimization and Control (math.OC); Machine Learning (stat.ML)
Wasserstein gradient flow (WGF) is a common method to perform optimization over the space of probability measures. While WGF is guaranteed to converge to a first-order stationary point, for nonconvex functionals the converged solution does not necessarily satisfy the second-order optimality condition; i.e., it could converge to a saddle point. In this work, we propose a new algorithm for probability measure optimization, perturbed Wasserstein gradient flow (PWGF), that achieves second-order optimality for general nonconvex objectives. PWGF enhances WGF by injecting noisy perturbations near saddle points via a Gaussian process-based scheme. By pushing the measure forward along a random vector field generated from a Gaussian process, PWGF helps the solution escape saddle points efficiently by perturbing the solution towards the smallest eigenvalue direction of the Wasserstein Hessian. We theoretically derive the computational complexity for PWGF to achieve a second-order stationary point. Furthermore, we prove that PWGF converges to a global optimum in polynomial time for strictly benign objectives.
[47]
arXiv:2509.17051
(cross-list from cs.LG)
[pdf, html, other]
Title:
Enhancing Performance and Calibration in Quantile Hyperparameter Optimization
Riccardo Doyle
Comments:
19 pages, 15 figures, 1 table
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Bayesian hyperparameter optimization relies heavily on Gaussian Process (GP) surrogates, due to robust distributional posteriors and strong performance on limited training samples. GPs however underperform in categorical hyperparameter environments or when assumptions of normality, heteroskedasticity and symmetry are excessively challenged. Conformalized quantile regression can address these estimation weaknesses, while still providing robust calibration guarantees. This study builds upon early work in this area by addressing feedback covariate shift in sequential acquisition and integrating a wider range of surrogate architectures and acquisition functions. Proposed algorithms are rigorously benchmarked against a range of state of the art hyperparameter optimization methods (GP, TPE and SMAC). Findings identify quantile surrogate architectures and acquisition functions yielding superior performance to the current quantile literature, while validating the beneficial impact of conformalization on calibration and search performance.
[48]
arXiv:2509.17154
(cross-list from math.NA)
[pdf, html, other]
Title:
Data-efficient Kernel Methods for Learning Hamiltonian Systems
Yasamin Jalalian, Mostafa Samir, Boumediene Hamzi, Peyman Tavallali, Houman Owhadi
Subjects:
Numerical Analysis (math.NA); Machine Learning (cs.LG); Dynamical Systems (math.DS); Machine Learning (stat.ML)
Hamiltonian dynamics describe a wide range of physical systems. As such, data-driven simulations of Hamiltonian systems are important for many scientific and engineering problems. In this work, we propose kernel-based methods for identifying and forecasting Hamiltonian systems directly from data. We present two approaches: a two-step method that reconstructs trajectories before learning the Hamiltonian, and a one-step method that jointly infers both. Across several benchmark systems, including mass-spring dynamics, a nonlinear pendulum, and the Henon-Heiles system, we demonstrate that our framework achieves accurate, data-efficient predictions and outperforms two-step kernel-based baselines, particularly in scarce-data regimes, while preserving the conservation properties of Hamiltonian dynamics. Moreover, our methodology provides theoretical a priori error estimates, ensuring reliability of the learned models. We also provide a more general, problem-agnostic numerical framework that goes beyond Hamiltonian systems and can be used for data-driven learning of arbitrary dynamical systems.
[49]
arXiv:2509.17175
(cross-list from cs.LG)
[pdf, html, other]
Title:
Detecting Urban PM$_{2.5}$ Hotspots with Mobile Sensing and Gaussian Process Regression
Niál Perry, Peter P. Pedersen, Charles N. Christensen, Emanuel Nussli, Sanelma Heinonen, Lorena Gordillo Dagallier, Raphaël Jacquat, Sebastian Horstmann, Christoph Franck
Comments:
39 pages, 12 figures
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
Low-cost mobile sensors can be used to collect PM$_{2.5}$ concentration data throughout an entire city. However, identifying air pollution hotspots from the data is challenging due to the uneven spatial sampling, temporal variations in the background air quality, and the dynamism of urban air pollution sources. This study proposes a method to identify urban PM$_{2.5}$ hotspots that addresses these challenges, involving four steps: (1) equip citizen scientists with mobile PM$_{2.5}$ sensors while they travel; (2) normalise the raw data to remove the influence of background ambient pollution levels; (3) fit a Gaussian process regression model to the normalised data and (4) calculate a grid of spatially explicit 'hotspot scores' using the probabilistic framework of Gaussian processes, which conveniently summarise the relative pollution levels throughout the city. We apply our method to create the first ever map of PM$_{2.5}$ pollution in Kigali, Rwanda, at a 200m resolution. Our results suggest that the level of ambient PM$_{2.5}$ pollution in Kigali is dangerously high, and we identify the hotspots in Kigali where pollution consistently exceeds the city-wide average. We also evaluate our method using simulated mobile sensing data for Beijing, China, where we find that the hotspot scores are probabilistically well calibrated and accurately reflect the 'ground truth' spatial profile of PM$_{2.5}$ pollution. Thanks to the use of open-source software, our method can be re-applied in cities throughout the world with a handful of low-cost sensors. The method can help fill the gap in urban air quality information and empower public health officials.
[50]
arXiv:2509.17180
(cross-list from cs.LG)
[pdf, html, other]
Title:
Regularizing Extrapolation in Causal Inference
David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, Avi Feller
Subjects:
Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)
Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.
[51]
arXiv:2509.17203
(cross-list from cs.SI)
[pdf, html, other]
Title:
Hodge Decomposition for Urban Traffic Flow: Limits on Dense OD Graphs and Advantages on Road Networks - Los Angeles Case
Yifei Sun
Subjects:
Social and Information Networks (cs.SI); Algebraic Topology (math.AT); Applications (stat.AP)
I study Hodge decomposition (HodgeRank) for urban traffic flow on two graph representations: dense origin--destination (OD) graphs and road-segment networks. Reproducing the method of Aoki et al., we observe that on dense OD graphs the curl and harmonic components are negligible and the potential closely tracks node divergence, limiting the added value of Hodge potentials. In contrast, on a real road network (UTD19, downtown Los Angeles; 15-minute resolution), potentials differ substantially from divergence and exhibit clear morning/evening reversals consistent with commute patterns. We quantify smoothness and discriminability via local/global variances derived from the graph spectrum, and propose flow-aware embeddings that combine topology, bidirectional volume, and net-flow asymmetry for clustering. Code and preprocessing steps are provided to facilitate reproducibility.
[52]
arXiv:2509.17228
(cross-list from cs.LG)
[pdf, html, other]
Title:
Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness
Zihan Liang, Ziwen Pan, Ruoxuan Xiong
Comments:
To appear in Proc. of EMNLP 2025 (18 pages)
Subjects:
Machine Learning (cs.LG); Computation and Language (cs.CL); Methodology (stat.ME)
Clinical notes contain rich patient information, such as diagnoses or medications, making them valuable for patient representation learning. Recent advances in large language models have further improved the ability to extract meaningful representations from clinical texts. However, clinical notes are often missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of patients have no available discharge summaries. In such cases, representations can be learned from other modalities such as structured data, chest X-rays, or radiology reports. Yet the availability of these modalities is influenced by clinical decision-making and varies across patients, resulting in modality missing-not-at-random (MMNAR) patterns. We propose a causal representation learning framework that leverages observed data and informative missingness in multimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion component that integrates structured data, imaging, and text while conditioning on missingness patterns to capture patient health and clinician-driven assignment; (2) a modality reconstruction component with contrastive learning to ensure semantic sufficiency in representation learning; and (3) a multitask outcome prediction model with a rectifier that corrects for residual bias from specific modality observation patterns. Comprehensive evaluations across MIMIC-IV and eICU show consistent gains over the strongest baselines, achieving up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission.
[53]
arXiv:2509.17625
(cross-list from cs.LG)
[pdf, html, other]
Title:
Comparing Data Assimilation and Likelihood-Based Inference on Latent State Estimation in Agent-Based Models
Blas Kolic, Corrado Monti, Gianmarco De Francisci Morales, Marco Pangallo
Subjects:
Machine Learning (cs.LG); Computers and Society (cs.CY); Physics and Society (physics.soc-ph); Methodology (stat.ME)
In this paper, we present the first systematic comparison of Data Assimilation (DA) and Likelihood-Based Inference (LBI) in the context of Agent-Based Models (ABMs). These models generate observable time series driven by evolving, partially-latent microstates. Latent states need to be estimated to align simulations with real-world data -- a task traditionally addressed by DA, especially in continuous and equation-based models such as those used in weather forecasting. However, the nature of ABMs poses challenges for standard DA methods. Solving such issues requires adaptation of previous DA techniques, or ad-hoc alternatives such as LBI. DA approximates the likelihood in a model-agnostic way, making it broadly applicable but potentially less precise. In contrast, LBI provides more accurate state estimation by directly leveraging the model's likelihood, but at the cost of requiring a hand-crafted, model-specific likelihood function, which may be complex or infeasible to derive. We compare the two methods on the Bounded-Confidence Model, a well-known opinion dynamics ABM, where agents are affected only by others holding sufficiently similar opinions. We find that LBI better recovers latent agent-level opinions, even under model mis-specification, leading to improved individual-level forecasts. At the aggregate level, however, both methods perform comparably, and DA remains competitive across levels of aggregation under certain parameter settings. Our findings suggest that DA is well-suited for aggregate predictions, while LBI is preferable for agent-level inference.
[54]
arXiv:2509.17729
(cross-list from cs.LG)
[pdf, other]
Title:
A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis
Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.
[55]
arXiv:2509.17752
(cross-list from cs.LG)
[pdf, html, other]
Title:
GEM-T: Generative Tabular Data via Fitting Moments
Miao Li, Phuc Nguyen, Christopher Tam, Alexandra Morgan, Kenneth Ge, Rahul Bansal, Linzi Yu, Rima Arnaout, Ramy Arnaout
Comments:
18 pages, 4 figures
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Tabular data dominates data science but poses challenges for generative models, especially when the data is limited or sensitive. We present a novel approach to generating synthetic tabular data based on the principle of maximum entropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for tables.'' GEM-T directly captures nth-order interactions -- pairwise, third-order, etc. -- among columns of training data. In extensive testing, GEM-T matches or exceeds deep neural network approaches previously regarded as state-of-the-art in 23 of 34 publicly available datasets representing diverse subject domains (68\%). Notably, GEM-T involves orders-of-magnitude fewer trainable parameters, demonstrating that much of the information in real-world data resides in low-dimensional, potentially human-interpretable correlations, provided that the input data is appropriately transformed first. Furthermore, MaxEnt better handles heterogeneous data types (continuous vs. discrete vs. categorical), lack of local structure, and other features of tabular data. GEM-T represents a promising direction for light-weight high-performance generative models for structured data.
[56]
arXiv:2509.18014
(cross-list from cs.CR)
[pdf, html, other]
Title:
Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data Synthesis
Joshua Ward, Xiaofeng Lin, Chi-Hua Wang, Guang Cheng
Subjects:
Cryptography and Security (cs.CR); Machine Learning (stat.ML)
Tabular Generative Models are often argued to preserve privacy by creating synthetic datasets that resemble training data. However, auditing their empirical privacy remains challenging, as commonly used similarity metrics fail to effectively characterize privacy risk. Membership Inference Attacks (MIAs) have recently emerged as a method for evaluating privacy leakage in synthetic data, but their practical effectiveness is limited. Numerous attacks exist across different threat models, each with distinct implementations targeting various sources of privacy leakage, making them difficult to apply consistently. Moreover, no single attack consistently outperforms the others, leading to a routine underestimation of privacy risk.
To address these issues, we propose a unified, model-agnostic threat framework that deploys a collection of attacks to estimate the maximum empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an open-source Python library that streamlines this auditing process through a novel testbed that integrates seamlessly into existing synthetic data evaluation pipelines through a Scikit-Learn-like API. Our software implements 13 attack methods through a Scikit-Learn-like API, designed to enable fast systematic estimation of privacy leakage for practitioners as well as facilitate the development of new attacks and experiments for researchers.
We demonstrate our framework's utility in the largest tabular synthesis privacy benchmark to date, revealing that higher synthetic data quality corresponds to greater privacy leakage, that similarity-based privacy metrics show weak correlation with MIA results, and that the differentially private generator PATEGAN can fail to preserve privacy under such attacks. This underscores the necessity of MIA-based auditing when designing and deploying Tabular Generative Models.
[57]
arXiv:2509.18025
(cross-list from math.OC)
[pdf, other]
Title:
Deep Learning as the Disciplined Construction of Tame Objects
Gilles Bareilles, Allen Gehret, Johannes Aspman, Jana Lepšová, Jakub Mareček
Comments:
35 pages, 8 figures
Subjects:
Optimization and Control (math.OC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic (math.LO); Machine Learning (stat.ML)
One can see deep-learning models as compositions of functions within the so-called tame geometry. In this expository note, we give an overview of some topics at the interface of tame geometry (also known as o-minimality), optimization theory, and deep learning theory and practice. To do so, we gradually introduce the concepts and tools used to build convergence guarantees for stochastic gradient descent in a general nonsmooth nonconvex, but tame, setting. This illustrates some ways in which tame geometry is a natural mathematical framework for the study of AI systems, especially within Deep Learning.
Replacement submissions (showing 56 of 56 entries)
[58]
arXiv:2202.12414
(replaced)
[pdf, html, other]
Title:
Automated Detection of Short-term Slow Slip Events in Southwest Japan
Yiming Ma, Andreas Anastasiou, Fabien Montiel
Subjects:
Applications (stat.AP); Methodology (stat.ME)
Inferring from the occurrence pattern of slow slip events (SSEs) the probability of triggering a damaging earthquake within the nearby velocity weakening portion of the plate interface is critical for hazard mitigation. Although robust methods exist to detect long-term SSEs consistently and efficiently, detecting short-term SSEs remains a challenge. In this study, we propose a novel statistical approach, called singular spectrum analysis isolate-detect (SSAID), for automatically estimating the start and end times of short-term SSEs in GPS data. The method recasts the problem of detecting SSEs as that of identifying change-points in a piecewise non-linear signal. This is achieved by obscuring the deviation from piecewise-linearity in the underlying SSE signals using added noise. We verify its effectiveness on a range of model-generated synthetic SSE data with different noise levels, and demonstrate its superior performance compared to two existing methods. We illustrate its capability in detecting short-term SSEs in observed GPS data from 36 stations in southwest Japan via the co-occurrence of non-volcanic tremors, hypothesis tests and fault estimation.
[59]
arXiv:2204.10488
(replaced)
[pdf, html, other]
Title:
The Equivariance Criterion in a Linear Model for Fixed-X Cases
Daowei Wang, Mian Wu, Haojin Zhou
Subjects:
Statistics Theory (math.ST)
The field of machine have seen rising applications of equivariance criterion. However, there is no systematic way to justify its usage, including why it works, whether there is an optimal solution and if so, what form it carries. In this article, we explored the usage of equivariance criterion in a normal linear model with fixed-$X$ and extended the model to allow multiple populations, which, in turn, leads to a multivariate invariant location-scale transformation group, compared than the commonly used univariate one. The minimum risk equivariant estimators of the coefficient vector and the diagonal covariance matrix were derived, which were consistent with literature works. This work serves as an early exploration of the usage of equivariance criterion in machine learning, where we confirmed that the least square approach widely used in machine learning indeed carries optimality in some sense at least in the framework of estimation.
Meanwhile, the problems can be shown to be equivalent to a mixture from $p$ independent normal samples and via the principle of functional equivariance, an alternative proof can be derived. However, such an approach carries its own limitation with a strong tie to equivariance criterion.
[60]
arXiv:2303.16008
(replaced)
[pdf, html, other]
Title:
Risk ratio, odds ratio, risk difference... Which causal measure is easier to generalize?
Bénédicte Colnet, Julie Josse, Gaël Varoquaux, Erwan Scornet
Subjects:
Methodology (stat.ME)
There are many measures to report so-called treatment or causal effects: absolute difference, ratio, odds ratio, number needed to treat, and so on. The choice of a measure, e.g. absolute versus relative, is often debated because it leads to different impressions of the benefit or risk of a treatment. Besides, different causal measures may lead to various treatment effect heterogeneity: some input variables may have an influence on some causal measures and no effect at all on others. In addition some measures -- but not all -- have appealing properties such as collapsibility, matching the intuition of a population summary. In this paper, we first review common causal measures and their pros and cons typically brought forward. Doing so, we clarify the notions of collapsibility and treatment effect heterogeneity, unifying existing definitions. Then, we show that for any causal measures there exists a discriminative model such that the conditional average treatment effect (CATE) captures the treatment effect. However, only the risk difference has its CATE and ATE (average treatment effect) disentangled from the baseline, regardless of the outcome type (continuous or binary). As our primary goal is the generalization of causal measures, we show that different sets of covariates are needed to generalize an effect to a target population depending on (i) the causal measure of interest, and (ii) the identification method chosen, that is generalizing either conditional outcome or local effects.
[61]
arXiv:2307.04527
(replaced)
[pdf, html, other]
Title:
Automatic Debiased Machine Learning for Covariate Shifts
Victor Chernozhukov, Michael Newey, Whitney K Newey, Rahul Singh, Vasilis Syrgkanis
Subjects:
Methodology (stat.ME)
We present machine learning estimators for causal and predictive parameters under covariate shift, where covariate distributions differ between training and target populations. One such parameter is the average effect of a policy that alters the covariate distribution, such as a treatment modifying surrogate covariates used to predict long-term outcomes. Another example is the average treatment effect for a population with a shifted covariate distribution, like the effect of a policy on the treated group.
We propose a debiased machine learning method to estimate a broad class of these parameters in a statistically reliable and automatic manner. Our method eliminates regularization biases arising from the use of machine learning tools in high-dimensional settings, relying solely on the parameter's defining formula. It employs data fusion by combining samples from target and training data to eliminate biases. We prove that our estimator is consistent and asymptotically normal. Computational experiments and an empirical study on the impact of minimum wage increases on teen employment--using the difference-in-differences framework with unconfoundedness--demonstrate the effectiveness of our method.
[62]
arXiv:2405.01206
(replaced)
[pdf, other]
Title:
Posterior contraction rates in a sparse non-linear mixed-effects model
Marion Naveau (MIA Paris-Saclay), Maud Delattre (MaIAGE), Laure Sansonnet (MIA Paris-Saclay)
Subjects:
Statistics Theory (math.ST)
Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the regression vector and the covariance matrix of the random effects are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at a rate similar to that established in the linear case.
[63]
arXiv:2411.03992
(replaced)
[pdf, html, other]
Title:
Sparse Bayesian joint modal estimation for exploratory item factor analysis
Keiichiro Hijikata, Motonori Oka, Kensuke Okada
Subjects:
Methodology (stat.ME); Computation (stat.CO)
This study presents a scalable Bayesian estimation algorithm for sparse estimation in exploratory item factor analysis based on a classical Bayesian estimation method, namely Bayesian joint modal estimation (BJME). BJME estimates the model parameters and factor scores that maximize the complete-data joint posterior density. Simulation studies show that the proposed algorithm has high computational efficiency and accuracy in variable selection over latent factors and the recovery of the model parameters. Moreover, we conducted a real data analysis using large-scale data from a psychological assessment that targeted the Big Five personality traits. This result indicates that the proposed algorithm achieves computationally efficient parameter estimation and extracts the interpretable factor loading structure.
[64]
arXiv:2411.17180
(replaced)
[pdf, html, other]
Title:
Validation-Free Sparse Learning: A Phase Transition Approach to Feature Selection
Sylvain Sardy, Maxime van Cutsem, Xiaoyu Ma
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The growing environmental footprint of artificial intelligence (AI), especially in terms of storage and computation, calls for more frugal and interpretable models. Sparse models (e.g., linear, neural networks) offer a promising solution by selecting only the most relevant features, reducing complexity, preventing over-fitting and enabling interpretation-marking a step towards truly intelligent AI.
The concept of a right amount of sparsity (without too many false positive or too few true positive) is subjective. So we propose a new paradigm previously only observed and mathematically studied for compressed sensing (noiseless linear models): obtaining a phase transition in the probability of retrieving the relevant features. We show in practice how to obtain this phase transition for a class of sparse learners. Our approach is flexible and applicable to complex models ranging from linear to shallow and deep artificial neural networks while supporting various loss functions and sparsity-promoting penalties. It does not rely on cross-validation or on a validation set to select its single regularization parameter. For real-world data, it provides a good balance between predictive accuracy and feature sparsity.
A Python package is available at this https URL containing all the simulations and ready-to-use models.
[65]
arXiv:2412.03486
(replaced)
[pdf, html, other]
Title:
Tight PAC-Bayesian Risk Certificates for Contrastive Learning
Anna Van Elst, Debarghya Ghoshdastidar
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Contrastive representation learning is a modern paradigm for learning representations of unlabeled data via augmentations -- precisely, contrastive models learn to embed semantically similar pairs of samples (positive pairs) closer than independently drawn samples (negative samples). In spite of its empirical success and widespread use in foundation models, statistical theory for contrastive learning remains less explored. Recent works have developed generalization error bounds for contrastive losses, but the resulting risk certificates are either vacuous (certificates based on Rademacher complexity or $f$-divergence) or require strong assumptions about samples that are unreasonable in practice. The present paper develops non-vacuous PAC-Bayesian risk certificates for contrastive representation learning, considering the practical considerations of the popular SimCLR framework. Notably, we take into account that SimCLR reuses positive pairs of augmented data as negative samples for other data, thereby inducing strong dependence and making classical PAC or PAC-Bayesian bounds inapplicable. We further refine existing bounds on the downstream classification loss by incorporating SimCLR-specific factors, including data augmentation and temperature scaling, and derive risk certificates for the contrastive zero-one risk. The resulting bounds for contrastive loss and downstream prediction are much tighter than those of previous risk certificates, as demonstrated by experiments on CIFAR-10.
[66]
arXiv:2412.09304
(replaced)
[pdf, html, other]
Title:
Nonparametric estimation of the total treatment effect with multiple outcomes in the presence of terminal events
Jessica Gronsbell, Zachary R. McCaw, Isabelle-Emmanuella Nogues, Xiangshan Kong, Tianxi Cai, Lu Tian, LJ Wei
Subjects:
Methodology (stat.ME)
As standards of care advance, patients are living longer and once-fatal diseases are becoming manageable. Clinical trials increasingly focus on reducing disease burden, which can be quantified by the timing and occurrence of multiple non-fatal clinical events. Most existing methods for the analysis of multiple event-time data require stringent modeling assumptions that can be difficult to verify empirically, leading to treatment efficacy estimates that forego interpretability when the underlying assumptions are not met. Moreover, most existing methods do not appropriately account for informative terminal events, such as premature treatment discontinuation or death, which prevent the occurrence of subsequent events. To address these limitations, we derive and validate estimation and inference procedures for the area under the mean cumulative function (AUMCF), an extension of the restricted mean survival time to the multiple event-time setting. The AUMCF is nonparametric, clinically interpretable, and properly accounts for terminal competing risks. To enable covariate adjustment, we also develop an augmentation estimator that provides efficiency at least equaling, and often exceeding, the unadjusted estimator. The utility and interpretability of the AUMCF are illustrated with extensive simulation studies and through an analysis of multiple heart-failure-related endpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST) clinical trial. Our open-source R package MCC makes conducting AUMCF analyses straightforward and accessible.
[67]
arXiv:2412.11554
(replaced)
[pdf, html, other]
Title:
Learning Massive-scale Partial Correlation Networks in Clinical Multi-omics Studies with HP-ACCORD
Sungdong Lee, Joshua Bang, Youngrae Kim, Hyungwon Choi, Sang-Yun Oh, Joong-Ho Won
Comments:
25 pages, 6 figures, Accepted for publication in Annals of Applied Statistics
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
Graphical model estimation from multi-omics data requires a balance between statistical estimation performance and computational scalability. We introduce a novel pseudolikelihood-based graphical model framework that reparameterizes the target precision matrix while preserving the sparsity pattern and estimates it by minimizing an $\ell_1$-penalized empirical risk based on a new loss function. The proposed estimator maintains estimation and selection consistency in various metrics under high-dimensional assumptions. The associated optimization problem allows for a provably fast computation algorithm using a novel operator-splitting approach and communication-avoiding distributed matrix multiplication. A high-performance computing implementation of our framework was tested using simulated data with up to one million variables, demonstrating complex dependency structures similar to those found in biological networks. Leveraging this scalability, we estimated a partial correlation network from a dual-omic liver cancer data set. The co-expression network estimated from the ultrahigh-dimensional data demonstrated superior specificity in prioritizing key transcription factors and co-activators by excluding the impact of epigenetic regulation, thereby highlighting the value of computational scalability in multi-omic data analysis.
[68]
arXiv:2501.00755
(replaced)
[pdf, html, other]
Title:
An AI-powered Bayesian generative modeling approach for causal inference in observational studies
Qiao Liu, Wing Hung Wong
Subjects:
Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)
Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome variables. The core innovation of CausalBGM lies in its ability to estimate the individual treatment effect (ITE) by learning individual-specific distributions of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This approach not only effectively mitigates confounding effects but also provides comprehensive uncertainty quantification, offering reliable and interpretable causal effect estimates at the individual level. CausalBGM adopts a Bayesian model and uses a novel iterative algorithm to update the model parameters and the posterior distribution of latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. Its Bayesian foundation ensures statistical rigor, providing robust and well-calibrated posterior intervals. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in modern applications in fields such as genomics, healthcare, and social sciences. CausalBGM is maintained at the website this https URL.
[69]
arXiv:2501.09483
(replaced)
[pdf, html, other]
Title:
Semiparametrics via parametrics and contiguity
Adam Lee, Emil A. Stoltenberg, Per A. Mykland
Subjects:
Statistics Theory (math.ST); Econometrics (econ.EM); Methodology (stat.ME)
Inference on the parametric part of a semiparametric model is no trivial task. If one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model and inference may proceed by the method of maximum likelihood. Under regularity conditions, the ensuing maximum likelihood estimator is asymptotically normal and efficient in the approximating parametric model. Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting 'semiparametric' estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous: we move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the conventional sieve literature by being more specific about the approximating parametric models, by working not only with but also under these when treating the parametric models, and by taking full advantage of the mutual contiguity that we require between the parametric and semiparametric models. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.
[70]
arXiv:2503.00448
(replaced)
[pdf, html, other]
Title:
Parametric MMD Estimation with Missing Values: Robustness to Missingness and Data Model Misspecification
Badr-Eddine Chérief-Abdellatif, Jeffrey Näf
Subjects:
Methodology (stat.ME)
In the missing data literature, the Maximum Likelihood Estimator (MLE) is celebrated for its ignorability property under missing at random (MAR) data. However, its sensitivity to misspecification of the (complete) data model, even under MAR, remains a significant limitation. This issue is further exacerbated by the fact that the MAR assumption may not always be realistic, introducing an additional source of potential misspecification through the missingness mechanism. To address this, we propose a novel M-estimation procedure based on the Maximum Mean Discrepancy (MMD), which is provably robust to both model misspecification and deviations from the assumed missingness mechanism. Our approach offers strong theoretical guarantees and improved reliability in complex settings. We establish the consistency and asymptotic normality of the estimator under missingness completely at random (MCAR), provide an efficient stochastic gradient descent algorithm, and derive error bounds that explicitly separate the contributions of model misspecification and missingness bias. Furthermore, we analyze missing not at random (MNAR) scenarios where our estimator maintains controlled error, including a Huber setting where both the missingness mechanism and the data model are contaminated. Our contributions refine the understanding of the limitations of the MLE and provide a robust and principled alternative for handling missing data.
[71]
arXiv:2503.04071
(replaced)
[pdf, html, other]
Title:
Conformal Prediction with Upper and Lower Bound Models
Miao Li, Michael Klamkin, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper studies a Conformal Prediction (CP) methodology for building prediction intervals in a regression setting, given only deterministic lower and upper bounds on the target variable. It proposes a new CP mechanism (CPUL) that goes beyond post-processing by adopting a model selection approach over multiple nested interval construction methods. Paradoxically, many well-established CP methods, including CPUL, may fail to provide adequate coverage in regions where the bounds are tight. To remedy this limitation, the paper proposes an optimal thresholding mechanism, OMLT, that adjusts CPUL intervals in tight regions with undercoverage. The combined CPUL-OMLT is validated on large-scale learning tasks where the goal is to bound the optimal value of a parametric optimization problem. The experimental results demonstrate substantial improvements over baseline methods across various datasets.
[72]
arXiv:2503.14177
(replaced)
[pdf, html, other]
Title:
Distributions and Direct Parametrization for Stable Stochastic State-Space Models
Mohamad Al Ahdab, Zheng-Hua Tan, John Leth
Subjects:
Methodology (stat.ME); Systems and Control (eess.SY)
We present a direct parametrization for continuous-time stochastic state-space models that ensures external stability via the stochastic bounded-real lemma. Our formulation facilitates the construction of probabilistic priors that enforce almost-sure stability, which are suitable for sampling-based Bayesian inference methods. We validate our work with a simulation example and demonstrate its ability to yield stable predictions with uncertainty quantification.
[73]
arXiv:2503.21639
(replaced)
[pdf, html, other]
Title:
Locally minimax optimal confidence sets for the best model
Ilmun Kim, Aaditya Ramdas
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)
This paper tackles a fundamental inference problem: given $n$ observations from a distribution $P$ over $\mathbb{R}^d$ with unknown mean $\boldsymbol{\mu}$, we must form a confidence set for the index (or indices) corresponding to the smallest component of $\boldsymbol{\mu}$. By duality, we reduce this to testing, for each $r$ in $1,\ldots,d$, whether $\mu_r$ is the smallest. Based on the sample splitting and self-normalization approach of Kim and Ramdas (2024), we propose "dimension-agnostic" tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in $\boldsymbol{\mu}$. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the \emph{local} minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Furthermore, we develop robust variants that continue to achieve the same minimax rate under heavy-tailed distributions with only finite second moments. While these results highlight the theoretical strength of our method, a practical concern is that sample splitting can reduce finite-sample power. We show that this drawback can be substantially alleviated by the multi-split aggregation method of Guo and Shah (2025). Finally, empirical results on simulated and real data illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.
[74]
arXiv:2504.07946
(replaced)
[pdf, html, other]
Title:
Characteristic function-based tests for spatial randomness
Yiran Zeng, Dale L. Zimmerman
Comments:
28 pages, 6 figures
Subjects:
Methodology (stat.ME); Computation (stat.CO)
We introduce a new type of test for complete spatial randomness that applies to mapped point patterns in a rectangle or a cube of any dimension. This is the first test of its kind to be based on characteristic functions and utilizes a weighted $L_2$-distance between the empirical and uniform characteristic functions. The test shows surprising connections to Ripley's $K$-function and Zimmerman's $\bar{\omega}^2$ statistic. It is also simple to calculate and does not require adjusting for edge effects. An efficient algorithm is developed to find the asymptotic null distribution of the test statistic under the Cauchy weight function. This makes the test fast to compute. In simulations, our test shows varying sensitivity to different levels of spatial interaction depending on the scale parameter of the Cauchy weight function. Tests with different parameter values can be combined to create a Bonferroni-corrected omnibus test, which is more powerful than the popular $L$-test and the Clark-Evans test in most simulation settings of heterogeneity, aggregation and regularity, especially when the sample size is large. The simplicity of the empirical characteristic function makes it straightforward to extend our test to non-rectangular or sparsely sampled point patterns.
[75]
arXiv:2505.00711
(replaced)
[pdf, html, other]
Title:
Global Activity Scores
Ruilong Yue, Giray Ökten
Subjects:
Statistics Theory (math.ST)
We introduce a new global sensitivity measure, the global activity scores. We establish its theoretical connection with Sobol' sensitivity indices and demonstrate its performance through numerical examples. In these examples, we compare global activity scores with Sobol' sensitivity indices, derivative-based sensitivity measures, and activity scores. The results show that in the presence of noise or high variability, global activity scores outperform derivative-based measures and activity scores, while in noiseless settings the three approaches yield similar results.
[76]
arXiv:2505.04773
(replaced)
[pdf, html, other]
Title:
Estimating the Heritability of Longitudinal Rate-of-Change: Genetic Insights into PSA Velocity in Prostate Cancer-Free Individuals
Pei Zhang, Xiaoyu Wang, Jianxin Shi, Paul S. Albert
Subjects:
Applications (stat.AP); Computation (stat.CO)
Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.
[77]
arXiv:2505.19741
(replaced)
[pdf, other]
Title:
Minimax Adaptive Online Nonparametric Regression over Besov Spaces
Paul Liautaud (LPSM (UMR\_8001), SU), Pierre Gaillard (Thoth, UGA), Olivier Wintenberger (LPSM (UMR\_8001), SU, ICP)
Subjects:
Statistics Theory (math.ST); Machine Learning (stat.ML)
We study online adversarial regression with convex losses against a rich class of continuous yet highly irregular prediction rules, modeled by Besov spaces $B\_{pq}^s$ with general parameters $1 \leq p,q \leq \infty$ and smoothness $s > \tfrac{d}{p}$. We introduce an adaptive wavelet-based algorithm that performs sequential prediction without prior knowledge of $(s,p,q)$, and establish minimax-optimal regret bounds against any comparator in $B\_{pq}^s$. We further design a locally adaptive extension capable of dynamically tracking spatially inhomogeneous smoothness. This adaptive mechanism adjusts the resolution of the predictions over both time and space, yielding refined regret bounds in terms of local regularity. Consequently, in heterogeneous environments, our adaptive guarantees can significantly surpass those obtained by standard global methods.
[78]
arXiv:2505.20022
(replaced)
[pdf, html, other]
Title:
Kernel Ridge Regression with Predicted Feature Inputs and Applications to Factor-Based Nonparametric Regression
Xin Bing, Xin He, Chao Wang
Subjects:
Statistics Theory (math.ST)
Kernel methods, particularly kernel ridge regression (KRR), are time-proven, powerful nonparametric regression techniques known for their rich capacity, analytical simplicity, and computational tractability. The analysis of their predictive performance has received continuous attention for more than two decades. However, in many modern regression problems where the feature inputs used in KRR cannot be directly observed and must instead be inferred from other measurements, the theoretical foundations of KRR remain largely unexplored. In this paper, we introduce a novel approach for analyzing KRR with predicted feature inputs. Our framework is not only essential for handling predicted feature inputs -- enabling us to derive risk bounds without imposing any assumptions on the error of the predicted feature -- but also strengthens existing analyses in the classical setting by allowing arbitrary model misspecification, requiring weaker conditions under the squared loss, particularly allowing both an unbounded response and an unbounded function class, and being flexible enough to accommodate other convex loss functions. We apply our general theory to factor-based nonparametric regression models and establish the minimax optimality of KRR when the feature inputs are predicted using principal component analysis. Our theoretical findings are further corroborated by simulation studies and real-data analyses using pretrained LLM embeddings for the downstream prediction task.
[79]
arXiv:2505.20946
(replaced)
[pdf, html, other]
Title:
Almost Unbiased Liu Type Estimator in Bell Regression Model: Theory, Simulation and Application
Caner Tanış, Yasin Asar
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
In this paper, we gain the new almost unbiased Liu-type estimators to literature for the Bell regression model. We provide the superiority of the proposed estimator to its competitors such as the maximum likelihood estimator and Liu-type estimators via some theorems. We also design an extensive Monte Carlo simulation study to show that the proposed estimators outperforms the competitors in terms of mean squared error theoretically. Finally, we present a real data study to assess the performance of the introduced estimators in modeling real-life data. The findings of both the simulation and the empirical study demonstrate that the proposed regression estimators surpasses its competitors based on the mean square error criterion.
[80]
arXiv:2506.23396
(replaced)
[pdf, html, other]
Title:
AICO: Feature Significance Tests for Supervised Learning
Kay Giesecke, Enguerrand Horel, Chartsiri Jirachotkulthorn
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The opacity of many supervised learning algorithms remains a key challenge, hindering scientific discovery and limiting broader deployment -- particularly in high-stakes domains. This paper develops model- and distribution-agnostic significance tests to assess the influence of input features in any regression or classification algorithm. Our method evaluates a feature's incremental contribution to model performance by masking its values across samples. Under the null hypothesis, the distribution of performance differences across a test set has a non-positive median. We construct a uniformly most powerful, randomized sign test for this median, yielding exact p-values for assessing feature significance and confidence intervals with exact coverage for estimating population-level feature importance. The approach requires minimal assumptions, avoids model retraining or auxiliary models, and remains computationally efficient even for large-scale, high-dimensional settings. Experiments on synthetic tasks validate its statistical and computational advantages, and applications to real-world data illustrate its practical utility.
[81]
arXiv:2507.12891
(replaced)
[pdf, html, other]
Title:
Refining the Notion of No Anticipation in Difference-in-Differences Studies
Marco Piccininni, Eric J. Tchetgen Tchetgen, Mats J. Stensrud
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
We address an ambiguity in identification strategies using difference-in-differences, which are widely applied in empirical research, particularly in economics. The assumption commonly referred to as the "no-anticipation assumption" states that treatment has no effect on outcomes before its implementation. However, because standard causal models rely on a temporal structure in which causes precede effects, such an assumption seems to be inherently satisfied. This raises the question of whether the assumption is repeatedly stated out of redundancy or because the formal statements fail to capture the intended subject-matter interpretation. We argue that confusion surrounding the no-anticipation assumption arises from ambiguity in the intervention considered and that current formulations of the assumption are ambiguous. Therefore, new definitions and identification results are proposed.
[82]
arXiv:2507.14457
(replaced)
[pdf, html, other]
Title:
Mean Shift for Clustering Functional Data: A Scalable Algorithm and Convergence Analysis
Ting-Li Chen, Toshinari Morimoto, Su-Yun Huang, Ruey S. Tsay
Subjects:
Methodology (stat.ME)
This paper extends the mean shift algorithm from vector-valued data to functional data, enabling effective clustering in infinite-dimensional settings. To address the computational challenges posed by large-scale datasets, we introduce a fast stochastic variant that significantly reduces computational complexity. We provide a rigorous analysis of convergence for the full functional mean shift procedure, establishing theoretical guarantees for its behavior. For the stochastic variant, we provide some partial justification for its use by showing that it approximates the full algorithm well when the subset size is sufficiently large. The proposed method is validated both through simulation studies and through real-data analysis, including hourly Taiwan PM$_{2.5}$ measurements and Argo oceanographic profiles. Our key contributions include: (1) a novel extension of the mean shift algorithm to functional data for clustering without the need to specify the number of clusters; (2) convergence analysis of the full functional mean shift algorithm in Hilbert space; (3) a scalable stochastic variant based on random partitioning, with partial theoretical justification; and (4) real-data applications demonstrating the method's scalability and practical usefulness.
[83]
arXiv:2507.15320
(replaced)
[pdf, other]
Title:
Does the draw matter in an incomplete round-robin tournament? The case of the UEFA Champions League
László Csató, András Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma
Comments:
29 pages, 12 figures, 3 tables
Subjects:
Applications (stat.AP); Optimization and Control (math.OC); Physics and Society (physics.soc-ph)
A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. If the teams are seeded based on their actual strengths, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.
[84]
arXiv:2507.17306
(replaced)
[pdf, other]
Title:
A principled approach for comparing Variable Importance
Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Variable importance measures (VIMs) aim to quantify the contribution of each input covariate to the predictability of a given output. With the growing interest in explainable AI, numerous VIMs have been proposed, many of which are heuristic in nature. This is often justified by the inherent subjectivity of the notion of importance. This raises important questions regarding usage: What makes a good VIM? How can we compare different VIMs?
In this paper, we address these questions by: (1) proposing an axiomatic framework that bridges the gap between variable importance and variable selection. This framework formalizes the intuitive principle that features providing no additional information should not be assigned importance. It helps avoid false positives due to spurious correlations, which can arise with popular methods such as Shapley values; and (2) introducing a general pipeline for constructing VIMs, which clarifies the objective of various VIMs and thus facilitates meaningful comparisons. This approach is natural in statistics, but the literature has diverged from it.
Finally, we provide an extensive set of examples to guide practitioners in selecting and estimating appropriate indices aligned with their specific goals and data.
[85]
arXiv:2507.22095
(replaced)
[pdf, html, other]
Title:
Simulating Posterior Bayesian Neural Networks with Dependent Weights
Nicola Apollonio, Giovanni Franzina, Giovanni Luca Torrisi
Comments:
6 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR)
In this paper we consider posterior Bayesian fully connected and feedforward deep neural networks with dependent weights. Particularly, if the likelihood is Gaussian, we identify the distribution of the wide width limit and provide an algorithm to sample from the network. In the shallow case we explicitly compute the distribution of the conditional output, proving that it is a Gaussian mixture. All the theoretical results are numerically validated.
[86]
arXiv:2508.01861
(replaced)
[pdf, html, other]
Title:
Tensor-Empowered Asset Pricing with Missing Data
Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen
Subjects:
Applications (stat.AP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with a latent factor model tailored for tensor-structured financial data. Results show that ACT-Tensor not only achieves accurate return forecasting but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.
[87]
arXiv:2508.16096
(replaced)
[pdf, html, other]
Title:
Quasi Instrumental Variable Methods for Stable Hidden Confounding and Binary Outcome
Zhonghua Liu, Baoluo Sun, Ting Ye, David Richardson, Eric Tchetgen Tchetgen
Subjects:
Methodology (stat.ME)
Instrumental variable (IV) methods are central to causal inference from observational data, particularly when a randomized experiment is not feasible. However, of the three conventional core IV identification conditions, only one, IV relevance, is empirically verifiable; often one or both of the other conditions, exclusion restriction and IV independence from unmeasured confounders, are unmet in real-world applications. These challenges are compounded when the outcome is binary, a setting for which robust IV methods remain underdeveloped. A fundamental contribution of this paper is the development of a general identification strategy justified under a structural equilibrium dynamic generative model of so-called stable confounding and a quasi instrumental variable (QIV), i.e. a variable that is only assumed to be predictive of the outcome. Such a model implies (a) stability of confounding on the multiplicative scale, and (b) stability of the additive average treatment effect among the treated (ATT), across levels of that QIV. The former is all that is necessary to ensure a valid test of the causal null hypothesis; together those two conditions establish nonparametric identification and estimation of the conditional and marginal ATT. To address the statistical challenges posed by the need for boundedness in binary outcomes, we introduce a generalized odds product re-parametrization of the observed data distribution, and we develop both a principled maximum likelihood estimator and a triply robust semiparametric locally efficient estimator, which we evaluate through simulations and an empirical application to the UK Biobank.
[88]
arXiv:2508.19535
(replaced)
[pdf, other]
Title:
A Structure-Preserving Assessment of VBPBB for Time Series Imputation Under Periodic Trends, Noise, and Missingness Mechanisms
Asmaa Ahmad, Eric J Rose, Michael Roy, Edward Valachovic
Comments:
24 pages, 6 figure and 3 tables
Subjects:
Applications (stat.AP)
Incomplete time series data present significant challenges to accurate statistical analysis, particularly when the underlying data exhibit periodic structures such as seasonal or monthly trends. Traditional imputation methods often fail to preserve these temporal dynamics, leading to biased estimates and reduced analytical integrity. In this study, we introduce and evaluate a structure-preserving imputation framework that incorporates significant periodic components into the multiple imputation process via the Variable Bandpass Periodic Block Bootstrap (VBPBB). We simulate time series data containing annual and monthly periodicities and introduce varying levels of noise representing low, moderate, and high signal-to-noise scenarios to mimic real world variability. Missing data are introduced under Missing Completely at Random (MCAR) mechanisms across a range of missingness proportions (5% - 70%). VBPBB is used to extract dominant periodic components at multiple frequencies, which are then bootstrapped and included as covariates in the Amelia II multiple imputation model. The performance of this periodicity-enhanced approach is compared against standard imputation methods that do not incorporate temporal structure. Our results demonstrate that the VBPBB-enhanced imputation framework consistently outperforms conventional approaches across all tested conditions, with the greatest performance gains observed in high-noise settings and when multiple periodic components are retained. This study addresses critical limitations in existing imputation techniques by offering a flexible, periodicity-aware solution that preserves temporal structure in incomplete time series. We further explore the methodological implications of incorporating frequency-based components and discuss future directions for advancing robust imputation in temporally correlated data environments.
[89]
arXiv:2509.05945
(replaced)
[pdf, html, other]
Title:
Simplicial clustering using the $α$--transformation
Michail Tsagris, Nikolaos Kontemeniotis
Subjects:
Methodology (stat.ME)
We introduce two simplicial clustering approaches for compositional data, that are adaptations of the $K$--means and of the Gaussian mixture models algorithms, by employing the $\alpha$--transformation. By utilizing clustering validation indices we can decide on the number of clusters and choose the value of $\alpha$ for the $K$--means, while for the model-based clustering approach information criteria complete this task. extensive simulation studies compare the performance of these two approaches and a real data set illustrates their performance in real world settings.
[90]
arXiv:2509.06287
(replaced)
[pdf, html, other]
Title:
Statistical Inference for Misspecified Contextual Bandits
Yongyi Guo, Ziping Xu
Subjects:
Statistics Theory (math.ST); Artificial Intelligence (cs.AI)
Contextual bandit algorithms have transformed modern experimentation by enabling real-time adaptation for personalized treatment and efficient use of data. Yet these advantages create challenges for statistical inference due to adaptivity. A fundamental property that supports valid inference is policy convergence, meaning that action-selection probabilities converge in probability given the context. Convergence ensures replicability of adaptive experiments and stability of online algorithms. In this paper, we highlight a previously overlooked issue: widely used algorithms such as LinUCB may fail to converge when the reward model is misspecified, and such non-convergence creates fundamental obstacles for statistical inference. This issue is practically important, as misspecified models -- such as linear approximations of complex dynamic system -- are often employed in real-world adaptive experiments to balance bias and variance.
Motivated by this insight, we propose and analyze a broad class of algorithms that are guaranteed to converge even under model misspecification. Building on this guarantee, we develop a general inference framework based on an inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic normality with a consistent variance estimator. Simulation studies confirm that the proposed method provides robust and data-efficient confidence intervals, and can outperform existing approaches that exist only in the special case of offline policy evaluation. Taken together, our results underscore the importance of designing adaptive algorithms with built-in convergence guarantees to enable stable experimentation and valid statistical inference in practice.
[91]
arXiv:2509.12691
(replaced)
[pdf, html, other]
Title:
Power-Dominance in Estimation Theory: A Third Pathological Axis
Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanpää
Comments:
5 pages, 1 figure
Subjects:
Methodology (stat.ME); Signal Processing (eess.SP); Statistics Theory (math.ST); Machine Learning (stat.ML)
This paper introduces a novel framework for estimation theory by introducing a second-order diagnostic for estimator design. While classical analysis focuses on the bias-variance trade-off, we present a more foundational constraint. This result is model-agnostic, domain-agnostic, and is valid for both parametric and non-parametric problems, Bayesian and frequentist frameworks. We propose to classify the estimators into three primary power regimes. We theoretically establish that any estimator operating in the `power-dominant regime' incurs an unavoidable mean-squared error penalty, making it structurally prone to sub-optimal performance. We propose a `safe-zone law' and make this diagnostic intuitive through two safe-zone maps. One map is a geometric visualization analogous to a receiver operating characteristic curve for estimators, and the other map shows that the safe-zone corresponds to a bounded optimization problem, while the forbidden `power-dominant zone' represents an unbounded optimization landscape. This framework reframes estimator design as a path optimization problem, providing new theoretical underpinnings for regularization and inspiring novel design philosophies.
[92]
arXiv:2509.14502
(replaced)
[pdf, html, other]
Title:
Rate doubly robust estimation for weighted average treatment effects
Yiming Wang, Yi Liu, Shu Yang
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
The weighted average treatment effect (WATE) defines a versatile class of causal estimands for populations characterized by propensity score weights, including the average treatment effect (ATE), treatment effect on the treated (ATT), on controls (ATC), and for the overlap population (ATO). WATE has broad applicability in social and medical research, as many datasets from these fields align with its framework. However, the literature lacks a systematic investigation into the robustness and efficiency conditions for WATE estimation. Although doubly robust (DR) estimators are well-studied for ATE, their applicability to other WATEs remains uncertain. This paper investigates whether widely used WATEs admit DR or rate doubly robust (RDR) estimators and assesses the role of nuisance function accuracy, particularly with machine learning. Using semiparametric efficient influence function (EIF) theory and double/debiased machine learning (DML), we propose three RDR estimators under specific rate and regularity conditions and evaluate their performance via Monte Carlo simulations. Applications to NHANES data on smoking and blood lead levels, and SIPP data on 401(k) eligibility, demonstrate the methods' practical relevance in medical and social sciences.
[93]
arXiv:2305.16189
(replaced)
[pdf, html, other]
Title:
Multi-scale clustering and source separation of InSight mission seismic data
Ali Siahkoohi, Rudy Morel, Randall Balestriero, Erwan Allys, Grégory Sainton, Taichi Kawamura, Maarten V. de Hoop
Subjects:
Machine Learning (cs.LG); Earth and Planetary Astrophysics (astro-ph.EP); Machine Learning (stat.ML)
Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of timescales exhibited by sources in time series data from planetary space missions. As such, a systematic multi-scale unsupervised approach is needed to identify and separate sources at different timescales. Existing methods typically rely on a preselected window size that determines their operating timescale, limiting their capacity to handle multi-scale sources. To address this issue, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering spectra that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial variational autoencoder that is trained to probabilistically cluster sources at different timescales. To perform source separation, we use samples from clusters at multiple timescales obtained via the factorial variational autoencoder as prior information and formulate an optimization problem in the wavelet scattering spectra representation space. When applied to the entire seismic dataset recorded during the NASA InSight mission on Mars, containing sources varying greatly in timescale, our approach disentangles such different sources, e.g., minute-long transient one-sided pulses (known as "glitches") and structured ambient noises resulting from atmospheric activities that typically last for tens of minutes, and provides an opportunity to conduct further investigations into the isolated sources.
[94]
arXiv:2310.18563
(replaced)
[pdf, html, other]
Title:
Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects
Tymon Słoczyński, S. Derya Uysal, Jeffrey M. Wooldridge
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
How should researchers adjust for covariates? We show that if the propensity score is estimated using a specific covariate balancing approach, inverse probability weighting (IPW), augmented inverse probability weighting (AIPW), and inverse probability weighted regression adjustment (IPWRA) estimators are numerically equivalent for the average treatment effect (ATE), and likewise for the average treatment effect on the treated (ATT). The resulting weights are inherently normalized, making normalized and unnormalized IPW and AIPW identical. We discuss implications for instrumental variables and difference-in-differences estimators and illustrate with two applications how these numerical equivalences simplify analysis and interpretation.
[95]
arXiv:2405.13535
(replaced)
[pdf, html, other]
Title:
Addressing the Inconsistency in Bayesian Deep Learning via Generalized Laplace Approximation
Yinsong Chen, Samson S. Yu, Zhong Li, Chee Peng Lim
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
In recent years, inconsistency in Bayesian deep learning has attracted significant attention. Tempered or generalized posterior distributions are frequently employed as direct and effective solutions. Nonetheless, the underlying mechanisms and the effectiveness of generalized posteriors remain active research topics. In this work, we interpret posterior tempering as a correction for model misspecification via adjustments to the joint probability, and as a recalibration of priors by reducing aleatoric uncertainty. We also introduce the generalized Laplace approximation, which requires only a simple modification to the Hessian calculation of the regularized loss and provides a flexible and scalable framework for high-quality posterior inference. We evaluate the proposed method on state-of-the-art neural networks and real-world datasets, demonstrating that the generalized Laplace approximation enhances predictive performance.
[96]
arXiv:2407.11678
(replaced)
[pdf, html, other]
Title:
Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation
Luwei Sun, Dongrui Shen, Han Feng
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
In this paper, we focus on analyzing the excess risk of the unpaired data generation model, called CycleGAN. Unlike classical GANs, CycleGAN not only transforms data between two unpaired distributions but also ensures the mappings are consistent, which is encouraged by the cycle-consistency term unique to CycleGAN. The increasing complexity of model structure and the addition of the cycle-consistency term in CycleGAN present new challenges for error analysis. By considering the impact of both the model architecture and training procedure, the risk is decomposed into two terms: approximation error and estimation error. These two error terms are analyzed separately and ultimately combined by considering the trade-off between them. Each component is rigorously analyzed; the approximation error through constructing approximations of the optimal transport maps, and the estimation error through establishing an upper bound using Rademacher complexity. Our analysis not only isolates these errors but also explores the trade-offs between them, which provides a theoretical insights of how CycleGAN's architecture and training procedures influence its performance.
[97]
arXiv:2409.10096
(replaced)
[pdf, html, other]
Title:
Robust Reinforcement Learning with Dynamic Distortion Risk Measures
Anthony Coache, Sebastian Jaimungal
Comments:
27 pages, 3 figures
Subjects:
Machine Learning (cs.LG); Computational Finance (q-fin.CP); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM); Machine Learning (stat.ML)
In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.
[98]
arXiv:2410.22069
(replaced)
[pdf, html, other]
Title:
Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks
Nikolaos Tsilivis, Eitan Gronich, Julia Kempe, Gal Vardi
Comments:
Fix the authors order
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).
[99]
arXiv:2411.04551
(replaced)
[pdf, html, other]
Title:
Measure-to-measure interpolation using Transformers
Borjan Geshkovski, Philippe Rigollet, Domènec Ruiz-Balet
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
Transformers are deep neural network architectures that underpin the recent successes of large language models. Unlike more classical architectures that can be viewed as point-to-point maps, a Transformer acts as a measure-to-measure map implemented as specific interacting particle system on the unit sphere: the input is the empirical measure of tokens in a prompt and its evolution is governed by the continuity equation. In fact, Transformers are not limited to empirical measures and can in principle process any input measure. As the nature of data processed by Transformers is expanding rapidly, it is important to investigate their expressive power as maps from an arbitrary measure to another arbitrary measure. To that end, we provide an explicit choice of parameters that allows a single Transformer to match $N$ arbitrary input measures to $N$ arbitrary target measures, under the minimal assumption that every pair of input-target measures can be matched by some transport map.
[100]
arXiv:2411.05850
(replaced)
[pdf, html, other]
Title:
Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? An Intercomparison for Temperature and Precipitation over Spain
Jose González-Abad, José Manuel Gutiérrez
Comments:
Published in Artificial Intelligence for the Earth Systems
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Machine Learning (cs.LG); Machine Learning (stat.ML)
Deep Learning (DL) has shown promise for downscaling global climate change projections under different approaches, including Perfect Prognosis (PP) and Regional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models are trained on observational data, so it remains an open question whether they can plausibly extrapolate unseen conditions and changes in future emissions scenarios. Here we focus on this problem as the main drawback for the operationalization of these methods and present the results of an intercomparison experiment to evaluate the performance and extrapolation capability of existing models using a common experimental framework, taking into account the sensitivity of results to different training replicas. We focus on minimum and maximum temperatures and precipitation over Spain, a region with a range of climatic conditions with different influential regional processes. We conclude with a discussion of the findings, limitations of existing methods, and prospects for future development.
[101]
arXiv:2412.13535
(replaced)
[pdf, html, other]
Title:
On the Maximum and Minimum of a Multivariate Poisson Distribution
Zheng Liu, Feifan Shi, Jing Yao, Yang Yang
Subjects:
Probability (math.PR); Statistics Theory (math.ST)
In this paper, we investigate the cumulative distribution functions (CDFs) of the maximum and minimum of multivariate Poisson distributions with three dependence structures, namely, the common shock, comonotonic shock and thinning-dependence models. In particular, we formulate the definition of a thinning-dependent multivariate Poisson distribution based on Wang and Yuen (2005). We derive explicit CDFs of the maximum and minimum of the multivariate Poisson random vectors and conduct asymptotic analyses on them. Our results reveal the substantial difference between the three dependence structures for multivariate Poisson distribution and may suggest an alternative method for studying the dependence for other multivariate distributions. We further provide numerical examples demonstrating obtained results.
[102]
arXiv:2502.02216
(replaced)
[pdf, html, other]
Title:
Flatten Graphs as Sequences: Transformers are Scalable Graph Generators
Dexiong Chen, Markus Krimmel, Karsten Borgwardt
Comments:
To appear at NeurIPS 2025
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at this https URL.
[103]
arXiv:2502.12013
(replaced)
[pdf, html, other]
Title:
Unsupervised Structural-Counterfactual Generation under Domain Shift
Krishn Vishwas Kher, Lokesh Venkata Siva Maruthi Badisa, Saksham Mittal, Kusampudi Venkata Datta Sri Harsha, Chitneedi Geetha Sowmya, SakethaNath Jagarlapudi
Comments:
Updated author ordering
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Motivated by the burgeoning interest in cross-domain learning, we present a novel generative modeling challenge: generating counterfactual samples in a target domain based on factual observations from a source domain. Our approach operates within an unsupervised paradigm devoid of parallel or joint datasets, relying exclusively on distinct observational samples and causal graphs for each domain. This setting presents challenges that surpass those of conventional counterfactual generation. Central to our methodology is the disambiguation of exogenous causes into effect-intrinsic and domain-intrinsic categories. This differentiation facilitates the integration of domain-specific causal graphs into a unified joint causal graph via shared effect-intrinsic exogenous variables. We propose leveraging Neural Causal models within this joint framework to enable accurate counterfactual generation under standard identifiability assumptions. Furthermore, we introduce a novel loss function that effectively segregates effect-intrinsic from domain-intrinsic variables during model training. Given a factual observation, our framework combines the posterior distribution of effect-intrinsic variables from the source domain with the prior distribution of domain-intrinsic variables from the target domain to synthesize the desired counterfactuals, adhering to Pearl's causal hierarchy. Intriguingly, when domain shifts are restricted to alterations in causal mechanisms without accompanying covariate shifts, our training regimen parallels the resolution of a conditional optimal transport problem. Empirical evaluations on a synthetic dataset show that our framework generates counterfactuals in the target domain that very closely resemble the ground truth.
[104]
arXiv:2502.14790
(replaced)
[pdf, html, other]
Title:
Bayesian Algorithms for Adversarial Online Learning: from Finite to Infinite Action Spaces
Alexander Terenin, Jeffrey Negrea
Comments:
This version renames the paper: its original name was "An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces"
Subjects:
Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Statistics Theory (math.ST); Machine Learning (stat.ML)
We develop a form Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling over the $d$-dimensional unit cube, using a certain Gaussian process prior widely-used in the Bayesian optimization literature, has a $\mathcal{O}\Big(\beta\sqrt{Td\log(1+\sqrt{d}\frac{\lambda}{\beta})}\Big)$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.
[105]
arXiv:2503.09767
(replaced)
[pdf, html, other]
Title:
Cover Learning for Large-Scale Topology Representation
Luis Scoccola, Uzu Lim, Heather A. Harrington
Comments:
29 pages, 19 figures, 5 tables; final version at ICML 2025
Subjects:
Machine Learning (cs.LG); Computational Geometry (cs.CG); Algebraic Topology (math.AT); Machine Learning (stat.ML)
Classical unsupervised learning methods like clustering and linear dimensionality reduction parametrize large-scale geometry when it is discrete or linear, while more modern methods from manifold learning find low dimensional representation or infer local geometry by constructing a graph on the input data. More recently, topological data analysis popularized the use of simplicial complexes to represent data topology with two main methodologies: topological inference with geometric complexes and large-scale topology visualization with Mapper graphs -- central to these is the nerve construction from topology, which builds a simplicial complex given a cover of a space by subsets. While successful, these have limitations: geometric complexes scale poorly with data size, and Mapper graphs can be hard to tune and only contain low dimensional information. In this paper, we propose to study the problem of learning covers in its own right, and from the perspective of optimization. We describe a method for learning topologically-faithful covers of geometric datasets, and show that the simplicial complexes thus obtained can outperform standard topological inference approaches in terms of size, and Mapper-type algorithms in terms of representation of large-scale topology.
[106]
arXiv:2504.15325
(replaced)
[pdf, other]
Title:
Significativity Indices for Agreement Values
Alberto Casagrande, Francesco Fabris, Rossano Girometti, Roberto Pagliarini
Comments:
27 pages, 6 figures
Journal-ref:
Casagrande, A., Fabris, F., Girometti, R. et al. Significativity Indices for Agreement Values. Stat Comput 35, 197 (2025)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Agreement measures, such as Cohen's kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a golden standard can be compared simply by using the order induced by their agreement measure with respect to the golden standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen's kappa, but they are mainly naïve, and their boundaries are arbitrary. This work proposes a general approach to evaluate the significativity of any agreement value between two classifiers and introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. Moreover, this manuscript addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.
[107]
arXiv:2505.02809
(replaced)
[pdf, html, other]
Title:
Towards Quantifying the Hessian Structure of Neural Networks
Zhaorui Dong, Yushun Zhang, Jianfeng Yao, Ruoyu Sun
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal that the reported Hessian structure comes from a mixture of two forces: a ``static force'' rooted in the architecture design, and a ''dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ''static force'' at random initialization. We study linear models and 1-hidden-layer networks for classification tasks with $C$ classes. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C$ becomes large. Our findings reveal that $C$ is one primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$.
[108]
arXiv:2505.03432
(replaced)
[pdf, html, other]
Title:
Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients
Stefano Bruno, Sotirios Sabanis
Comments:
Accepted for publication in Transactions on Machine Learning Research
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)
Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.
[109]
arXiv:2506.02318
(replaced)
[pdf, html, other]
Title:
Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models
Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang
Subjects:
Machine Learning (cs.LG); Signal Processing (eess.SP); Statistics Theory (math.ST)
Discrete state space diffusion models have shown significant advantages in applications involving discrete data, such as text and image generation. It has also been observed that their performance is highly sensitive to the choice of rate matrices, particularly between uniform and absorbing rate matrices. While empirical results suggest that absorbing rate matrices often yield better generation quality compared to uniform rate matrices, existing theoretical works have largely focused on the uniform rate matrices case. Notably, convergence guarantees and error analyses for absorbing diffusion models are still missing. In this work, we provide the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices. We begin by deriving an upper bound on the KL divergence of the forward process, introducing a surrogate initialization distribution to address the challenge posed by the absorbing stationary distribution, which is a singleton and causes the KL divergence to be ill-defined. We then establish the first convergence guarantees for both the $\tau$-leaping and uniformization samplers under absorbing rate matrices, demonstrating improved rates over their counterparts using uniform rate matrices. Furthermore, under suitable assumptions, we provide convergence guarantees without early stopping. Our analysis introduces several new technical tools to address challenges unique to absorbing rate matrices. These include a Jensen-type argument for bounding forward process convergence, novel techniques for bounding absorbing score functions, and a non-divergent upper bound on the score near initialization that removes the need of early-stopping.
[110]
arXiv:2506.08274
(replaced)
[pdf, html, other]
Title:
The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks
João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Ricardo V. Godoy, Leonardo André Ambrosio, Marcelo Becker
Comments:
36 pages
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.
[111]
arXiv:2507.16370
(replaced)
[pdf, other]
Title:
Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning
Lucas de Lara (IECL)
Subjects:
Artificial Intelligence (cs.AI); Statistics Theory (math.ST)
Counterfactual reasoning aims at answering contrary-to-fact questions like ``Would have Alice recovered had she taken aspirin?'' and corresponds to the most fine-grained layer of causation. Critically, while many counterfactual statements cannot be falsified-even by randomized experiments-they underpin fundamental concepts like individual-wise fairness. Therefore, providing models to formalize and implement counterfactual beliefs remains a fundamental scientific problem. In the Markovian setting of Pearl's causal framework, we propose an alternative approach to structural causal models to represent counterfactuals compatible with a given causal graphical model. More precisely, we introduce counterfactual models, also called canonical representations of structural causal models. They enable analysts to choose a counterfactual assumption via random-process probability distributions with preassigned marginals and characterize the counterfactual equivalence class of structural causal models. Using these representations, we present a normalization procedure to disentangle the (arbitrary and unfalsifiable) counterfactual choice from the (typically testable) interventional constraints. In contrast to structural causal models, this allows to implement many counterfactual assumptions while preserving interventional knowledge, and does not require any estimation step at the individual-counterfactual layer: only to make a choice. Finally, we illustrate the specific role of counterfactuals in causality and the benefits of our approach on theoretical and numerical examples.
[112]
arXiv:2509.05760
(replaced)
[pdf, html, other]
Title:
Rethinking Beta: A Causal Take on CAPM
Naftali Cohen
Subjects:
Theoretical Economics (econ.TH); Pricing of Securities (q-fin.PR); Statistical Finance (q-fin.ST); Applications (stat.AP)
The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.
[113]
arXiv:2509.06599
(replaced)
[pdf, html, other]
Title:
Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems
Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanpää
Comments:
15 pages, 1 figure, 2 photographs
Subjects:
Machine Learning (cs.LG); Computational Complexity (cs.CC); Signal Processing (eess.SP); Systems and Control (eess.SY); Statistics Theory (math.ST)
Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.
Total of 113 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack