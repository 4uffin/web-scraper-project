Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Friday, 26 September 2025
Total of 86 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 27 of 27 entries)
[1]
arXiv:2509.20404
[pdf, other]
Title:
Sample completion, structured correlation, and Netflix problems
Leonardo N. Coregliano, Maryanthe Malliaris
Comments:
97 pages, 1 figure
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Logic (math.LO); Statistics Theory (math.ST)
We develop a new high-dimensional statistical learning model which can take advantage of structured correlation in data even in the presence of randomness. We completely characterize learnability in this model in terms of VCN${}_{k,k}$-dimension (essentially $k$-dependence from Shelah's classification theory). This model suggests a theoretical explanation for the success of certain algorithms in the 2006~Netflix Prize competition.
[2]
arXiv:2509.20433
[pdf, html, other]
Title:
Distinguishability of causal structures under latent confounding and selection
Ryan Carey, Marina Maciel Ansanelli, Elie Wolfe, Robin J. Evans
Comments:
26 pages, 16 figures
Subjects:
Statistics Theory (math.ST)
Statistical relationships in observed data can arise for several different reasons: the observed variables may be causally related, they may share a latent common cause, or there may be selection bias. Each of these scenarios can be modelled using different causal graphs. Not all such causal graphs, however, can be distinguished by experimental data. In this paper, we formulate the equivalence class of causal graphs as a novel graphical structure, the selected-marginalized directed graph (smDG). That is, we show that two directed acyclic graphs with latent and selected vertices have the same smDG if and only if they are indistinguishable, even when allowing for arbitrary interventions on the observed variables. As a substitute for the more familiar d-separation criterion for DAGs, we provide an analogous sound and complete separation criterion in smDGs for conditional independence relative to passive observations. Finally, we provide a series of sufficient conditions under which two causal structures are indistinguishable when there is only access to passive observations.
[3]
arXiv:2509.20506
[pdf, html, other]
Title:
Identification and Estimation of Joint Potential Outcome Distributions from a Single Study
Zach Shahn, David Madigan
Subjects:
Methodology (stat.ME)
Most causal inference methods focus on estimating marginal average treatment effects, but many important causal estimands depend on the joint distribution of potential outcomes, including the probability of causation and proportions benefiting from or harmed by treatment. Wu et al (2025) recently established nonparametric identification of this joint distribution for categorical outcomes under binary treatment by leveraging variation across multiple studies. We demonstrate that their multi-study framework can be implemented within a single study by using a baseline covariate that is associated with untreated potential outcomes but does not modify treatment effects conditional on those outcomes. This reframing substantially broadens the practical applicability of their results, as it eliminates the need for multiple independent datasets and gives analysts control over covariate selection to satisfy key identifying assumptions. We provide complete identification and estimation theory for the single-study setting, including a Neyman-orthogonal estimator for cases where the conditional independence assumption only holds after adjusting for covariates. We validate the estimator in a simulation and apply it to data from a large field experiment assessing the effect of mailings on voter turnout.
[4]
arXiv:2509.20508
[pdf, html, other]
Title:
Fast Estimation of Wasserstein Distances via Regression on Sliced Wasserstein Distances
Khai Nguyen, Hai Nguyen, Nhat Ho
Comments:
35 pages, 20 figures, 4 tables
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We address the problem of efficiently computing Wasserstein distances for multiple pairs of distributions drawn from a meta-distribution. To this end, we propose a fast estimation method based on regressing Wasserstein distance on sliced Wasserstein (SW) distances. Specifically, we leverage both standard SW distances, which provide lower bounds, and lifted SW distances, which provide upper bounds, as predictors of the true Wasserstein distance. To ensure parsimony, we introduce two linear models: an unconstrained model with a closed-form least-squares solution, and a constrained model that uses only half as many parameters. We show that accurate models can be learned from a small number of distribution pairs. Once estimated, the model can predict the Wasserstein distance for any pair of distributions via a linear combination of SW distances, making it highly efficient. Empirically, we validate our approach on diverse tasks, including Gaussian mixtures, point-cloud classification, and Wasserstein-space visualizations for 3D point clouds. Across various datasets such as MNIST point clouds, ShapeNetV2, MERFISH Cell Niches, and scRNA-seq, our method consistently provides a better approximation of Wasserstein distance than the state-of-the-art Wasserstein embedding model, Wasserstein Wormhole, particularly in low-data regimes. Finally, we demonstrate that our estimator can also accelerate Wormhole training, yielding \textit{RG-Wormhole}.
[5]
arXiv:2509.20555
[pdf, html, other]
Title:
Robust Estimation of Moderated Causal Excursion Odds Ratio in Micro-Randomized Trials
Jiaxin Yu, Tianchen Qian
Subjects:
Methodology (stat.ME)
Micro-randomized trials (MRTs) have become increasingly popular for developing and evaluating mobile health interventions that promote healthy behaviors and manage chronic conditions. The recently proposed causal excursion effects have become the standard measure for interventions' marginal and moderated effect in MRTs. Existing methods for MRTs with binary outcomes focus on causal excursion effects on the relative risk scale. However, a causal excursion effect on the odds ratio scale is attractive for its interpretability and valid predicted probabilities, making it a valuable supplement to causal excursion relative risk. In this paper, we propose two novel estimators for the moderated causal excursion odds ratio for MRTs with longitudinal binary outcomes. When the prespecified moderator fully captures the way interventions are sequentially randomized, we propose a doubly robust estimator that remains consistent if either of two nuisance models is correctly specified. For more general settings in which treatment randomization depends on variables beyond the chosen moderator, we propose a general estimator that incorporates an association nuisance model. We further establish the general estimator's robustness to the misspecification of the association nuisance model under no causal effect, and extend the general estimator to accommodate any link functions. We establish the consistency and asymptotic normality of both estimators and demonstrate their performance through simulation studies. We apply the methods to Drink Less, a 30-day MRT for developing mobile health interventions to help reduce alcohol consumption, where the proximal outcome is whether the user opens the app in the hour following the notification.
[6]
arXiv:2509.20586
[pdf, html, other]
Title:
Incorporating External Controls for Estimating the Average Treatment Effect on the Treated with High-Dimensional Data: Retaining Double Robustness and Ensuring Double Safety
Chi-Shian Dai, Chao Ying, Yang Ning, Jiwei Zhao
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
Randomized controlled trials (RCTs) are widely regarded as the gold standard for causal inference in biomedical research. For instance, when estimating the average treatment effect on the treated (ATT), a doubly robust estimation procedure can be applied, requiring either the propensity score model or the control outcome model to be correctly specified. In this paper, we address scenarios where external control data, often with a much larger sample size, are available. Such data are typically easier to obtain from historical records or third-party sources. However, we find that incorporating external controls into the standard doubly robust estimator for ATT may paradoxically result in reduced efficiency compared to using the estimator without external controls. This counterintuitive outcome suggests that the naive incorporation of external controls could be detrimental to estimation efficiency. To resolve this issue, we propose a novel doubly robust estimator that guarantees higher efficiency than the standard approach without external controls, even under model misspecification. When all models are correctly specified, this estimator aligns with the standard doubly robust estimator that incorporates external controls and achieves semiparametric efficiency. The asymptotic theory developed in this work applies to high-dimensional confounder settings, which are increasingly common with the growing prevalence of electronic health record data. We demonstrate the effectiveness of our methodology through extensive simulation studies and a real-world data application.
[7]
arXiv:2509.20587
[pdf, html, other]
Title:
Unsupervised Domain Adaptation with an Unobservable Source Subpopulation
Chao Ying, Jun Jin, Haotian Zhang, Qinglong Tian, Yanyuan Ma, Yixuan Li, Jiwei Zhao
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
We study an unsupervised domain adaptation problem where the source domain consists of subpopulations defined by the binary label $Y$ and a binary background (or environment) $A$. We focus on a challenging setting in which one such subpopulation in the source domain is unobservable. Naively ignoring this unobserved group can result in biased estimates and degraded predictive performance. Despite this structured missingness, we show that the prediction in the target domain can still be recovered. Specifically, we rigorously derive both background-specific and overall prediction models for the target domain. For practical implementation, we propose the distribution matching method to estimate the subpopulation proportions. We provide theoretical guarantees for the asymptotic behavior of our estimator, and establish an upper bound on the prediction error. Experiments on both synthetic and real-world datasets show that our method outperforms the naive benchmark that does not account for this unobservable source subpopulation.
[8]
arXiv:2509.20618
[pdf, html, other]
Title:
A Gapped Scale-Sensitive Dimension and Lower Bounds for Offset Rademacher Complexity
Zeyu Jia, Yury Polyanskiy, Alexander Rakhlin
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
We study gapped scale-sensitive dimensions of a function class in both sequential and non-sequential settings. We demonstrate that covering numbers for any uniformly bounded class are controlled above by these gapped dimensions, generalizing the results of \cite{anthony2000function,alon1997scale}. Moreover, we show that the gapped dimensions lead to lower bounds on offset Rademacher averages, thereby strengthening existing approaches for proving lower bounds on rates of convergence in statistical and online learning.
[9]
arXiv:2509.20636
[pdf, html, other]
Title:
A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates in Spatial Compositional Data
Joaquim Valerio Teixeira, Ed Reznik, Sudpito Banerjee, Wesley Tansey
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
The analysis of spatial data from biological imaging technology, such as imaging mass spectrometry (IMS) or imaging mass cytometry (IMC), is challenging because of a competitive sampling process which convolves signals from molecules in a single pixel. To address this, we develop a scalable Bayesian framework that leverages natural sparsity in spatial signal patterns to recover relative rates for each molecule across the entire image. Our method relies on the use of a heavy-tailed variant of the graphical lasso prior and a novel hierarchical variational family, enabling efficient inference via automatic differentiation variational inference. Simulation results show that our approach outperforms state-of-the-practice point estimate methodologies in IMS, and has superior posterior coverage than mean-field variational inference techniques. Results on real IMS data demonstrate that our approach better recovers the true anatomical structure of known tissue, removes artifacts, and detects active regions missed by the standard analysis approach.
[10]
arXiv:2509.20638
[pdf, html, other]
Title:
An Interpretable Single-Index Mixed-Effects Model for Non-Gaussian National Survey Data
Qingyang Liu, Debdeep Pati, Dipankar Bandyopadhyay
Comments:
39 pages, 13 figures
Subjects:
Methodology (stat.ME); Applications (stat.AP)
This manuscript presents an innovative statistical model to quantify periodontal disease in the context of complex medical data. A mixed-effects model incorporating skewed random effects and heavy-tailed residuals is introduced, ensuring robust handling of non-normal data distributions. The fixed effect is modeled as a combination of a slope parameter and a single index function, constrained to be monotonic increasing for meaningful interpretation. This approach captures different dimensions of periodontal disease progression by integrating Clinical Attachment Level (CAL) and Pocket Depth (PD) biomarkers within a unified analytical framework. A variable selection method based on the grouped horseshoe prior is employed, addressing the relatively high number of risk factors. Furthermore, survey weight information typically provided with large survey data is incorporated to ensure accurate inference. This comprehensive methodology significantly advances the statistical quantification of periodontal disease, offering a nuanced and precise assessment of risk factors and disease progression. The proposed methodology is implemented in the \textsf{R} package \href{this https URL}{\textsc{MSIMST}}.
[11]
arXiv:2509.20698
[pdf, other]
Title:
Online Sequential Leveraging Sampling Method for Streaming Autoregressive Time Series with Application to Seismic Data
Rui Xie, T. N. Sriram, Wei Biao Wu, Ping Ma
Comments:
Accepted by the Annals of Applied Statistics
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Seismic data contain complex temporal information that arrives at high speed and has a large, even potentially unbounded volume. The explosion of temporally correlated streaming data from advanced seismic sensors poses analytical challenges due to its sheer volume and real-time nature. Sampling, or data reduction, is a natural yet powerful tool for handling large streaming data while balancing estimation accuracy and computational cost. Currently, data reduction methods and their statistical properties for streaming data, especially streaming autoregressive time series, are not well-studied in the literature. In this article, we propose an online leverage-based sequential data reduction algorithm for streaming autoregressive time series with application to seismic data. The proposed Sequential Leveraging Sampling (SLS) method selects only one consecutively recorded block from the data stream for inference. While the starting point of the SLS block is chosen using a random mechanism based on streaming leverage scores of data, the block size is determined by a sequential stopping rule. The SLS block offers efficient sample usage, as evidenced by our results confirming asymptotic normality for the normalized least squares estimator in both linear and nonlinear autoregressive settings. The SLS method is applied to two seismic datasets: the 2023 Turkey-Syria earthquake doublet data on the macroseismic scale and the Oklahoma seismic data on the microseismic scale. We demonstrate the ability of the SLS method to efficiently identify seismic events and elucidate their intricate temporal dependence structure. Simulation studies are presented to evaluate the empirical performance of the SLS method.
[12]
arXiv:2509.20702
[pdf, html, other]
Title:
Incorporating LLM Embeddings for Variation Across the Human Genome
Hongqian Niu, Jordan Bryan, Xihao Li, Didong Li
Subjects:
Applications (stat.AP); Artificial Intelligence (cs.AI); Genomics (q-bio.GN)
Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.
[13]
arXiv:2509.20753
[pdf, html, other]
Title:
RAPTOR-GEN: RApid PosTeriOR GENerator for Bayesian Learning in Biomanufacturing
Wandi Xu, Wei Xie
Comments:
80 pages, 6 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Biopharmaceutical manufacturing is vital to public health but lacks the agility for rapid, on-demand production of biotherapeutics due to the complexity and variability of bioprocesses. To overcome this, we introduce RApid PosTeriOR GENerator (RAPTOR-GEN), a mechanism-informed Bayesian learning framework designed to accelerate intelligent digital twin development from sparse and heterogeneous experimental data. This framework is built on a multi-scale probabilistic knowledge graph (pKG), formulated as a stochastic differential equation (SDE)-based foundational model that captures the nonlinear dynamics of bioprocesses. RAPTOR-GEN consists of two ingredients: (i) an interpretable metamodel integrating linear noise approximation (LNA) that exploits the structural information of bioprocessing mechanisms and a sequential learning strategy to fuse heterogeneous and sparse data, enabling inference of latent state variables and explicit approximation of the intractable likelihood function; and (ii) an efficient Bayesian posterior sampling method that utilizes Langevin diffusion (LD) to accelerate posterior exploration by exploiting the gradients of the derived likelihood. It generalizes the LNA approach to circumvent the challenge of step size selection, facilitating robust learning of mechanistic parameters with provable finite-sample performance guarantees. We develop a fast and robust RAPTOR-GEN algorithm with controllable error. Numerical experiments demonstrate its effectiveness in uncovering the underlying regulatory mechanisms of biomanufacturing processes.
[14]
arXiv:2509.20803
[pdf, html, other]
Title:
Statistical Learning of Trade Credit Insurance Network Data with Applications to Ratemaking and Reserving
Woongchae Yoo, Spark C. Tseung, Tsz Chai Fung
Subjects:
Applications (stat.AP)
Trade credit insurance (TCI) is a specialized line of property and casualty insurance, protecting businesses against financial losses due to buyer's insolvency. Predictive modeling for TCI claims poses formidable challenges due to the data's complexity, yet remains underexplored in the literature. Leveraging six years of detailed TCI data from an Asian TCI insurer, we develop a bivariate, network-augmented Generalized Linear Mixed Model (GLMM) to jointly model claim probability and reporting time gaps. Our model integrates extended-order degree centrality and random effects at the business and policy levels, adjusted for data incompleteness, to capture claim histories, reporting time gaps, and network relationships specific to TCI data. To implement a feasible workaround for the high-dimensional integrations required by individual random effects, we propose a scalable Stochastic Expectation-Maximization (SEM) algorithm. Data analysis using this TCI dataset demonstrates that our model significantly outperforms benchmark models in both model fit and predictive accuracy, highlighting the effectiveness of our approach for improved ratemaking and reserving in TCI. Supplementary materials for this article are available as an online supplement.
[15]
arXiv:2509.20831
[pdf, html, other]
Title:
Modi linear failure rate distribution with application to survival time data
Lazhar Benkhelifa
Subjects:
Methodology (stat.ME); Applications (stat.AP)
A new lifetime model, named the Modi linear failure rate distribution, is suggested. This flexible model is capable of accommodating a wide range of hazard rate shapes, including decreasing, increasing, bathtub, upside-down bathtub, and modified bathtub forms, making it particularly suitable for modeling diverse survival and reliability data. Our proposed model contains the Modi exponential distribution and the Modi Rayleigh distribution as sub-models. Numerous mathematical and reliability properties are derived, including the $r^{th}$ moment, moment generating function, $r^{th}$ conditional moment, quantile function, order statistics, mean deviations, Rényi entropy, and reliability function. The method of maximum likelihood is employed to estimate the model parameters. Monte Carlo simulations are presented to examine how these estimators perform. The superior fit of our newly introduced model is proved through two real-world survival data sets.
[16]
arXiv:2509.20850
[pdf, html, other]
Title:
Detecting gene-environment interactions to guide personalized intervention: boosting distributional regression for polygenic scores
Qiong Wu, Hannah Klinkhammer, Kiran Kunwar, Christian Staerk, Carlo Maj, Andreas Mayr
Comments:
18 pages with 8 figures
Subjects:
Applications (stat.AP); Genomics (q-bio.GN); Quantitative Methods (q-bio.QM); Computation (stat.CO)
Polygenic risk scores can be used to model the individual genetic liability for human traits. Current methods primarily focus on modeling the mean of a phenotype neglecting the variance. However, genetic variants associated with phenotypic variance can provide important insights to gene-environment interaction studies. To overcome this, we propose snpboostlss, a cyclical gradient boosting algorithm for a Gaussian location-scale model to jointly derive sparse polygenic models for both the mean and the variance of a quantitative phenotype. To improve computational efficiency on high-dimensional and large-scale genotype data (large n and large p), we only consider a batch of most relevant variants in each boosting step. We investigate the effect of statins therapy (the environmental factor) on low-density lipoprotein in the UK Biobank cohort using the new snpboostlss algorithm. We are able to verify the interaction between statins usage and the polygenic risk scores for phenotypic variance in both cross sectional and longitudinal analyses. Particularly, following the spirit of target trial emulation, we observe that the treatment effect of statins is more substantial in people with higher polygenic risk scores for phenotypic variance, indicating gene-environment interaction. When applying to body mass index, the newly constructed polygenic risk scores for variance show significant interaction with physical activity and sedentary behavior. Therefore, the polygenic risk scores for phenotypic variance derived by snpboostlss have potential to identify individuals that could benefit more from environmental changes (e.g. medical intervention and lifestyle changes).
[17]
arXiv:2509.20928
[pdf, html, other]
Title:
Conditionally Whitened Generative Models for Probabilistic Time Series Forecasting
Yanfeng Yang, Siwei Chen, Pingping Hu, Zhaotong Shen, Yingjie Zhang, Zhuoran Sun, Shuai Li, Ziqi Chen, Kenji Fukumizu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Probabilistic forecasting of multivariate time series is challenging due to non-stationarity, inter-variable dependencies, and distribution shifts. While recent diffusion and flow matching models have shown promise, they often ignore informative priors such as conditional means and covariances. In this work, we propose Conditionally Whitened Generative Models (CW-Gen), a framework that incorporates prior information through conditional whitening. Theoretically, we establish sufficient conditions under which replacing the traditional terminal distribution of diffusion models, namely the standard multivariate normal, with a multivariate normal distribution parameterized by estimators of the conditional mean and covariance improves sample quality. Guided by this analysis, we design a novel Joint Mean-Covariance Estimator (JMCE) that simultaneously learns the conditional mean and sliding-window covariance. Building on JMCE, we introduce Conditionally Whitened Diffusion Models (CW-Diff) and extend them to Conditionally Whitened Flow Matching (CW-Flow). Experiments on five real-world datasets with six state-of-the-art generative models demonstrate that CW-Gen consistently enhances predictive performance, capturing non-stationary dynamics and inter-variable correlations more effectively than prior-free approaches. Empirical results further demonstrate that CW-Gen can effectively mitigate the effects of distribution shift.
[18]
arXiv:2509.20985
[pdf, other]
Title:
Empirical PAC-Bayes bounds for Markov chains
Vahe Karagulyan, Pierre Alquier
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The core of generalization theory was developed for independent observations. Some PAC and PAC-Bayes bounds are available for data that exhibit a temporal dependence. However, there are constants in these bounds that depend on properties of the data-generating process: mixing coefficients, mixing time, spectral gap... Such constants are unknown in practice. In this paper, we prove a new PAC-Bayes bound for Markov chains. This bound depends on a quantity called the pseudo-spectral gap. The main novelty is that we can provide an empirical bound on the pseudo-spectral gap when the state space is finite. Thus, we obtain the first fully empirical PAC-Bayes bound for Markov chains. This extends beyond the finite case, although this requires additional assumptions. On simulated experiments, the empirical version of the bound is essentially as tight as the non-empirical one.
[19]
arXiv:2509.21015
[pdf, other]
Title:
Unbiased Parameter Estimation of Partially Observed Diffusions using Diffusion Bridges
Miguel Alvarez, Ajay Jasra
Subjects:
Methodology (stat.ME); Numerical Analysis (math.NA)
In this article we consider the estimation of static parameters for partially observed diffusion processes with discrete-time observations over a fixed time interval. In particular, when one only has access to time-discretized solutions of the diffusions we build upon the works of \cite{ub_par,ub_grad} to devise a method that can estimate the parameters without time-discretization bias. We leverage an identity associated to the gradient of the log-likelihood associated to diffusion bridges, which has not been used before. Contrary to the afore mentioned methods, the diffusion coefficient can depend on the parameters and our approach facilitates the use of more efficient Markov chain sampling algorithms. We prove that our estimator is unbiased with finite variance and demonstrate the efficacy of our methodology in several examples.
[20]
arXiv:2509.21041
[pdf, html, other]
Title:
A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data
Maeve Upton, Eamonn Organ, Amanda Lenzi, James Sweeney
Subjects:
Applications (stat.AP)
Accurate estimation of solar irradiance is essential for reliable modelling of solar photovoltaic (PV) power production. In Ireland's highly variable maritime climate, where ground-based measurement stations are sparsely distributed, selecting an appropriate solar irradiance dataset presents a significant challenge. This study introduces a novel Bayesian spatio-temporal modelling framework for predicting solar irradiance at hourly and sub-hourly (10-minute) resolutions across Ireland. Cross-validation demonstrates that our model is statistically robust across all temporal resolutions with hourly showing highest prediction precision whereas 10-minute resolution encounters higher errors but better uncertainty quantification. In separate evaluations, we compare our model against alternative data sources, including reanalysis datasets and nearest-station interpolation, and find that it consistently provides superior site-specific accuracy. At the hourly scale, our model outperforms ERA5 in agreement with ground-based observations. At the sub-hourly scale, 10-minute resolution estimates provide solar PV power outputs consistent with residential and industrial solar PV installations in Ireland. Beyond surpassing existing datasets, our model delivers full uncertainty quantification, scalability and the capacity for real-time implementation, offering a powerful tool for solar energy prediction and the estimation of losses due to overload clipping from inverter undersizing.
[21]
arXiv:2509.21091
[pdf, html, other]
Title:
Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute
Junpei Komiyama, Daisuke Oba, Masafumi Oyamada
Subjects:
Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \to \infty$, which we denote as Best-of-$\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.
[22]
arXiv:2509.21160
[pdf, html, other]
Title:
WISER: Segmenting watermarked region - an epidemic change-point perspective
Soham Bonnerjee, Sayar Karmakar, Subhrajyoty Roy
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
With the increasing popularity of large language models, concerns over content authenticity have led to the development of myriad watermarking schemes. These schemes can be used to detect a machine-generated text via an appropriate key, while being imperceptible to readers with no such keys. The corresponding detection mechanisms usually take the form of statistical hypothesis testing for the existence of watermarks, spurring extensive research in this direction. However, the finer-grained problem of identifying which segments of a mixed-source text are actually watermarked, is much less explored; the existing approaches either lack scalability or theoretical guarantees robust to paraphrase and post-editing. In this work, we introduce a unique perspective to such watermark segmentation problems through the lens of epidemic change-points. By highlighting the similarities as well as differences of these two problems, we motivate and propose WISER: a novel, computationally efficient, watermark segmentation algorithm. We theoretically validate our algorithm by deriving finite sample error-bounds, and establishing its consistency in detecting multiple watermarked segments in a single text. Complementing these theoretical results, our extensive numerical experiments show that WISER outperforms state-of-the-art baseline methods, both in terms of computational speed as well as accuracy, on various benchmark datasets embedded with diverse watermarking schemes. Our theoretical and empirical findings establish WISER as an effective tool for watermark localization in most settings. It also shows how insights from a classical statistical problem can lead to a theoretically valid and computationally efficient solution of a modern and pertinent problem.
[23]
arXiv:2509.21174
[pdf, html, other]
Title:
Breaking the curse of dimensionality for linear rules: optimal predictors over the ellipsoid
Alexis Ayme, Bruno Loureiro
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
In this work, we address the following question: What minimal structural assumptions are needed to prevent the degradation of statistical learning bounds with increasing dimensionality? We investigate this question in the classical statistical setting of signal estimation from $n$ independent linear observations $Y_i = X_i^{\top}\theta + \epsilon_i$. Our focus is on the generalization properties of a broad family of predictors that can be expressed as linear combinations of the training labels, $f(X) = \sum_{i=1}^{n} l_{i}(X) Y_i$. This class -- commonly referred to as linear prediction rules -- encompasses a wide range of popular parametric and non-parametric estimators, including ridge regression, gradient descent, and kernel methods. Our contributions are twofold. First, we derive non-asymptotic upper and lower bounds on the generalization error for this class under the assumption that the Bayes predictor $\theta$ lies in an ellipsoid. Second, we establish a lower bound for the subclass of rotationally invariant linear prediction rules when the Bayes predictor is fixed. Our analysis highlights two fundamental contributions to the risk: (a) a variance-like term that captures the intrinsic dimensionality of the data; (b) the noiseless error, a term that arises specifically in the high-dimensional regime. These findings shed light on the role of structural assumptions in mitigating the curse of dimensionality.
[24]
arXiv:2509.21191
[pdf, html, other]
Title:
Not All Accuracy Is Equal: Prioritizing Diversity in Infectious Disease Forecasting
Carson Dudley, Marisa Eisenberg
Comments:
5 pages, 2 figures
Subjects:
Applications (stat.AP); Quantitative Methods (q-bio.QM)
Ensemble forecasts have become a cornerstone of large-scale disease response, underpinning decision making at agencies such as the US Centers for Disease Control and Prevention (CDC). Their growing use reflects the goal of combining multiple models to improve accuracy and stability versus using a single model. However, recent experience shows these benefits are not guaranteed. During the COVID-19 pandemic, the CDC's multi-model forecasting ensemble outperformed the best single model by only 1%, and CDC flu forecasting ensembles have often ranked below multiple individual models.
This raises a key question: why are ensembles underperforming? We posit that a central reason is that both model developers and ensemble builders typically focus on stand-alone accuracy. Models are fit to minimize their own forecasting error, and ensembles are often weighted according to those same scores. However, most epidemic forecasts are built from a small set of approaches and trained on the same surveillance data, leading to highly correlated errors. This redundancy limits the benefit of ensembling and may explain why large ensembles sometimes deliver only marginal gains.
To realize the potential of ensembles, both modelers and ensemblers should prioritize models that contribute complementary information rather than replicating existing approaches. Ensembles built with this principle in mind move beyond size for its own sake toward true diversity, producing forecasts that are more robust and more valuable for epidemic preparedness and response.
[25]
arXiv:2509.21225
[pdf, html, other]
Title:
A Latent Variable Framework for Multiple Imputation with Non-ignorable Missingness: Analyzing Perceptions of Social Justice in Europe
Siliang Zhang, Yunxiao Chen, Jouni Kuha
Subjects:
Methodology (stat.ME)
This paper proposes a general multiple imputation approach for analyzing large-scale data with missing values. An imputation model is derived from a joint distribution induced by a latent variable model, which can flexibly capture associations among variables of mixed types. The model also allows for missingness which depends on the latent variables and is thus non-ignorable with respect to the observed data. We develop a frequentist multiple imputation method for this framework and provide asymptotic theory that establishes valid inference for a broad class of analysis models. Simulation studies confirm the method's theoretical properties and robust practical performance. The procedure is applied to a cross-national analysis of individuals' perceptions of justice and fairness of income distributions in their societies, using data from the European Social Survey which has substantial nonresponse. The analysis demonstrates that failing to account for non-ignorable missingness can yield biased conclusions; for instance, complete-case analysis is shown to exaggerate the correlation between personal income and perceived fairness of income distributions in society. Code implementing the proposed methodology is publicly available at this https URL.
[26]
arXiv:2509.21226
[pdf, html, other]
Title:
Bayesian inference for velocity-jump models for movement
Paul G. Blackwell
Subjects:
Methodology (stat.ME)
The velocity-jump model is a specific type of piecewise deterministic Markov process in which an individual's velocity is constant except at times that form the events of some point process. It represents an interpretable continuous-time version of the discrete-time `step and turn' models widely used in analysing wildlife telemetry. In this paper, I derive a reversible jump Markov chain Monte Carlo algorithm to carry out exact Bayesian inference for velocity-jump models by reconstructing the trajectories between observations, and illustrate its use in analysing real and simulated telemetry data. The method uses a proposal distribution for updating velocities that is constructed by approximating the movement model with a multivariate normal distribution and then conditioning that distribution on the data. The velocity-jump models considered can incorporate measurement error and Markov dependence between successive velocities.
[27]
arXiv:2509.21228
[pdf, html, other]
Title:
Response to Promises and Pitfalls of Deep Kernel Learning
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This note responds to "Promises and Pitfalls of Deep Kernel Learning" (Ober et al., 2021). The marginal likelihood of a Gaussian process can be compartmentalized into a data fit term and a complexity penalty. Ober et al. (2021) shows that if a kernel can be multiplied by a signal variance coefficient, then reparametrizing and substituting in the maximized value of this parameter sets a reparametrized data fit term to a fixed value. They use this finding to argue that the complexity penalty, a log determinant of the kernel matrix, then dominates in determining the other values of kernel hyperparameters, which can lead to data overcorrelation. By contrast, we show that the reparametrization in fact introduces another data-fit term which influences all other kernel hyperparameters. Thus, a balance between data fit and complexity still plays a significant role in determining kernel hyperparameters.
Cross submissions (showing 12 of 12 entries)
[28]
arXiv:2509.20417
(cross-list from eess.IV)
[pdf, html, other]
Title:
Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations
D. Doutsas, B. Figliuzzi
Journal-ref:
2024 14th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)
Subjects:
Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Applications (stat.AP)
We propose a novel approach based on optimal transport (OT) for tackling the problem of highly mixed data in blind hyperspectral unmixing. Our method constrains the distribution of the estimated abundance matrix to resemble a targeted Dirichlet distribution more closely. The novelty lies in using OT to measure the discrepancy between the targeted and true abundance distributions, which we incorporate as a regularization term in our optimization problem. We demonstrate the efficiency of our method through a case study involving an unsupervised deep learning approach. Our experiments show that the proposed approach allows for a better estimation of the endmembers in the presence of highly mixed data, while displaying robustness to the choice of target abundance distribution.
[29]
arXiv:2509.20574
(cross-list from cs.LG)
[pdf, html, other]
Title:
The Sensitivity of Variational Bayesian Neural Network Performance to Hyperparameters
Scott Koermer, Natalie Klein
Comments:
18 pages, 6 figures
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
In scientific applications, predictive modeling is often of limited use without accurate uncertainty quantification (UQ) to indicate when a model may be extrapolating or when more data needs to be collected. Bayesian Neural Networks (BNNs) produce predictive uncertainty by propagating uncertainty in neural network (NN) weights and offer the promise of obtaining not only an accurate predictive model but also accurate UQ. However, in practice, obtaining accurate UQ with BNNs is difficult due in part to the approximations used for practical model training and in part to the need to choose a suitable set of hyperparameters; these hyperparameters outnumber those needed for traditional NNs and often have opaque effects on the results. We aim to shed light on the effects of hyperparameter choices for BNNs by performing a global sensitivity analysis of BNN performance under varying hyperparameter settings. Our results indicate that many of the hyperparameters interact with each other to affect both predictive accuracy and UQ. For improved usage of BNNs in real-world applications, we suggest that global sensitivity analysis, or related methods such as Bayesian optimization, should be used to aid in dimensionality reduction and selection of hyperparameters to ensure accurate UQ in BNNs.
[30]
arXiv:2509.20634
(cross-list from econ.EM)
[pdf, html, other]
Title:
Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities
Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, Subhadeep Paul
Subjects:
Econometrics (econ.EM); Artificial Intelligence (cs.AI); General Economics (econ.GN); Methodology (stat.ME)
We find AI embeddings obtained using a pre-trained transformer-based Large Language Model (LLM) of 80,000-120,000 written affirmations and correction exchanges among residents in low-security correctional facilities to be highly predictive of recidivism. The prediction accuracy is 30\% higher with embedding vectors than with only pre-entry covariates. However, since the text embedding vectors are high-dimensional, we perform Zero-Shot classification of these texts to a low-dimensional vector of user-defined classes to aid interpretation while retaining the predictive power. To shed light on the social dynamics inside the correctional facilities, we estimate peer effects in these LLM-generated numerical representations of language with a multivariate peer effect model, adjusting for network endogeneity. We develop new methodology and theory for peer effect estimation that accommodate sparse networks, multivariate latent variables, and correlated multivariate outcomes. With these new methods, we find significant peer effects in language usage for interaction and feedback.
[31]
arXiv:2509.20678
(cross-list from cs.LG)
[pdf, html, other]
Title:
Bispectral OT: Dataset Comparison using Symmetry-Aware Optimal Transport
Annabel Ma, Kaiying Hou, David Alvarez-Melis, Melanie Weber
Comments:
Accepted to NeurIPS 2025 Workshop on Symmetry and Geometry in Neural Representations (NeurReps)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Optimal transport (OT) is a widely used technique in machine learning, graphics, and vision that aligns two distributions or datasets using their relative geometry. In symmetry-rich settings, however, OT alignments based solely on pairwise geometric distances between raw features can ignore the intrinsic coherence structure of the data. We introduce Bispectral Optimal Transport, a symmetry-aware extension of discrete OT that compares elements using their representation using the bispectrum, a group Fourier invariant that preserves all signal structure while removing only the variation due to group actions. Empirically, we demonstrate that the transport plans computed with Bispectral OT achieve greater class preservation accuracy than naive feature OT on benchmark datasets transformed with visual symmetries, improving the quality of meaningful correspondences that capture the underlying semantic label structure in the dataset while removing nuisance variation not affecting class or content.
[32]
arXiv:2509.20721
(cross-list from cs.LG)
[pdf, html, other]
Title:
Scaling Laws are Redundancy Laws
Yuda Bi, Vince D Calhoun
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Scaling laws, a defining feature of deep learning, reveal a striking power-law improvement in model performance with increasing dataset and model size. Yet, their mathematical origins, especially the scaling exponent, have remained elusive. In this work, we show that scaling laws can be formally explained as redundancy laws. Using kernel regression, we show that a polynomial tail in the data covariance spectrum yields an excess risk power law with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail and 1/beta measures redundancy. This reveals that the learning curve's slope is not universal but depends on data redundancy, with steeper spectra accelerating returns to scale. We establish the law's universality across boundedly invertible transformations, multi-modal mixtures, finite-width approximations, and Transformer architectures in both linearized (NTK) and feature-learning regimes. This work delivers the first rigorous mathematical explanation of scaling laws as finite-sample redundancy laws, unifying empirical observations with theoretical foundations.
[33]
arXiv:2509.20993
(cross-list from cs.LG)
[pdf, html, other]
Title:
Learning Ising Models under Hard Constraints using One Sample
Rohan Chauhan, Ioannis Panageas
Subjects:
Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)
We consider the problem of estimating inverse temperature parameter $\beta$ of an $n$-dimensional truncated Ising model using a single sample. Given a graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability distribution over the $n$-dimensional hypercube $\{-1,1\}^n$ where each configuration $\mathbf{\sigma}$ is constrained to lie in a truncation set $S \subseteq \{-1,1\}^n$ and has probability $\Pr(\mathbf{\sigma}) \propto \exp(\beta\mathbf{\sigma}^\top A\mathbf{\sigma})$ with $A$ being the adjacency matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where the truncation set $S$ can be expressed as the set of satisfying assignments of a $k$-SAT formula. Given a single sample $\mathbf{\sigma}$ from a truncated Ising model, with inverse parameter $\beta^*$, underlying graph $G$ of bounded degree $\Delta$ and $S$ being expressed as the set of satisfying assignments of a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\hat{\beta}$ that is $O(\Delta^3/\sqrt{n})$-consistent with the true parameter $\beta^*$ for $k \gtrsim \log(d^2k)\Delta^3.$
Our estimator is based on the maximization of the pseudolikelihood, a notion that has received extensive analysis for various probabilistic models without [Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA '24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC '19, Galanis et al. SODA '24], to confront the more challenging setting of the truncated Ising model.
[34]
arXiv:2509.21021
(cross-list from cs.LG)
[pdf, html, other]
Title:
Efficient Ensemble Conditional Independence Test Framework for Causal Discovery
Zhengkang Guan, Kun Kuang
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Constraint-based causal discovery relies on numerous conditional independence tests (CITs), but its practical applicability is severely constrained by the prohibitive computational cost, especially as CITs themselves have high time complexity with respect to the sample size. To address this key bottleneck, we introduce the Ensemble Conditional Independence Test (E-CIT), a general and plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate strategy: it partitions the data into subsets, applies a given base CIT independently to each subset, and aggregates the resulting p-values using a novel method grounded in the properties of stable distributions. This framework reduces the computational complexity of a base CIT to linear in the sample size when the subset size is fixed. Moreover, our tailored p-value combination method offers theoretical consistency guarantees under mild conditions on the subtests. Experimental results demonstrate that E-CIT not only significantly reduces the computational burden of CITs and causal discovery but also achieves competitive performance. Notably, it exhibits an improvement in complex testing scenarios, particularly on real-world datasets.
[35]
arXiv:2509.21096
(cross-list from econ.EM)
[pdf, html, other]
Title:
Overidentification testing with weak instruments and heteroskedasticity
Stuart Lane, Frank Windmeijer
Comments:
45 pages
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
Exogeneity is key for IV estimators, which can assessed via overidentification (OID) tests. We discuss the Kleibergen-Paap (KP) rank test as a heteroskedasticity-robust OID test and compare to the typical J-test. We derive the heteroskedastic weak-instrument limiting distributions for J and KP as special cases of the robust score test estimated via 2SLS and LIML respectively. Monte Carlo simulations show that KP usually performs better than J, which is prone to severe size distortions. Test size depends on model parameters not consistently estimable with weak instruments, so a conservative approach is recommended. This generalises recommendations to use LIML-based OID tests under homoskedasticity. We then revisit the classic problem of estimating the elasticity of intertemporal substitution (EIS) in lifecycle consumption models. Lagged macroeconomic indicators should provide naturally valid but frequently weak instruments. The literature provides a wide range of estimates for this parameter, and J frequently rejects the null of valid instruments. J often rejects the null whereas KP does not; we suggest that J over-rejects, sometimes severely. We argue that KP-test should be used over the J-test. We also argue that instrument invalidity/misspecification is unlikely the cause of the range of EIS estimates in the literature.
[36]
arXiv:2509.21132
(cross-list from q-bio.QM)
[pdf, other]
Title:
Detecting disease progression from animal movement using hidden Markov models
Dongmin Kim, Théo Michelot, Katherine Mertes, Jared A. Stabach, John Fieberg
Subjects:
Quantitative Methods (q-bio.QM); Applications (stat.AP)
Understanding disease dynamics is crucial for managing wildlife populations and assessing spillover risk to domestic animals and humans, but infection data on free-ranging animals are difficult to obtain. Because pathogen and parasite infections can alter host movement, infection status may be inferred from animal trajectories. We present a hidden Markov model (HMM) framework that links observed movement behaviors to unobserved infection states, consistent with epidemiological compartmental models (e.g., susceptible, infected, recovered, dead). Using movement data from 84 reintroduced scimitar-horned oryx (Oryx dammah), 38 confirmed dead in the field and 6 sampled for disease testing, we demonstrate how HMMs can incorporate epidemiological structure through (1) constrained transition probabilities (e.g., to preclude or allow recovery), (2) covariate effects on transmission, and (3) hierarchically structured HMMs (HHMMs) for multi-scale transitions. Comparing veterinary diagnostic reports with model outputs, we found that HMMs with epidemiological constraints successfully identified infection-associated reductions in movement, whereas unconstrained models failed to capture disease progression. Simulations further showed that constrained HMMs accurately classified susceptible, infected, and recovered states. By illustrating flexible formulations and a workflow for model selection, we provide a transferable approach for detecting infection from movement data. This framework can enhance wildlife disease surveillance, guide population management, and improve understanding of disease dynamics.
[37]
arXiv:2509.21172
(cross-list from cs.LG)
[pdf, html, other]
Title:
Inverse Reinforcement Learning Using Just Classification and a Few Regressions
Lars van der Laan, Nathan Kallus, Aurélien Bibaut
Subjects:
Machine Learning (cs.LG); Econometrics (econ.EM); Optimization and Control (math.OC); Machine Learning (stat.ML)
Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a linear fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.
[38]
arXiv:2509.21181
(cross-list from cs.LG)
[pdf, html, other]
Title:
Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias
Shuofeng Zhang, Ard Louis
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
For overparameterized linear regression with isotropic Gaussian design and minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\{ \lVert \widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s which plateau from those that continue to grow with an explicit exponent.
This unified solution resolves the scaling of *all* $\ell_r$ norms within the family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.
We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\alpha$ to an effective $p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.
Given that many generalization proxies depend on $\lVert \widehat {w_p} \rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.
[39]
arXiv:2509.21296
(cross-list from cs.LG)
[pdf, html, other]
Title:
No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks
Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, Itay Safran
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.
Replacement submissions (showing 47 of 47 entries)
[40]
arXiv:2405.13844
(replaced)
[pdf, html, other]
Title:
Counterfactual Cocycles: A Framework for Robust and Coherent Counterfactual Transports
Hugh Dance, Benjamin Bloem-Reddy
Comments:
Fixed typo on p.10 + minor update to experimental results
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
Estimating joint distributions (a.k.a. couplings) over counterfactual outcomes is central to personalized decision-making and treatment risk assessment. Two emergent frameworks with identifiability guarantees are: (i) bijective structural causal models (SCMs), which are flexible but brittle to mis-specified latent noise; and (ii) optimal-transport (OT) methods, which avoid latent noise assumptions but can produce incoherent counterfactual transports which fail to identify higher-order couplings. In this work, we bridge the gap with \emph{counterfactual cocycles}: a framework for counterfactual transports that use algebraic structure to provide coherence and identifiability guarantees. Every counterfactual cocycle corresponds to an equivalence class of SCMs, however the cocycle is invariant to the latent noise distribution, enabling us to sidestep various mis-specification problems. We characterize the structure of all identifiable counterfactual cocycles; propose flexible model parameterizations; introduce a novel cocycle estimator that avoids any distributional assumptions; and derive mis-specification robustness properties of the resulting counterfactual inference method. We demonstrate state-of-the-art performance and noise-robustness of counterfactual cocycles across synthetic benchmarks and a 401(k) eligibility study.
[41]
arXiv:2406.19186
(replaced)
[pdf, html, other]
Title:
Asymptotic independence in higher dimensions and its implications on risk management
Bikramjit Das, Vicky Fasen-Hartmann
Comments:
25 pages
Subjects:
Statistics Theory (math.ST)
In the study of extremes, the presence of asymptotic independence signifies that extreme events across multiple variables are probably less likely to occur together. Although well-understood in a bivariate context, the concept remains relatively unexplored when addressing the nuances of the joint occurrence of extremes in higher dimensions. In this paper, we propose a notion of mutual asymptotic independence to capture the behavior of joint extremes in dimensions larger than two and contrast it with the classical notion of (pairwise) asymptotic independence. Additionally, we define k-wise asymptotic independence, which captures the tail dependence between pairwise and mutual asymptotic independence. The concepts are compared using examples of Archimedean, Gaussian and Marshall-Olkin copulas, among others. Finally, we discuss the implications of these new notions of asymptotic independence on assessing the risk of complex systems under distributional ambiguity.
[42]
arXiv:2407.07338
(replaced)
[pdf, other]
Title:
Towards Complete Causal Explanation with Expert Knowledge
Aparajithan Venkateswaran, Emilija Perković
Comments:
85 pages (main paper 24 pages, supplementary material 61 pages), 21 figures, 7 algorithms, 3 tables
Subjects:
Machine Learning (stat.ML); Discrete Mathematics (cs.DM); Machine Learning (cs.LG); Methodology (stat.ME)
We study the problem of restricting a Markov equivalence class of maximal ancestral graphs (MAGs) to only those MAGs that contain certain edge marks, which we refer to as expert or orientation knowledge. Such a restriction of the Markov equivalence class can be uniquely represented by a restricted essential ancestral graph. Our contributions are several-fold. First, we prove certain properties for the entire Markov equivalence class including a conjecture from Ali et al. (2009). Second, we present several new sound graphical orientation rules for adding orientation knowledge to an essential ancestral graph. We also show that some orientation rules of Zhang (2008b) are not needed for restricting the Markov equivalence class with orientation knowledge. Third, we provide an algorithm for including this orientation knowledge and show that in certain settings the output of our algorithm is a restricted essential ancestral graph. Finally, outside of the specified settings, we provide an algorithm for checking whether a graph is a restricted essential graph and discuss its runtime. This work can be seen as a generalization of Meek (1995) to settings which allow for latent confounding.
[43]
arXiv:2410.07548
(replaced)
[pdf, html, other]
Title:
Hybrid Summary Statistics
T. Lucas Makinen, Ce Sui, Benjamin D. Wandelt, Natalia Porqueres, Alan Heavens
Comments:
7 pages, 4 figures. Accepted to ML4PS2024 at NeurIPS 2024. Code available at this https URL
Subjects:
Machine Learning (stat.ML); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Information Theory (cs.IT); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)
We present a way to capture high-information posteriors from training sets that are sparsely sampled over the parameter space for robust simulation-based inference. In physical inference problems, we can often apply domain knowledge to define traditional summary statistics to capture some of the information in a dataset. We show that augmenting these statistics with neural network outputs to maximise the mutual information improves information extraction compared to neural summaries alone or their concatenation to existing summaries and makes inference robust in settings with low training data. We introduce 1) two loss formalisms to achieve this and 2) apply the technique to two different cosmological datasets to extract non-Gaussian parameter information.
[44]
arXiv:2410.18730
(replaced)
[pdf, other]
Title:
Nonparametric Clustering Stopping Rule Based on Multivariate Median
Hend Gabr, Brian H Willis, Mohammed Baragilly
Subjects:
Computation (stat.CO)
This paper introduces a novel nonparametric criterion for determining the appropriate number of clusters, which is derived from the spatial median. The method is constructed to reconcile two competing objectives of cluster analysis: the preservation of internal homogeneity within clusters and the maximization of heterogeneity across clusters. To this end, the proposed algorithm optimizes the ratio of inter-cluster to intra-cluster variability, incorporating adjustments for both the sample size and the number of clusters. Unlike conventional techniques, the method is distribution-free and demonstrates robustness in the presence of outliers. Its properties were first examined through extensive simulation studies, followed by empirical evaluations on three applied datasets. To further assess comparative performance, the proposed procedure was benchmarked against 13 established algorithms for cluster number determination. In 11 of these comparisons, the proposed criterion exhibited superior performance, thereby underscoring its utility as a reliable and rigorous alternative for multivariate clustering applications.
[45]
arXiv:2411.12840
(replaced)
[pdf, other]
Title:
The Aldous$\unicode{x2013}$Hoover Theorem in Categorical Probability
Leihao Chen, Tobias Fritz, Tomáš Gonda, Andreas Klingler, Antonio Lorenzin
Comments:
39 pages, v2: minor changes per referees' suggestions
Subjects:
Statistics Theory (math.ST); Logic in Computer Science (cs.LO); Category Theory (math.CT); Probability (math.PR)
The Aldous-Hoover Theorem concerns an infinite matrix of random variables whose distribution is invariant under finite permutations of rows and columns. It states that, up to equality in distribution, each random variable in the matrix can be expressed as a function only depending on four key variables: one common to the entire matrix, one that encodes information about its row, one that encodes information about its column, and a fourth one specific to the matrix entry.
We state and prove the theorem within a category-theoretic approach to probability, namely the theory of Markov categories. This makes the proof more transparent and intuitive when compared to measure-theoretic ones. A key role is played by a newly identified categorical property, the Cauchy--Schwarz axiom, which also facilitates a new synthetic de Finetti Theorem.
We further provide a variant of our proof using the ordered Markov property and the d-separation criterion, both generalized from Bayesian networks to Markov categories. We expect that this approach will facilitate a systematic development of more complex results in the future, such as categorical approaches to hierarchical exchangeability.
[46]
arXiv:2502.11820
(replaced)
[pdf, html, other]
Title:
A Diagnostic to Find and Help Combat Stochastic Positivity Issues -- with a Focus on Continuous Treatments
Katharina Ring, Michael Schomaker
Comments:
33 pages (24 without appendix), 12 figures (7 without appendix)
Subjects:
Applications (stat.AP); Other Statistics (stat.OT)
The positivity assumption is central in the identification of a causal effect, and especially the stochastic variant is an issue many applied researchers face, yet is rarely discussed, especially in conjunction with continuous treatments or Modified Treatment Policies. One common recommendation for dealing with a violation is to change the estimand. However, an applied researcher is faced with two problems: First, how can she tell whether there is a stochastic positivity violation given her estimand of interest, preferably without having to estimate a model first? Second, if she finds a problem with stochastic positivity, how should she change her estimand in order to arrive at an estimand which does not face the same issues? We suggest a novel diagnostic which allows the researcher to answer both questions by providing insights into how well an estimation for a certain estimand can be made for each observation using the data at hand. We provide a simulation study on the general behaviour of different Modified Treatment Policies (MTPs) at different levels of stochastic positivity violations and show how the diagnostic helps understand where bias is to be expected. We illustrate the application of our proposed diagnostic in a pharmacoepidemiological study based on data from CHAPAS-3, a trial comparing different treatment regimens for children living with HIV.
[47]
arXiv:2503.16737
(replaced)
[pdf, html, other]
Title:
Revenue Maximization Under Sequential Price Competition Via The Estimation Of s-Concave Demand Functions
Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)
We consider price competition among multiple sellers over a selling horizon of $T$ periods. In each period, sellers simultaneously offer their prices (which are made public) and subsequently observe their respective demand (not made public). The demand function of each seller depends on all sellers' prices through a private, unknown, and nonlinear relationship. We propose a dynamic pricing policy that uses semi-parametric least-squares estimation and show that when the sellers employ our policy, their prices converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers would reach if they were fully informed. Each seller incurs a regret of $O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution of our work is proving the existence of equilibrium under shape-constrained demand functions via the concept of $s$-concavity and establishing regret bounds of our proposed policy. Technically, we also establish new concentration results for the least squares estimator under shape constraints. Our findings offer significant insights into dynamic competition-aware pricing and contribute to the broader study of non-parametric learning in strategic decision-making.
[48]
arXiv:2505.08654
(replaced)
[pdf, html, other]
Title:
An Efficient Multi-scale Leverage Effect Estimator under Dependent Microstructure Noise
Ziyang Xiong, Zhao Chen, Christina Dan Wang
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Statistical Finance (q-fin.ST)
Estimating the leverage effect from high-frequency data is vital but challenged by complex, dependent microstructure noise, often exhibiting non-Gaussian higher-order moments. This paper introduces a novel multi-scale framework for efficient and robust leverage effect estimation under such flexible noise structures. We develop two new estimators, the Subsampling-and-Averaging Leverage Effect (SALE) and the Multi-Scale Leverage Effect (MSLE), which adapt subsampling and multi-scale approaches holistically using a unique shifted window technique. This design simplifies the multi-scale estimation procedure and enhances noise robustness without requiring the pre-averaging approach. We establish central limit theorems and stable convergence, with MSLE achieving convergence rates of an optimal $n^{-1/4}$ and a near-optimal $n^{-1/9}$ for the noise-free and noisy settings, respectively. A cornerstone of our framework's efficiency is a specifically designed MSLE weighting strategy that leverages covariance structures across scales. This significantly reduces asymptotic variance and, critically, yields substantially smaller finite-sample errors than existing methods under both noise-free and realistic noisy settings. Extensive simulations and empirical analyses confirm the superior efficiency, robustness, and practical advantages of our approach.
[49]
arXiv:2505.08683
(replaced)
[pdf, html, other]
Title:
Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models
Stefania Scheurer, Philipp Reiser, Tim Brünnette, Wolfgang Nowak, Anneli Guthke, Paul-Christian Bürkner
Comments:
27 pages, 15 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Bayesian inference typically relies on a large number of model evaluations to estimate posterior distributions. Established methods like Markov Chain Monte Carlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally challenging. While ABI enables fast inference after training, generating sufficient training data still requires thousands of model simulations, which is infeasible for expensive models. Surrogate models offer a solution by providing approximate simulations at a lower computational cost, allowing the generation of large data sets for training. However, the introduced approximation errors and uncertainties can lead to overconfident posterior estimates. To address this, we propose Uncertainty-Aware Surrogate-based Amortized Bayesian Inference (UA-SABI) -- a framework that combines surrogate modeling and ABI while explicitly quantifying and propagating surrogate uncertainties through the inference pipeline. Our experiments show that this approach enables reliable, fast, and repeated Bayesian inference for computationally expensive models, even under tight time constraints.
[50]
arXiv:2506.02413
(replaced)
[pdf, html, other]
Title:
Tensor State Space-based Dynamic Multilayer Network Modeling
Tian Lan, Jie Guo, Chen Zhang
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Understanding the complex interactions within dynamic multilayer networks is critical for advancements in various scientific domains. Existing models often fail to capture such networks' temporal and cross-layer dynamics. This paper introduces a novel Tensor State Space Model for Dynamic Multilayer Networks (TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric Tucker decomposition to represent latent node features, their interaction patterns, and layer transitions. Then by fixing the latent features and allowing the interaction patterns to evolve over time, TSSDMN uniquely captures both the temporal dynamics within layers and across different layers. The model identifiability conditions are discussed. By treating latent features as variables whose posterior distributions are approximated using a mean-field variational inference approach, a variational Expectation Maximization algorithm is developed for efficient model inference. Numerical simulations and case studies demonstrate the efficacy of TSSDMN for understanding dynamic multilayer networks.
[51]
arXiv:2506.03104
(replaced)
[pdf, html, other]
Title:
Two-Phase Treatment with Noncompliance: Identifying the Cumulative Average Treatment Effect via Multisite Instrumental Variables
Guanglei Hong, Xu Qin, Zhengyan Xu, Fan Yang
Comments:
44 pages; 1 figure; 4 tables
Subjects:
Methodology (stat.ME); Applications (stat.AP)
When evaluating a two-phase intervention, the cumulative average treatment effect (ATE) is often the primary causal estimand of interest. However, some individuals who do not respond well to the Phase I treatment may subsequently display noncompliant behaviours. At the same time, exposure to the Phase I treatment is expected to directly influence an individual's potential outcomes, thereby violating the exclusion restriction. Building on an instrumental variable (IV) strategy for multisite trials, we clarify the conditions under which the cumulative ATE of a two-phase treatment can be identified by employing the random assignment of the Phase I treatment as the instrument. Our strategy relaxes both the conventional exclusion restriction and sequential ignorability assumptions. We assess the performance of the new strategy through simulation studies. Additionally, we reanalyze data from the Tennessee class size study, in which students and teachers were randomly assigned to either small or regular class types in kindergarten (Phase I) with noncompliance emerging in Grade 1 (Phase II). Applying our new strategy, we estimate the cumulative ATE of receiving two consecutive years of instruction in a small versus regular class.
[52]
arXiv:2506.08616
(replaced)
[pdf, html, other]
Title:
Generalizing while preserving monotonicity in comparison-based preference learning models
Julien Fageot, Peva Blanchard, Gilles Bareilles, Lê-Nguyên Hoang
Subjects:
Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)
If you tell a learning model that you prefer an alternative $a$ over another alternative $b$, then you probably expect the model to be monotone, that is, the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps surprisingly, many widely deployed comparison-based preference learning models, including large language models, fail to have this guarantee. Until now, the only comparison-based preference learning algorithms that were proved to be monotone are the Generalized Bradley-Terry models. Yet, these models are unable to generalize to uncompared data. In this paper, we advance the understanding of the set of models with generalization ability that are monotone. Namely, we propose a new class of Linear Generalized Bradley-Terry models with Diffusion Priors, and identify sufficient conditions on alternatives' embeddings that guarantee monotonicity. Our experiments show that this monotonicity is far from being a general guarantee, and that our new class of generalizing models improves accuracy, especially when the dataset is limited.
[53]
arXiv:2507.03406
(replaced)
[pdf, html, other]
Title:
Testing Hypotheses regarding Covariance and Correlation matrices with the R package CovCorTest
Paavo Sattler, Svenja Jedhoff
Subjects:
Computation (stat.CO); Methodology (stat.ME)
In addition to the commonly analyzed measures of location, dispersion measurements such as variance and correlation provide many valuable information. Consequently, they play a crucial role in multivariate statistics, which leads to tests regarding covariance and correlation matrices. Furthermore, also the structure of these matrices leads to important hypotheses of interest, since it contains substantial information about the underlying model. In fact, assumptions regarding the structures of covariance and correlation matrices are often fundamental in statistical modelling and testing.
In this context, semi-parametric settings with minimal distributional assumptions and very general hypotheses are essential for enabling manifold usage. The free available package CovCorTest provides suitable tests addressing all aforementioned issues, using bootstrap and similar techniques to achieve good performance, particularly in small samples. Additionally, the package offers flexible specification options for the hypotheses under investigation in two central tests, accommodating users with varying levels of expertise, which results in high flexibility and user-friendliness at the same time. This paper also presents the application of \textbf{CovCorTest} for various issues, illustrated by multiple examples, where the tests are applied to a real-world dataset.
[54]
arXiv:2507.20975
(replaced)
[pdf, html, other]
Title:
Locally Adaptive Conformal Inference for Operator Models
Trevor Harris, Yan Liu
Comments:
9 pages, 2 figures, 2 tables, Preprint
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Operator models are regression algorithms between Banach spaces of functions. They have become an increasingly critical tool for spatiotemporal forecasting and physics emulation, especially in high-stakes scenarios where robust, calibrated uncertainty quantification is required. We introduce Local Sliced Conformal Inference (LSCI), a distribution-free framework for generating function-valued, locally adaptive prediction sets for operator models. We prove finite-sample validity and derive a data-dependent upper bound on the coverage gap under local exchangeability. On synthetic Gaussian-process tasks and real applications (air quality monitoring, energy demand forecasting, and weather prediction), LSCI yields tighter sets with stronger adaptivity compared to conformal baselines. We also empirically demonstrate robustness against biased predictions and certain out-of-distribution noise regimes.
[55]
arXiv:2509.04691
(replaced)
[pdf, html, other]
Title:
Inferring Piece Value in Chess and Chess Variants
Steven Pav
Comments:
58 pages
Subjects:
Applications (stat.AP)
We use logistic regression to estimate the value of the pieces in standard chess and several chess variants, namely Chess 960, Atomic chess, Antichess, and Horde chess. We perform our regressions on several years of data from Lichess, the free and open-source internet chess server. We use the published player ratings to control for the confounding effect of differential player skill. We adjust for the attenuation bias in regressions due to the noise in observed ratings. We find that major piece values, relative to the value of a pawn, are fairly consistent with historical valuation systems. However we find slightly higher value to bishops than knights. We find that piece values are smaller, in absolute value, in Atomic and Antichess than standard chess. We also present approximate values of the pieces to equalize odds when players of varying skill face off. We briefly consider self-play experiments using the Stockfish engine, which give a contrasting view of piece value.
[56]
arXiv:2509.07885
(replaced)
[pdf, other]
Title:
Clustering methods for Categorical Time Series and Sequences : A scoping review
Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, François Petit
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( this https URL )
Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases.
[57]
arXiv:2509.17543
(replaced)
[pdf, html, other]
Title:
Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality
Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett
Comments:
43 pages, 20 figures. Included acknowledgements
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.
[58]
arXiv:2509.19889
(replaced)
[pdf, html, other]
Title:
Improving Disease Risk Estimation in Small Areas by Accounting for Spatiotemporal Local Discontinuities
G. Santafé, A. Adin, M.D. Ugarte
Subjects:
Methodology (stat.ME)
This work proposes a two-step method to enhance disease risk estimation in small areas by integrating spatiotemporal cluster detection within a Bayesian hierarchical spatiotemporal model. First, we introduce an efficient scan-statistic-based clustering algorithm that employs a greedy search within the scan window, enabling flexible cluster detection across large spatial domains. We then integrate these detected clusters into a Bayesian spatiotemporal model to estimate relative risks, explicitly accounting for identified risk discontinuities. We apply this methodology to large-scale cancer mortality data at the municipality level across continental Spain. Our results show our method offers superior cluster detection accuracy compared to SaTScan. Furthermore, integrating cluster information into a Bayesian spatiotemporal model significantly improves model fit and risk estimate performance, as evidenced by better DIC, WAIC, and logarithmic scores than SaTScan-based or standard BYM2 models. This methodology provides a powerful tool for epidemiological analysis, offering a more precise identification of high- and low-risk areas and enhancing the accuracy of risk estimation models.
[59]
arXiv:2007.04568
(replaced)
[pdf, html, other]
Title:
Learning to Bid Optimally and Efficiently in Adversarial First-price Auctions
Yanjun Han, Zhengyuan Zhou, Aaron Flores, Erik Ordentlich, Tsachy Weissman
Subjects:
Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Information Theory (cs.IT); Machine Learning (stat.ML)
First-price auctions have very recently swept the online advertising industry, replacing second-price auctions as the predominant auction mechanism on many platforms. This shift has brought forth important challenges for a bidder: how should one bid in a first-price auction, where unlike in second-price auctions, it is no longer optimal to bid one's private value truthfully and hard to know the others' bidding behaviors? In this paper, we take an online learning angle and address the fundamental problem of learning to bid in repeated first-price auctions, where both the bidder's private valuations and other bidders' bids can be arbitrary. We develop the first minimax optimal online bidding algorithm that achieves an $\widetilde{O}(\sqrt{T})$ regret when competing with the set of all Lipschitz bidding policies, a strong oracle that contains a rich set of bidding strategies. This novel algorithm is built on the insight that the presence of a good expert can be leveraged to improve performance, as well as an original hierarchical expert-chaining structure, both of which could be of independent interest in online learning. Further, by exploiting the product structure that exists in the problem, we modify this algorithm--in its vanilla form statistically optimal but computationally infeasible--to a computationally efficient and space efficient algorithm that also retains the same $\widetilde{O}(\sqrt{T})$ minimax optimal regret guarantee. Additionally, through an impossibility result, we highlight that one is unlikely to compete this favorably with a stronger oracle (than the considered Lipschitz bidding policies). Finally, we test our algorithm on three real-world first-price auction datasets obtained from Verizon Media and demonstrate our algorithm's superior performance compared to several existing bidding algorithms.
[60]
arXiv:2110.02248
(replaced)
[pdf, html, other]
Title:
Contextual Combinatorial Bandits with Changing Action Sets via Gaussian Processes
Andi Nika, Sepehr Elahi, Cem Tekin
Comments:
34 pages, 7 figures
Subjects:
Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process (GP) indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$ regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$ is the maximum information gain associated with the sets of base arm contexts $\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximum cardinality of any feasible action over all rounds, and $\lambda^*(K)$ is the maximum eigenvalue of all covariance matrices of selected actions up to time $T$, which is a function of $K$. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.
[61]
arXiv:2303.05838
(replaced)
[pdf, html, other]
Title:
Rosenthal-type inequalities for linear statistics of Markov chains
Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Marina Sheshukova
Comments:
Revised exposition and updated the LSA example. Main results are unchanged
Subjects:
Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)
In this paper, we establish novel concentration inequalities for additive functionals of geometrically ergodic Markov chains similar to Rosenthal inequalities for sums of independent random variables. We pay special attention to the dependence of our bounds on the mixing time of the corresponding chain. Precisely, we establish explicit bounds that are linked to the constants from the martingale version of the Rosenthal inequality, as well as the constants that characterize the mixing properties of the underlying Markov kernel. Finally, our proof technique is, up to our knowledge, new and is based on a recurrent application of the Poisson decomposition.
[62]
arXiv:2401.02080
(replaced)
[pdf, html, other]
Title:
Energy based diffusion generator for efficient sampling of Boltzmann distributions
Yan Wang, Ling Guo, Hao Wu, Tao Zhou
Subjects:
Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)
Sampling from Boltzmann distributions, particularly those tied to high dimensional and complex energy functions, poses a significant challenge in many fields. In this work, we present the Energy-Based Diffusion Generator (EDG), a novel approach that integrates ideas from variational autoencoders and diffusion models. EDG uses a decoder to generate Boltzmann-distributed samples from simple latent variables, and a diffusion-based encoder to estimate the Kullback-Leibler divergence to the target distribution. Notably, EDG is simulation-free, eliminating the need to solve ordinary or stochastic differential equations during training. Furthermore, by removing constraints such as bijectivity in the decoder, EDG allows for flexible network design. Through empirical evaluation, we demonstrate the superior performance of EDG across a variety of sampling tasks with complex target distributions, outperforming existing methods.
[63]
arXiv:2404.12589
(replaced)
[pdf, html, other]
Title:
Geometry and factorization of multivariate Markov chains with applications to MCMC acceleration
Michael C.H. Choi, Youjia Wang, Geoffrey Wolfer
Comments:
41 pages, 4 figures
Subjects:
Probability (math.PR); Information Theory (cs.IT); Optimization and Control (math.OC); Computation (stat.CO)
This paper analyzes the factorizability and geometry of transition matrices of multivariate Markov chains. Specifically, we demonstrate that the induced chains on factors of a product space can be regarded as information projections with respect to the Kullback-Leibler divergence. This perspective yields Han-Shearer type inequalities and submodularity of the entropy rate of Markov chains, as well as applications in the context of large deviations and mixing time comparison. As concrete algorithmic applications in Markov chain Monte Carlo (MCMC), we provide two illustrations based on lifted MCMC and swapping algorithm respectively to demonstrate projection samplers improve mixing over the original samplers. The projection sampler based on the swapping algorithm resamples the highest-temperature coordinate at stationarity at each step, and we prove that such practice accelerates the mixing time by multiplicative factors related to the number of temperatures and the dimension of the underlying state space when compared with the original swapping algorithm. Through simple numerical experiments on a bimodal target distribution, we show that the projection samplers mix effectively, in contrast to lifted MCMC and the swapping algorithm, which mix less well.
[64]
arXiv:2406.08286
(replaced)
[pdf, other]
Title:
Copy-composition for Probabilistic Graphical Models
Toby St Clere Smithe (Verses AI)
Comments:
In Proceedings ACT 2024, arXiv:2509.18357
Journal-ref:
EPTCS 429, 2025, pp. 146-173
Subjects:
Category Theory (math.CT); Statistics Theory (math.ST)
In probabilistic modelling, joint distributions are often of more interest than their marginals, but the standard composition of stochastic channels is defined by marginalization. Last year at ACT, the notion of 'copy-composition' was introduced in order to circumvent this problem and express the chain rule of the relative entropy fibrationally, but while that goal was achieved, copy-composition lacked a satisfactory origin story. Here, we supply such a story for two standard probabilistic tools: directed and undirected graphical models. We explain that (directed) Bayesian networks may be understood as ''stochastic terms'' of product type, in which context copy-composition amounts to a pull-push operation. Likewise, we show that (undirected) factor graphs compose by copy-composition. In each case, our construction yields a double fibration of decorated (co)spans. Along the way, we introduce a useful bifibration of measure kernels, to provide semantics for the notion of stochastic term, which allows us to generalize probabilistic modelling from product to dependent types.
[65]
arXiv:2409.02604
(replaced)
[pdf, html, other]
Title:
Context-Aware Reasoning On Parametric Knowledge for Inferring Causal Variables
Ivaxi Sheth, Sahar Abdelnabi, Mario Fritz
Comments:
EMNLP'25 Findings
Subjects:
Machine Learning (cs.LG); Methodology (stat.ME)
Scientific discovery catalyzes human intellectual advances, driven by the cycle of hypothesis generation, experimental design, evaluation, and assumption refinement. Central to this process is causal inference, uncovering the mechanisms behind observed phenomena. While randomized experiments provide strong inferences, they are often infeasible due to ethical or practical constraints. However, observational studies are prone to confounding or mediating biases. While crucial, identifying such backdoor paths is expensive and heavily depends on scientists' domain knowledge to generate hypotheses. We introduce a novel benchmark where the objective is to complete a partial causal graph. We design a benchmark with varying difficulty levels with over 4000 queries. We show the strong ability of LLMs to hypothesize the backdoor variables between a cause and its effect. Unlike simple knowledge memorization of fixed associations, our task requires the LLM to reason according to the context of the entire graph.
[66]
arXiv:2410.24206
(replaced)
[pdf, other]
Title:
Understanding Optimization in Deep Learning with Central Flows
Jeremy M. Cohen, Alex Damian, Ameet Talwalkar, J. Zico Kolter, Jason D. Lee
Comments:
First two authors contributed equally; author order determined by coin flip. This is the full version of a paper published at ICLR 2025. We encourage readers to explore the blog version of this paper, with animated optimization trajectories, at this https URL. Our code can be found at this https URL
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)
Traditional theories of optimization cannot describe the dynamics of optimization in deep learning, even in the simple setting of deterministic training. The challenge is that optimizers typically operate in a complex, oscillatory regime called the "edge of stability." In this paper, we develop theory that can describe the dynamics of optimization in this regime. Our key insight is that while the *exact* trajectory of an oscillatory optimizer may be challenging to analyze, the *time-averaged* (i.e. smoothed) trajectory is often much more tractable. To analyze an optimizer, we derive a differential equation called a "central flow" that characterizes this time-averaged trajectory. We empirically show that these central flows can predict long-term optimization trajectories for generic neural networks with a high degree of numerical accuracy. By interpreting these central flows, we are able to understand how gradient descent makes progress even as the loss sometimes goes up; how adaptive optimizers "adapt" to the local loss landscape; and how adaptive optimizers implicitly navigate towards regions where they can take larger steps. Our results suggest that central flows can be a valuable theoretical tool for reasoning about optimization in deep learning.
[67]
arXiv:2501.18792
(replaced)
[pdf, html, other]
Title:
Bayesian Optimization with Preference Exploration using a Monotonic Neural Network Ensemble
Hanyang Wang, Juergen Branke, Matthias Poloczek
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
Many real-world black-box optimization problems have multiple conflicting objectives. Rather than attempting to approximate the entire set of Pareto-optimal solutions, interactive preference learning allows to focus the search on the most relevant subset. However, few previous studies have exploited the fact that utility functions are usually monotonic. In this paper, we address the Bayesian Optimization with Preference Exploration (BOPE) problem and propose using a neural network ensemble as a utility surrogate model. This approach naturally integrates monotonicity and supports pairwise comparison data. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches and exhibits robustness to noise in utility evaluations. An ablation study highlights the critical role of monotonicity in enhancing performance.
[68]
arXiv:2502.09151
(replaced)
[pdf, html, other]
Title:
Regularization can make diffusion models more efficient
Mahsa Taheri, Johannes Lederer
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Diffusion models are one of the key architectures of generative AI. Their main drawback, however, is the computational costs. This study indicates that the concept of sparsity, well known especially in statistics, can provide a pathway to more efficient diffusion pipelines. Our mathematical guarantees prove that sparsity can reduce the input dimension's influence on the computational complexity to that of a much smaller intrinsic dimension of the data. Our empirical findings confirm that inducing sparsity can indeed lead to better samples at a lower cost.
[69]
arXiv:2503.15477
(replaced)
[pdf, html, other]
Title:
What Makes a Reward Model a Good Teacher? An Optimization Perspective
Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason D. Lee, Sanjeev Arora
Comments:
Accepted to NeurIPS 2025; Code available at this https URL
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)
The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. However, while this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient~optimization.
[70]
arXiv:2505.12462
(replaced)
[pdf, html, other]
Title:
Provably Sample-Efficient Robust Reinforcement Learning with Average Reward
Zachary Roch, Chi Zhang, George Atia, Yue Wang
Comments:
Preprint, work in progress
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Robust reinforcement learning (RL) under the average-reward criterion is essential for long-term decision-making, particularly when the environment may differ from its specification. However, a significant gap exists in understanding the finite-sample complexity of these methods, as most existing work provides only asymptotic guarantees. This limitation hinders their principled understanding and practical deployment, especially in data-limited scenarios. We close this gap by proposing \textbf{Robust Halpern Iteration (RHI)}, a new algorithm designed for robust Markov Decision Processes (MDPs) with transition uncertainty characterized by $\ell_p$-norm and contamination models. Our approach offers three key advantages over previous methods: (1). Weaker Structural Assumptions: RHI only requires the underlying robust MDP to be communicating, a less restrictive condition than the commonly assumed ergodicity or irreducibility; (2). No Prior Knowledge: Our algorithm operates without requiring any prior knowledge of the robust MDP; (3). State-of-the-Art Sample Complexity: To learn an $\epsilon$-optimal robust policy, RHI achieves a sample complexity of $\tilde{\mathcal O}\left(\frac{SA\mathcal H^{2}}{\epsilon^{2}}\right)$, where $S$ and $A$ denote the numbers of states and actions, and $\mathcal H$ is the robust optimal bias span. This result represents the tightest known bound. Our work hence provides essential theoretical understanding of sample efficiency of robust average reward RL.
[71]
arXiv:2505.15064
(replaced)
[pdf, html, other]
Title:
Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy
Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda
Subjects:
Machine Learning (cs.LG); Dynamical Systems (math.DS); Machine Learning (stat.ML)
Why and when is deep better than shallow? We answer this question in a framework that is agnostic to network implementation. We formulate a deep model as an abstract state-transition semigroup acting on a general metric space, and separate the implementation (e.g., ReLU nets, transformers, and chain-of-thought) from the abstract state transition. We prove a bias-variance decomposition in which the variance depends only on the abstract depth-$k$ network and not on the implementation (Theorem 1). We further split the bounds into output and hidden parts to tie the depth dependence of the variance to the metric entropy of the state-transition semigroup (Theorem 2). We then investigate implementation-free conditions under which the variance grow polynomially or logarithmically with depth (Section 4). Combining these with exponential or polynomial bias decay identifies four canonical bias-variance trade-off regimes (EL/EP/PL/PP) and produces explicit optimal depths $k^\ast$. Across regimes, $k^\ast>1$ typically holds, giving a rigorous form of depth supremacy. The lowest generalization error bound is achieved under the EL regime (exp-decay bias + log-growth variance), explaining why and when deep is better, especially for iterative or hierarchical concept classes such as neural ODEs, diffusion/score models, and chain-of-thought reasoning.
[72]
arXiv:2505.21073
(replaced)
[pdf, html, other]
Title:
Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity
Pierre Houedry, Nicolas Courty, Florestan Martin-Baillon, Laetitia Chapel, Titouan Vayer
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Trees and the associated shortest-path tree metrics provide a powerful framework for representing hierarchical and combinatorial structures in data. Given an arbitrary metric space, its deviation from a tree metric can be quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing algorithms that bridge an arbitrary metric to its closest tree metric is still a vivid subject of interest, as most common approaches are either heuristical and lack guarantees, or perform moderately well. In this work, we introduce a novel differentiable optimization framework, coined DeltaZero, that solves this problem. Our method leverages a smooth surrogate for Gromov's $\delta$-hyperbolicity which enables a gradient-based optimization, with a tractable complexity. The corresponding optimization procedure is derived from a problem with better worst case guarantees than existing bounds, and is justified statistically. Experiments on synthetic and real-world datasets demonstrate that our method consistently achieves state-of-the-art distortion.
[73]
arXiv:2506.01052
(replaced)
[pdf, html, other]
Title:
A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections or Strong Convexity
Wei-Cheng Lee, Francesco Orabona
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
We investigate the finite-time convergence properties of Temporal Difference (TD) learning with linear function approximation, a cornerstone algorithm in the field of reinforcement learning.
We are interested in the so-called ``robust'' setting, where the convergence guarantee does not depend on the minimal curvature of the potential function.
While prior work has established convergence guarantees in this setting, these results typically rely on the assumption that each iterate is projected onto a bounded set, a condition that is both artificial and does not match the current practice.
In this paper, we challenge the necessity of such an assumption and present a refined analysis of TD learning. For the first time, we show that the simple projection-free variant converges with a rate of $\widetilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence of Markovian noise. Our analysis reveals a novel self-bounding property of the TD updates and exploits it to guarantee bounded iterates.
[74]
arXiv:2506.01408
(replaced)
[pdf, html, other]
Title:
Modeling temporal hypergraphs
Jürgen Lerner, Marian-Gabriel Hâncean, Matjaz Perc
Subjects:
Physics and Society (physics.soc-ph); Applications (stat.AP)
Networks representing social, biological, technological or other systems are often characterized by higher-order interaction involving any number of nodes. Temporal hypergraphs are given by ordered sequences of hyperedges representing sets of nodes interacting at given points in time. In this paper we discuss how a recently proposed model family for time-stamped hyperedges - relational hyperevent models (RHEM) - can be employed to define tailored null distributions for temporal hypergraphs and to test and control for complex dependencies in hypergraph dynamics. RHEM can be specified with a given vector of temporal hyperedge statistics - functions that quantify the structural position of hyperedges in the history of previous hyperedges - and equate expected values of these statistics with their empirically observed values. This allows, for instance, to analyze the overrepresentation or underrepresentation of temporal hyperedge configurations in a model that reproduces the observed distributions of possibly complex sub-configurations, including but going beyond node degrees. Concrete examples include, but are not limited to, preferential attachment, repetition of subsets of any given size, triadic closure, homophily, and degree assortativity for subsets of any order.
[75]
arXiv:2506.08415
(replaced)
[pdf, html, other]
Title:
Improved Scaling Laws in Linear Regression via Data Reuse
Licong Lin, Jingfeng Wu, Peter L. Bartlett
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Neural scaling laws suggest that the test error of large language models trained online decreases polynomially as the model size and data size increase. However, such scaling can be unsustainable when running out of new data. In this work, we show that data reuse can improve existing scaling laws in linear regression. Specifically, we derive sharp test error bounds on $M$-dimensional linear models trained by multi-pass stochastic gradient descent (multi-pass SGD) on $N$ data with sketched features. Assuming that the data covariance has a power-law spectrum of degree $a$, and that the true parameter follows a prior with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$, where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting, one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse (i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are also provided to verify our theoretical findings.
[76]
arXiv:2506.10374
(replaced)
[pdf, html, other]
Title:
Optimal Non-Adaptive Group Testing with One-Sided Error Guarantees
Daniel McMorrow, Jonathan Scarlett
Subjects:
Information Theory (cs.IT); Statistics Theory (math.ST)
The group testing problem consists of determining a sparse subset of defective items from within a larger set of items via a series of tests, where each test outcome indicates whether at least one defective item is included in the test. We study the approximate recovery setting, where the recovery criterion of the defective set is relaxed to allow a small number of items to be misclassified. In particular, we consider one-sided approximate recovery criteria, where we allow either only false negative or only false positive misclassifications. Under false negatives only (i.e., finding a subset of defectives), we show that there exists an algorithm matching the optimal threshold of two-sided approximate recovery. Under false positives only (i.e., finding a superset of the defectives), we provide a converse bound showing that the better of two existing algorithms is optimal.
[77]
arXiv:2506.17326
(replaced)
[pdf, html, other]
Title:
CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction
Agnideep Aich, Md Monzur Murshed, Sameera Hewage, Amanda Mayeaux
Subjects:
Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people are affected by it. Early detection can significantly lower this risk. Despite significant advancements in machine learning for identifying diabetic cases, results can still be influenced by the imbalanced nature of the data. To address this challenge, our study considered copula-based data augmentation, which preserves the dependency structure when generating data for the minority class and integrates it with machine learning (ML) techniques. We selected the Pima Indian dataset and generated data using A2 copula, then applied five machine learning algorithms: logistic regression, random forest, gradient boosting, extreme gradient boosting, and Multilayer Perceptron. Overall, our findings show that Random Forest with A2 copula oversampling (theta = 10) achieved the best performance, with improvements of 5.3% in accuracy, 9.5% in precision, 5.7% in recall, 7.6% in F1-score, and 1.1% in AUC compared to the standard SMOTE method. Furthermore, we statistically validated our results using the McNemar's test. This research represents the first known use of A2 copulas for data augmentation and serves as an alternative to the SMOTE technique, highlighting the efficacy of copulas as a statistical method in machine learning applications.
[78]
arXiv:2506.17940
(replaced)
[pdf, html, other]
Title:
An entropy-optimal path to humble AI
Davide Bassetti, Lukáš Pospíšil, Michael Groom, Terence J. O'Kane, Illia Horenko
Comments:
39 pages, 5 figures
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Progress of AI has led to very successful, but by no means humble models and tools, especially regarding (i) the huge and further exploding costs and resources they demand, and (ii) the over-confidence of these tools with the answers they provide. Here we introduce a novel mathematical framework for a non-equilibrium entropy-optimizing reformulation of Boltzmann machines based on the exact law of total probability and the exact convex polytope representations. We show that it results in the highly-performant, but much cheaper, gradient-descent-free learning framework with mathematically-justified existence and uniqueness criteria, and cheaply-computable confidence/reliability measures for both the model inputs and the outputs. Comparisons to state-of-the-art AI tools in terms of performance, cost and the model descriptor lengths on a broad set of synthetic and real-world problems with varying complexity reveal that the proposed method results in more performant and slim models, with the descriptor lengths being very close to the intrinsic complexity scaling bounds for the underlying problems. Applying this framework to historical climate data results in models with systematically higher prediction skills for the onsets of important La Niña and El Niño climate phenomena, requiring just few years of climate data for training - a small fraction of what is necessary for contemporary climate prediction tools.
[79]
arXiv:2507.01761
(replaced)
[pdf, html, other]
Title:
Enhanced Generative Model Evaluation with Clipped Density and Coverage
Nicolas Salvy, Hugues Talbot, Bertrand Thirion
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Although generative models have made remarkable progress in recent years, their use in critical applications has been hindered by an inability to reliably evaluate the quality of their generated samples. Quality refers to at least two complementary concepts: fidelity and coverage. Current quality metrics often lack reliable, interpretable values due to an absence of calibration or insufficient robustness to outliers. To address these shortcomings, we introduce two novel metrics: Clipped Density and Clipped Coverage. By clipping individual sample contributions, as well as the radii of nearest neighbor balls for fidelity, our metrics prevent out-of-distribution samples from biasing the aggregated values. Through analytical and empirical calibration, these metrics demonstrate linear score degradation as the proportion of bad samples increases. Thus, they can be straightforwardly interpreted as equivalent proportions of good samples. Extensive experiments on synthetic and real-world datasets demonstrate that Clipped Density and Clipped Coverage outperform existing methods in terms of robustness, sensitivity, and interpretability when evaluating generative models.
[80]
arXiv:2507.05482
(replaced)
[pdf, html, other]
Title:
Training-Free Stein Diffusion Guidance: Posterior Correction for Sampling Beyond High-Density Regions
Van Khoa Nguyen, Lionel Blondé, Alexandros Kalousis
Comments:
Revised version with additional results
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Training free diffusion guidance provides a flexible way to leverage off-the-shelf classifiers without additional training. Yet, current approaches hinge on posterior approximations via Tweedie's formula, which often yield unreliable guidance, particularly in low-density regions. Stochastic optimal control (SOC), in contrast, provides principled posterior simulation but is prohibitively expensive for fast sampling. In this work, we reconcile the strengths of these paradigms by introducing Stein Diffusion Guidance (SDG), a novel training-free framework grounded in a surrogate SOC objective. We establish a theoretical bound on the value function, demonstrating the necessity of correcting approximate posteriors to faithfully reflect true diffusion dynamics. Leveraging Stein variational inference, SDG identifies the steepest descent direction that minimizes the Kullback-Leibler divergence between approximate and true posteriors. By incorporating a principled Stein correction mechanism and a novel running cost functional, SDG enables effective guidance in low-density regions. Experiments on molecular low-density sampling tasks suggest that SDG consistently surpasses standard training-free guidance methods, highlighting its potential for broader diffusion-based sampling beyond high-density regions.
[81]
arXiv:2508.11727
(replaced)
[pdf, html, other]
Title:
Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks
Songyao Jin, Biwei Huang
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Multivariate Hawkes process provides a powerful framework for modeling temporal dependencies and event-driven interactions in complex systems. While existing methods primarily focus on uncovering causal structures among observed subprocesses, real-world systems are often only partially observed, with latent subprocesses posing significant challenges. In this paper, we show that continuous-time event sequences can be represented by a discrete-time causal model as the time interval shrinks, and we leverage this insight to establish necessary and sufficient conditions for identifying latent subprocesses and the causal influences. Accordingly, we propose a two-phase iterative algorithm that alternates between inferring causal relationships among discovered subprocesses and uncovering new latent subprocesses, guided by path-based conditions that guarantee identifiability. Experiments on both synthetic and real-world datasets show that our method effectively recovers causal structures despite the presence of latent subprocesses.
[82]
arXiv:2508.19441
(replaced)
[pdf, html, other]
Title:
Data-Augmented Few-Shot Neural Emulator for Computer-Model System Identification
Sanket Jantre, Deepak Akhare, Zhiyuan Wang, Xiaoning Qian, Nathan M. Urban
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)
Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long-horizon rollout of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local "stencil" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories. Finally, with only 10 solver steps' worth of augmented stencil data, our approach outperforms traditional ML emulators trained on thousands of trajectories in long-horizon rollout accuracy and stability.
[83]
arXiv:2509.06154
(replaced)
[pdf, html, other]
Title:
Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs. Neural Operators
Dibyajyoti Nayak, Somdatta Goswami
Comments:
23 pages including references. Supplementary Information provided
Subjects:
Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)
Developing accurate, data-efficient surrogate models is central to advancing AI for Science. Neural operators (NOs), which approximate mappings between infinite-dimensional function spaces using conventional neural architectures, have gained popularity as surrogates for systems driven by partial differential equations (PDEs). However, their reliance on large datasets and limited ability to generalize in low-data regimes hinder their practical utility. We argue that these limitations arise from their global processing of data, which fails to exploit the local, discretized structure of physical systems. To address this, we propose Graph Neural Simulators (GNS) as a principled surrogate modeling paradigm for time-dependent PDEs. GNS leverages message-passing combined with numerical time-stepping schemes to learn PDE dynamics by modeling the instantaneous time derivatives. This design mimics traditional numerical solvers, enabling stable long-horizon rollouts and strong inductive biases that enhance generalization. We rigorously evaluate GNS on four canonical PDE systems: (1) 2D scalar Burgers', (2) 2D coupled Burgers', (3) 2D Allen-Cahn, and (4) 2D nonlinear shallow-water equations, comparing against state-of-the-art NOs including Deep Operator Network (DeepONet) and Fourier Neural Operator (FNO). Results demonstrate that GNS is markedly more data-efficient, achieving less than 1% relative L2 error using only 3% of available trajectories, and exhibits dramatically reduced error accumulation over time (82.5% lower autoregressive error than FNO, 99.9% lower than DeepONet). To choose the training data, we introduce a PCA combined with KMeans trajectory selection strategy. These findings provide compelling evidence that GNS, with its graph-based locality and solver-inspired design, is the most suitable and scalable surrogate modeling framework for AI-driven scientific discovery.
[84]
arXiv:2509.09362
(replaced)
[pdf, html, other]
Title:
Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation
Hanfei Zhou, Lei Shi
Subjects:
Numerical Analysis (math.NA); Machine Learning (cs.LG); Machine Learning (stat.ML)
A key challenge in scientific machine learning is solving partial differential equations (PDEs) on complex domains, where the curved geometry complicates the approximation of functions and their derivatives required by differential operators. This paper establishes the first simultaneous approximation theory for deep neural networks on manifolds. We prove that a constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property that plays a crucial role in controlling generalization error--can approximate any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for $k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero parameters, a rate that overcomes the curse of dimensionality by depending only on the intrinsic dimension $d$. These results readily extend to functions in Hölder-Zygmund spaces. We complement this result with a matching lower bound, proving our construction is nearly optimal by showing the required number of parameters matches up to a logarithmic factor. Our proof of the lower bound introduces novel estimates for the Vapnik-Chervonenkis dimension and pseudo-dimension of the network's high-order derivative classes. These complexity bounds provide a theoretical cornerstone for learning PDEs on manifolds involving derivatives. Our analysis reveals that the network architecture leverages a sparse structure to efficiently exploit the manifold's low-dimensional geometry. Finally, we corroborate our theoretical findings with numerical experiments.
[85]
arXiv:2509.12527
(replaced)
[pdf, html, other]
Title:
Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design
Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Large language models frequently generate confident but incorrect outputs, requiring formal uncertainty quantification with abstention guarantees. We develop information-lift certificates that compare model probabilities to a skeleton baseline, accumulating evidence into sub-gamma PAC-Bayes bounds valid under heavy-tailed distributions. Across eight datasets, our method achieves 77.2\% coverage at 2\% risk, outperforming recent 2023-2024 baselines by 8.6-15.1 percentage points, while blocking 96\% of critical errors in high-stakes scenarios vs 18-31\% for entropy methods. Limitations include skeleton dependence and frequency-only (not severity-aware) risk control, though performance degrades gracefully under corruption.
[86]
arXiv:2509.13548
(replaced)
[pdf, html, other]
Title:
Mixture-of-Experts Framework for Field-of-View Enhanced Signal-Dependent Binauralization of Moving Talkers
Manan Mittal, Thomas Deppisch, Joseph Forrer, Chris Le Sueur, Zamir Ben-Hur, David Lou Alon, Daniel D.E. Wong
Comments:
5 pages, 3 figures
Subjects:
Sound (cs.SD); Machine Learning (stat.ML)
We propose a novel mixture of experts framework for field-of-view enhancement in binaural signal matching. Our approach enables dynamic spatial audio rendering that adapts to continuous talker motion, allowing users to emphasize or suppress sounds from selected directions while preserving natural binaural cues. Unlike traditional methods that rely on explicit direction-of-arrival estimation or operate in the Ambisonics domain, our signal-dependent framework combines multiple binaural filters in an online manner using implicit localization. This allows for real-time tracking and enhancement of moving sound sources, supporting applications such as speech focus, noise reduction, and world-locked audio in augmented and virtual reality. The method is agnostic to array geometry offering a flexible solution for spatial audio capture and personalized playback in next-generation consumer audio devices.
Total of 86 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack