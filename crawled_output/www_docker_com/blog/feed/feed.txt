Docker
https://www.docker.com
Fri, 19 Sep 2025 14:26:15 +0000
en-US
hourly
1
https://wordpress.org/?v=6.7.1
https://www.docker.com/app/uploads/2024/02/cropped-docker-logo-favicon-32x32.png
Docker
https://www.docker.com
32
32
Silent Component Updates & Redesigned Update Experience
https://www.docker.com/blog/docker-desktop-silent-component-updates/
Kat Tomrushka
Fri, 19 Sep 2025 13:07:13 +0000
Engineering
Products
Docker
Docker Desktop
Docker Desktop release
https://www.docker.com/?p=77598
Following on from our previous initiative to improve how Docker Desktop delivers updates, we are excited to announce another major improvement to how Docker Desktop keeps your development tools up to date. Starting with Docker Desktop 4.46, we're introducing automatic component updates and a completely redesigned update experience that puts your productivity first. Why We're...
<p></p>
<p>Following on from our <a href="https://www.docker.com/blog/docker-desktop-updates-every-two-weeks/">previous initiative</a> to improve how Docker Desktop delivers updates, we are excited to announce another major improvement to how Docker Desktop keeps your development tools up to date. Starting with Docker Desktop 4.46, we&#8217;re introducing automatic component updates and a completely redesigned update experience that puts your productivity first.<br></p>
<h2 class="wp-block-heading"><strong>Why We&#8217;re Making This Change</strong></h2>
<p></p>
<p>Your development workflow shouldn&#8217;t be interrupted by update notifications and restart requirements. With our new approach, you get:</p>
<p></p>
<ul class="wp-block-list">
<li>Zero workflow interruption – components update automatically in the background when a Docker Desktop restart is not required</li>
<li>Always-current tools – Scout, Compose, Ask Gordon, and Model Runner stay up-to-date without manual intervention</li>
<li>Better security posture – automatic updates mean you&#8217;re always running the latest, most secure versions</li>
<li>Enterprise control &#8211; admin console cloud setting to control the update behaviour.&nbsp;</li>
</ul>
<p></p>
<h2 class="wp-block-heading"><strong>What&#8217;s New in Docker Desktop 4.46</strong></h2>
<h3 class="wp-block-heading"><strong>Silent Component Updates</strong></h3>
<p>Independent tools now update automatically in the background without any user interaction required and without impact on running containers:</p>
<p></p>
<ul class="wp-block-list">
<li>Docker Scout – Latest vulnerability scanning capabilities</li>
<li>Docker Compose – New features and bug fixes</li>
<li>Ask Gordon – Enhanced AI assistance improvements</li>
<li>Model Runner – Updated model support and performance optimizations</li>
</ul>
<p></p>
<p>Note that the component list above may change in the future as we add or remove features.&nbsp;</p>
<p></p>
<h3 class="wp-block-heading"><strong>Redesigned Update Experience</strong></h3>
<p>We have completely re-imagined how Docker Desktop communicates updates to you:</p>
<p></p>
<ul class="wp-block-list">
<li>Streamlined update flow with clearer messaging</li>
<li>In-app release highlights showcasing key improvements you actually care about</li>
<li>Reduced notification fatigue through more thoughtful update communications</li>
<li>[Coming soon] Smart timing – GUI-only updates happen automatically when you close and reopen Docker&nbsp;</li>
</ul>
<p></p>
<h2 class="wp-block-heading"><strong>Full Control When You Need It</strong></h2>
<p></p>
<h3 class="wp-block-heading"><strong>Individual User Control</strong></h3>
<p></p>
<p>Want to manage updates yourself? You have complete control:</p>
<ol class="wp-block-list">
<li>Go to Docker Desktop Settings</li>
<li>Navigate to Software Updates</li>
<li>Toggle &#8220;Automatically update components&#8221; on or off</li>
</ol>
<div class="wp-block-ponyo-image">
<img fetchpriority="high" decoding="async" width="1222" height="961" src="https://www.docker.com/app/uploads/2025/09/image3.png" class="attachment-full size-full" alt="image3" srcset="https://www.docker.com/app/uploads/2025/09/image3.png 1222w, https://www.docker.com/app/uploads/2025/09/image3-381x300.png 381w, https://www.docker.com/app/uploads/2025/09/image3-1110x873.png 1110w" sizes="(max-width: 1222px) 100vw, 1222px" title="- image3">
</div>
<p><em>Software updates: new setting to control opt in or out of automatic component updates.</em><br></p>
<h3 class="wp-block-heading"><strong>Enterprise Management</strong></h3>
<p></p>
<p>For Docker Business subscribers, administrators maintain full governance through the admin console:</p>
<p></p>
<ol class="wp-block-list">
<li>Access <a href="https://app.docker.com" rel="nofollow noopener" target="_blank">Admin Console</a> &gt; Desktop Settings Management</li>
<li>Edit your global policy</li>
<li>Configure &#8220;Automatically update components&#8221; to enable, disable, lock, or set defaults for your entire organization</li>
</ol>
<p></p>
<p>This ensures enterprises can maintain their preferred update policies while giving individual developers the productivity benefits of seamless updates.</p>
<p></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="992" height="508" src="https://www.docker.com/app/uploads/2025/09/image1.png" class="attachment-full size-full" alt="image1" srcset="https://www.docker.com/app/uploads/2025/09/image1.png 992w, https://www.docker.com/app/uploads/2025/09/image1-586x300.png 586w" sizes="(max-width: 992px) 100vw, 992px" title="- image1">
</div>
<p><em>Admin console: desktop settings management policy contains a new silent update setting for enterprise control.</em><br></p>
<h2 class="wp-block-heading"><strong>We Want Your Feedback</strong></h2>
<p>The redesigned update workflow is rolling out to the majority of our users as we gather feedback and refine the experience. We&#8217;re committed to getting this right, so please share your thoughts:</p>
<p></p>
<ul class="wp-block-list">
<li>In-app feedback popup – we do read those!</li>
<li>Docker <a href="https://communityinviter.com/apps/dockercommunity/docker-community" rel="nofollow noopener" target="_blank">Slack community</a> – join the conversation with other developers</li>
<li><a href="https://github.com/docker/for-mac/issues" rel="nofollow noopener" target="_blank">GitHub</a> issues – report specific bugs or feature requests</li>
</ul>
<p></p>
<h2 class="wp-block-heading"><strong>Getting Started</strong></h2>
<p></p>
<p>Docker Desktop 4.46 with silent component updates is available now. The new update experience will gradually roll out to all users over the coming weeks.</p>
<p></p>
<p>Already using Docker Desktop? Update in-app to get the latest features.&nbsp;</p>
<p></p>
<p>New to Docker? <a href="https://www.docker.com/products/docker-desktop/">Download Docker Desktop here</a> to experience the most seamless development environment we&#8217;ve ever built.</p>
<p></p>
Beyond Containers: llama.cpp Now Pulls GGUF Models Directly from Docker Hub
https://www.docker.com/blog/llama-cpp-pulls-gguf-models-from-docker-hub/
Eric Curtin
Fri, 19 Sep 2025 12:01:31 +0000
Engineering
AI/ML
Docker Hub
Docker Model Runner
https://www.docker.com/?p=77801
The world of local AI is moving at an incredible pace, and at the heart of this revolution is llama.cpp—the powerhouse C++ inference engine that brings Large Language Models (LLMs) to everyday hardware (and it’s also the inference engine that powers Docker Model Runner). Developers love llama.cpp for its performance and simplicity. And we at...
<p>The world of local AI is moving at an incredible pace, and at the heart of this revolution is llama.cpp—the powerhouse C++ inference engine that brings Large Language Models (LLMs) to everyday hardware (and it’s also the inference engine that powers <a href="https://docs.docker.com/ai/model-runner/" rel="nofollow noopener" target="_blank">Docker Model Runner</a>). Developers love llama.cpp for its performance and simplicity. And we at Docker are obsessed with making developer workflows simpler.</p>
<p>That&#8217;s why we&#8217;re thrilled to announce a game-changing new feature in llama.cpp: native support for pulling and running GGUF models directly from Docker Hub.</p>
<p>This isn&#8217;t about running llama.cpp <em>in</em> a Docker container. This is about using Docker Hub as a powerful, versioned, and centralized repository for your AI models, just like you do for your container images.</p>
<h2 class="wp-block-heading"><strong>Why Docker Hub for AI Models?</strong></h2>
<p>Managing AI models can be cumbersome. You&#8217;re often dealing with direct download links, manual version tracking, and scattered files. By integrating with Docker Hub, llama.cpp leverages a mature and robust ecosystem to solve these problems.</p>
<ul class="wp-block-list">
<li><strong>Rock-Solid Versioning</strong>: The familiar repository:tag syntax you use for images now applies to models. Easily switch between gemma3 and smollm2:135M-Q4_0 with complete confidence.</li>
<li><strong>Centralized &amp; Discoverable</strong>: Docker Hub can become the canonical source for your team&#8217;s models. No more hunting for the &#8220;latest&#8221; version on a shared drive or in a chat history.</li>
<li><strong>Simplified Workflow</strong>: Forget curl, wget or manually downloading from web UIs. A single command-line flag now handles discovery, download, and caching.</li>
<li><strong>Reproducibility</strong>: By referencing a model with its immutable digest or tag, you ensure that your development, testing, and production environments are all using the exact same artifact, leading to more consistent and reproducible results.</li>
</ul>
<h2 class="wp-block-heading"><strong>How It Works Under the Hood&nbsp;</strong></h2>
<p>This new feature cleverly uses the Open Container Initiative (OCI) specification, which is the foundation of Docker images. The GGUF model file is treated as a layer within an OCI manifest, identified by a special media type like application/vnd.docker.ai.gguf.v3. For more details on why the OCI standard matters for models, check out our <a href="https://www.docker.com/blog/oci-artifacts-for-ai-model-packaging/">blog</a>.</p>
<p>When you use the new &#8211;docker-repo flag, llama.cpp performs the following steps:</p>
<ol class="wp-block-list">
<li><strong>Authentication</strong>: It first requests an authentication token from the Docker registry to authorize the download.</li>
<li><strong>Manifest Fetch</strong>: It then fetches the manifest for the specified model and tag (e.g., ai/gemma3:latest).</li>
<li><strong>Layer Discovery</strong>: It parses the manifest to find the specific layer that contains the GGUF model file by looking for the correct media type.</li>
<li><strong>Blob Download</strong>: Using the layer&#8217;s unique digest (a sha256 hash), it downloads the model file directly from the registry&#8217;s blob storage.</li>
<li><strong>Caching</strong>: The model is saved to a local cache, so subsequent runs are instantaneous.</li>
</ol>
<p>This entire process is seamless and happens automatically in the background.<br></p>
<h2 class="wp-block-heading"><strong>Get Started in Seconds</strong></h2>
<p>Ready to try it? If you have a <a href="https://github.com/ggml-org/llama.cpp/pull/15790" rel="nofollow noopener" target="_blank">recent build</a> of llama.cpp, you can serve a model from Docker Hub with one simple command. The new flag is &#8211;docker-repo (or -dr).</p>
<p>Let&#8217;s run <a href="https://hub.docker.com/r/ai/gemma3" rel="nofollow noopener" target="_blank">gemma3</a>, a model available from Docker Hub.</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
# Now, serve a model from Docker Hub!
llama-server -dr gemma3
</pre></div>
<p>The first time you execute this, you&#8217;ll see llama.cpp log the download progress. After that, it will use the cached version. It&#8217;s that easy! The default organization is ai/, so gemma3 is resolved to ai/gemma3. The default tag is :latest, but a tag can be specified like :1B-Q4_K_M.</p>
<p>For a complete Docker-integrated experience with OCI pushing and pulling support try out Docker Model Runner. The docker model runner equivalent for chatting is:</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
# Pull, serve and chat to a model from Docker Hub!
docker model run ai/gemma3
</pre></div>
<h2 class="wp-block-heading"><strong>The Future of AI Model Distribution</strong></h2>
<p>This integration represents a powerful shift in how we think about distributing and managing AI artifacts. By using OCI-compliant registries like Docker Hub, the AI community can build more robust, reproducible, and scalable MLOps pipelines.</p>
<p>This is just the beginning. We envision a future where models, datasets, and the code that runs them are all managed through the same streamlined, developer-friendly workflow that has made Docker an essential tool for millions.</p>
<p>Check out the latest llama.cpp to try it out, and explore the growing collection of models on <a href="https://hub.docker.com/u/ai" rel="nofollow noopener" target="_blank">Docker Hub</a> today!</p>
<h3 class="wp-block-heading">Learn more</h3>
<ul class="wp-block-list">
<li>Read our quickstart guide to<a href="https://www.docker.com/blog/run-llms-locally/"> Docker Model Runner</a>.</li>
<li>Visit our<a href="https://github.com/docker/model-runner" rel="nofollow noopener" target="_blank"> Model Runner GitHub repo</a>! Docker Model Runner is open-source, and we welcome collaboration and contributions from the community!</li>
<li>Discover curated models on <a href="https://hub.docker.com/u/ai" rel="nofollow noopener" target="_blank">Docker Hub</a></li>
</ul>
Build and Distribute AI Agents and Workflows with cagent
https://www.docker.com/blog/cagent-build-and-distribute-ai-agents-and-workflows/
Oleg Selajev
Thu, 18 Sep 2025 16:00:00 +0000
Products
AI/ML
cagent
developers
https://www.docker.com/?p=77658
cagent is a new open-source project from Docker that makes it simple to build, run, and share AI agents, without writing a single line of code. Instead of writing code and wrangling Python versions and dependencies when creating AI agents, you define your agent's behavior, tools, and persona in a single YAML file, making it...
<p></p>
<p><a href="https://github.com/docker/cagent" rel="nofollow noopener" target="_blank">cagent</a> is a new open-source project from Docker that makes it simple to build, run, and share AI agents, without writing a single line of code. Instead of writing code and wrangling Python versions and dependencies when creating AI agents, you define your agent&#8217;s behavior, tools, and persona in a single YAML file, making it incredibly straightforward to create and share personalized AI assistants.<br></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1600" height="1004" src="https://www.docker.com/app/uploads/2025/09/Cagent-1.png" class="attachment-full size-full" alt="Cagent 1" srcset="https://www.docker.com/app/uploads/2025/09/Cagent-1.png 1600w, https://www.docker.com/app/uploads/2025/09/Cagent-1-478x300.png 478w, https://www.docker.com/app/uploads/2025/09/Cagent-1-1110x697.png 1110w, https://www.docker.com/app/uploads/2025/09/Cagent-1-1536x964.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px" title="- Cagent 1">
</div>
<p class="has-sm-font-size">Figure 1: cagent is a powerful, easy to use, customizable multi-agent runtime that orchestrates AI agents with specialized capabilities and tools, and the interactions between agents.</p>
<p></p>
<p>cagent can use OCI registries to share and pull agents created by the community, so not only can you elegantly solve the agent creation problem, but also the agent distribution problem.&nbsp;</p>
<p></p>
<p>Let&#8217;s dive into what makes cagent special and explore some real-world use cases.</p>
<p></p>
<h2 class="wp-block-heading">What is cagent?</h2>
<p>At its core, <a href="https://docs.docker.com/ai/cagent/" rel="nofollow noopener" target="_blank">cagent</a> is a command-line utility that runs AI agents defined in cagent.yaml files. The philosophy is simple: <strong>declare what you want your agent to do</strong>, and cagent handles the rest.&nbsp;</p>
<p></p>
<p>There are a few features that you&#8217;ll probably like for authoring your agents.&nbsp;</p>
<ul class="wp-block-list">
<li>Declarative and Simple: Define models, instructions, and agent behavior in one YAML file. This &#8220;single artifact&#8221; approach makes agents portable, easy to version, and easy to share.</li>
<li>Flexible Model Support: You’re not tied to a specific provider. You can run remote models or even local ones using Docker Model Runner, ideal for privacy reasons.&nbsp;</li>
<li>Powerful Tool Integration: cagent includes built-in tools for common tasks (like shell commands or filesystem access) and supports external tools via MCP, enabling agents to connect to virtually any API.&nbsp;</li>
<li>Multi-Agent Systems: You&#8217;re also not limited to a single agent. Cagent allows you to define a team of agents that can collaborate and delegate tasks to one another, with each agent having its own specialized skills and tools.&nbsp;<br></li>
</ul>
<h2 class="wp-block-heading">Practical use cases for agent</h2>
<p>I&#8217;ve lived with and used cagent for a few weeks now, and in this article, I want to share two of my practically useful agents that I actually use.&nbsp;</p>
<p></p>
<h3 class="wp-block-heading"><strong>A GitHub Task Tracker</strong></h3>
<p>Let’s start with a practical, developer-centric example. While tracking GitHub issues with AI might not be revolutionary, it’s surprisingly useful and demonstrates cagent’s capabilities in a real-world workflow.&nbsp;</p>
<p></p>
<p>There&#8217;s no shortage of task tracking solutions to integrate with, but one of the most useful for developers is GitHub. We&#8217;ll use a repository in GitHub and issues on it as our to-do list. Does it have the best UX? It doesn&#8217;t actually matter; we&#8217;ll consume and create issues with AI, so the actual underlying UX is irrelevant.&nbsp;</p>
<p></p>
<p>I have a GitHub repo: <a href="http://github.com/shelajev/todo" rel="nofollow noopener" target="_blank">github.com/shelajev/todo</a>, which has issues enabled, and we&#8217;d like an agent that can, among other things, create issues, list issues, and close issues.&nbsp;</p>
<p></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1600" height="842" src="https://www.docker.com/app/uploads/2025/09/Cagent-2.png" class="attachment-full size-full" alt="cagent session creating a to-do agent; prompt engineering leads to a YAML config for a GitHub Issue Manager." srcset="https://www.docker.com/app/uploads/2025/09/Cagent-2.png 1600w, https://www.docker.com/app/uploads/2025/09/Cagent-2-570x300.png 570w, https://www.docker.com/app/uploads/2025/09/Cagent-2-1110x584.png 1110w, https://www.docker.com/app/uploads/2025/09/Cagent-2-285x150.png 285w, https://www.docker.com/app/uploads/2025/09/Cagent-2-1536x808.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px" title="- Cagent 2">
</div>
<p class="has-sm-font-size">Figure 2</p>
<p></p>
<p>Here’s the YAML for a GitHub-based to-do list agent. The instructions for the agent were generated with the agent new command, and then I refined the instructions it generated by manually asking Gemini to make them shorter.&nbsp;</p>
<p><br>YAML</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
version: &quot;2&quot;
models:
gpt:
provider: openai
model: gpt-5
max_tokens: 64000
agents:
root:
model: gpt
description: &quot;GitHub Issue Manager - An agent that connects to GitHub to use a repo as a todo-list&quot;
instruction: |
You are a to-do list agent, and your purpose is to help users manage their tasks in their &quot;todo&quot; GitHub repository.
# Primary Responsibilities
- Connect to the user&#039;s &quot;todo&quot; GitHub repository and fetch their to-do items, which are GitHub issues.
- Identify and present the to-do items for the current day.
- Provide clear summaries of each to-do item, including its priority and any labels.
- Help the user organize and prioritize their tasks.
- Assist with managing to-do items, for example, by adding comments or marking them as complete.
# Key Behaviors
- Always start by stating the current date to provide context for the day&#039;s tasks.
- Focus on open to-do items.
- Use labels such as &quot;urgent,&quot; &quot;high priority,&quot; etc., to highlight important tasks.
- Summarize to-do items with their title, number, and any relevant labels.
- Proactively suggest which tasks to tackle first based on their labels and context.
- Offer to help with actions like adding notes to or closing tasks.
# User Interaction Flow
When the user asks about their to-do list:
1. List the open items from the &quot;todo&quot; repository.
2. Highlight any urgent or high-priority tasks.
3. Offer to provide more details on a specific task or to help manage the list.
add_date: true
toolsets:
- type: mcp
command: docker
args: &#x5B;mcp, gateway, run]
tools:
&#x5B;
&quot;get_me&quot;,
&quot;add_issue_comment&quot;,
&quot;create_issue&quot;,
&quot;get_issue&quot;,
&quot;list_issues&quot;,
&quot;search_issues&quot;,
&quot;update_issue&quot;,
]
</pre></div>
<p>It’s a good example of a well-crafted prompt that defines the agent’s persona, responsibilities, and behavior, ensuring it acts predictably and helpfully. The best part is editing and running it is fast and frictionless, just save the YAML and run:&nbsp;<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
cagent run github-todo.yaml
</pre></div>
<p>This development loop works without any IDE setup. I&#8217;ve done several iterations in Vim, all from the same terminal window where I was running the agent.&nbsp;</p>
<p>This agent also uses a streamlined tools configuration. A lot of examples show adding MCP servers from the Docker MCP toolkit like this:&nbsp;</p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
toolsets:
- type: mcp
ref: docker:github-official
</pre></div>
<p></p>
<p>This would run the GitHub MCP server from the MCP catalog, but as a separate &#8220;toolkit&#8221; from your Docker Desktop&#8217;s MCP toolkit setup.</p>
<p></p>
<p>Using the manual command to connect to the MCP toolkit makes it easy to use OAuth login support in Docker Desktop.&nbsp;</p>
<p></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1600" height="1050" src="https://www.docker.com/app/uploads/2025/09/Cagent-3.png" class="attachment-full size-full" alt="Docker Desktop MCP Toolkit — GitHub Official server configuration with OAuth selected and authentication enabled." srcset="https://www.docker.com/app/uploads/2025/09/Cagent-3.png 1600w, https://www.docker.com/app/uploads/2025/09/Cagent-3-457x300.png 457w, https://www.docker.com/app/uploads/2025/09/Cagent-3-1110x728.png 1110w, https://www.docker.com/app/uploads/2025/09/Cagent-3-1536x1008.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px" title="- Cagent 3">
</div>
<p class="has-sm-font-size">Figure 3</p>
<p></p>
<p>Also, the official GitHub MCP server is awfully verbose. Powerful, but verbose. So, for the issue-related agents, it makes a lot of sense to limit the list of tools exposed to the agent:&nbsp;<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
tools:
&#x5B;
&quot;get_me&quot;,
&quot;add_issue_comment&quot;,
&quot;create_issue&quot;,
&quot;get_issue&quot;,
&quot;list_issues&quot;,
&quot;search_issues&quot;,
&quot;update_issue&quot;,
]
</pre></div>
<p></p>
<p>That list I made with running:&nbsp;<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
docker mcp tools list | grep &quot;issue&quot;
</pre></div>
<p>And asking AI to format it as an array.&nbsp;</p>
<p></p>
<p>This todo-agent is available on Docker Hub, so it&#8217;s a simple agent pull command away:&nbsp;</p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
cagent run docker.io/olegselajev241/github-todo:latest
</pre></div>
<p>Just enable the GitHub MCP server in the MCP toolkit first, and well, make sure the repo exists.</p>
<h3 class="wp-block-heading"><strong>The Advocu Captains Agent</strong></h3>
<p>At Docker, we use <strong>Advocu</strong> to track our Docker Captains, ambassadors who create content, speak at conferences, and engage with the community. We use Advocu to track their information details and contributions, such as blog posts, videos, and conference talks about Docker&#8217;s technologies.</p>
<p></p>
<p>Manually searching through Advocu is time-consuming. For a long time, I wondered: what if we could build an AI assistant to do it for us?&nbsp;</p>
<p></p>
<p>My first attempt was to build a custom MCP server for our Advocu instance: <a href="https://github.com/shelajev/mcp-advocu" rel="nofollow noopener" target="_blank">https://github.com/shelajev/mcp-advocu</a></p>
<p></p>
<p>It&#8217;s largely &#8220;vibe-coded&#8221;, but in a nutshell, running<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
docker run -i -rm -e ADVOCU_CLIENT_SECRET=your-secret-here olegselajev241/mcp-advocu:stdio
</pre></div>
<p>will run the MCP server with tools that expose information about Docker Captains, allowing MCP clients to search through their submitted activities.&nbsp;<br></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1600" height="848" src="https://www.docker.com/app/uploads/2025/09/Cagent-4.png" class="attachment-full size-full" alt="Advocu agent in cagent querying Docker Captains by country; tool call confirmation dialog visible in terminal." srcset="https://www.docker.com/app/uploads/2025/09/Cagent-4.png 1600w, https://www.docker.com/app/uploads/2025/09/Cagent-4-566x300.png 566w, https://www.docker.com/app/uploads/2025/09/Cagent-4-1110x588.png 1110w, https://www.docker.com/app/uploads/2025/09/Cagent-4-285x150.png 285w, https://www.docker.com/app/uploads/2025/09/Cagent-4-1536x814.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px" title="- Cagent 4">
</div>
<p class="has-sm-font-size">Figure 4</p>
<p></p>
<p>However, sharing the actual agent, and especially the configuration required to run it, was a bit awkward.&nbsp;</p>
<p></p>
<p>cagent solved this for me in a much neater way. Here is the complete cagent.yaml for my Advocu agent:</p>
<p></p>
<p>YAML</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
#!/usr/bin/env cagent run
version: &quot;2&quot;
agents:
root:
model: anthropic/claude-sonnet-4-0
description: Agent to help with finding information on Docker Captains and their recent contributions to Docker
toolsets:
- type: mcp
command: docker
args:
- run
- -i
- --rm
- --env-file
- ./.env
- olegselajev241/mcp-advocu:stdio
instruction: You have access to Advocu - a platform where Docker Captains log their contributions. You can use tools to query and process that information about captains themselves, and their activities like articles, videos, and conference sessions. You help the user to find relevant information and to connect to the captains by topic expertise, countries, and so on. And to have a hand on the pulse of their contributions, so you can summarize them or answer questions about activities and their content
</pre></div>
<p>With this file, we have a powerful, personalized assistant that can query Captain info, summarize their contributions, and find experts by topic. It’s a perfect example of how cagent can automate a specific internal workflow.</p>
<p></p>
<p>Users simply need to create a .env file with the appropriate secret. Even for less technical team members, I can give a shell one-liner to get them set up quickly.&nbsp;</p>
<p></p>
<p>Now, everyone at Docker can ask questions about Docker captains without pinging the person running the program (hi, Eva!) or digging through giant spreadsheets.&nbsp;</p>
<p></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1600" height="841" src="https://www.docker.com/app/uploads/2025/09/cagent-5.png" class="attachment-full size-full" alt="cagent terminal output summarizing recent MCP activities and sentiment — example of report-style results." srcset="https://www.docker.com/app/uploads/2025/09/cagent-5.png 1600w, https://www.docker.com/app/uploads/2025/09/cagent-5-571x300.png 571w, https://www.docker.com/app/uploads/2025/09/cagent-5-1110x583.png 1110w, https://www.docker.com/app/uploads/2025/09/cagent-5-285x150.png 285w, https://www.docker.com/app/uploads/2025/09/cagent-5-1536x807.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px" title="- cagent 5">
</div>
<p class="has-sm-font-size">Figure 5</p>
<p></p>
<p>I&#8217;m also excited about the upcoming <a href="https://github.com/docker/cagent/blob/bffc9979029831467af778d5bd2ddfb3cd155326/pkg/environment/1password.go#L17" rel="nofollow noopener" target="_blank">cagent 1Password integration</a>, which will simplify the setup even more.&nbsp;&nbsp;</p>
<p></p>
<p>All in all, agents are really just a combination of:</p>
<ul class="wp-block-list">
<li>A system prompt</li>
<li>An integration with a model (ideally, the most efficient one that gets the job done)</li>
<li>And the right tools via MCP<br></li>
</ul>
<p>With cagent, it&#8217;s incredibly easy to manage all three in a clean, Docker &#8211; native way.&nbsp;</p>
<p></p>
<h3 class="wp-block-heading"><strong>Get Started Today!</strong></h3>
<p>cagent empowers you to build your own fleet of AI assistants, tailored to your exact needs. </p>
<p></p>
<p>It&#8217;s a tool designed for developers who want to leverage the power of AI without getting bogged down in complexity.<br></p>
<p>You can get started right now by heading over to the<a href="https://github.com/docker/cagent" rel="nofollow noopener" target="_blank"> cagent GitHub repository</a>. Download the latest release and start building your first agent in minutes.&nbsp;</p>
<p></p>
<p>Give the repository a star, try it out, and let us know what amazing agents you build!</p>
<p></p>
Docker and CNCF: Partnering to Power the Future of Open Source
https://www.docker.com/blog/docker-cncf-partnership/
Aurélien Suarez
Thu, 18 Sep 2025 16:00:00 +0000
Community
Company
CNCF
Docker
open source
https://www.docker.com/?p=73008
At Docker, open source is not just something we support; it's a core part of our culture. It’s part of our DNA. From foundational projects like Docker Compose (35.5k stars, 5.4k forks) and Moby (69.8k stars, 18.8k forks) to our continued code contributions, we remain committed to strengthening the open-source ecosystem. Today, we are announcing...
<p>At Docker, open source is not just something we support; it&#8217;s a core part of our culture. It’s part of our DNA. From foundational projects like Docker Compose (35.5k stars, 5.4k forks) and Moby (69.8k stars, 18.8k forks) to our continued code contributions, we remain committed to strengthening the open-source ecosystem.</p>
<p>Today, we are announcing a new milestone in that journey: an official partnership between Docker and the Cloud Native Computing Foundation (CNCF). This partnership brings more than just resources for open-source projects. It also reflects the CNCF’s recognition of Docker as the leading distribution platform for containerized software and as a trusted partner in modern software supply chain security.</p>
<div class="organism wp-block-ponyo-galen">
<div class="container left" style="">
<div class="galen-content">
<h3>&#8220;Docker&#8217;s mission has always been to empower developers, and we know that trust is earned through consistency, openness, and listening. This partnership with CNCF reflects a broader commitment to the open source community by helping maintainers grow their projects, reach more developers through Docker Hub, and deliver value to their communities faster with improved tools, automation, and support.&#8221;</h3>
<div>
<p class="name">Michael Donovan</p>
<p class="title">VP Products, Docker</p>
</div>
</div>
</div>
</div>
<h2 class="wp-block-heading">Why this Partnership Matters</h2>
<p>This partnership reflects CNCF’s support of Docker as an industry leader and a strategic partner, trusted to deliver the scale, visibility, and security that today’s cloud-native ecosystem demands.</p>
<p>Docker Hub is the most widely used container registry in the world, serving over 22 billion image downloads per month and hosting more than 14 million images. For CNCF projects, using Docker is a natural choice, offering a trusted, reliable distribution platform with unmatched reach and adoption across the developer community.</p>
<div class="organism wp-block-ponyo-galen">
<div class="container left" style="">
<div class="galen-content">
<h3>&#8220;Docker was a founding member of CNCF, and we&#8217;ve maintained a long-term open collaboration over the past decade. This partnership marks a step forward for CNCF projects and we&#8217;re glad to work together to further secure the open source supply chain.&#8221;</h3>
<div>
<p class="name">Chris Aniszczyk</p>
<p class="title">CTO, CNCF</p>
</div>
</div>
</div>
</div>
<p>For Docker, this partnership is a reinforcement of our commitment to the open source community.&nbsp; We are also excited by the opportunity to deepen collaboration with the maintainers and developers building the future of cloud-native software. For maintainers, it’s an opportunity to gain access to premium infrastructure and support tailored to the needs of open-source projects.</p>
<h2 class="wp-block-heading">Maintainers: Unlock Full Access to DSOS Benefits</h2>
<div class="wp-block-ponyo-image">
<img decoding="async" width="700" height="467" src="https://www.docker.com/app/uploads/2025/06/JamesSpurin.jpg" class="attachment-full size-full" alt="Docker Captain James Spurin speaking at an event" srcset="https://www.docker.com/app/uploads/2025/06/JamesSpurin.jpg 700w, https://www.docker.com/app/uploads/2025/06/JamesSpurin-450x300.jpg 450w" sizes="(max-width: 700px) 100vw, 700px" title="- JamesSpurin">
</div>
<p><em>Figure: Docker Captain James Spurin providing a talk on Docker.</em></p>
<p>During the following days, all CNCF projects will have direct access to a dedicated bundle of Docker services through the Docker Sponsored Open Source (DSOS) program. Some of the key benefits of the program are:</p>
<ul class="wp-block-list">
<li>Unlimited image pulls</li>
<li><em>Sponsored OSS</em> status for increased trust and discoverability</li>
<li>Access to Docker usage metrics and engagement insights</li>
<li>Streamlined support through Docker’s open-source channels</li>
</ul>
<p>These benefits help you scale your project, grow your community, and ensure reliable access for your users.</p>
<div class="organism wp-block-ponyo-galen">
<div class="container left" style="">
<div class="galen-content">
<h3>&#8220;Docker Desktop has long been a key part of my Cloud Native workflows, and extending the Docker Sponsored Open Source Program to CNCF projects will be a game-changer for maintainers and contributors alike.&#8221;</h3>
<div>
<p class="name">James Spurin</p>
<p class="title">Docker Captain &amp; CNCF Ambassador</p>
</div>
</div>
</div>
</div>
<h2 class="wp-block-heading">What the Partnership Offers CNCF Projects</h2>
<h3 class="wp-block-heading">Docker: Official CNCF Project Services Provider</h3>
<p>As part of this collaboration, Docker will be listed as an official service provider on the CNCF<a href="https://contribute.cncf.io/resources/project-services/" rel="nofollow noopener" target="_blank"> Project Services page</a>. This showcasing enhances the discoverability of Docker’s tools and services for CNCF maintainers, reinforcing Docker’s role as a trusted infrastructure partner. For projects, it means easier access to vetted, high-impact resources already recognized and recommended by the CNCF community.</p>
<h3 class="wp-block-heading">Security with Docker Scout</h3>
<p>CNCF projects now have unlimited access to <a href="https://www.docker.com/products/docker-scout/"><strong>Docker Scout</strong></a>, our image analysis and policy evaluation tool. Scout is a critical security layer aligned with modern supply chain practices, helping projects detect vulnerabilities, enforce policies, and maintain healthy, secure containers.</p>
<h3 class="wp-block-heading">Automated Builds</h3>
<p>CNCF projects can streamline their development pipelines with Docker autobuilds, enabling automated image creation directly from source code.</p>
<h3 class="wp-block-heading">OSS Status</h3>
<p>All participating projects receive a <em>Sponsored OSS</em> badge on Docker Hub, increasing trust and visibility among users.</p>
<h3 class="wp-block-heading">Unlimited Image Pulls</h3>
<p>DSOS members benefit from unrestricted public image pulls, ensuring reliable access for users and reducing friction for project adoption.</p>
<h3 class="wp-block-heading">Docker Usage Metrics</h3>
<p>Access to pull data and adoption metrics provides deeper visibility into community engagement and image usage trends.</p>
<h3 class="wp-block-heading">Support and Communication Channels</h3>
<p>DSOS projects receive priority support through Docker’s open-source outreach channels.</p>
<h2 class="wp-block-heading">Reinforcing Docker’s Role in the Open-Source Supply Chain</h2>
<p>Security and trust are foundational to sustainable open source. Docker’s continued investment in secure tooling, developer experience, and supply chain integrity reflects our long-term commitment to supporting the infrastructure that open-source projects and their users rely on. Through tools like <a href="https://www.docker.com/products/docker-scout/"><strong>Docker Scout</strong></a>, now available to all CNCF projects, Docker is helping maintainers adopt secure development practices in a way that integrates naturally into their existing workflows.</p>
<p>Also the recent launch of <a href="https://www.docker.com/products/hardened-images/"><strong>Docker Hardened Images</strong></a>, curated, security-enhanced base images, has drawn intense interest from both the open-source community and enterprise users.&nbsp;</p>
<p>By continuing to invest in security, reliability, and open collaboration, Docker aims to help the ecosystem move forward with confidence.</p>
<h2 class="wp-block-heading">Moving Forward</h2>
<p>This partnership with CNCF is more than a program expansion. It is a signal that Docker Hub is the preferred distribution platform for the projects that matter most in the cloud-native ecosystem. It enables us to collaborate more deeply with maintainers, deliver better tools, and ensure open-source infrastructure is built on a strong, secure foundation.</p>
<p>If you&#8217;re a CNCF maintainer, now is the time to make sure your project is fully supported.</p>
<p>In the following days, your project will feature the DSOS badge on Docker Hub. If not, contact the CNCF <a href="https://github.com/cncf/servicedesk" rel="nofollow noopener" target="_blank">Service Desk</a> to get started. In case you don’t want to become part of the DSOS program, you can also use the same method of contact.</p>
<p>We’re proud to support the projects powering the modern internet, and we’re just getting started.</p>
<h3 class="wp-block-heading" style="margin-top:0;margin-right:0;margin-bottom:0;margin-left:0;padding-top:0;padding-right:0;padding-bottom:0;padding-left:0">Learn More</h3>
<ul class="wp-block-list">
<li>Apply to the <a href="https://www.docker.com/community/open-source/application/">Docker Sponsored Open-Source Program</a></li>
<li>Learn about <a href="https://www.docker.com/community/open-source/">Docker’s Open Source tools</a></li>
<li>Read the <a href="https://www.cncf.io/announcements/2025/09/18/cncf-expands-infrastructure-support-for-project-maintainers-through-partnership-with-docker/?ajs_aid=38a0ac94-4c51-44af-8634-9bad4b7f9ab8" rel="nofollow noopener" target="_blank">CNCF blog</a> about the partnership</li>
</ul>
Docker Model Runner General Availability
https://www.docker.com/blog/announcing-docker-model-runner-ga/
Francesco Corti
Thu, 18 Sep 2025 12:19:05 +0000
Products
AI/ML
Docker Desktop
Docker Hub
Docker Model Runner
https://www.docker.com/?p=77633
We’re excited to share that Docker Model Runner is now generally available (GA)! In April 2025, Docker introduced the first Beta release of Docker Model Runner, making it easy to manage, run, and distribute local AI models (specifically LLMs). Though only a short time has passed since then, the product has evolved rapidly, with continuous...
<p>We’re excited to share that <a href="https://www.docker.com/products/model-runner/">Docker Model Runner</a> is now generally available (GA)! In April 2025, Docker introduced the <a href="https://www.docker.com/blog/introducing-docker-model-runner/">first Beta release</a> of Docker Model Runner, making it easy to manage, run, and distribute local AI models (specifically LLMs). Though only a short time has passed since then, the product has evolved rapidly, with continuous enhancements driving the product to a reliable level of maturity and stability.</p>
<p>This blog post takes a look back at the most important and widely appreciated capabilities Docker Model Runner brings to developers, and looks ahead to share what they can expect in the near future.</p>
<h2 class="wp-block-heading">What is Docker Model Runner?</h2>
<p>Docker Model Runner (DMR) is built for developers first, making it easy to pull, run, and distribute large language models (LLMs) directly from <a href="https://hub.docker.com/u/ai" rel="nofollow noopener" target="_blank">Docker Hub</a> (in an OCI-compliant format) or <a href="https://huggingface.co/" rel="nofollow noopener" target="_blank">HuggingFace</a> (if models are available in the GGUF format, in which case they will be packaged as OCI Artifacts on-the-fly by the HuggingFace backend).</p>
<p>Tightly integrated with Docker Desktop and Docker Engine, DMR lets you serve models through OpenAI-compatible APIs, package GGUF files as OCI artifacts, and interact with them using either the command line, a graphical interface, or developer-friendly (REST) APIs.</p>
<p>Whether you’re creating <a href="https://www.docker.com/blog/genai-vs-agentic-ai/">generative AI</a> applications, experimenting with machine learning workflows, or embedding AI into your software development lifecycle, Docker Model Runner delivers a consistent, secure, and efficient way to <a href="https://www.docker.com/blog/run-llms-locally/">work with AI models locally</a>.</p>
<p>Check the <a href="https://docs.docker.com/ai/model-runner/" rel="nofollow noopener" target="_blank">official documentation</a> to learn more about Docker Model Runner and its capabilities.</p>
<h2 class="wp-block-heading">Why Docker Model Runner?</h2>
<p>Docker Model Runner makes it easier for developers to experiment and build AI application, including agentic apps, using the same Docker commands and workflows they already use every day. No need to learn a new tool!</p>
<p>Unlike many new AI tools that introduce complexity or require additional approvals, Docker Model Runner fits cleanly into existing enterprise infrastructure. It runs within your current security and compliance boundaries, so teams don’t have to jump through hoops to adopt it.</p>
<p>Model Runner supports OCI-packaged models, allowing you to store and distribute models through any OCI-compatible registry, including Docker Hub. And for teams using Docker Hub, enterprise features like Registry Access Management (RAM) provide policy-based access controls to help enforce guardrails at scale.</p>
<h2 class="wp-block-heading">11 Docker Model Runner Features Developers Love Most</h2>
<p>Below are the features that stand out the most and have been highly valued by the community.</p>
<h3 class="wp-block-heading">1. Powered by llama.cpp&nbsp;</h3>
<p>Currently, DMR is built on top of <a href="https://github.com/ggml-org/llama.cpp" rel="nofollow noopener" target="_blank">llama.cpp</a>, which we plan to continue supporting. At the same time, DMR is designed with flexibility in mind, and support for additional inference engines (such as MLX or vLLM) is under consideration for future releases.</p>
<h3 class="wp-block-heading">2. GPU acceleration across macOS and Windows platforms&nbsp;</h3>
<p>Harness the full power of your hardware with GPU support: Apple Silicon on macOS, NVIDIA GPUs on Windows, and even ARM/Qualcomm acceleration — all seamlessly managed through Docker Desktop.</p>
<h3 class="wp-block-heading">3. Native Linux support&nbsp;</h3>
<p>Run DMR on Linux with Docker CE, making it ideal for automation, CI/CD pipelines, and production workflows.</p>
<h3 class="wp-block-heading">4. CLI and UI experience&nbsp;</h3>
<p>Use DMR from the Docker CLI (on both Docker Desktop and Docker CE) or through Docker Desktop’s UI. The UI provides guided onboarding to help even first-time AI developers start serving models smoothly, with automatic handling of available resources (RAM, GPU, etc.).</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1000" height="629" src="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-1.png" class="attachment-full size-full" alt="MR GA figure 1" srcset="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-1.png 1000w, https://www.docker.com/app/uploads/2025/09/MR-GA-figure-1-477x300.png 477w" sizes="(max-width: 1000px) 100vw, 1000px" title="- MR GA figure 1">
</div>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1000" height="322" src="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-2.png" class="attachment-full size-full" alt="MR GA figure 2" srcset="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-2.png 1000w, https://www.docker.com/app/uploads/2025/09/MR-GA-figure-2-730x235.png 730w" sizes="(max-width: 1000px) 100vw, 1000px" title="- MR GA figure 2">
</div>
<p class="has-xs-font-size">Figure 1: Docker Model Runner works both in Docker Desktop and the CLI, letting you run models locally with the same familiar Docker commands and workflows you already know</p>
<p></p>
<h3 class="wp-block-heading">5. Flexible model distribution&nbsp;</h3>
<p>Pull and push models from <a href="https://hub.docker.com/u/ai" rel="nofollow noopener" target="_blank">Docker Hub</a> in OCI format, or pull directly from <a href="https://huggingface.co/" rel="nofollow noopener" target="_blank">HuggingFace</a> repositories hosting models in GGUF format for maximum flexibility in sourcing and sharing models.</p>
<h3 class="wp-block-heading">6. Open Source and free&nbsp;</h3>
<p>DMR is fully open source and free for everyone, lowering the barrier to entry for developers experimenting with or building on AI.</p>
<h3 class="wp-block-heading">7. Secure and controlled&nbsp;</h3>
<p>DMR runs in an isolated, controlled environment that doesn’t interfere with the main system or user data (sandboxing). Developers and IT admins can fine-tune security and availability by enabling/disabling DMR or configuring options like host-side TCP support and CORS.</p>
<h3 class="wp-block-heading">8. Configurable inference settings&nbsp;</h3>
<p>Developers can customize context length and llama.cpp runtime flags to fit their use cases, with more configuration options coming soon.</p>
<h3 class="wp-block-heading">9. Debugging support&nbsp;</h3>
<p>Built-in request/response tracing and inspect capabilities make it easier to understand token usage and framework/library behaviors, helping developers debug and optimize their applications.</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1000" height="664" src="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-3.png" class="attachment-full size-full" alt="MR GA figure 3" srcset="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-3.png 1000w, https://www.docker.com/app/uploads/2025/09/MR-GA-figure-3-452x300.png 452w" sizes="(max-width: 1000px) 100vw, 1000px" title="- MR GA figure 3">
</div>
<p class="has-xs-font-size">Figure 2: Built-in tracing and inspect tools in Docker Desktop make debugging easier, giving developers clear visibility into token usage and framework behavior</p>
<h3 class="wp-block-heading">10. Integrated with the Docker ecosystem&nbsp;</h3>
<p>DMR works out of the box with <a href="https://docs.docker.com/ai/compose/models-and-compose/" rel="nofollow noopener" target="_blank">Docker Compose</a> and is fully integrated with other Docker products, such as <a href="https://docs.docker.com/offload/" rel="nofollow noopener" target="_blank">Docker Offload</a> (cloud offload service) and <a href="https://docs.docker.com/testcontainers/" rel="nofollow noopener" target="_blank">Testcontainers</a>, extending its reach into both local and distributed workflows.</p>
<h3 class="wp-block-heading">11. Up-to-date model catalog&nbsp;</h3>
<p>Access a <a href="https://hub.docker.com/u/ai" rel="nofollow noopener" target="_blank">curated catalog</a> of the most popular and powerful AI models on Docker Hub. These models can be pulled for free and used across development, pipelines, staging, or even production environments.</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="1000" height="873" src="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-4.png" class="attachment-full size-full" alt="MR GA figure 4" srcset="https://www.docker.com/app/uploads/2025/09/MR-GA-figure-4.png 1000w, https://www.docker.com/app/uploads/2025/09/MR-GA-figure-4-344x300.png 344w" sizes="(max-width: 1000px) 100vw, 1000px" title="- MR GA figure 4">
</div>
<p class="has-xs-font-size">Figure 3: Curated model catalog on Docker Hub, packaged as OCI Artifacts and ready to run</p>
<h2 class="wp-block-heading">The road ahead</h2>
<p>The future is bright for Docker Model Runner, and the recent GA version is only the first milestone. Below are some of the future enhancements that you should expect to be released soon.</p>
<h3 class="wp-block-heading">Streamlined User Experience&nbsp;</h3>
<p>Our goal is to make DMR simple and intuitive for developers to use and debug. This includes richer response rendering in the chat-like interface within Docker Desktop and the CLI, multimodal support in the UI (already available through the API), integration with MCP tools, and enhanced debugging features, alongside expanded configuration options for greater flexibility. Last but not least, we aim to provide smoother and more seamless integration with third-party tools and solutions across the AI ecosystem.</p>
<h3 class="wp-block-heading">Enhancements and better ability to execute&nbsp;</h3>
<p>We remain focused on continuously improving DMR’s performance and flexibility for running local models. Upcoming enhancements include support for the most widely used inference libraries and engines, advanced configuration options at the engine and model level, and the ability to deploy Model Runner independently from Docker Engine for production-grade use cases, along with many more improvements on the horizon.</p>
<h3 class="wp-block-heading">Frictionless Onboarding&nbsp;</h3>
<p>We want first-time AI developers to start building their applications right away, and to do so with the right foundations. To achieve this, we plan to make onboarding into DMR even more seamless. This will include a guided, step-by-step experience to help developers get started quickly, paired with a set of sample applications built on DMR. These samples will highlight real-world use cases and best practices, providing a smooth entry point for experimenting with and adopting DMR in everyday workflows.</p>
<h3 class="wp-block-heading">Staying on Top of Model Launch&nbsp;</h3>
<p>As we continue to enhance inference capabilities, we remain committed to maintaining a first-class catalog of AI models directly in Docker Hub, the leading registry for OCI artifacts, including models. Our goal is to ensure that new, relevant models are available in Docker Hub and runnable through DMR as soon as they are publicly released.</p>
<h3 class="wp-block-heading">Conclusion</h3>
<p>Docker Model Runner has come a long way in a short time, evolving from its Beta release into a mature and stable inference engine that’s now generally available. At its core, the mission has always been clear: make it simple, consistent, and secure for developers to pull, run, and serve AI models locally,. using familiar Docker CLI commands and tools they already love!</p>
<p>Now is the perfect time to get started. If you haven’t already, <a href="https://docs.docker.com/get-started/" rel="nofollow noopener" target="_blank">install Docker Desktop</a> and try out Docker Model Runner today. Follow the <a href="https://docs.docker.com/ai/model-runner/" rel="nofollow noopener" target="_blank">official documentation</a> to explore its capabilities and see for yourself how DMR can accelerate your journey into building AI-powered applications.</p>
<h4 class="wp-block-heading">Learn more</h4>
<ul class="wp-block-list">
<li>Read our quickstart guide to<a href="https://www.docker.com/blog/run-llms-locally/"> Docker Model Runner</a>.</li>
<li>Visit our<a href="https://github.com/docker/model-runner" rel="nofollow noopener" target="_blank"> Model Runner GitHub repo</a>! Docker Model Runner is open-source, and we welcome collaboration and contributions from the community!</li>
<li>Learn how Compose<a href="https://www.docker.com/blog/build-ai-agents-with-docker-compose/"> works with Model runner, making building AI apps and agents easier</a></li>
<li>Learn how to <a href="https://www.docker.com/blog/how-to-build-an-ai-tutor-with-model-runner/">build an AI tutor</a></li>
<li>Explore <a href="https://www.docker.com/blog/hybrid-ai-and-how-it-runs-in-docker/">how to use both local and remote models</a> in hybrid AI workflows&nbsp;</li>
<li><a href="https://www.docker.com/blog/building-ai-agents-with-goose-and-docker/">Building AI agents</a> made easy with Goose and Docker</li>
<li><a href="https://www.docker.com/blog/docker-model-runner-on-hugging-face/">Using Model Runner</a> on Hugging Face</li>
<li><a href="https://www.docker.com/blog/ai-powered-mock-apis-for-testing-with-docker-and-microcks/">Powering AI generated testing</a> with Docker Model Runner</li>
<li><a href="https://www.docker.com/blog/build-genai-app-with-java-spring-ai-docker-model-runner/">Build a GenAI App</a> With Java Using Spring AI and Docker Model Runner</li>
<li><a href="https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/">Tool Calling with Local LLMs</a>: A Practical Evaluation</li>
<li><a href="https://www.docker.com/blog/behind-the-scenes-how-we-designed-docker-model-runner-and-whats-next/">Behind the scenes: How we designed Docker Model Runner and what’s next</a></li>
<li><a href="https://www.docker.com/blog/why-docker-chose-oci-artifacts-for-ai-model-packaging/">Why Docker Chose OCI Artifacts for AI Model Packaging</a></li>
<li><a href="https://youtu.be/904fQI6aNos" rel="nofollow noopener" target="_blank">What’s new with Docker Model Runner&nbsp;</a></li>
<li><a href="https://www.docker.com/blog/publish-ai-models-on-docker-hub/">Publishing AI models to Docker Hub</a></li>
<li><a href="https://www.docker.com/blog/how-to-make-ai-chatbot-from-scratch/" data-type="link" data-id="https://www.docker.com/blog/how-to-make-ai-chatbot-from-scratch/">How to Build and Run a GenAI ChatBot from Scratch using Docker Model Runner</a> </li>
</ul>
How to Build Secure AI Coding Agents with Cerebras and Docker Compose
https://www.docker.com/blog/cerebras-docker-compose-secure-ai-coding-agents/
Oleg Selajev
Wed, 17 Sep 2025 16:00:00 +0000
Products
Docker Compose
Docker MCP Gateway
MCP server
https://www.docker.com/?p=77621
In the recent article, Building Isolated AI Code Environments with Cerebras and Docker Compose, our friends at Cerebras showcased how one can build a coding agent to use worlds fastest Cerebras’ AI inference API, Docker Compose, ADK-Python, and MCP servers. In this post, we’ll dive deeper into the underlying technologies and show how the pieces...
<p>In the recent article, <a href="https://www.cerebras.ai/blog/DockerCompose" data-type="link" data-id="https://www.cerebras.ai/blog/DockerCompose" rel="nofollow noopener" target="_blank">Building Isolated AI Code Environments with Cerebras and Docker Compose</a>, our friends at Cerebras showcased how one can build a coding agent to use worlds fastest Cerebras’ AI inference API, Docker Compose, ADK-Python, and MCP servers.</p>
<p>In this post, we’ll dive deeper into the underlying technologies and show how the pieces come together to build an AI agent environment that’s portable, secure, and fully containerized. You&#8217;ll learn how to create multi-agent systems, run some agents with local models in Docker Model Runner, and integrate custom tools as MCP servers into your AI agent&#8217;s workflow.</p>
<p>We’ll also touch on how to build a secure sandbox for executing the code your agent writes, an ideal use case for containers in real-world development.&nbsp;</p>
<h2 class="wp-block-heading">Getting Started</h2>
<p>To begin, clone the <a href="https://github.com/dockersamples/docker-cerebras-demo" rel="nofollow noopener" target="_blank">repository from GitHub</a> and navigate into the project directory.</p>
<p>Get the code for the agent, and prepare the .env file to provide your Cerebras API key:&nbsp;</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: bash; gutter: false; title: ; notranslate">
git clone https://github.com/dockersamples/docker-cerebras-demo &amp;&amp; cd docker-cerebras-demo
</pre></div>
<p>Next, prepare the .env file to provide your Cerebras API key. You can get a key from the<a href="https://cloud.cerebras.ai/" rel="nofollow noopener" target="_blank"> Cerebras Cloud platform</a>.</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: bash; gutter: false; title: ; notranslate">
# This copies the sample environment file to your local .env file
cp .env-sample .env
</pre></div>
<p>Now, open the <code>.env</code> file in your favorite editor and add your API key to the <code>CEREBRAS_API_KEY</code> line. Once that&#8217;s done, run the system using Docker Compose:<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: bash; gutter: false; title: ; notranslate">
docker compose up --build
</pre></div>
<p>The first run may take a few minutes to pull the model and containers. Once it’s up, you can see the agent at <code>localhost:8000</code>.</p>
<p>The first run may take a few minutes to pull the necessary Docker images and the AI model. Once it’s running, you can access the agent&#8217;s interface at <code>http://localhost:8000</code>. From there, you can interact with your agent and issue commands like &#8220;write code,&#8221; &#8220;initialize the sandbox environment,&#8221; or request specific tools like &#8220;cerebras, curl <a href="http://docker.com" rel="nofollow noopener" target="_blank">docker.com</a> for me please.&#8221;</p>
<h2 class="wp-block-heading">Understanding the Architecture</h2>
<p>This demo follows the architecture from our<a href="https://www.google.com/search?q=https://github.com/dockersamples/compose-for-agents" rel="nofollow noopener" target="_blank"> Compose for Agents repository</a>, which breaks down an agent into three core components:</p>
<ol class="wp-block-list">
<li>The Agentic Loop: This is the main application logic that orchestrates the agent&#8217;s behavior. In our case, it&#8217;s an ADK-Python-based application. The ADK-Python framework also includes a visualizer that lets you inspect tool calls and trace how the system reached specific decisions.</li>
</ol>
<div class="wp-block-ponyo-image">
<img decoding="async" width="5102" height="2810" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27.png" class="attachment-full size-full" alt="DevDuck architecture" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27.png 5102w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27-545x300.png 545w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27-1110x611.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27-1536x846.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.29.27-2048x1128.png 2048w" sizes="(max-width: 5102px) 100vw, 5102px" title="- Screenshot 2025 09 15 at 13.29.27">
</div>
<ol start="2" class="wp-block-list">
<li>The MCP Tools: These are the external tools the agent can use. We provide them securely via the Docker MCP Gateway. In this app we use context7 and node sandbox MCP servers.&nbsp;</li>
<li>The AI Model: You can define any local or remote AI model you want to use. Here, we&#8217;re using a local Qwen model for routing between the local agent and the powerful Cerebras agent which will use Cerebras API.&nbsp;</li>
</ol>
<p><a href="https://cloud.cerebras.ai/" rel="nofollow noopener" target="_blank">Cerebras Cloud</a> serves as a specialized, high-performance inference backend. It can run massive models, like a half-trillion parameter Qwen coder, at thousands of tokens per second. While our simple demo doesn&#8217;t require this level of speed, such performance is a game-changer for real-world applications.</p>
<p>Most of the prompts and responses are a few hundred tokens long, as they are simple commands to initialize a sandbox or write some JavaScript code in it. You&#8217;re welcome to make the agent work harder and see Cerebras&#8217; performance on more verbose requests.&nbsp;</p>
<p>For example, you can ask the Cerebras agent to write some JavaScript code, and see it call the functions from the MCP tools to read and write the files and run them as you see on the screenshot below.&nbsp;</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="5106" height="2814" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57.png" class="attachment-full size-full" alt="DevDuck architecture calling MCP tools" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57.png 5106w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57-544x300.png 544w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57-1110x612.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57-1536x847.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.57-2048x1129.png 2048w" sizes="(max-width: 5106px) 100vw, 5106px" title="- Screenshot 2025 09 15 at 13.30.57">
</div>
<h2 class="wp-block-heading">Building a Custom Sandbox as an MCP Server</h2>
<p>A key feature of this setup is the ability to create a secure sandbox for code execution. To do this, we&#8217;ll build a custom MCP server. In our example, we enable two MCP servers:</p>
<ul class="wp-block-list">
<li><code>context7</code>: This gives our agent access to the latest documentation for various application frameworks.</li>
<li><code>node-code-sandbox</code>: This is our custom-made sandbox for executing the code our agent writes.</li>
</ul>
<p>You can find the implementation of our Node.js sandbox server in the<a href="https://github.com/shelajev/node-sandbox-mcp" rel="nofollow noopener" target="_blank"> node-sandbox-mcp GitHub repository</a>. It&#8217;s a Quarkus application written in Java that exposes itself as an <code>stdio</code> mcp-server and uses the awesome <a href="http://testcontainers.com" rel="nofollow noopener" target="_blank">Testcontainers library</a> to create and manage the sandbox containers programmatically.</p>
<p>An important detail is that you have full control over the sandbox configuration. We start the container with a common Node.js development image and, as a crucial security measure, disable its networking. But since it&#8217;s a custom MCP server, you can enable any security measures you deem necessary.&nbsp;</p>
<p>Here&#8217;s a snippet of the Testcontainers-java code used to create the container:</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: java; gutter: false; title: ; notranslate">
GenericContainer sandboxContainer = new GenericContainer&lt;&gt;(&quot;mcr.microsoft.com/devcontainers/javascript-node:20&quot;)
.withNetworkMode(&quot;none&quot;) // disable network!!
.withWorkingDirectory(&quot;/workspace&quot;)
.withCommand(&quot;sleep&quot;, &quot;infinity&quot;);
sandboxContainer.start();
</pre></div>
<p>Testcontainers provides a flexible, idiomatic API to interact with the sandbox. Running a command or writing a file becomes a simple one-line method call:</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: java; gutter: false; title: ; notranslate">
// To execute a command inside the sandbox
sandbox.execInContainer(command);
// To write a file into the sandbox
sandbox.copyFileToContainer(Transferable.of(contents.getBytes()), filename);
</pre></div>
<p>The actual implementation has a bit more glue code for managing background processes or selecting the correct sandbox if you&#8217;ve created multiple, but these one-liners are the core of the interaction.<br></p>
<h2 class="wp-block-heading">Packaging and Using the Custom Server</h2>
<p>To use our custom server, we first need to package it as a Docker image. For Quarkus applications, a single command does the trick:</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: bash; gutter: false; title: ; notranslate">
./mvnw package -DskipTests=true -Dquarkus.container-image.build=true
</pre></div>
<p>This command produces a local Docker image and outputs its name, something like:<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
&#x5B;INFO] &#x5B;io.quarkus.container.image.docker.deployment.DockerProcessor] Built container image shelajev/node-sandbox:1.0.0-SNAPSHOT
</pre></div>
<p>Since we&#8217;re running everything locally, we don&#8217;t even need to push this image to a remote registry. You can inspect this image in Docker Desktop and find its hash, which we&#8217;ll use in the next step.</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="2796" height="1536" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20.png" class="attachment-full size-full" alt="DevDuck - Docker Desktop image layers" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20.png 2796w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20-546x300.png 546w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20-1110x610.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20-1536x844.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.48.20-2048x1125.png 2048w" sizes="(max-width: 2796px) 100vw, 2796px" title="- Screenshot 2025 09 15 at 13.48.20">
</div>
<h2 class="wp-block-heading">Integrating the Sandbox via the MCP Gateway</h2>
<p>With our custom MCP server image ready, it&#8217;s time to plug it into <a href="https://github.com/docker/mcp-gateway" rel="nofollow noopener" target="_blank">the MCP Gateway</a>. We&#8217;ll create a custom catalog file (<a href="https://github.com/dockersamples/docker-cerebras-demo/blob/main/mcp-gateway-catalog.yaml" rel="nofollow noopener" target="_blank">mcp-gateway-catalog.yaml</a>) that enables both the standard <code>context7</code> server and our new <code>node-code-sandbox</code>.</p>
<p>Currently, creating this file is a manual process, but we&#8217;re working on simplifying it. The result is a portable catalog file that mixes standard and custom MCP servers.</p>
<p>Notice two key things in the configuration for the node-code-sandbox MCP server in the catalog:</p>
<ul class="wp-block-list">
<li><code>longLived: true</code>: This tells the gateway that our server needs to persist between the tool calls to track the sandbox&#8217;s state.&nbsp;</li>
<li><code>image:</code>: We reference the specific Docker image using its <code>sha256</code> hash to ensure reproducibility.</li>
</ul>
<p>If you&#8217;re building the custom server for the sandbox MCP, you can replace the image reference with the one your build step produced.&nbsp;</p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
longLived: true
image: olegselajev241/node-sandbox@sha256:44437d5b61b6f324d3bb10c222ac43df9a5b52df9b66d97a89f6e0f8d8899f67
</pre></div>
<p>Finally, we update our <code>docker-compose.yml</code> to mount this catalog file and enable both servers:<br></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
mcp-gateway:
# mcp-gateway secures your MCP servers
image: docker/mcp-gateway:latest
use_api_socket: true
command:
- --transport=sse
# add any MCP servers you want to use
- --servers=context7,node-code-sandbox
- --catalog=/mcp-gateway-catalog.yaml
volumes:
- ./mcp-gateway-catalog.yaml:/mcp-gateway-catalog.yaml:ro
</pre></div>
<p>When you run <code>docker compose up</code>, the gateway starts, which in turn starts our <code>node-sandbox</code> MCP server. When the agent requests a sandbox, a third container is launched &#8211; the actual isolated environment.&nbsp;<br></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="3724" height="1532" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13.png" class="attachment-full size-full" alt="DevDuck launched node-sandbox in isolated container" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13.png 3724w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13-730x300.png 730w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13-1110x457.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13-1536x632.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.30.13-2048x843.png 2048w" sizes="(max-width: 3724px) 100vw, 3724px" title="- Screenshot 2025 09 15 at 13.30.13">
</div>
<p>You can use tools like Docker Desktop to inspect all running containers, view files, or even open a shell for debugging.<br></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="2690" height="1962" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35.png" class="attachment-full size-full" alt="DevDuck Docker Desktop inspect running containers" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35.png 2690w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35-411x300.png 411w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35-1110x810.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35-1536x1120.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-14.00.35-2048x1494.png 2048w" sizes="(max-width: 2690px) 100vw, 2690px" title="- Screenshot 2025 09 15 at 14.00.35">
</div>
<h2 class="wp-block-heading">The Security Benefits of Containerized Sandboxes&nbsp;</h2>
<p>This containerized sandbox approach is a significant security win. Containers provide a well-understood security boundary with a smaller vulnerability profile than running random internet code on your host machine, and you can harden them as needed.</p>
<p>Remember how we disabled networking in the sandbox container? This means any code the agent generates cannot leak local secrets or data to the internet. If you ask the agent to run code that tries to access, for example, <code>google.com</code>, it will fail.</p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="5104" height="2816" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32.png" class="attachment-full size-full" alt="DevDuck containerized sandbox showing inability to access google.com" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32.png 5104w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32-544x300.png 544w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32-1110x612.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32-1536x847.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.31.32-2048x1130.png 2048w" sizes="(max-width: 5104px) 100vw, 5104px" title="- Screenshot 2025 09 15 at 13.31.32">
</div>
<p>This demonstrates a key advantage: granular control. While the sandbox is cut off from the network, other tools are not. The <code>context7</code> MCP server can still access the internet to fetch documentation, allowing the agent to write better code without compromising the security of the execution environment.<br></p>
<div class="wp-block-ponyo-image">
<img decoding="async" width="5102" height="2816" src="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54.png" class="attachment-full size-full" alt="DevDuck demo showing sandbox access to provided docs" srcset="https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54.png 5102w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54-544x300.png 544w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54-1110x613.png 1110w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54-1536x848.png 1536w, https://www.docker.com/app/uploads/2025/09/Screenshot-2025-09-15-at-13.32.54-2048x1130.png 2048w" sizes="(max-width: 5102px) 100vw, 5102px" title="- Screenshot 2025 09 15 at 13.32.54">
</div>
<p>Oh, and a neat detail is that when you stop the containers managed by compose, it also kills the sandbox MCP server, and that in turn triggers Testcontainers to clean up all the sandbox containers, just like it cleans after a typical test run.&nbsp;<br></p>
<h2 class="wp-block-heading">Next Steps and Extensibility</h2>
<p>This coding agent is a great starting point, but it isn&#8217;t production-ready. For a real-world application, you might want to grant controlled access to resources like the npm registry. You could, for example, achieve this by mapping your local npm cache from the host system into the sandbox. This way, you, the developer, control exactly which npm libraries are accessible.</p>
<p>Because the sandbox is a custom MCP server, the possibilities are endless. You can build it yourself, tweak it however you want, and integrate any tools or constraints you need.</p>
<h2 class="wp-block-heading">Conclusion</h2>
<p>In this post, we demonstrated how to build a secure and portable AI coding agent using Docker Compose and the MCP Toolkit. By creating a custom MCP server with Testcontainers, we built a sandboxed execution environment that offers granular security controls, like disabling network access, without limiting the agent&#8217;s other tools.&nbsp; We connect this coding agent to Cerebras API, so we get incredible inference speed.&nbsp;This architecture provides a powerful and secure foundation for building your own AI agents. We encourage you to clone the repository and experiment with the code! You probably already <a href="https://docs.docker.com/get-started/get-docker/" rel="nofollow noopener" target="_blank">have Docker</a> and can sign up for a Cerebras API key <a href="http://inference.cerebras.ai" rel="nofollow noopener" target="_blank">here</a>.</p>
MCP Security: A Developer’s Guide
https://www.docker.com/blog/mcp-security-explained/
Saurabh Davala
Tue, 16 Sep 2025 13:00:30 +0000
Engineering
Products
AI/ML
Docker Desktop
Docker Hub
Docker MCP Gateway
https://www.docker.com/?p=77545
Since its release by Anthropic in November 2024, Model Context Protocol (MCP) has gained massive adoption and is quickly becoming the connective tissue between AI agents and the tools, APIs, and data they act on.&#160; With just a few lines of configuration, an agent can search code, open tickets, query SaaS systems, or even deploy...
<p>Since its release by Anthropic in November 2024, Model Context Protocol (MCP) has gained massive adoption and is quickly becoming the connective tissue between AI agents and the tools, APIs, and data they act on.&nbsp;</p>
<p>With just a few lines of configuration, an agent can search code, open tickets, query SaaS systems, or even deploy infrastructure. That kind of flexibility is powerful but it also introduces new security challenges. In fact, security researchers analyzing the MCP ecosystem found command injection flaws <a href="https://www.docker.com/blog/mcp-security-issues-threatening-ai-infrastructure/#:~:text=Security%20researchers%20analyzing%20the%20MCP%20ecosystem%20found%20that%20OAuth%2Drelated%20vulnerability%20represent%20the%20most%20severe%20attack%20class%2C%20with%20command%20injection%20flaws%20affecting%2043%25%20of%20analyzed%20servers.">affecting 43% of analyzed servers</a>. A single misconfigured or malicious server can exfiltrate secrets, trigger unsafe actions, or quietly change how an agent behaves.&nbsp;</p>
<p>This guide is for developers and platform teams building with agents. We’ll unpack what makes MCP workflows uniquely risky for AI infrastructure, highlight common missteps like prompt injection or shadow tooling, and show how secure defaults, like <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/">containerized MCP servers</a> and <a href="https://www.docker.com/blog/docker-mcp-gateway-secure-infrastructure-for-agentic-ai/">policy-based gateways</a>, can help you govern every tool call without slowing your AI roadmap.</p>
<h2 class="wp-block-heading"><strong>What is MCP security?</strong></h2>
<p><a href="https://www.docker.com/blog/mcp-security-issues-threatening-ai-infrastructure/">Model Context Protocol</a> is a standardized interface that enables AI agents to interact with external tools, databases, and services. MCP security refers to the controls and risks that govern how agents <strong>discover, connect to, and execute </strong>MCP servers. These security risks span across the entire development lifecycle and involve:</p>
<ul class="wp-block-list">
<li><strong>Supply chain</strong>: how servers are packaged, signed, versioned, and approved.</li>
<li><strong>Runtime isolation</strong>: how they’re executed on the host vs. in containers, with CPU/memory/network limits.</li>
<li><strong>Brokered access</strong>: how calls are mediated, logged, blocked, or transformed in real time.</li>
<li><strong>Client trust</strong>: which tools a given IDE/agent is allowed to see and use.</li>
</ul>
<h3 class="wp-block-heading">Why does MCP security matter?</h3>
<p>Securing MCP workflows has become more important than ever because AI agents blur the line between “code” and “runtime.” A prompt or tool description can change what your system is capable of without a code release.&nbsp;</p>
<p>This means that security practices have to move up a layer, from static analysis to <strong>policy over agent‑tool interactions</strong>. Docker codifies that policy in a gateway and makes secure defaults practical for everyday developers.</p>
<p>Docker’s approach is to make MCP both <strong>easy</strong> and <strong>safe</strong> through containerized execution, a policy‑enforcing <strong>MCP Gateway</strong>, and a curated <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/"><strong>MCP Catalog &amp; Toolkit</strong></a> that helps teams standardize what agents can do. If you’re building with agents, this guide will help you understand the risks, why traditional tools fall short, and how Docker reduces blast radius without slowing your AI roadmap.</p>
<h2 class="wp-block-heading"><strong>Understanding MCP security risks</strong></h2>
<p>While MCP risks can show up in various ways across the dev lifecycle, there are specific categories they typically fall into. The section below highlights how these risks surface in real workflows, their impact, and practical guardrails that mitigate without slowing teams down.&nbsp;</p>
<h3 class="wp-block-heading"><strong>Misconfigurations &amp; weak defaults</strong></h3>
<ul class="wp-block-list">
<li><strong>Running servers directly on the host</strong> with broad privileges or a persistent state.</li>
<li><strong>Unrestricted network egress</strong> from tools to the public internet.</li>
<li><strong>Unvetted catalogs/registries</strong> in client configs, exposing agents to unknown tools.</li>
<li><strong>No audit trail</strong> for tool calls-hard to investigate or respond.</li>
</ul>
<p><strong>Impact:</strong> Lateral movement, data exfiltration, and irreproducible behavior.</p>
<p><strong>Mitigation:</strong> Always follow <a href="https://www.docker.com/blog/mcp-server-best-practices/">MCP server best practices</a> such as leveraging containerization, applying resource and network limits, maintaining an allowlist of approved tools, and capturing call logs centrally.</p>
<h3 class="wp-block-heading"><strong>Malicious or compromised servers (supply chain)</strong></h3>
<ul class="wp-block-list">
<li><strong>Typosquatting/poisoned images</strong> or unsigned builds.</li>
<li><strong>Hidden side effects</strong> or altered tool metadata that nudges agents into risky actions.</li>
</ul>
<p><strong>Impact:</strong> Covert behavior change, credential theft, persistent access.</p>
<p><strong>Mitigation:</strong> Require signature verification, pin versions/digests, and pull from curated sources such as the <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/"><strong>MCP Catalog &amp; Toolkit</strong></a>.</p>
<h3 class="wp-block-heading"><strong>Secret management failures</strong></h3>
<ul class="wp-block-list">
<li><strong>Plaintext credentials</strong> in environment variables, prompts, or tool arguments.</li>
<li><strong>Leakage</strong> via tool outputs or model completions.</li>
</ul>
<p><strong>Impact:</strong> Account takeover, data loss.</p>
<p><strong>Mitigation:</strong> Use managed secrets, minimize prompt exposure, and redact or block sensitive values at the broker.</p>
<h3 class="wp-block-heading"><strong>Prompt injection &amp; tool poisoning</strong></h3>
<ul class="wp-block-list">
<li><strong>Prompt injection</strong>: hostile content instructs the model to exfiltrate data or call dangerous tools.</li>
<li><strong>Tool poisoning/shadowing</strong>: misleading tool descriptions or unexpected defaults that steer the agent.</li>
</ul>
<p><strong>Impact:</strong> Agents do the wrong thing, confidently.</p>
<p><strong>Mitigation:</strong> Strict tool allowlists, pre/post‑call interceptors, and output filtering at the gateway. Docker’s MCP Gateway provides <a href="https://github.com/docker/mcp-gateway/blob/main/docs/security.md#active-security" rel="nofollow noopener" target="_blank">active security capabilities</a> (signature checks, call logging, secret and network controls, interceptors).</p>
<h2 class="wp-block-heading"><strong>What makes MCP security challenging?</strong></h2>
<ul class="wp-block-list">
<li><strong>Dynamic &amp; non‑deterministic behavior</strong>: the same prompt may lead to different tool calls.</li>
<li><strong>Instruction vs. data ambiguity</strong>: LLMs can treat content (including tool docs) as instructions.</li>
<li><strong>Growing, shifting attack surface</strong>: every new tool expands what the agent can do instantly.</li>
<li><strong>Traditional AppSec gaps</strong>: Static analysis tools don’t see agentic tool calls or MCP semantics; you need mediation between agents and tools, not just better prompts.</li>
</ul>
<p><strong>Implication for developers:</strong> You need a guardrail that lives at the agent–tool boundary, verifying what runs, brokering what’s allowed, and recording what happened.</p>
<h2 class="wp-block-heading"><strong>How to prevent and mitigate MCP server security concerns</strong></h2>
<p>Use this practitioner checklist to raise the floor:</p>
<ol class="wp-block-list">
<li><strong>Containerize every MCP server</strong><strong><br></strong>Run servers in containers (not on the host) with <strong>CPU/memory caps</strong> and a read‑only filesystem where possible. Treat each server as untrusted code with the least privilege necessary.<br>Why it helps<em>:</em> limits blast radius and makes behavior reproducible.</li>
<li><strong>Centralize enforcement at a gateway (broker)</strong><strong><br></strong>Place a <strong>policy‑enforcing gateway</strong> between clients (IDE/agent) and servers. Use it to:
<ul class="wp-block-list">
<li>Verify <strong>signatures</strong> before running servers.</li>
<li>Maintain a <strong>tool allowlist</strong> (only approved servers are discoverable).</li>
<li>Apply <strong>network egress controls</strong> and <strong>secret redaction</strong>.</li>
<li><strong>Log</strong> requests/responses for audit and incident response.</li>
</ul>
</li>
<li><strong>Govern secrets end‑to‑end</strong><strong><br></strong>Store secrets in a managed system; avoid .env files. Prefer short‑lived tokens. Sanitize prompts and tool outputs to reduce exposure.</li>
<li><strong>Defend the prompt layer</strong><strong><br></strong>Use <strong>pre‑call interceptors</strong> (argument/type checks, safety classifiers) and <strong>post‑call interceptors</strong> (redaction, PII scrub). Combine with strict tool scoping to reduce prompt‑injection blast radius.</li>
<li><strong>Harden the supply chain</strong><strong><br></strong>Pull servers from curated sources (e.g., <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/"><strong>MCP Catalog &amp; Toolkit</strong></a>), require signatures, and pin to immutable versions.</li>
<li><strong>Monitor and rehearse</strong><strong><br></strong>Alert on anomalous tool sequences (e.g., sudden credential access), and run tabletop exercises to rotate tokens and revoke access.</li>
</ol>
<h2 class="wp-block-heading"><strong>How Docker makes MCP security practical</strong></h2>
<p>Turning MCP security from theory into practice means putting guardrails where agents meet tools and making trusted servers easy to adopt for agentic workflows. Docker’s MCP stack does both: Docker Gateway enforces policy and observability on every call, while the Docker MCP Catalog &amp; Toolkit curates, verifies, and versions the servers your team can safely use.</p>
<h3 class="wp-block-heading"><strong>Docker MCP Gateway: Your enforcement point</strong></h3>
<p>The gateway sits between clients and servers to provide <strong>verification, policy, and observability</strong> for every tool call. It supports active security measures like <strong>signature verification, call logging, secret and network controls, and pre/post-interceptors</strong> so you can block or transform risky actions before they reach your systems.&nbsp;</p>
<p>Learn more in <a href="https://www.docker.com/blog/docker-mcp-gateway-secure-infrastructure-for-agentic-ai/"><strong>Docker MCP Gateway: Unified, Secure Infrastructure for Agentic AI</strong></a> and the <a href="https://github.com/docker/mcp-gateway/blob/main/docs/security.md#active-security" rel="nofollow noopener" target="_blank"><strong>Gateway Active Security</strong></a> documentation.</p>
<h3 class="wp-block-heading"><strong>Docker MCP Catalog &amp; Toolkit: Curation and convenience</strong></h3>
<p>Use the <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/"><strong>MCP Catalog &amp; Toolkit</strong></a> to standardize the servers your organization trusts. The catalog helps reduce supply‑chain risk (publisher verification, versioning, provenance) and makes it straightforward for developers to pull approved tools into their workflow. With a growing selection of 150+ curated MCP servers, MCP Catalog is a safe and easy way to get started with MCP.</p>
<p>Looking for a broader view of how Docker helps with AI development? Check out <a href="https://www.docker.com/solutions/docker-ai/"><strong>Docker for AI</strong></a>.</p>
<h3 class="wp-block-heading">Putting it all Together: A practical flow</h3>
<ol class="wp-block-list">
<li><strong>Choose servers from the Catalog</strong> and pin them by digest.</li>
<li><strong>Register servers with the Gateway</strong> so clients only see approved tooling.</li>
<li><strong>Enable active security</strong>: verify signatures, log all calls, redact/deny secrets, and restrict egress.</li>
<li><strong>Add pre/post interceptors</strong>: validate arguments (before), redact/normalize outputs (after).</li>
<li><strong>Monitor and tune</strong>: review call logs, alert on anomalies, rotate secrets, and update allowlists as new tools are introduced.</li>
</ol>
<h3 class="wp-block-heading">Conclusion</h3>
<p>MCP unlocks powerful agentic workflows but also introduces new classes of risk, from prompt injection to tool poisoning and supply‑chain tampering. MCP security isn’t just better prompts; it’s <strong>secure packaging, verified distribution, and a brokered runtime with policy</strong>.</p>
<p><strong>Key takeaways</strong></p>
<ul class="wp-block-list">
<li>Treat MCP as a <strong>governed toolchain</strong>, not just an SDK.</li>
<li>Put a <strong>policy gateway</strong> between agents and tools to verify, mediate, and observe.</li>
<li>Pull servers from the <a href="https://www.docker.com/products/mcp-catalog-and-toolkit/"><strong>MCP Catalog &amp; Toolkit</strong></a> and pin versions/digests.</li>
<li>Use <strong>active security</strong> features such as <a href="https://github.com/docker/mcp-gateway/blob/main/docs/security.md#active-security" rel="nofollow noopener" target="_blank"><strong>signature checks, interceptors, logging, and secret/network controls</strong></a> to reduce blast radius.</li>
</ul>
<h3 class="wp-block-heading">Learn more</h3>
<p>Browse the<a href="https://hub.docker.com/mcp" rel="nofollow noopener" target="_blank"><strong> </strong><strong>MCP Catalog</strong></a>: Discover 200+ containerized, security-hardened MCP servers</p>
<p>Download the MCP Toolkit in<a href="https://www.docker.com/products/docker-desktop/"><strong> </strong><strong>Docker Desktop</strong></a>: Get immediate access to secure credential management and container isolation</p>
<p>Submit Your Server: Help build the secure, containerized MCP ecosystem.<a href="https://github.com/docker/mcp-registry" rel="nofollow noopener" target="_blank"> Check our submission guidelines</a> for more.</p>
<p>Follow Our Progress:<a href="https://github.com/docker/mcp-gateway" rel="nofollow noopener" target="_blank"> Star our repository</a> for the latest security updates and threat intelligence</p>
The Nine Rules of AI PoC Success: How to Build Demos That Actually Ship
https://www.docker.com/blog/ai-poc-success-rules/
Jim Clark
Mon, 15 Sep 2025 13:00:00 +0000
Engineering
AI/ML
developers
productivity
https://www.docker.com/?p=76416
That study claiming "95% of AI POCs fail" has been making the rounds. It's clickbait nonsense, and frankly, it's not helping anyone. The real number? Nobody knows, because nobody's tracking it properly. But here's what I do know after years of watching teams build AI systems: the study masks a much more important problem. Teams...
<p>That study claiming &#8220;95% of AI POCs fail&#8221; has been making the rounds. It&#8217;s clickbait nonsense, and frankly, it&#8217;s not helping anyone. The real number? Nobody knows, because nobody&#8217;s tracking it properly. But here&#8217;s what I do know after years of watching teams build AI systems: the study masks a much more important problem.<br></p>
<p><strong>Teams are confused about how to design POCs that survive beyond the demo stage. There is no playbook.</strong></p>
<p></p>
<p>Most AI POCs die because they were designed to die. They&#8217;re built as disposable demos, optimized for executive presentations rather than production reality. They burn through cloud credits, rely on perfect conditions and perfectly structured data, and quickly collapse when real users start to touch them. If they don’t collapse then, often under scale they collapse when the design problems emerge under strain, leading to more serious failure.</p>
<p></p>
<p>But it doesn&#8217;t have to be this way.&nbsp;</p>
<p></p>
<p>After watching hundreds of AI projects at Docker and beyond, I&#8217;ve seen the patterns that separate the 5% that make it from the 95% that don&#8217;t. Here&#8217;s the playbook I wish every platform and MLOps team had from day one.</p>
<p></p>
<h2 class="wp-block-heading"><strong>The New Foundation: Remocal Workflows</strong></h2>
<p></p>
<p>Before we dive into the rules, let&#8217;s talk about the biggest shift in how successful teams approach AI development: <strong>remocal workflows</strong> (remote + local).</p>
<p></p>
<p>Running AI locally isn&#8217;t just about saving money—though it absolutely does that. It&#8217;s about maintaining developer velocity and avoiding the demo theater trap. Here&#8217;s how the best teams structure their work:</p>
<p></p>
<ul class="wp-block-list">
<li><strong>Test locally on laptops</strong> for fast iteration. No waiting for cloud resources, no surprise bills, no network latency killing your flow. The nature of building with AI should be making the process feel very interactive.</li>
<li><strong>Burst to remote resources</strong> for scale testing, production-like validation, or when you actually need those H100s. It should feel easy to move AI workloads around.</li>
<li><strong>Keep costs transparent</strong> from day one. You know exactly what each experiment costs because you&#8217;re only paying for remote compute when you choose to.</li>
</ul>
<p></p>
<p>POCs that incorporate this pattern from day zero avoid both runaway bills and the classic &#8220;it worked in the demo&#8221; disaster. They&#8217;re grounded in reality because they&#8217;re built with production constraints baked in.</p>
<p></p>
<h2 class="wp-block-heading"><strong>The Nine Rules of POC Survival</strong></h2>
<p></p>
<h3 class="wp-block-heading"><strong>1. Start Small, Stay Small</strong></h3>
<p></p>
<p>Your first instinct is wrong. You don&#8217;t need the biggest model, the complete dataset, or every possible feature. Bite-sized everything: models that fit on a laptop, datasets you can actually inspect, and scope narrow enough that you can explain the value in one sentence.</p>
<p></p>
<p>Early wins compound trust. A small thing that works beats a big thing that might work.</p>
<p></p>
<h3 class="wp-block-heading"><strong>2. Design for Production from Day Zero</strong></h3>
<p></p>
<p>Logging, monitoring, versioning, and guardrails aren&#8217;t &#8220;nice to haves&#8221; you add later. They&#8217;re the foundation that determines whether your POC can grow up to be a real system.</p>
<p></p>
<p>If your POC doesn&#8217;t have structured logging and basic metrics – observability –&nbsp; from the first commit, you&#8217;re building a disposable demo, not a prototype of a production system.</p>
<p></p>
<h3 class="wp-block-heading"><strong>3. Optimize for Repeatability and Model Improvement, Not Novelty</strong></h3>
<p></p>
<p>Infrastructure should be templated. Prompt testing should be in CI/CD. Model comparisons should be apples-to-apples benchmarks, not &#8220;it felt better this time.&#8221; What’s more, POC designs can and should assume existing model families will continue to rapidly improve. That includes larger context windows, greater accuracy, lower latency and smaller resource consumption.</p>
<p></p>
<p>The sexiest part of AI isn&#8217;t the novel algorithm—it&#8217;s how we&#8217;re learning to frame problems in ways&nbsp; that make AI more reliable at scale.</p>
<p></p>
<h3 class="wp-block-heading"><strong>4. Think in Feedback Loops</strong></h3>
<p></p>
<p>This is the big one that separates amateur hour from production-ready systems. Separate your non-deterministic AI components from your deterministic business logic. Build in layers of control and validation. Domain knowledge is still your magic ingredient.</p>
<p></p>
<p>In a remocal setup, this becomes natural: your agent loops can run locally for fast iteration, while tool execution and heavy compute burst to remote resources only when needed. You get reliability from layered control, not from hoping your model has a good day.</p>
<p></p>
<h3 class="wp-block-heading"><strong>5. Solve Pain, Not Impress</strong></h3>
<p></p>
<p>Anchor everything to measurable business pain. Real users with real problems they&#8217;re willing to pay to solve. If your POC&#8217;s main value proposition is &#8220;look how cool this is,&#8221; you&#8217;re building the wrong thing.</p>
<p></p>
<p>Kill the vanity demos that only look good in slideware. Build the boring solutions that save people actual time and money.</p>
<p></p>
<h3 class="wp-block-heading"><strong><strong>6. Embed Cost and Risk Awareness Early</strong></strong></h3>
<p></p>
<p>Track unit economics from day one. What does each request cost? Each user? Each workflow?</p>
<p></p>
<p>Benchmark small vs. large models. Cloud vs. local execution. Know your trade-offs with real numbers, not hand-waving about &#8220;cloud scale.&#8221;</p>
<p></p>
<h3 class="wp-block-heading"><strong>7. Make Ownership Clear</strong></h3>
<p></p>
<p>Who owns this thing when it breaks at 2 AM? What are the SLAs? Who&#8217;s responsible for retraining the model? Who pays for the compute?</p>
<p></p>
<p>Don&#8217;t let POCs drift in the organizational void between research labs and operations teams. Assign owners, responsibilities, and lifecycle management from day one.</p>
<p></p>
<h3 class="wp-block-heading"><strong>8. Control Costs Upfront</strong></h3>
<p></p>
<p>Transparent cost per request, user, and workflow. Hard budget caps and kill switches. No surprises in the monthly cloud bill.</p>
<p></p>
<p>Remocal workflows make this natural: you default to local execution and only burst remote when you consciously choose to spend money. Your costs are predictable because they&#8217;re intentional.</p>
<p></p>
<h3 class="wp-block-heading"><strong>9. Involve Users From Day Zero</strong></h3>
<p></p>
<p>Co-design with real users, not executives who saw a ChatGPT demo and want &#8220;AI for everything.&#8221; Measure adoption and time saved, not just accuracy scores.</p>
<p></p>
<p>The best AI POCs feel like natural extensions of existing workflows because they were built with the people who actually do the work.</p>
<p></p>
<h2 class="wp-block-heading"><strong>Why This Actually Matters</strong></h2>
<p></p>
<p>Most failed AI POCs never had a chance. They were too big, too expensive, too disconnected from real problems, and too optimized for demo day rather than daily use.</p>
<p>By flipping the script—starting small, designing for production, involving real users, and building on remocal workflows—you dramatically increase your odds of building something that ships and scales.</p>
<p></p>
<p>The difference between a successful AI POC and a failed one isn&#8217;t the sophistication of the model. It&#8217;s the boring engineering decisions you make on day zero.</p>
<p></p>
<p><strong>Stop treating AI POCs as disposable demos. Treat them as the first draft of a production system.</strong></p>
<p></p>
<p><em>Jim Clark is Principal Engineer for AI at Docker, where he helps teams build AI systems that actually make it to production. He&#8217;s spent the last decade watching the gap between AI demos and AI products, and occasionally bridging it.</em></p>
<p></p>
From Hallucinations to Prompt Injection: Securing AI Workflows at Runtime
https://www.docker.com/blog/secure-ai-agents-runtime-security/
Andy Ramirez
Wed, 10 Sep 2025 13:00:00 +0000
Products
AI/ML
Docker
security
https://www.docker.com/?p=76370
How developers are embedding runtime security to safely build with AI agents Introduction: When AI Workflows Become Attack Surfaces The AI tools we use today are powerful, but also unpredictable and exploitable. You prompt an LLM and it generates a Dockerfile. It looks correct. A shell script? Reasonable. You run it in dev. Then something...
<p><strong>How developers are embedding runtime security to safely build with AI agents</strong></p>
<h3 class="wp-block-heading"><br><strong>Introduction: When AI Workflows Become Attack Surfaces</strong></h3>
<p></p>
<p>The AI tools we use today are powerful, but also unpredictable and exploitable.</p>
<p></p>
<p>You prompt an LLM and it generates a Dockerfile. It looks correct. A shell script? Reasonable. You run it in dev. Then something breaks: a volume is deleted. A credential leaks into a log. An outbound request hits a production API. Nothing in your CI pipeline flagged it, because the risk only became real <em>at runtime</em>.</p>
<p></p>
<p>This is the new reality of AI-native development: fast-moving code, uncertain behavior, and an expanding attack surface.</p>
<p></p>
<p>Hallucinations in LLM output are only part of the story. As developers build increasingly autonomous agentic tools, they’re also exposed to <strong>prompt injection</strong>, <strong>jailbreaks</strong>, and <strong>deliberate misuse</strong> of model outputs by adversaries. A malicious user, through a cleverly crafted input, can hijack an AI agent and cause it to modify files, exfiltrate secrets, or run unauthorized commands.</p>
<p></p>
<p>In one recent case, a developer ran an LLM-generated script that silently deleted a production database, an issue that went undetected until customer data was already lost. In another, an internal AI assistant was prompted to upload sensitive internal documents to an external file-sharing site, triggered entirely through user input.</p>
<p></p>
<p>These failures weren’t caught in static analysis, code review, or CI. They surfaced only when the code <em>ran</em>.</p>
<p></p>
<p>In this post, we’ll explore how developers are addressing both accidental failures and intentional threats by shifting runtime security into the development loop, embedding observability, policy enforcement, and threat detection directly into their workflows using Docker.</p>
<p></p>
<h3 class="wp-block-heading"><strong>The Hidden Risks of AI-Generated Code</strong></h3>
<p></p>
<p>LLMs and AI agents are great at generating text, but they don’t always know what they’re doing. Whether you&#8217;re using GitHub Copilot, LangChain, or building with OpenAI APIs, your generated outputs might include:</p>
<p></p>
<ul class="wp-block-list">
<li>Shell scripts that escalate privileges or misconfigure file systems<br></li>
<li>Dockerfiles that expose unnecessary ports or install outdated packages<br></li>
<li>Infra-as-code templates that connect to production services by default<br></li>
<li>Hardcoded credentials or tokens hidden deep in the output<br></li>
<li>Command sequences that behave differently depending on the context<br></li>
</ul>
<p>The problem is compounded when teams start running autonomous agents, AI tools designed to take actions, not just suggest code. These agents can:</p>
<p></p>
<ul class="wp-block-list">
<li>Execute file writes and deletions<br></li>
<li>Make outbound API calls<br></li>
<li>Spin up or destroy containers<br></li>
<li>Alter configuration state mid-execution<br></li>
<li>Execute dangerous database queries<br></li>
</ul>
<p>These risks only surface at runtime, after your build has passed and your pipeline has shipped. And that’s a problem developers are increasingly solving inside the dev loop.</p>
<p></p>
<h3 class="wp-block-heading"><strong>Why Runtime Security Belongs in the Developer Workflow</strong></h3>
<p></p>
<p>Traditional security tooling focuses on build-time checks, SAST, SCA, linters, compliance scanners. These are essential, but they don’t protect you from what AI-generated agents do at execution time.</p>
<p>Developers need runtime security that fits their workflow, not a blocker added later.</p>
<p></p>
<p><strong>What runtime security enables:</strong></p>
<p></p>
<ul class="wp-block-list">
<li>Live detection of dangerous system calls or file access<br></li>
<li>Policy enforcement when an agent attempts unauthorized actions<br></li>
<li>Observability into AI-generated code behavior in real environments<br></li>
<li>Isolation of high-risk executions in containerized sandboxes<br></li>
</ul>
<p><strong>Why it matters:</strong></p>
<p></p>
<ul class="wp-block-list">
<li>Faster feedback loops: See issues before your CI/CD fails<br></li>
<li>Reduced incident risk: Catch privilege escalation, data exposure, or network calls early<br></li>
<li>Higher confidence: Ship LLM-generated code without guesswork<br></li>
<li>Secure experimentation: Enable safe iteration without slowing down teams<br></li>
</ul>
<p><strong>Developer ROI:</strong> Catching a misconfigured agent in dev avoids hours of triage and mitigates production risk and reputation risk; saving time, cost, and compliance exposure.</p>
<p></p>
<h3 class="wp-block-heading"><strong>Building Safer AI Workflows with Docker</strong></h3>
<p>Docker provides the building blocks to develop, test, and secure modern agentic applications:</p>
<p></p>
<ul class="wp-block-list">
<li><strong>Docker Desktop</strong> gives you an isolated, local runtime for testing unsafe code<br></li>
<li><strong>Docker Hardened Images.</strong> Secure, minimal, production-ready images<br></li>
<li><strong>Docker Scout</strong> scans container images for vulnerabilities and misconfigurations<br></li>
<li><strong>Runtime policy enforcement</strong> (with upcoming MCP Defender integration) provides live detection and guardrails while code executes<br></li>
</ul>
<h3 class="wp-block-heading"><strong>Step-by-Step: Safely Test AI-Generated Scripts</strong></h3>
<p></p>
<p><strong>1. Run your agent or script in a hardened container</strong></p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
docker run --rm -it \
--security-opt seccomp=default.json \
--cap-drop=ALL \
-v $(pwd):/workspace \
python:3.11-slim
</pre></div>
<p></p>
<ul class="wp-block-list">
<li>Applies syscall restrictions and drops unnecessary capabilities<br></li>
<li>Runs with no persistent volume changes<br></li>
<li>Enables safe, repeatable testing of LLM output</li>
</ul>
<p></p>
<p><strong>2. Scan the container with Docker Scout</strong></p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
docker scout cves my-agent:latest
</pre></div>
<ul class="wp-block-list">
<li>Surfaces known CVEs and outdated dependencies<br></li>
<li>Detects unsafe base images or misconfigured package installs<br></li>
<li>Available both locally and inside CI/CD workflows</li>
</ul>
<p></p>
<p><strong>3. Add runtime policy (beta) to block unsafe behavior</strong></p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
scout policy add deny-external-network \
--rule &quot;deny outbound to *&quot;
</pre></div>
<p>This would catch an AI agent that unknowingly makes an outbound request to an internal system, third-party API, or external data store.</p>
<p></p>
<p><strong>Note:</strong> Runtime policy enforcement in Docker Scout is currently in development. CLI and behavior may change upon release.</p>
<p></p>
<p><strong>Best Practices for Securing AI Agent Containers</strong></p>
<p></p>
<p></p>
<p></p>
<div style="--row-column-count: 2;" class="wp-block-ponyo-table style__default">
<table class="responsive-table">
<tbody class="wp-block-ponyo-table-body">
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span><strong>Practice</strong></span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span><strong>Why it matters</strong></span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Use slim, verified base images</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Minimizes attack surface and dependency drift</span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Avoid downloading from unverified sources</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Prevents LLMs from introducing shadow dependencies</span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Use </span><span>.dockerignore</span><span> and secrets management</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Keeps secrets out of containers</span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Run containers with dropped capabilities</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Limits impact of unexpected commands</span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Apply runtime seccomp profiles</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Enforces syscall-level sandboxing</span></p>
</td>
</tr>
<tr class="wp-block-ponyo-table-row">
<td class="wp-block-ponyo-cell">
<p><span>Log agent behavior for analysis</span></p>
</td>
<td class="wp-block-ponyo-cell">
<p><span>Builds observability into experimentation</span></p>
</td>
</tr>
</tbody>
</table>
</div>
<p></p>
<p><strong>Integrating Into Your Cloud-Native Workflow</strong></p>
<p></p>
<p>Runtime security for AI tools isn’t just for local testing, it fits cleanly into cloud-native and CI/CD workflows too.</p>
<p></p>
<p><strong>GitHub Actions Integration Example:</strong></p>
<p></p>
<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
jobs:
security-scan:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v3
- name: Build container
run: docker build -t my-agent:latest .
- name: Scan for CVEs
run: docker scout cves my-agent:latest
</pre></div>
<p></p>
<p><strong>Works across environments:</strong></p>
<p></p>
<ul class="wp-block-list">
<li>Local dev via Docker Desktop<br></li>
<li>Remote CI/CD via GitHub Actions, GitLab, Jenkins<br></li>
<li>Kubernetes staging environments with policy enforcement and agent isolation<br></li>
<li>Cloud Development Environments (CDEs) with Docker + secure agent sandboxes<br></li>
</ul>
<p>Dev teams using ephemeral workspaces and Docker containers in cloud IDEs or CDEs can now enforce the same policies across local and cloud environments.</p>
<p></p>
<h3 class="wp-block-heading"><strong>Real-World Example: AI-Generated Infra Gone Wrong</strong></h3>
<p></p>
<p>A platform team uses an LLM agent to auto-generate Kubernetes deployment templates. A developer reviews the YAML and merges it. The agent-generated config opens an internal-only service to the internet via <code>LoadBalancer</code>. The CI pipeline passes. The deploy works. But a customer database is now exposed.</p>
<p></p>
<p>Had the developer run this template inside a containerized sandbox with outbound policy rules, the attempt to expose the service would have triggered an alert, and the policy would have prevented escalation.</p>
<p></p>
<p><strong>Lesson:</strong> You can’t rely on static review alone. You need to see what AI-generated code <em>does</em>, not just what it looks like.</p>
<p></p>
<h3 class="wp-block-heading"><strong>Why This Matters: Secure-by-Default for AI-Native Dev Teams</strong></h3>
<p></p>
<p>As LLM-powered tools evolve from suggestion to action, runtime safety becomes a baseline requirement, not an optional add-on.</p>
<p>The future of secure AI development starts in the inner loop, with runtime policies, observability, and smart defaults that don’t slow you down.</p>
<p>Docker’s platform gives you:</p>
<ul class="wp-block-list">
<li>Developer-first workflows with built-in security<br></li>
<li>Runtime enforcement to catch AI mistakes early<br></li>
<li>Toolchain integration across build, test, deploy<br></li>
<li>Cloud-native flexibility across local dev, CI/CD, and CDEs<br></li>
</ul>
<p>Whether you&#8217;re building AI-powered automations, agent-based platforms, or tools that generate infrastructure, you need a runtime layer that sees what AI can’t, and blocks what it shouldn’t do.</p>
<p></p>
<h3 class="wp-block-heading"><strong>What’s Next</strong></h3>
<p></p>
<p>Runtime protection is moving left, into your dev environment. With Docker, developers can:</p>
<ul class="wp-block-list">
<li>Run LLM-generated code in secure, ephemeral containers<br></li>
<li>Observe runtime behavior before pushing to CI<br></li>
<li>Enforce policies that prevent high-risk actions<br></li>
<li>Reduce the risk of silent security failures in AI-powered apps<br></li>
</ul>
<p>Docker is working to bring MCP Defender into our platform to provide this protection out-of-the-box, so hallucinations don’t turn into incidents.</p>
<p></p>
<h3 class="wp-block-heading"><strong>Ready to Secure Your AI Workflow?</strong></h3>
<p></p>
<ul class="wp-block-list">
<li>Sign up for early access to Docker’s runtime security capabilities<br></li>
<li>Watch our Tech Talk on “Building Safe AI Agents with Docker”<br></li>
<li>Explore Docker Scout for real-time vulnerability insights<br></li>
<li>Join the community conversation on Docker Community Slack or GitHub Discussions</li>
</ul>
<p></p>
<p>Let’s build fast, and safely.</p>
<p></p>
Docker Acquisition of MCP Defender Helps Meet Challenges of Securing the Agentic Future
https://www.docker.com/blog/docker-acquires-mcp-defender-ai-agent-security/
Andy Ramirez
Fri, 05 Sep 2025 13:00:00 +0000
Company
Agentic apps
Docker
MCP
https://www.docker.com/?p=76255
Docker, Inc.®, a provider of cloud-native and AI-native development tools, infrastructure, and services, today announced the acquisition of MCP Defender, a company founded to secure AI applications. The rapid evolution of AI-from simple generative models to powerful agentic tools-has transformed software development in extraordinary ways. But as with all powerful technologies, new capabilities bring new...
<p>Docker, Inc.®, a provider of cloud-native and AI-native development tools, infrastructure, and services, today announced the acquisition of MCP Defender, a company founded to secure AI applications.</p>
<p>The rapid evolution of AI-from simple generative models to powerful agentic tools-has transformed software development in extraordinary ways. But as with all powerful technologies, new capabilities bring new security challenges. <a href="https://www.docker.com/blog/mcp-security-issues-threatening-ai-infrastructure/">We recently highlighted critical MCP security issues on the Docker blog</a>, emphasizing how essential it is to secure our emerging AI infrastructure. Building on that discussion, we want to offer our perspective on the current state of AI security, outline its trajectory, consider what this means for organizations developing AI agents and tools, and explore Docker&#8217;s vision for securely empowering these new AI workloads.</p>
<p></p>
<p>Today’s AI security landscape mirrors the early days of container adoption: rapid innovation, widespread enthusiasm, but significant uncertainty around risks. AI agents now routinely execute critical tasks &#8211; from automated code generation and system administration to customer interaction &#8211; often interfacing directly with sensitive data and critical infrastructure. The security stakes have never been higher.</p>
<p></p>
<p>Looking ahead, securing AI infrastructure will require a significant shift towards runtime monitoring, real-time threat detection, and continuous security evaluation. Organizations will increasingly adopt tools designed specifically to detect and respond dynamically to threats occurring at runtime. Instead of relying solely on preventative measures, security strategies will embrace active monitoring and intelligent automation.</p>
<p></p>
<p>For companies developing AI agents and MCP tools, these security shifts are profound. Security can no longer be a late-stage consideration-it must be embedded from the earliest design phase. These solutions must transparently enforce policies, providing clear guardrails that reduce the cognitive load on development teams. Security for AI agents should be frictionless &#8211; built seamlessly into the workflows developers already use every day.</p>
<p></p>
<p>Docker’s mission has always been to simplify application development while ensuring security and portability. Extending that mission to agentic AI means integrating security deeply into the infrastructure itself. Docker’s vision is clear: secure-by-default AI infrastructure where every interaction is automatically verified, every threat proactively detected, and every policy transparently enforced. Docker’s commitment to security extends beyond AI, with products such as Docker Scout and Docker Hardened Images.</p>
<p></p>