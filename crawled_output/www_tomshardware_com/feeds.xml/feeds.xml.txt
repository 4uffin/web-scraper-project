<![CDATA[ Latest from Tom's Hardware ]]>
https://www.tomshardware.com
All the latest content from the Tom's Hardware team
Fri, 19 Sep 2025 21:51:04 +0000
en
<![CDATA[ Microsoft hikes Xbox Series X price, again, to $649 — second price increase of 2025 comes as shifting tariffs continue to plague tech prices ]]>
<p>Microsoft <a data-analytics-id="inline-link" href="https://support.xbox.com/en-US/help/hardware-network/console/may-2025-pricing-updates">has announced</a> a new price increase for its Xbox Series X and S consoles coming on October 3, a scant five months after the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/xbox/microsoft-hikes-prices-of-xbox-consoles-controllers-headsets-and-games-worldwide-cites-market-conditions-and-price-of-development">previous hike in May</a>. The already-pricey Xbox Series X now commands $649.99, the Series S goes for $399.99 (with 512 GB of storage), and the capacious Series X 2 TB Galaxy Black Special Edition has a price tag as long as its name, at $799.99.</p><p>However, this time around, things are a little different. The sting is pointed only at U.S. customers, and mercifully covers only the consoles. This is in contrast to the May increase, when Microsoft raised prices on its console wares across the board, including accessories and games, and applied them worldwide.</p><div ><table><caption>Revised Xbox Series U.S. pricing</caption><thead><tr><th class="firstcol " ><p>Model</p></th><th
><p>New price</p></th><th
><p>Old price</p></th><th
><p>Difference</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Xbox Series X</p></td><td
><p>$649.99</p></td><td
><p>$599.99</p></td><td
><p>$50</p></td></tr><tr><td class="firstcol " ><p>Xbox Series X Digital</p></td><td
><p>$599.99</p></td><td
><p>$549.99</p></td><td
><p>$50</p></td></tr><tr><td class="firstcol " ><p>Xbox Series S 1 TB</p></td><td
><p>$449.99</p></td><td
><p>$429.99</p></td><td
><p>$20</p></td></tr><tr><td class="firstcol " ><p>Xbox Series S 512 GB</p></td><td
><p>$399.99</p></td><td
><p>$379.99</p></td><td
><p>$20</p></td></tr><tr><td class="firstcol " ><p>Xbox Series X 2 TB Galaxy Black Special Edition</p></td><td
><p>$799.99</p></td><td
><p>$729.99</p></td><td
><p>$70</p></td></tr></tbody></table></div><p>Predictably, Microsoft pins the blame for the increase on "changes in the macroeconomics environment", corporate-speak that likely means tariffs. While that might be a decent justification in of itself, some would argue that it's a lack of forethought and/or poor optics to have price increases twice in the same year for similar reasons.</p><p>Microsoft might be simply following the market on the heels of Sony, which <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/console-gaming/sony-hikes-ps5-prices-by-usd50-starting-tomorrow-sony-adds-up-to-10-percent-to-the-price-of-every-model-from-august-21">raised prices</a> for its PlayStation 5 lineup by $50 roughly a month ago. As it stands, the standard PlayStation 5 currently sits at $549.99, undercutting the Xbox Series X by a solid $100. That's quite the bitter pill for prospective Halo gamers to swallow, and just like Microsoft's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/microsoft-follows-nvidias-lead-surpasses-usd4-trillion-market-capitalization-on-soaring-demand-for-cloud-services-multi-front-ai-endeavors">Q4 revenue</a>, it's more than a little rich in light of PlayStation having a lion's share of the market.</p><p>If that wasn't enough, the time is ripe for affordable PCs, and some napkin math puts a machine equal or better to the Xbox Series X at around the $600 mark, give or take, as pricing winds allow. Given how hard Microsoft is pushing its cross-platform Xbox Game Pass, it's become harder still to justify the price of a Series X.</p><p>Back in August, <a data-analytics-id="inline-link" href="https://www.sony.com/en/SonyInfo/IR/library/presen/er/pdf/25q1_supplement.pdf">Sony reported</a> that it had sold 80.3 million PlayStation 5 consoles. There are no sales figures for the Xbox Series consoles, but if Take-Two's <a data-analytics-id="inline-link" href="https://ir.take2games.com/static-files/e8841ad4-15c4-4572-b4c4-0df5548df6ba">financial report</a> from May is to be believed, a total of around 107 million ninth-generation consoles have been sold. Simple math would put the Xbox Series offerings at over 27 million.</p><p>Considering the elapsed time since then and assuming a very generous 30 million, that works out to 62.6% versus 37.3% for Sony. Good thing Microsoft's services are raking in cash hand over fist.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/xbox/microsoft-hikes-xbox-series-x-price-again-to-usd649-second-price-increase-of-2025-comes-as-shifting-tariffs-continue-to-plague-tech-prices
Microsoft jacked up the price of its Xbox Series lineup by $20 to $50, placing the Series X at a cool $649.99.
ejyK9BxYGmPZjy3quMm5JY
Fri, 19 Sep 2025 21:51:04 +0000 Xbox
Video Games
Console Gaming
Bruno Ferreira
Getty Images/Future Publishing
Xbox Series X and PS5 next to each other on a desk.
Xbox Series X and PS5 next to each other on a desk.
<![CDATA[ OpenAI is reportedly poaching Apple talent to build its first consumer hardware device — could potentially be a smart speaker ]]>
<p>OpenAI has signed a manufacturing agreement with Luxshare to build its first consumer hardware product, according to a new report from<em> </em><a data-analytics-id="inline-link" href="https://www.theinformation.com/articles/openai-raids-apple-hardware-talent-manufacturing-partners"><em>The Information</em></a>. The device — described by suppliers as something that resembles a screenless smart speaker (in other words, an Alexa-adjacent device) — is part of a broader <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/jony-ive-confirms-he-is-working-on-an-openai-hardware-design-project">push into hardware</a> that also includes exploratory work on glasses, a wearable pin, and a digital voice recorder. Sources familiar with the roadmap say OpenAI is targeting a late 2026 or early 2027 launch window.</p><p>Luxshare isn't the only Apple supplier involved. Goertek, which builds AirPods, HomePods, and Apple Watch components, has been approached to provide speaker modules for OpenAI's first product. The company is also said to be engaging with other Chinese supply chain partners Apple has spent years cultivating, including those in tooling and precision audio.</p><p>According to <em>The Register</em>, the hardware project is being led by Tang Tan, who left Apple in 2023 after a 25-year tenure that included design oversight of various flagship products. Tan now serves as OpenAI's chief hardware officer and reports directly to CEO Sam Altman. In May, OpenAI acquired io Products — the hardware design studio Tan co-founded with Jony Ive — in a $6.5 billion deal backed by SoftBank. Court documents from an ongoing trademark dispute describe OpenAI's goal as building a family of AI-native products under the io brand.</p><p>Since the io acquisition, OpenAI has stepped up recruiting across Apple's product and operations teams. At least 25 former Apple employees are understood to have joined the effort in 2025 alone, with backgrounds in human interface design, audio, wearables, and manufacturing scale-up. Among them are senior figures like Cyrus Daniel Irani, a longtime Siri interface designer, and Matt Theobald, who spent 17 years on Apple's manufacturing design team. Others include engineers from Apple's camera and Watch divisions. Most hires have joined Tan's org inside OpenAI, though some remain associated with LoveFrom, Ive's separate design firm.</p><p>The poaching of talent could complicate the company's relationship with Apple, which, since 2024, has integrated OpenAI's models into Siri and iOS. That partnership now runs parallel to a hardware effort built by Apple's own former talent and vendors.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/openai-reportedly-poaching-apple-talent-to-build-first-consumer-device
OpenAI has signed a manufacturing agreement with Luxshare to build its first consumer hardware product, according to a report by The Information on September 19.
6QHMYhS7oHjK3kraMVXRrX
Fri, 19 Sep 2025 20:50:46 +0000 Tech Industry
lukejamesalden@gmail.com (Luke James)
Luke James
Getty Images
ChatGPT
ChatGPT
<![CDATA[ How often should you expect to replace your headset, keyboard, or mouse — buying tips for picking longer-lasting gear ]]>
<p>A couple of weeks ago, my colleague Matt Safford showed up to our morning meeting wearing half a headset. "Guess what," he announced. "My headset — the one that I <em>only </em>use for these meetings — just snapped in half. While I was wearing it."</p><p>I asked if he'd been putting it on, or fiddling with the adjustments, and he confirmed that he wasn't touching it — it was on his head and it suddenly felt loose, and when he reached up to see what was going on, he discovered it was in two pieces. RIP, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/razer-barracuda-pro"><u>Razer Barracuda Pro</u></a> — the included rigid carrying case isn't quite as much of a benefit if the headset is so fragile it's going to snap in half while it's being used exactly as intended.</p><p>This isn't the first time one of us has shown up to the morning meeting in need of a new peripheral, whether it's a headset, keyboard, mouse, or, I don't know... a chair or a desk. And peripherals, especially gaming peripherals, aren't meant to last forever — they wear out (even if you're not a gamer who's prone to fits of rage), physically, and they become dated, technologically. At the rate gaming companies update their lineups, it seems like you should be shopping for new accessories every 2 - 3 years. Of course, gaming companies want to sell you stuff, so that's not a great way to estimate the expected lifespan of your gear.</p><p>But you might be wondering just how long your peripherals <em>should </em>last, assuming you're not a competitive eSports athlete or a gaming peripherals editor.</p><h2 id="headsets-2">Headsets</h2><p>Matt's Razer Barracuda lasted about 2.5 years, which is definitely shorter than you should expect a gaming headset to last (and you shouldn't expect any gaming headset to snap at the hinge while you're just... wearing it). But the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/best-gaming-headsets"><u>best gaming headsets</u></a> won't necessarily last forever, depending on a few factors.</p><p>Wireless headsets tend to lose their steam faster because the battery degrades — but you should still be able to get a solid 3 - 4 years out of a wireless headset, if not longer. Wired headsets can last significantly longer if they're well-built, but most gaming headsets are... not that well-built. Still, a decent wired headset should get you a solid 5 - 6 years of use — longer if it has replaceable parts.</p><p>Here's what you should look for if you're looking for a headset that won't snap in half:</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="8b8G6LNTB4NCuBRpNF4SfW" name="IMG_7090.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/8b8G6LNTB4NCuBRpNF4SfW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Asus ROG Delta II comes with replaceable earpads</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="gV3EfRaSYiaMuVn6wAuNkW" name="IMG_3155.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/gV3EfRaSYiaMuVn6wAuNkW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Logitech's G Pro X 2 Lightspeed also comes with replaceable earpads.</p><p><strong>Replaceable earpads:</strong> <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/corsair-virtuoso-rgb-wireless-xt"><u>Corsair's Virtuoso RGB Wireless XT</u></a> has long been a favorite among Tom's Hardware editors. The only gripe? The earpads, which are leatherette-covered memory foam, wear out fairly quickly and are difficult to replace — mostly because Corsair didn't sell them for a long time (though I believe they do, now). Earpads tend to wear out after 1 - 2 years of regular use (or sooner, depending on what they're made of and whether you sweat), so you should look for a gaming headset with replaceable earpads. Some gaming headsets, like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/logitech-g-pro-x-2-lightspeed-review"><u>Logitech's G Pro X 2 Lightspeed</u></a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/asus-rog-delta-ii-review"><u>Asus' ROG Delta II</u></a>, even come with alternate / replacement earpads in the box.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="XkkQLhpij7eYkPXrQjyT6W" name="IMG_3928.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/XkkQLhpij7eYkPXrQjyT6W.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="XrDh29isMZAEmscxzoaFMW" name="IMG_3927.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/XrDh29isMZAEmscxzoaFMW.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Beyerdynamic's MMX 300 Pro features snap-off headband padding.</p><p><strong>Replaceable headband:</strong> Headsets' headband padding also wears out over time (though not as quickly as earpads). A lot of gaming headsets have built-in padding, but we're starting to see more replaceable options — such as the replaceable padding on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/corsair-void-wireless-v2-review"><u>Corsair Void Wireless v2</u></a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/beyerdynamic-mmx-300-pro-review"><u>Beyerdynamic MMX 300 Pro</u></a>. You'll also find headsets with alternate headband options, such as the leather strap on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/audeze-maxwell"><u>Audeze Maxwell</u></a> or the "ski-band" strap on <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/steelseries-arctis-nova-5-wireless-review"><u>SteelSeries Arctis Nova</u></a> series.</p><p><strong>Analog connection:</strong> Wireless technology and batteries can fail, but a wired analog connection will basically always work. You might think an analog connection is a given on all headsets, but we're seeing more and more wireless headsets ditch this option completely. Still, there are some, like the Asus ROG Delta II and Logitech G Pro X 2 Lightspeed, that still come with wires.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="KNB86TAg6r93jK42BVGzAW" name="IMG_3396" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/KNB86TAg6r93jK42BVGzAW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>HyperX's Cloud III S Wireless has sturdy aluminum forks.</p><p><strong>Not plastic:</strong> Well, not <em>all </em>plastic. Ultra lightweight headsets like the Razer Barracuda and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/turtle-beach-atlas-air-review"><u>Turtle Beach's Atlas Air</u></a> are comfortable and, well, lightweight, because they're almost entirely plastic. And while this is great for comfort, it's not so great for durability — if you're looking for a headset that will last, look for one with a well-built metal headband, like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/steelseries-arctis-nova-pro-wireless-headset"><u>SteelSeries' Arctis Nova Pro</u></a>, and sturdy hinges or forks, like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-headsets/hyperx-cloud-iii-s-wireless-review"><u>HyperX Cloud III S Wireless</u></a>.</p><h2 id="keyboards-2">Keyboards</h2><p>A <a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/rama-works-u80-a-custom-keyboard"><u>well-built mechanical keyboard</u></a> can last forever — well,
maybe not <em>forever, </em>but at least a few decades (the IBM Model M is basically older than the internet). But while the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/best-gaming-keyboards"><u>best gaming keyboards</u></a> today are mechanical, they're probably not going to last decades (maybe <em>a </em>decade). I tend to refresh my keyboard every 4 - 5 years, but that's usually because it just feels like it's time for an update (or upgrade) — not because the keyboard itself is unusable.</p><p>Here's what you should look for if you're after a keyboard that will last more than a couple of years:</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="AfJRkCSnvDnXD8hqR6BdgW" name="IMG_3134" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/AfJRkCSnvDnXD8hqR6BdgW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Asus' ROG Strix Scope II 96 Wireless' keycaps are still going strong, shine-free.</p><p><strong>Double-shot PBT keycaps:</strong> ABS keycaps, which are prone to shine, are common on mainstream gaming keyboards, though we've seen double-shot PBT keycaps on several premium gaming keyboards, such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/asus-rog-strix-scope-ii-96-wireless"><u>Asus ROG Strix Scope II 96 Wireless</u></a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/logitech-g915-x-tkl-review"><u>Logitech G915 X TKL</u></a>.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="5iwRdx2XHrbstKycvuktAW" name="IMG_4394.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/5iwRdx2XHrbstKycvuktAW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Glorious' GMMK 3 Pro series has an optional hot-swappable PCB that accepts both magnetic and mechanical switches.</p><p><strong>Hot-swappable switches:</strong> Many gaming companies have keyboards with hot-swappable switches, but they're not all the same. Some keyboards, such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/asus-rog-azoth-x-review"><u>Asus ROG Azoth X</u></a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/lemokey-l3"><u>Lemokey L3</u></a>, have standard hot-swappable PCBs that accept any and all mechanical switches. Keyboards with magnetic (Hall Effect) switches are often hot-swappable, but most (like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/mechanical-keyboards/arbiter-studio-polar-75-pro-review"><u>Arbiter Studio's Polar 75 Pro</u></a>) are only compatible with certain magnetic switches, but <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/glorious-gmmk-3-review-customized-mainstream-gaming"><u>Glorious' GMMK 3</u></a> series does offer a hot-swappable PCB that works with both mechanical and magnetic switches.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="J6BynJdUA3LwsYevNbFVPW" name="IMG_2845" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/J6BynJdUA3LwsYevNbFVPW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The Meletrix Boog75's aluminum case isn't going anywhere, at least.</p><p><strong>A solid aluminum build: </strong>You don't <em>need </em>a keyboard with a solid aluminum case like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/meletrix-boog75-review"><u>Meletrix Boog75</u></a> or the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/keychron-q1-he-review"><u>Keychron Q1 HE</u></a>, but a better build will probably lend itself to a longer lifespan, though perhaps not for the reasons you think (after all, keyboards don't really move much). But aluminum keyboards tend to be better built overall and better able to handle aggressive typing or gaming.</p><h2 id="mice-2">Mice</h2><p><a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-gaming-mouse"><u>Gaming mice</u></a> tend to follow a pretty regular update cycle of about 1.5 - 2.5 years, and this is the one category where the update cycle isn't a terrible indicator of expected lifespan. Your mouse gets a lot more wear than your keyboard does, because it has significantly fewer buttons and it's always moving. (Also, if you are the type of gamer who gets heated, your mouse is probably the easiest thing to slam down on your desk or throw across the room.) Mouse switches and keyboard switches have similar click ratings, but you'll reach that number a lot quicker when you only have two buttons to choose from instead of 88 or 104 (or however many keys there are on your keyboard). That said, while updating your mouse every 2 - 3 years isn't crazy, you should still be able to stretch a gaming mouse's lifespan to 3 - 4 years as long as you're not excessively slamming or squeezing it.</p><p>But you still want your gaming mouse to last the full 2 - 3 years, so here's what to look for:</p><p><strong>Solid build: </strong>This might seem like a no-brainer, but it's still worth mentioning — if you want your mouse to last, it needs to be well-built and sturdy-feeling right out of the box. If the sides are flexing or creaking, or the scroll wheel feels indecisive, or the switches are mushy... it only gets worse, not better.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="wzAJ6EW7ybqVksm59kRtpW" name="IMG_9931" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/wzAJ6EW7ybqVksm59kRtpW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>For wired mice, a drag-free cable like the one on HyperX's Pulsefire Haste 2 is essential</p><p><strong>Drag-free or detachable cable:</strong> The cable can make or break a mouse's wired experience, and most wired mice come with their cable permanently attached. If you're shopping for a wired mouse, look for one with a lightweight, flexible, drag-free paracord cable, like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/hyperx-pulsefire-haste-2-its-fine-but-thats-it"><u>HyperX's Pulsefire Haste 2</u></a>, or one that has a detachable cable — like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/steelseries-aerox-3"><u>SteelSeries' Aerox</u></a> series.</p><p><strong>Onboard memory: </strong>You probably do want a gaming mouse that's at least somewhat customizable, so you can adjust DPI steps and tweak various settings to calibrate the feel perfectly. But once that's set up, you're probably going to go back in and readjust things... never. While the execution of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-mice/cherry-xtrfy-m64-pro-wireless-mouse-review"><u>Cherry Xtrfy M64 Pro's</u></a> plug-and-play isn't perfect, the idea isn't a bad one — nobody wants a mouse that's forever tied to some clunky peripheral software that stops working whenever you actually try to use it. If you want your mouse to last through Synapse's 243 yearly updates, make sure it has several onboard profiles.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="dEVfN42PYKWX5mbSzYUckW" name="IMG_4581.JPEG" alt="How long should your peripherals last" src="https://cdn.mos.cms.futurecdn.net/dEVfN42PYKWX5mbSzYUckW.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The fancy carrying case and carbon fiber shell aren't going to help the Asus ROG Harpe Ace Extreme significantly outlive its competitors.</p><p><strong>Not made of carbon fiber:</strong> You don't want a mouse that flexes or creaks, but that doesn't mean you need one made of carbon fiber. The $250 <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/asus-rog-harpe-ace-extreme-review-47g-and-usd250"><u>Asus ROG Harpe Ace Extreme</u></a> is an impressive, ultra-lightweight mouse with a carbon fiber chassis (well, half carbon fiber), but it's going to be subject to the same problems that other gaming mice face as they age: switch issues, an outdated sensor, and connectivity and battery life issues. Splurging on a pricey gaming mouse made of carbon fiber or, I don't know, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/razer-showcases-24k-gold-viper-signature-mini-gaming-mouse"><u><em>gold</em></u></a><em> </em>isn't going to get you nearly as far as it will in other categories.</p><p>Of course, not all peripherals need regular updates. Audio equipment that isn't worn on your head, such as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-pc-speakers"><u>speakers</u></a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-microphones,6247.html"><u>microphones</u></a>, can easily last a decade or more — both physically and technologically. <a data-analytics-id="inline-link" href="https://www.logitech.com/en-us/shop/c/speakers"><u>Logitech's top-of-the-line 5.1 surround Z906 system</u></a>, for example, is almost 15 years old, while <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/rode-nt1-5th-generation-mic"><u>Rode's NT1 5th generation</u></a> got a tech-y update with USB input and 32-bit float audio recording, but is otherwise largely similar to the 34-year-old original NT1. And while the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-webcams"><u>best webcams</u></a> are all 4K resolution <em>now</em>, it took a pandemic to spur that leap in technology.</p>
https://www.tomshardware.com/peripherals/how-often-should-you-expect-to-replace-your-headset-keyboard-or-mouse-buying-tips-for-picking-longer-lasting-gear
How long should your gaming headset, keyboard, and mouse last before you should think about replacing them?
cAJb5fUVWbcan2v8bGdvSb
Fri, 19 Sep 2025 20:30:19 +0000 Peripherals
Sarah Jacobsson Purewal
Tom&#039;s Hardware
How long should your peripherals last
How long should your peripherals last
<![CDATA[ GPU sales skyrocketed 27% last quarter — tariff jitters sparked an odd gaming hardware spending surge in Q2 '25 ]]>
<p>The middle of the year is usually a sleepy season for PC hardware sales, and so summer quarter shipments tend to sag a bit as people hold off until holiday deals. Of course, 2025 has been anything but typical. Instead of the usual slowdown, <a data-analytics-id="inline-link" href="https://www.jonpeddie.com/news/q225-pc-graphics-add-in-board-shipments-increased-27-0-from-last-quarter/">the latest numbers</a> from Jon Peddie Research (JPR) show a surprising surge in CPU and GPU shipments. The culprit? Fear of tariffs, according to the analyst.</p><p>With <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/usd320-camera-lens-buyers-hit-with-usd2-000-delivery-fee-in-tariffs-fight-some-sellers-implement-exorbitant-shipping-costs-to-dissuade-us-customers">new U.S. import tariffs</a> hanging over tech imports, PC makers and consumers alike went into buy-ahead mode. The result was a markedly unseasonal rush on gaming hardware that pulled demand forward, creating what could be called a "panic‑build quarter" in the client PC industry.</p><p>The numbers tell the story, as CPU shipments were up about 8% quarter‑over‑quarter and 13% year‑over‑year. Desktop CPUs increased their share versus laptops by 9% to grab 33% of the market share, a healthy bump in a segment that's been <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/best-gaming-laptops">overshadowed by laptops</a> in recent years.</p><p>Total GPU shipments jumped 8.4% from last quarter, up to 74.7 million units. Nvidia scooped up more share at the expense of AMD and Intel, likely due in part to the superior availability of its GPUs. However, Intel still sells more GPUs than either of the other two combined, thanks to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/amd-vs-intel-cpus">its dominance</a> in the laptop CPU market.</p><p>The standout statistic was discrete desktop graphics cards, or add-in boards (AIBs), up a wild 27% quarter‑to‑quarter and 22% year‑over‑year, due in part to impressive new hardware from all three vendors. Nvidia's grip tightened further, rising to 94% of the AIB market <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-dominates-gpu-shipments-with-94-percent-share-shipment-surge-likely-caused-by-customers-getting-ahead-of-tariffs">as we previously reported</a>.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:889px;"><p class="vanilla-image-block" style="padding-top:67.94%;"><img id="EvZvMjWsaEq7qjPFMViRmn" name="PC AIB GPU shipments growth rate" alt="A graph depicting volatile AIB shipment growth over the last ten years." src="https://cdn.mos.cms.futurecdn.net/EvZvMjWsaEq7qjPFMViRmn.png" mos="" align="middle" fullscreen="1" width="889" height="604" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">PC GPU AIB shipments are highly volatile, but this continued growth for three quarters is abnormal. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Jon Peddie Research)</span></figcaption></figure><p>That's not normal for Q2. Usually, shipments drift lower before rebounding at year's end, but this time, tariffs flipped the script. Retailers and distributors didn't want to get caught flat-footed if <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/trump-announces-100-percent-semiconductor-tariffs-theres-no-charge-for-chips-built-in-the-u-s">regulations suddenly raised costs</a>, and enthusiasts picked up on the same cues. Nobody wanted to be the one paying 15–25% more for a graphics card a few months later, so they bought early, clearing stock and pushing prices higher at the high end.</p><p>JPR calls this "buying ahead of tariffs," and it comes with a warning: demand that gets pulled forward can leave a hole later. Q3 and Q4 might look weaker because so many buyers have already opened their wallets.</p><p>This wasn't just about economics, though; it was also about timing. Nvidia's new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-5060-is-up-to-25-percent-faster-than-rtx-4060-with-frame-generation-in-new-gpu-preview">entry- and mid-range</a> Blackwell cards, as well as AMD's new RDNA 4 GPUs, were filling shelves right as the tariff panic set in. Gamers who might have otherwise waited for the latest hardware to mature instead rushed to lock down fresh GPUs while they could.</p><p>Interestingly, Dr. Peddie notes that the midrange stayed relatively affordable as vendors leaned on it to keep shipments moving. Still, flagship GPUs saw rising prices and severe shortages, reflecting what we've seen with the GeForce RTX 5090, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-confirms-its-recently-delisted-rtx-50-series-founders-edition-cards-are-not-discontinued-limited-edition-products-were-removed-because-they-were-out-of-stock">which is unobtanium</a> (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/usd3-700-rtx-5090-gpus-have-found-new-homes-after-sitting-on-us-retailers-shelves">at least, at MSRP</a>). The psychology is easy to trace: better to grab a new graphics card today than risk paying even more tomorrow.</p><p>Trump's tariffs didn't just bend economic charts; they <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/newegg-blames-tariffs-for-rtx-5090-and-rtx-5080-price-hikes">bent gamer behavior</a>. The 2025 mid-year surge appears more like panic buying than organic growth. Whether the past quarter turns out to be a quirky blip or the start of a volatile cycle will depend on how hard the comedown hits in Q3.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/pc-gaming/gpu-sales-skyrocketed-27-percent-last-quarter-tariff-jitters-sparked-an-odd-gaming-hardware-spending-surge-in-q2-25
A report from Jon Peddie Research highlights an unusual PC hardware spending surge in the first half of 2025.
37tm9VSjaqQNBbNtpPRdHM
Fri, 19 Sep 2025 19:43:43 +0000 PC Gaming
Video Games
Zak Killian
Tom&#039;s Hardware
A photograph of Micro Center store shelves stocked with PC graphics cards.
A photograph of Micro Center store shelves stocked with PC graphics cards.
<![CDATA[ Building in the Thermaltake Tower 250 made me furious about one design flaw that could be easily solved — cramped cable cut-outs and inaccessible I/O ]]>
<p>As I am sure many readers can relate, I heard the hallowed, somewhat daunting words that friends and loved ones say to the resident PC geek or tech enthusiast in their lives —
"Can you make me a PC?"</p><p>Of course, I've done it dozens of times, but this was the first time that my wife, who regularly plays (or attempts to run) AAA games on a basic RTX 3050 <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/best-gaming-laptops">gaming laptop</a>, wanted something higher-end. I would figure out the components, and she could pick out a case, specifying other aesthetic frills she would like. It couldn't be a big, black, boring box, and it had to be an interesting color. My wife eventually settled on the Thermaltake Tower 250 in the eye-catching "Hydrangea Blue" colorway. The PC case is an evolution of the original Tower 100, which <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/thermaltake-the-tower-100-review/2">we reviewed in 2019</a>.</p><p>Things began to come together, and being a resourceful sort, we managed to corral all of the components to build a (slightly larger than usual) <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-mini-itx-pc-cases">ITX-based</a> system.</p><p>This isn't a super-high-end build, sporting an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-5-9600x-cpu-review">AMD Ryzen 5 9600X</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/nvidia-geforce-rtx-4080-review">RTX 4080 Founders Edition GPU</a>, which I had intended to use in a different build. With that, alongside a 2TB PCIe 4.0 SSD from Sabrent and 32GB of Klevv RAM, it is quite a performant system, which should easily run just about every modern AAA title without issue at 1440p.</p><p>Once the case arrived and we started the build process, I noticed something unusual. The Thermaltake Tower 250 rotates the motherboard 90 degrees, compared to a standard configuration. This allows for a vertically-placed GPU, a motherboard placed in the center, and then support for an all-in-one cooler's radiator on the right. This isn't an issue in itself, but it does cause the motherboard's I/O to be pointing to the top of the case, in addition to any GPU display cables.</p><h2 id="where-for-art-thou-i-o-2">Where for art thou, I/O</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2048px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="H6ck3dBnEzh7vzMmLtcCRU" name="Thermaltake Tower 250 IO" alt="Tower 250 IO" src="https://cdn.mos.cms.futurecdn.net/H6ck3dBnEzh7vzMmLtcCRU.jpg" mos="" align="middle" fullscreen="" width="2048" height="1152" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The top of the Thermaltake Tower 250 is quite crowded; there are two preinstalled exhaust fans, in addition to the front I/O panel of the case. There's also a cut-out for cables, in addition to around an inch of clearance next to the fan shroud for anything coming from the motherboard IO, which feeds into a hole for the plastic shroud that covers it, leading out of the case.</p><p>So, if you want to plug anything into your motherboard, you'll have to navigate a frustrating labyrinth of cut-outs every single time. Many people will set and forget their rear I/O, ports, and with two USB-A and a Type-C port at the front, many just won't need it. Since this PC uses Wi-Fi, routing the antenna through was very simple, and I added a handful of USB-C cables to be routed to the desk to have a couple of chargers, in case she needs to plug things in.</p><p>What I didn't anticipate was the sheer number of random cables my wife would want to plug into her system at any given time. With the front I/O already populated, plugging in USB devices became more difficult than it really should be. When factoring in the heft of a DisplayPort cable in addition to everything else, things got pretty crowded.</p><p>So, the process of taking the top of the case off and wrangling around inside to plug in devices continued. This slight inconvenience turned into annoyance, and I answered the repeated calls of "Can you plug this in please?" repeatedly for a whole week, until I buckled and bought a generic USB hub from Amazon.</p><p>You shouldn't really have to do that for the sake of convenience, especially when a motherboard has enough IO for everything. But now that the saga is over, it also highlights another design issue for the Thermaltake Tower 250.</p><h2 id="building-cleanly-still-results-in-visible-cable-mess-2">Building cleanly still results in visible cable mess</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2048px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="uJ5PpN3twpQSrT52tYk6Yh" name="Tower 250 Cables" alt="Thermaltake Tower 250 PC case internals" src="https://cdn.mos.cms.futurecdn.net/uJ5PpN3twpQSrT52tYk6Yh.jpg" mos="" align="middle" fullscreen="" width="2048" height="1152" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>To Thermaltake's credit, the Tower 250 is broadly well-designed and very simple to build inside, and it took less than an hour for me to assemble a full system, including an all-in-one cooler and RGB frills, which were listed as "mandatory" under my wife's requirements. The case has consistent and simple channels for almost everything, including support for a full-sized PSU. Cable channelling for the GPU is well-thought-out, meaning we hid the majority of a white 12V2x6 cable into a corner, which cannot be seen from most angles. With everything put together, it looked and ran well, until I realized that the I/O pointing out at the top can become a bit of an eyesore.</p><p>With several cables running out at the top, the effort put in elsewhere to hide cables so efficiently was seemingly for naught. Above the I/O, there's an inch or two of clearance where cables can be routed. The problem is that with the Hydrangea Blue colorway of the case, the internal metal sheets are white, so if you have any black cables, they stick out like a sore thumb. This is admittedly minor compared to my other grievance, but it also has a fairly simple solution.</p><p>If Thermaltake built a detachable shroud for this part of the Tower 250, which puts some effort into hiding the cabling, it would also allow for a larger cable routing channel through the rear, making it simpler to actually make use of the motherboard I/O, or offer a wider cut-out, which makes accessing the rear IO much more accessible. Adding more steps between allowing a user to make use of the features of the components within the system is a black mark on an otherwise well-designed and good-looking <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-pc-cases,4183.html">PC case</a>.</p>
https://www.tomshardware.com/pc-components/pc-cases/building-in-the-thermaltake-tower-250-made-me-furious-about-one-design-flaw-that-could-be-easily-solved-cramped-cable-cut-outs-and-inaccessible-i-o
After building in the Thermaltake Tower 250, I found that the ITX PC case has one issue that drove me mad: the Motherboard I/O is difficult to access.
yVxhVK3mCPENiHjMB5N2iP
Fri, 19 Sep 2025 18:47:55 +0000 PC Cases
PC Components
sayem.ahmed@futurenet.com (Sayem Ahmed)
Sayem Ahmed
Tom&#039;s Hardware
Thermaltake Tower 250 on a desk with accessories
Thermaltake Tower 250 on a desk with accessories
<![CDATA[ Asus gives us the PCIe finger — teases new concept that boosts motherboard GPU slot power to 250W ]]>
<p>Since PCIe's inception in the early 2000s, the high-speed connectivity standard has been limited to 75 watts of peak power from the physical slot. This is enough to power some entry-level graphics cards from the physical slot alone, but most graphics cards require auxiliary power to get enough juice. However, Asus wants to change that — <a data-analytics-id="inline-link" href="https://www.ithome.com/0/884/319.htm">IT Home</a> reports that the GPU maker has teased a new concept design allowing the PCIe slot to deliver a whopping 250 watts of power output through modifications to the PCIe front finger.</p><p>The concept reportedly takes advantage of the unused part of the front PCIe finger to triple power output. The five 12V lines attached to the PCIe finger are "merged", with each line featuring enhanced width and thickness as well as more conductive materials to boost the slot's current-carrying capacity.</p><p>Top-down images of Asus' modifications show the changes; the five 12V pins at the front are significantly larger, to the point where they are visible to the naked eye. This is in contrast to a regular PCIe slot, where the pins are so small that they are virtually impossible to spot. To feed the modified PCIe slot, most of the additional power is allegedly fed by an extra 8-pin PCIe connector on the motherboard.</p><p>Asus's PCIe concept could finally make cableless graphics cards more mainstream, assuming adoption is high. PCIe's 75-watt limit has left all but the most power-efficient entry-level graphics cards as the only mainstream GPUs that can do away with auxiliary power cables.</p><p>Technically, Asus already has an internal proprietary GPU power connector for its cableless <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/asus-reveals-rtx-5090-and-rtx-5070-ti-gpus-with-a-detachable-1000w-power-connector">BTF</a> products (the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/asus-gpu-power-connector-delivers-1-000w-for-cableless-builds-gc-hpwr-has-a-retractable-design">GC-HPWR</a>). But Asus is likely building its aforementioned PCIe modification as a cheaper method to create cableless GPUs, particularly for the mainstream market. Modifying the PCIe slot on a motherboard or graphics card is cheaper than installing an extra internal finger for internal power delivery.</p><p>250 watts is not a lot of power in an era where most high-performance GPUs consume more than 350 watts. But 250 watts would be more than sufficient for all entry-level and lower-mid-range graphics cards. For example, AMD's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9060-xt-16gb-review">Radeon RX 9060 XT</a> and<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9070-xt-review"> RX 9070</a> consume just 160 and 220 watts, respectively, making them prime candidates for Asus' PCIe modification.</p><p>Again, though, Asus' concept is just that, a concept. There's no knowing how widely adopted Asus' design will be if it ever gets finalized. Asus' concept is also a modification of the PCIe standard, which could make adoption even harder. Even if it is adopted, board makers and AIB makers would need to build new graphics cards and motherboard models specifically designed to take advantage of Asus' concept, which would increase costs and manufacturing complexity.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/asus-gives-us-the-pcie-finger-teases-new-concept-that-boosts-motherboard-gpu-slot-power-to-250w
Asus teases a new concept design that pushes a PCIe slot to 250 watts of power delivery through modifications to its five 12V pins. The concept could make cableless GPUs more mainstream if it becomes widely adopted in the future.
xcp43JMyWM6Ga4Ghv7PGeF
Fri, 19 Sep 2025 18:43:13 +0000 GPUs
PC Components
editors@tomshardware.com (Aaron Klotz)
Aaron Klotz
Asus
Asus Tuf Gaming Z790-BTF
Asus Tuf Gaming Z790-BTF
<![CDATA[ Nvidia CEO Huang says upcoming DGX Spark systems are powered by N1 silicon — confirms GB10 Superchip and N1/N1X SoCs are identical ]]>
<p>Yesterday, Nvidia and Intel <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">lifted the curtain on a historic collaboration</a> that will see the two chipmakers jointly develop a myriad of CPU and GPU products. While future solutions like the "Intel x86 RTX SoC" were the focus of the announcement, some clarification was also shed on existing projects. Chief among these was Nvidia CEO Jensen Huang saying that the upcoming, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-arm-soc-for-windows-machines-reportedly-debuting-in-q4-featuring-n1x-with-n1-to-follow-in-early-2026">long-rumored N1 SoC</a> is essentially the same as the GB10 Superchip that's been out for a while.</p><p>For some context, Nvidia has never officially unveiled the N1/N1X SoCs, but speculation sparked from CES 2025's announcement of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-project-digits-desktop-ai-supercomputer-fits-in-the-palm-of-your-hand-usd3-000-to-bring-1-pflops-of-performance-home?utm_source=chatgpt.com">Project DIGITS</a>, where the company revealed its collaboration with MediaTek. From that came the GB10 "Superchip," which is part of the company's DGX Spark lineup, and multiple vendors have <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/acer-unveils-project-digits-supercomputer-featuring-nvidias-gb10-superchip-with-128gb-of-lpddr5x">already released their iterations of it</a>. The GB10 is aimed squarely at AI workloads, offering supercomputer-like performance at home. It includes a 20-core ARM-based CPU developed in conjunction with MediaTek, along with a powerful Blackwell-based GPU chiplet.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="SRJQsHv2hAhyzggBhL4y7N" name="NVIDIA-DGX-Spark-and-NVIDIA-DGX-Station-blackwell-hero.jpg" alt="Nvidia" src="https://cdn.mos.cms.futurecdn.net/SRJQsHv2hAhyzggBhL4y7N.jpg" mos="" align="middle" fullscreen="" width="1600" height="900" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Nvidia DGX Spark and DGX Station </span><span class="credit" itemprop="copyrightHolder">(Image credit: Nvidia)</span></figcaption></figure><p>The N1 SoC shares the same specs, at least <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidias-20-core-n1x-leaks-with-3000-single-core-geekbench-score-arm-chip-could-rival-intel-and-amds-laptop-offerings">according to previous leaks and rumors</a>, featuring 6,144 CUDA cores for its GPU - same as the desktop RTX 5070 - and a 20-core CPU split across two clusters, built using Nvidia's Grace architecture. Back in July, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-n1x-soc-leaks-with-the-same-number-of-cuda-cores-as-an-rtx-5070-n1x-specs-align-with-the-gb10-superchip">we saw a Geekbench score</a> surface for the N1X, which allegedly confirmed these specs, giving credence to the fact that GB10 and N1 are intrinsically tied. Of course, just because two products are closely linked to each other doesn't mean they're the same, but all signs pointed toward identical chips being used across the board.</p><p>That notion has just been legitimized by Jensen Huang, who said the following in a webcast last night, "We also have a new ARM product that's called N1. And that product is - that processor is going to go into the DGX Spark and many other versions of products like that. And so we're super excited about the ARM road map, and this doesn't affect any of that."</p><p>According to Nvidia's CEO, the silicon powering the GB10 — which itself is what powers DGX Spark — is identical to the N1/N1X SoC. Especially the part about "many other versions" confirms that N1 could simply be a slightly lower-binned version of the full-fat GB10. After all, the latter is meant for client devices like laptops and desktops, whereas the GB10 targets professionals. The distinction matters because N1 represents Nvidia's first serious attempt at taking their in-house CPU cores mainstream (following Tegra).</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="WME5ju3tjZ2AANMrps5jeV" name="Nvidia-Tegra-X1.jpg" alt="Nvidia's Tegra X1 SoC prcoessor." src="https://cdn.mos.cms.futurecdn.net/WME5ju3tjZ2AANMrps5jeV.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Nvidia Tegra X1 </span><span class="credit" itemprop="copyrightHolder">(Image credit: Nvdia)</span></figcaption></figure><p>Unfortunately, that's the only statement pertaining to N1, so we still don't know when it will actually launch. But at least it's out there now that GB10, which should already be in the hands of some, is what Nvidia will eventually release in the future, just with a different target audience in mind. Given Nvidia's new deal with Intel, the interest in developing ARM-based products might collide with x86-based solutions that Intel specializes in. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/teams-at-nvidia-and-intel-have-been-working-in-secret-on-jointly-developed-processors-for-a-year-the-trump-administration-has-no-involvement-in-this-partnership-at-all">However, that's apparently not an issue,</a> and both roadmaps will continue forward at full force, unaffected by each other.</p><p>The N1 SoC has <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/nvidias-upcoming-arm-based-n1x-soc-leaks-again-this-time-on-furmark-modest-benchmark-score-indicates-early-engineering-sample-but-confirms-windows-evaluation">already been tested on Windows</a>, which suggests that the chip is getting closer to its Windows-on-ARM destination day by day. The GB10, on the other hand, is not exactly intended for Microsoft's operating system; rather, it is a Linux-based DGX OS that's optimized for local AI, datacenter, and other professional workloads. With that said, since the N1 technically doesn't even exist yet, there is no confirmation for it eventually running on Windows (despite the obvious implication), and Jensen Huang did not comment on it either.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/nvidia-ceo-huang-says-upcoming-dgx-spark-systems-are-powered-by-n1-silicon-confirms-gb10-superchip-and-n1-n1x-socs-are-identical
Nvidia's long-rumoed N1 SoC, featuring ARM-based CPU cores from MediaTek and Blackwell-based GPU cores, has now been confirmed as the same chip as the GB10 Superchip. They both share similar, if not identical, specs and Nvidia CEO Jensen Huang has just said that DGX Spark (which has the GB10) is powered by N1.
VKeQAuoj4yfogpMEj5oKk5
Fri, 19 Sep 2025 18:27:42 +0000 CPUs
PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Nvidia
Nvidia GB10
Nvidia GB10
<![CDATA[ GeForce RTX 50-series GPUs are finally selling at and below MSRP — RTX 5070 dips below $549 ]]>
<p>The <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-announces-rtx-50-series-at-up-to-usd1-999">GeForce RTX 50 series</a> (codenamed Blackwell) offers some of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gpus,4380.html">best graphics cards</a> you can find on the market today. It's great news that the prices for some of these <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-blackwell-architecture-deep-dive-a-closer-look-at-the-upgrades-coming-with-rtx-50-series-gpus">Blackwell</a>-based graphics cards have finally leveled out, making it easier for everyone to purchase them at or even below Nvidia's MSRP.</p><p>The <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5070-review-founders-edition">GeForce RTX 5070</a> launched several months ago at an MSRP of $549; however, it has proven challenging for consumers to acquire it at that specified price point. Nonetheless, a selection of customized GeForce RTX 5070 models from some of Nvidia's leading partners is presently available for purchase at prices below $549.</p><p>The Asus Prime RTX 5070 12GB GDDR7, in particular, is among the notable highlights of the day. This 2.5-slot graphics card, which conforms to the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-announces-sff-ready-graphics-card-and-case-guidelines-cram-an-enthusiast-class-gpu-into-your-mini-itx-system">Nvidia SFF-ready</a> standard, is presently available for purchase at <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DS6V1YSY">$523</a>, representing a 14% discount from its usual retail price. However, for enthusiasts preferring Nvidia's design, the GeForce RTX 5070 Founders Edition has been restocked at Best Buy for <a data-analytics-id="inline-link" href="https://www.bestbuy.com/product/nvidia-geforce-rtx-5070-12gb-gddr7-graphics-card-graphite-grey/J3GWYHGP8K">$549</a>.</p><div class="product"><a data-dimension112="b58bd615-0eac-4db3-a65e-858790d0b67d" data-action="Deal Block" data-label="The Prime RTX 5070 12GB GDDR7 is equipped with an Asus triple-fan cooling system utilizing Axial-tech fans. This graphics card operates at a base clock speed of 2,512 MHz and attains a boost clock of up to 2,542 MHz." data-dimension48="The Prime RTX 5070 12GB GDDR7 is equipped with an Asus triple-fan cooling system utilizing Axial-tech fans. This graphics card operates at a base clock speed of 2,512 MHz and attains a boost clock of up to 2,542 MHz." data-dimension25="$523.79" href="https://www.amazon.com/dp/B0DS6V1YSY" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2400px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="vZsXxKStgMPB3juvGnUhuT" name="descarga" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/vZsXxKStgMPB3juvGnUhuT.png" mos="" align="middle" fullscreen="" width="2400" height="2400" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>The Prime RTX 5070 12GB GDDR7 is equipped with an Asus triple-fan cooling system utilizing Axial-tech fans. This graphics card operates at a base clock speed of 2,512 MHz and attains a boost clock of up to 2,542 MHz.<a class="view-deal button" href="https://www.amazon.com/dp/B0DS6V1YSY" target="_blank" rel="nofollow" data-dimension112="b58bd615-0eac-4db3-a65e-858790d0b67d" data-action="Deal Block" data-label="The Prime RTX 5070 12GB GDDR7 is equipped with an Asus triple-fan cooling system utilizing Axial-tech fans. This graphics card operates at a base clock speed of 2,512 MHz and attains a boost clock of up to 2,542 MHz." data-dimension48="The Prime RTX 5070 12GB GDDR7 is equipped with an Asus triple-fan cooling system utilizing Axial-tech fans. This graphics card operates at a base clock speed of 2,512 MHz and attains a boost clock of up to 2,542 MHz." data-dimension25="$523.79">View Deal</a></p></div><p>If you are seeking a more economical option, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5060-ti-16gb-review">GeForce RTX 5060 Ti</a> with 16GB and 8GB, which have MSRPs of $429 and $379, respectively, may present a compelling choice. The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus is now available at a price of <a data-analytics-id="inline-link" href="https://www.newegg.com/msi-rtx-5060-ti-8g-ventus-2x-plus-geforce-rtx-5060-ti-8gb-graphics-card-double-fans/p/N82E16814137968">$339</a> after applicable discounts and a $20 rebate card. This graphics card is currently being offered at an 11% reduction relative to the official MSRP.</p><p>The 16GB model has maintained its value, resisting a decline below the MSRP. However, if you are considering that SKU, the Asus Prime GeForce RTX 5060 Ti 16GB GDDR7 is available for <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F4RZDFD5">$429</a> on Amazon, representing a 10% discount from the usual price.</p><div class="product"><a data-dimension112="937f13ba-7e40-47b0-91c0-885f7befad5c" data-action="Deal Block" data-label="The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus features a dual-slot, minimalistic design. The graphics card exhibits a modest factory overclock of 30 MHz higher than the reference specification." data-dimension48="The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus features a dual-slot, minimalistic design. The graphics card exhibits a modest factory overclock of 30 MHz higher than the reference specification." data-dimension25="$339.99" href="https://www.newegg.com/msi-rtx-5060-ti-8g-ventus-2x-plus-geforce-rtx-5060-ti-8gb-graphics-card-double-fans/p/N82E16814137968?utm_medium=affiliate&utm_campaign=afc-ran-com-_-PCPartPicker&utm_source=afc-PCPartPicker&AFFID=2558510&AFFNAME=PCPartPicker&ACRID=1&ASUBID=&ASID=https%3A%2F%2Fpcpartpicker.com%2Fproduct%2FQBrp99%2Fmsi-ventus-2x-plus-geforce-rtx-5060-ti-8-gb-video-card-rtx-5060-ti-8g-ventus-2x-plus" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:59.06%;"><img id="gRUe33eTyZJ8JYmsDRAKbL" name="14-137-968-01" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/gRUe33eTyZJ8JYmsDRAKbL.webp" mos="" align="middle" fullscreen="" width="1280" height="756" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus features a dual-slot, minimalistic design. The graphics card exhibits a modest factory overclock of 30 MHz higher than the reference specification.<a class="view-deal button" href="https://www.newegg.com/msi-rtx-5060-ti-8g-ventus-2x-plus-geforce-rtx-5060-ti-8gb-graphics-card-double-fans/p/N82E16814137968?utm_medium=affiliate&utm_campaign=afc-ran-com-_-PCPartPicker&utm_source=afc-PCPartPicker&AFFID=2558510&AFFNAME=PCPartPicker&ACRID=1&ASUBID=&ASID=https%3A%2F%2Fpcpartpicker.com%2Fproduct%2FQBrp99%2Fmsi-ventus-2x-plus-geforce-rtx-5060-ti-8-gb-video-card-rtx-5060-ti-8g-ventus-2x-plus" target="_blank" rel="nofollow" data-dimension112="937f13ba-7e40-47b0-91c0-885f7befad5c" data-action="Deal Block" data-label="The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus features a dual-slot, minimalistic design. The graphics card exhibits a modest factory overclock of 30 MHz higher than the reference specification." data-dimension48="The MSI GeForce RTX 5060 Ti 8G Ventus 2X OC Plus features a dual-slot, minimalistic design. The graphics card exhibits a modest factory overclock of 30 MHz higher than the reference specification." data-dimension25="$339.99">View Deal</a></p></div><p>Several custom <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-announces-geforce-rtx-5060-ti-and-rtx-5060-starting-at-usd379-and-usd299">GeForce RTX 5060</a> models are retailing at the MSRP of $299. Examples include the <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F8LDHQ7Y">Gigabyte GeForce RTX 5060 WindForce OC 8G</a>, the <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F4LP8VH5">MSI GeForce RTX 5060 8G Shadow 2X OC</a>, and the <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F77H7NBK">Asus Prime GeForce RTX 5060 8GB GDDR7 OC Edition</a>, all of which are available on Amazon.</p><p>The pricing for specific high-performance Blackwell SKUs, including the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5070-ti-review-asus">GeForce RTX 5070 Ti</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5080-review">GeForce RTX 5080,</a> has begun to stabilize. It is now more feasible to locate the GeForce RTX 5070 Ti and GeForce RTX 5080 in stock at their designated MSRPs of $749 and $999, respectively, compared to a few months prior.</p><p>The <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DS6WFRBP">Asus Prime GeForce RTX 5070 Ti 16GB GDDR7</a>, <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DXWQ22CQ">Gigabyte GeForce RTX 5070 Ti WindForce SFF 16G</a>, and <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DXL7GSYC">PNY GeForce RTX 5070 Ti Triple Fan</a> are presently available for purchase at $749.</p><p>Meanwhile, the <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DTPG3B1N">Asus Prime GeForce RTX 5080 16GB GDDR7</a>, <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0DTJFZ4YS">PNY GeForce RTX 5080 OC Triple Fan</a>, and <a data-analytics-id="inline-link" href="https://www.newegg.com/gigabyte-windforce-gv-n5080wf3-16gd-geforce-rtx-5080-16gb-graphics-card-triple-fans/p/N82E16814932780?utm_medium=affiliate&utm_campaign=afc-ran-com-_-PCPartPicker&utm_source=afc-PCPartPicker&AFFID=2558510&AFFNAME=PCPartPicker&ACRID=1&ASUBID=&ASID=https%3A%2F%2Fpcpartpicker.com%2Fproduct%2FmGG2FT%2Fgigabyte-windforce-sff-geforce-rtx-5080-16-gb-video-card-gv-n5080wf3-16gd">Gigabyte GeForce RTX 5080 WindForce SFF 16 G</a> have finally settled at the $999 mark.</p><p>Amidst speculation regarding the alleged release of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-rtx-50-super-lineup-leak-hints-at-increased-vram-of-up-to-24gb-and-415w-tgp">GeForce RTX 50 Super series</a>, it is plausible that retailers have strategically reduced pricing on some of the Blackwell graphics cards. Rather than official price cuts, these promotional offers are likely aimed at clearing inventory in anticipation of the rumored Super refresh. However, many consumers have expressed concerns that Nvidia's Blackwell gaming graphics cards are priced excessively; therefore, bringing the pricing back down to MSRP may not effectively incentivize purchases. Perhaps these Blackwell GPUs will look more convincing when retailers start offering genuine discounts rather than merely reverting elevated prices to MSRP.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/geforce-rtx-50-series-gpus-are-finally-selling-at-and-below-msrp-rtx-5070-dips-below-usd549
Pricing for some of Nvidia's GeForce RTX 50-series (codenamed Blackwell) gaming graphics cards has stabilized.
fGZqwT9sPGTVsnKWi5DhY
Fri, 19 Sep 2025 16:35:44 +0000 GPUs
PC Components
Zhiye Liu
Nvidia
GeForce RTX 5090
GeForce RTX 5090
<![CDATA[ AMD silently launches RX 7700 non-XT with 16 GB VRAM — New RDNA 3 GPU uses nerfed Navi 32 die, offers reduced performance and increased power draw ]]>
<p>If you thought RDNA 3 was done, think again, because AMD has just refreshed the lineup with a brand new SKU — <a data-analytics-id="inline-link" href="https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7700-xt.html" target="_blank">the RX 7700 non-XT</a>. Listed silently on its website without any announcement, the RX 7700 comes with 16 GB of GDDR6 memory saturated across a 256-bit bus, compared to the 7700 XT's relatively meager 12 GB (across 192-bit). AMD has used 19.5 Gbps chips, so, combined with the aforementioned specs, that puts the memory bandwidth on par with the 7800 XT, at 624 GB/s.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3216px;"><p class="vanilla-image-block" style="padding-top:64.68%;"><img id="EsC5duAZgj3s9peMoDQiNm" name="Screenshot 2025-09-19 at 7.48.01 PM" alt="AMD Radeon RX 7700 specs" src="https://cdn.mos.cms.futurecdn.net/EsC5duAZgj3s9peMoDQiNm.png" mos="" align="middle" fullscreen="" width="3216" height="2080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>The GPU under the hood is a cut-down version of Navi 32 with just 2,560 Stream Processors and 40 Compute Units. The 7700 XT uses the same Navi 32 but has 3,456 Stream Processors and 54 CUs; it also has more ROPs and TMUs (see table at the end), and 48 MB Infinity Cache compared to just 40 MB on the RX 7700. Moreover, there are 80 AI accelerators on the RX 7700, which isn't impressive on its own, but when you combine that with the 16 GB memory pool, this could become a sleeper option for local AI applications.</p><p>In terms of performance, AMD lists FPS numbers for a bunch of different games, showing a ~20% decrease in gaming compared to the 7700 XT (when taking into account games that overlap). Not only that, but the RX 7700 seems to be less efficient too, eating up 263W of power (versus 245W on the 7700 XT) despite the reduced performance. That leaves local AI inference the only real selling point for this SKU, if it's priced right to begin with. There are dual 8-pin connectors on board, and AMD recommends a 700W power supply for the RX 7700.</p><div ><table><caption>RDNA 3 70-class gaming performance</caption><thead><tr><th class="firstcol " ><p>Game</p></th><th
><p>RX 7700 XT FPS</p></th><th
><p>RX 7700 FPS</p></th><th
><p>Percentage Difference</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Resident Evil 4 (RT High)</p></td><td
><p>84</p></td><td
><p>70</p></td><td
><p>+20.0%</p></td></tr><tr><td class="firstcol " ><p>Dying Light 2 (RT)</p></td><td
><p>75</p></td><td
><p>63</p></td><td
><p>+19.0%</p></td></tr><tr><td class="firstcol " ><p>Hogwarts Legacy</p></td><td
><p>79</p></td><td
><p>64</p></td><td
><p>+23.4%</p></td></tr><tr><td class="firstcol " ><p>Marvel’s Spider-Man (Miles Morales / 2)</p></td><td
><p>80</p></td><td
><p>66</p></td><td
><p>+21.2%</p></td></tr></tbody></table></div><p>So far, AMD has not listed any availability, suggesting that this is perhaps an OEM product meant to be supplied to SIs and in prebuilts across the world. That being said, <a data-analytics-id="inline-link" href="https://asrock.com/Graphics-Card/AMD/Radeon%20RX%207700%20Challenger%2016GB/index.asp#Specification" target="_blank">ASRock has already unveiled its custom "Challenger" design</a> and—while the company didn't share price or availability either—we did learn that the card is clocked at 2,041 MHz (Game), boosting up to 2,459 MHz.</p><p>At the moment, AMD already has the RX 9060 XT 16 GB slotted nicely in the midrange, that's slightly faster (even its 8 GB variant would dwarf the RX 7700) and has a richer featureset. Therefore, launching the RX 7700 non-XT at anything more than ~$350 would seriously cannibalise the lineup, rendering it DoA.</p><div ><table><caption>RDNA 3 70-class specs</caption><thead><tr><th class="firstcol " ><p>GPU Specs</p></th><th
><p>Radeon RX 7700</p></th><th
><p>Radeon RX 7700 XT</p></th><th
><p>Radeon RX 7800 XT</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Stream Processors</p></td><td
><p>2,560</p></td><td
><p>3,456</p></td><td
><p>3,840</p></td></tr><tr><td class="firstcol " ><p>Compute Units</p></td><td
><p>40</p></td><td
><p>54</p></td><td
><p>60</p></td></tr><tr><td class="firstcol " ><p>VRAM</p></td><td
><p>16GB GDDR6</p></td><td
><p>12GB GDDR6</p></td><td
><p>16GB GDDR6</p></td></tr><tr><td class="firstcol " ><p>Memory Bus</p></td><td
><p>256-bit</p></td><td
><p>192-bit</p></td><td
><p>256-bit</p></td></tr><tr><td class="firstcol " ><p>Bandwidth</p></td><td
><p>624 GB/sec</p></td><td
><p>432 GB/sec</p></td><td
><p>624 GB/sec</p></td></tr><tr><td class="firstcol " ><p>GPU Game / Boost Clock</p></td><td
><p>2,041 MHz / 2,459 MHz</p></td><td
><p>2,171 MHz / 2,544MHz</p></td><td
><p>2,124 MHz / 2,430 MHz</p></td></tr><tr><td class="firstcol " ><p>Infinity Cache</p></td><td
><p>40 MB</p></td><td
><p>48 MB</p></td><td
><p>64 MB</p></td></tr><tr><td class="firstcol " ><p>AI Accelerators</p></td><td
><p>80 (1st Gen)</p></td><td
><p>108 (1st Gen)</p></td><td
><p>120 (1st Gen)</p></td></tr><tr><td class="firstcol " ><p>Texture Mapping Units (TMU)</p></td><td
><p>64</p></td><td
><p>96</p></td><td
><p>96</p></td></tr><tr><td class="firstcol " ><p>Render Output Units (ROP)</p></td><td
><p>160</p></td><td
><p>216</p></td><td
><p>240</p></td></tr><tr><td class="firstcol " ><p>TBP</p></td><td
><p>263 W</p></td><td
><p>245 W</p></td><td
><p>263 W</p></td></tr></tbody></table></div><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/amd-silently-launches-rx-7700-non-xt-with-16-gb-vram-new-rdna-3-gpu-uses-nerfed-navi-32-die-offers-reduced-performance-and-increased-power-draw
AMD has just launched the RX 7700 non-XT with 16 GB memory and binned down specs compared to the RX 7700 XT. It's considerably slower in gaming, takes more power than the RX 7700 XT, but has increased VRAM that could make it a hit for AI. Pricing and availability are up in the air.
DG6ivtVCXKT2nUEVtRNVS
Fri, 19 Sep 2025 15:27:45 +0000 GPUs
PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
AMD
AMD Radeon RX 7700
AMD Radeon RX 7700
<![CDATA[ [Updated] Intel says it remains committed to its Arc graphics project — 'Intel will continue to have GPU product offerings' ]]>
<p><strong>09/19/2025 update</strong>: <em>After publication, an Intel representative reached out with the following statement regarding GPU development: "While we’re not sharing specific roadmaps at this time, everything we discussed aligns with and complements Intel’s existing strategy. This collaboration with NVIDIA enables us to deliver additional custom solutions that accelerate AI workloads and broaden our reach across high-performance computing segments in client and data center.</em></p><p><em>We remain committed to our GPU roadmap. We’ll be collaborating with NVIDIA to serve specific market segments, but we’re also continuing to execute on our own path."</em></p><p><em>The original article follows below.</em></p><p>All talk in town right now is all about the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">Intel-Nvidia partnership</a>, and a question on many minds is whether that means Intel might be scrapping its plans for upcoming Arc graphics cards or iGPUs. The company is attempting to assuage those fears, as one of its spokespeople told <a data-analytics-id="inline-link" href="https://www.pcworld.com/article/2913872/intel-nvidia-deal-doesnt-change-its-roadmap.html" target="_blank">PCWorld</a>, “We’re not discussing specific roadmaps at this time, but the collaboration is complementary to Intel’s roadmap and Intel will continue to have GPU product offerings."</p><p>Intel also reiterated that its collaboration with Nvidia is "complementary" to the company's extant roadmap. Both those statements seem clear at face value, but some might find it a little hard to believe, given everything that's happened with Intel in the past year, namely but not only the major cuts to its workforce and a refocusing on profitability. The wording of the statement is also curious, as the company stating it will still have GPUs doesn't clarify what type they will be, or who will design them.</p><p>Last December, Intel's ex-CEO Pat Gelsinger left abruptly. Shortly after, former CEO of Products Michelle Holthaus stated that Intel remained committed to Arc. In March, Intel named Lip-Bu Tan as its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-appoints-lip-bu-tan-as-permanent-ceo">new CEO</a>, who issued more staff cuts and reportedly issued an order that any of Intel's new products have a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/intel-draws-a-line-in-the-sand-to-boost-gross-margins-new-products-must-deliver-50-percent-to-get-the-green-light">50% gross margin</a>. Additionally, Holthaus was <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/intel-ousts-ceo-of-products-as-part-of-the-latest-executive-shake-up-ending-30-year-career-company-also-establishes-new-custom-chip-design-unit">ousted recently</a>, after a mere 10 months in charge of Products. The company offered no updates to its Arc roadmap in the meantime.</p><p>All those events would be enough to question Arc's continued existence, but now the Nvidia partnership has added further concern, especially in light of Intel saying that it intends to use RTX chiplets in mobile and gaming devices, casting a pall over Arc iGPUs. Judging by the latest <a data-analytics-id="inline-link" href="https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam" target="_blank">Steam hardware survey</a>, Intel's share has remained fairly steady over the past couple of years, sitting at 7.9%. If RTX chiplets in mobile devices prove popular, they would certainly eat away at it.</p><p>Intel and Nvidia both claim their partnership won't affect <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/desktop-gpu-roadmap-nvidia-rubin-amd-udna-and-intel-xe3-celestial" target="_blank"><u>existing roadmaps</u></a>, but some in the enthusiast community fear<strong> </strong>that a $5 billion cash influx from Nvidia could come with pressure for Intel not to compete with RTX cards — assuming Nvidia is still fully focused on the gaming market, given how it now represents a small fraction of the company's business.</p><p>We reviewed the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/intel-arc-b580-review-the-new-usd249-gpu-champion-has-arrived/">Arc B580</a> in December and found it to have the best value in its price bracket despite some misgivings about its drivers. The $250 price point is hotly contested, and a high volume of sales in this bracket would certainly translate into additional market share for Intel, which has seen the size of its slice remain steady over the past couple of years. The existing Arc roadmap looks promising, but despite Intel's statement, many feel its future is under threat.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/intel-says-it-remains-committed-to-its-arc-graphics-project-intel-will-continue-to-have-gpu-product-offerings
Intel issued a press statement stating that it remains committed to developing its Arc GPUs.
BxT9ukCtgEMG3CENPsX3QH
Fri, 19 Sep 2025 14:57:12 +0000 Tech Industry
Bruno Ferreira
Intel
Intel Arc Pro Battlemage
Intel Arc Pro Battlemage
<![CDATA[ Researchers 3D print lightweight ceramic fuel cell — suggests alternative power source for the aerospace industry ]]>
<p>The world of fuel cells just got a jolt thanks to researchers from the Technical University of Denmark (DTU). The team has been hard at work developing a new technology that could completely change the way we approach fuel cell manufacturing. The new process involves 3D printing and results in what the team refers to as the "Monolithic Gyroidal Solid Oxide Cell" (or just "The Monolith" for short). This work was reported by <a data-analytics-id="inline-link" href="https://orbit.dtu.dk/en/publications/monolithic-gyroidal-solid-oxide-cells-by-additive-manufacturing" target="_blank">Interesting Engineering,</a> along with a journal published on the <a data-analytics-id="inline-link" href="https://orbit.dtu.dk/en/publications/monolithic-gyroidal-solid-oxide-cells-by-additive-manufacturing" target="_blank">DTU</a> website.</p><p>The team implemented a custom design inspired by the natural construction of coral. This shape optimizes surface area while the material composition allows for a much lighter-weight end product. Most fuel cells are comprised of metal, which contributes greatly to their weight. This fuel cell is apparently completely ceramic.</p><p>The intricate design is known as a gyroid and is a type of triply periodic minimal surface (shortened to TPMS). These surfaces are intended to provide as much surface area as possible. It's beneficial, particularly in this case, as the surface provides more optimal heat dispersion. According to the development team, the cell is capable of producing more than a watt of power for each gram of its own weight.</p><p>The material also has a surprisingly noteworthy amount of durability. When testing the fuel cell's ability to withstand temperature fluctuations, it managed to handle temperatures as high as 212° F (100° C). It also maintained its structural integrity when alternating between both power-storing and generating modes.</p><p>The fuel cell also features something called "Electrolysis Mode" which increases the hydrogen production rate almost tenfold compared to standard fuel cells. The 3D printing aspect of the design also helps make the manufacturing process easier than regular fuel cells.</p><p>If you want to get into the world of 3D printing but aren't sure where to start, check out our list of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-3d-printers">best 3D printers</a> to see what we recommend and why. You probably won't start off 3D printing fuel cells, but it's a great first step.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/3d-printing/researchers-3d-print-lightweight-ceramic-fuel-cell-suggests-alternative-power-source-for-the-aerospace-industry
Researchers from the Technical University of Denmark have created a 3D printing process that creates efficient, lightweight fuel cells that could transform the aerospace industry.
JME4eQwj7iGjvSKqT6jBYK
Fri, 19 Sep 2025 13:56:55 +0000 3D Printing
Ash Hill
Getty / Daniel Garrido
Airplane landing
Airplane landing
<![CDATA[ I've been using Linux for a quarter of a century, so why do I keep coming back to Ubuntu? ]]>
<p>It scares me to say this, but I have been using Linux for a quarter of a century! Long before the Raspberry Pi was my main hobby, installing Linux on a myriad of devices was my jam. From the early days, when an AMD K6-2 333 and 128MB RAM powered my initial exploration of the world of Unix and Linux, to today, which sees Linux running on my <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/amd-ryzen-5-5600x-zen-3-review" target="_blank"><u>Ryzen 5600X</u></a> system, a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/raspberry-pi/the-dream-of-a-raspberry-pi-laptop-becomes-a-reality-argonone-up-review" target="_blank"><u>Raspberry Pi laptop,</u></a> and my beloved <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/steam-deck-oled" target="_blank"><u>Steam Deck</u></a>.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:640px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="CSsJ2fYQ3X5szQkQSRaQmU" name="dapper1" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/CSsJ2fYQ3X5szQkQSRaQmU.jpg" mos="" link="" align="" fullscreen="" width="640" height="480" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:640px;"><p class="vanilla-image-block" style="padding-top:62.50%;"><img id="KrtChyUcJuiGjSwrPKLKkU" name="dapper2" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/KrtChyUcJuiGjSwrPKLKkU.jpg" mos="" link="" align="" fullscreen="" width="640" height="400" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="L4XSeSct6EwJADoj4NdanU" name="dapper3" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/L4XSeSct6EwJADoj4NdanU.jpg" mos="" link="" align="" fullscreen="" width="1024" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="VcLoZDpZME4dsG7m8XcgnU" name="dapper4" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/VcLoZDpZME4dsG7m8XcgnU.jpg" mos="" link="" align="" fullscreen="" width="1024" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="kNm4hPMmBdNXLLPgCwEzoU" name="dapper5" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/kNm4hPMmBdNXLLPgCwEzoU.jpg" mos="" link="" align="" fullscreen="" width="1024" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>In those 25 years, I have tried many different Linux distributions; in fact, here is a list of distros that I have used over the years. This list is not exhaustive, because I reviewed a lot of Linux distros for Linux Format magazine.</p><ul><li>Corel Linux</li><li>Mandrake Linux (Mandriva)</li><li>Open Suse / Suse Linux</li><li>Ubuntu (2006 onwards)</li><li>Debian</li><li>Crunchbang Linux (Debian)</li><li>Fedora</li><li>Manjaro</li><li>Arch</li><li>MX Linux</li><li>Bodhi</li><li>Raspberry Pi OS</li><li>Armbian</li><li>DietPi</li><li>AnduinOS</li><li>Bazzite</li><li>CachyOS</li><li>Linux Mint</li><li>ZorinOS</li></ul><p>The keen-eyed amongst you will spot that the majority of these distros are Debian-based. Yes, I prefer Debian-based distros, chiefly because I know apt rather well. But I can confidently use Fedora, or Arch (Aur) based systems too. So with all this confidence, why do I keep coming back to Ubuntu? Well, it boils down to a few reasons.</p><h2 id="ubuntu-just-works-2">Ubuntu “just works”</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="sCBtewgZiBsrLcEZdTsjtU" name="ubuntu1" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/sCBtewgZiBsrLcEZdTsjtU.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="w3xTmPm5SvFP3FFcayq6qU" name="ubuntu2" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/w3xTmPm5SvFP3FFcayq6qU.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="MbDqv3HLsMtoaEYsKMshuU" name="ubuntu3" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/MbDqv3HLsMtoaEYsKMshuU.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="WHukfgaQRNTecGMLe3sduU" name="ubuntu4" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/WHukfgaQRNTecGMLe3sduU.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>I’ve run Ubuntu on everything that I could. My laptops all ran it, something called an O2 Joggler (a rebranded OpenPeak device), which was a photo frame with an early Intel Atom CPU, also ran Ubuntu.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1824px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="RZFsVyTp6ztmrPEYJcrj4V" name="joggler" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/RZFsVyTp6ztmrPEYJcrj4V.jpg" mos="" align="middle" fullscreen="" width="1824" height="1368" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>My Asus eeePC ran it for a short while, until I moved to Cruncheee, a version of Crunchbang Linux (Debian) for the eeePC. What links all of these devices is that Ubuntu just works. Sure, in the early days, some Wi-Fi cards and obscure hardware forced me to tinker in the terminal, but in recent years, I’ve not had to delve into the terminal to fix a critical issue. For newcomers and those who prize reliability over the “bleeding edge”, Ubuntu is hard to beat. I can drop my nearly 70-year-old dad in front of Ubuntu, and he can use it easily. It just works.</p><h2 id="ease-of-use-no-matter-your-level-of-knowledge-2">Ease of use, no matter your level of knowledge</h2><p>That leads me nicely into this section, and Ubuntu’s ease of use spans the gamut of users.</p><p>Linux is often seen as the difficult and, dare I say, “nerdy” OS choice, and typically those attracted to Linux are more computer-savvy than others. However, Linux is open to everyone. Ubuntu is proof of that. From my perspective, Ubuntu offers the ease of use that macOS provides, but without the walled garden that Apple promotes. Ubuntu is free, and it works on older hardware. The installation process has been streamlined, particularly over the last couple of years, with a redesign and tweaks to reduce friction for new users.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:640px;"><p class="vanilla-image-block" style="padding-top:62.50%;"><img id="WqL2vPUYcNViBVkehiHdnU" name="Corel" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/WqL2vPUYcNViBVkehiHdnU.jpg" mos="" align="middle" fullscreen="" width="640" height="400" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>I recall installing Corel Linux and Mandrake Linux (now known as Mandriva) back in the early 2000s. They both had a great installer for the time. Remember, this was a time when the installer would scrutinize your system and ask you to make every decision along the way (including writing a custom <a data-analytics-id="inline-link" href="http://x.org" target="_blank"><u>X.org</u></a> file), which would scare some from trying. I mention Corel Linux (a Debian-based distro) because that was the first Linux distro that worked for me. The installer felt, dare I say it? More Windows than Linux, and for someone moving over, it was easier to use.</p><p>Corl Linux was great fun, and the CDE (a variant of KDE) user interface was similar to Windows of that era, so not too much for me to learn. Corel Linux just gave me a basic install, not too many apps to choose from, but enough. Unlike the time that I downloaded Mandrake Linux (two days to download two CDs!) and chose to install everything. The app menus were rammed full of stuff that I had no idea about.</p><p>I loved Mandrake Linux. It felt great and worked well on the hardware of the era. However, I tried a few out, and some, like the KDE CD burning tool K3B, stuck and became a favorite.</p><h2 id="the-choice-of-flavors-2">The choice of flavors</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:62.50%;"><img id="amKFb4PfJWAbcR7tRHW7qU" name="Kubuntu" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/amKFb4PfJWAbcR7tRHW7qU.jpg" mos="" link="" align="" fullscreen="" width="1280" height="800" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:62.50%;"><img id="dtkgbDACaJLQoPq8KJszoU" name="Kubuntu2" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/dtkgbDACaJLQoPq8KJszoU.jpg" mos="" link="" align="" fullscreen="" width="1280" height="800" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1366px;"><p class="vanilla-image-block" style="padding-top:56.22%;"><img id="nBQmn2JhFf9XfykYJtKzoU" name="xubuntu1" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/nBQmn2JhFf9XfykYJtKzoU.jpg" mos="" link="" align="" fullscreen="" width="1366" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Linux, at its most basic level, is all about “choice.” You can choose your distro, your window manager, desktop, apps, and more. Where Ubuntu, well, actually the Ubuntu community, does things a little differently is in the flavors of the OS. Each flavor, be it official or community-maintained, has “buntu” in its name.</p><div ><table><thead><tr><th class="firstcol " ><p>Flavor</p></th><th
><p>Desktop Environment</p></th><th
><p>Best For</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Ubuntu</p></td><td
><p>GNOME</p></td><td
><p>Newcomers</p></td></tr><tr><td class="firstcol " ><p>Kubuntu</p></td><td
><p>KDE Plasma</p></td><td
><p>Newcomers / General Use</p></td></tr><tr><td class="firstcol " ><p>Xubuntu</p></td><td
><p>Xfce</p></td><td
><p>Low spec machines</p></td></tr><tr><td class="firstcol " ><p>Lubuntu</p></td><td
><p>LXQt</p></td><td
><p>Low spec machines</p></td></tr><tr><td class="firstcol " ><p>Ubuntu MATE</p></td><td
><p>MATE</p></td><td
><p>Low spec machines / users who prefer older GNOME UI</p></td></tr><tr><td class="firstcol " ><p>Ubuntu Budgie</p></td><td
><p>Budgie</p></td><td
><p>Low spec machines</p></td></tr><tr><td class="firstcol " ><p>Ubuntu Studio</p></td><td
><p>KDE Plasma</p></td><td
><p>Creators</p></td></tr></tbody></table></div><p>This makes them easily identifiable to the end user. Sure, there are other distros based on Ubuntu that put their own name and spin on things (Pop! OS springs to mind), but the above list are the Ubuntu distros that many think of. I’ve also placed some suggested use cases for each distro, but ultimately that choice lies with you.</p><p>If I wished, I could install Ubuntu and then install another desktop environment, all via the software center or the terminal. In the past, I used to install Ubuntu and then install KDE. Now I just install Kubuntu and enjoy the KDE Plasma user interface.</p><h2 id="great-documentation-2">Great documentation</h2><p>Good documentation is what every project needs, and Ubuntu sets a high standard for documentation. If you were to Google “How can I change directory in a Linux terminal?”, chances are that you will get an answer that refers to Ubuntu. Either a direct Canonical / Ubuntu-backed document or something from a forum post. Ubuntu has been around so long now that it permeates the search results. Raspberry Pi is also doing the same thing, which I love to see!</p><p>The<a data-analytics-id="inline-link" href="https://help.ubuntu.com/"><u> official Ubuntu documentation</u></a> is excellent and covers trivial and technical issues with clarity. However, the <a data-analytics-id="inline-link" href="https://help.ubuntu.com/community/CommunityHelpWiki" target="_blank"><u>unofficial documentation</u></a> is also top-notch and provides knowledge for those uncommon issues, particularly when using applications not provided by Ubuntu.</p><p>As somebody who has over a decade of experience writing and editing technical documentation and tutorials, I really appreciate good documentation.</p><h2 id="you-re-just-an-ubuntu-fanboy-2">“You’re just an Ubuntu fanboy!”</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1824px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="ynBsnVmUWkH9JsLEupKnzU" name="how" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/ynBsnVmUWkH9JsLEupKnzU.jpg" mos="" link="" align="" fullscreen="" width="1824" height="1368" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3648px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="iYVZsXWVLAgA4mqePXhPmW" name="disks" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/iYVZsXWVLAgA4mqePXhPmW.jpg" mos="" link="" align="" fullscreen="" width="3648" height="2736" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3648px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="afcAY5qSriDMjyVWsnVghW" name="ogg" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/afcAY5qSriDMjyVWsnVghW.jpg" mos="" link="" align="" fullscreen="" width="3648" height="2736" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3648px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="7btNp2YcWnLhE4Jucii5iV" name="madlab" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/7btNp2YcWnLhE4Jucii5iV.jpg" mos="" link="" align="" fullscreen="" width="3648" height="2736" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1824px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="gLRyQc2VDiY8ntm8XPRw7V" name="bcb" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/gLRyQc2VDiY8ntm8XPRw7V.jpg" mos="" link="" align="" fullscreen="" width="1824" height="1368" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 6</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2736px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="VEKqJp9NxKjkCT5vi7AA9V" name="laptop" alt="Ubuntu" src="https://cdn.mos.cms.futurecdn.net/VEKqJp9NxKjkCT5vi7AA9V.jpg" mos="" link="" align="" fullscreen="" width="2736" height="2736" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>I’ll support any cause that aims to open up computing to everyone. Free/libre software has proven itself to be used in science (NASA, CERN, ESA), technology (<em>waving arms around to mime “Internet”</em>), entertainment, etc. Why pay for an operating system when you can get a great one for free?</p><p>My recent <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/live/building-a-linux-gaming-pc" target="_blank"><u>Bazzite experiment</u></a> has proven to me that Linux is now a capable gaming platform, and yes, I already own a Steam Deck, so I should’ve realised. But using a 4060Ti and 12th-gen Intel system to game is a bit different from what the Steam Deck can offer, and it has a bigger impact.</p><p>Digging into my personal history with Ubuntu, and in the early 2010s, I ran installfests at locations as diverse as Barcamp Blackpool, Oggcamp, and a disused paint warehouse in the center of Liverpool. I also co-created UCubed, an Ubuntu-centric unconference which later became the genesis for the Raspberry Jam movement, thanks to Alan O’Donohoe, who used UCubed as a template for the worldwide Jam movement.</p><p>Nobody at Canonical paid or asked me to do that. I felt that, after using Ubuntu for many years and being part of a larger tech community that included numerous software developers and system administrators, I had to give something back.</p><h2 id="wrapping-up-2">Wrapping Up</h2><p>You may not agree with me, heck, I love that you will probably comment about “Why distro X is better than Y!” and “GNOME sucks, I use Openbox!” but this is just my opinion, based on experience and life events that have ultimately shaped how I use Linux. Your journey will be different, and so will your choices. I respect them all and urge you to continue your Linux journey.</p>
https://www.tomshardware.com/software/operating-systems/ive-been-using-linux-for-a-quarter-of-a-century-so-why-do-i-keep-coming-back-to-ubuntu
Ubuntu Linux has formed a large part of my Linux journey, and I think it is the best Linux for all levels of users.
vBHyEkMpf96EjvnbT6PPd
Fri, 19 Sep 2025 13:54:21 +0000 Operating Systems
Software
Les Pounder
Tom&#039;s Hardware
Ubuntu
Ubuntu
<![CDATA[ Nvidia wants 10Gbps HBM4 to blunt AMD’s MI450, report claims — company said to be pushing suppliers for more bandwidth ]]>
<p>Nvidia is pressing its memory vendors to push beyond JEDEC’s official HBM4 baseline. According to <a data-analytics-id="inline-link" href="https://www.trendforce.com/presscenter/news/20250918-12719.html" target="_blank"><em>TrendForce</em></a>, the company has requested 10Gb/s-per-pin stacks for its 2026 Vera Rubin platform, a move designed to raise per-GPU bandwidth ahead of AMD’s next-generation MI450 Helios systems.</p><p>At 8Gb/s per pin — the rate JEDEC specifies for HBM4 — a single stack delivers just under 2 TB/s across the new 2,048-bit interface. Raising that to 10Gb/s bumps the total to 2.56 TB/s per stack. With six stacks, a single GPU clears 15 TB/s of raw bandwidth. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-rubin-cpx-die-shot-reveals-graphics-specific-hardware-blocks-not-needed-for-an-ai-gpu-rubin-cpx-may-form-the-foundation-of-next-gen-rtx-6090">Rubin CPX</a>, Nvidia's compute-optimized config built to handle the most demanding inference workloads, is advertised with 1.7 petabytes per second across a full NVL144 rack. The higher the pin speed, the less margin Nvidia needs elsewhere to hit those numbers.</p><p>But driving 10Gb/s HBM4 isn’t a given. Faster I/O brings higher power, tighter timing, and more strain on the base die. TrendForce notes that Nvidia may segment Rubin SKUs by HBM tier if costs or thermals spike. That means 10Gb/s parts for Rubin CPX and lower-speed stacks for the standard Rubin configuration. The fallback is already in view: staggered supplier qualification and extended validation windows to stretch yield.</p><p>SK hynix remains Nvidia’s dominant HBM supplier and says it has <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/dram/sk-hynix-completes-development-of-hbm4-2-048-bit-interface-and-10-gt-s-speeds-promised">completed HBM4 development</a> and is ready for mass production. The company has referenced “over 10Gb/s” capability but hasn’t published die specs, power targets, or process details.</p><p>Samsung, by contrast, is more aggressive on node migration. Its HBM4 base die is moving to 4nm FinFET, a logic-class node intended to support higher clock speeds and lower switching power. That could give Samsung an edge at the high end, even if SK hynix ships more volume. Micron has confirmed sampling of HBM4 with a 2,048-bit interface and bandwidth exceeding 2 TB/s, but hasn’t said whether 10Gb/s is in scope.</p><p><a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/amd-preps-rack-scale-instinct-mi450x-if128-with-128-gpus-to-challenge-nvidias-vr200-nvl144-in-2026">AMD’s MI450 is still on the horizon</a>, but the memory spec is already known. Helios racks are expected to support up to 432GB of HBM4 per GPU, giving AMD a route to match or exceed Nvidia on raw capacity. With CDNA 4, it also gains architectural upgrades that aim squarely at Rubin’s inference advantage.</p><p>Nvidia clearly wants to make memory faster. But the more it leans on 10Gb/s HBM4, the more exposed it becomes to supplier variation, yield risks, and rack-level power constraints at a time when the margin for error is shrinking.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/nvidia-wants-10gbps-hbm4-to-rival-amd-mi450
Nvidia is reportedly pressing its memory vendors to push beyond JEDEC’s official HBM4 baseline, reportedly requesting 10Gb/s-per-pin stacks for its 2026 Vera Rubin platform.
X3s3sKcEkaaNpPKYQZz2rZ
Fri, 19 Sep 2025 13:52:00 +0000 GPUs
PC Components
lukejamesalden@gmail.com (Luke James)
Luke James
Nvidia
Nvidia
Nvidia
<![CDATA[ $115 million ransomware hacker arrested over extortion attacks — Scattered Spider alumnus allegedly involved in over 120 computer network intrusions targeting 47 U.S. entities ]]>
<p>A teenage hacker from the UK has been arrested on charges of money laundering conspiracy, computer fraud, and wire fraud conspiracy, and stands accused of being part of a hacking group that extorted over $115 million from close to 50 victims in America, alongside various attacks in the UK, according to the <a data-analytics-id="inline-link" href="https://www.justice.gov/opa/pr/united-kingdom-national-charged-connection-multiple-cyber-attacks-including-critical" target="_blank">Department of Justice</a>. Alleged to be part of the infamous "Scattered Spider" hacking group, it seems that 19-year-old Thalha Jubair wasn't one of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/cyber-security/hacker-ransomware-groups-announce-retirement-to-enjoy-their-golden-parachutes-no-further-attacks-planned-future-attributed-activities-will-relate-to-undisclosed-past-breaches">lucky few who were able to ride off into the sunset</a> after its retirement announcement earlier this month.</p><p>Jubair is alleged to have been involved in at least 120 different computer network intrusions over a three-year period, starting in 2022. The complaint, filed with the District of New Jersey, accuses Jubair of targeting 47 U.S. entities. He and the group he was a part of, Scattered Spider, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/cyber-security/jaguar-land-rover-shuts-down-production-due-to-ransomware-attack-scattered-lapsus-usd-hunters-takes-responsibility">were well known for utilizing social engineering techniques to gain access to corporate networks</a> and then steal data or use ransomware attacks to blackmail and extort the businesses for profit.</p><p>Over just the past few years, Jubair is alleged to have extorted over $115 million from victims all over the world. Although the FBI Newark field office is investigating the case, it's doing so in conjunction with the UK National Crime Agency and City of London Police, as well as police agencies in the Netherlands, Romania, Canada, and Australia.</p><p>He also seems to have had particular involvement in controlling the money extracted from victims. Portions of the ransom payments were sent to cryptocurrency wallets controlled by Jubair. In July 2024, when law enforcement seized the server said to be storing the wallets containing cryptocurrency worth some $36 million, Jubair is alleged to have transferred some of those tokens, with a reported net worth of $8.4 million, to another wallet.</p><p>“The arrest of Thalha Jubair underscores an undeniable truth: no matter how elusive or destructive these cyber-criminal syndicates are, we will continue to pursue those who allegedly extort our businesses and ensure they are held accountable,” said Special Agent in Charge Stefanie Roddy for the FBI.</p><p>Jubair was arrested in London and was formally charged on September 18. Alongside the charges in U.S. courts, he also stands accused of an <a data-analytics-id="inline-link" href="" target="_blank">attack against Transport for London (TFL) infrastructure</a> in August 2024, where customer data, including names, contact details, and addresses, were compromised.</p><p>London police also arrested a fellow accused hacker, 18-year-old Owen Flowers, who also stands accused of involvement with the TFL hack. Flowers faces additional charges in the U.S., too, where he's accused of attacking health companies SSM Health Care Corporation and Sutter Health.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/usd115-million-ransomware-hacker-arrested-over-extortion-attacks-scattered-spider-alumnus-allegedly-involved-in-over-120-computer-network-intrusions-targeting-47-u-s-entities
A British National has been arrested and accused of helping to facilitate over $155 million in exploitation and blackmail attacks against U.S. and British companies. The 19-year-old has ties to the infamous Scattered Spider hacking group.
QsGtyWVN3m9zzcuSPWpv2R
Fri, 19 Sep 2025 11:56:09 +0000 Cybersecurity
Tech Industry
Jon Martindale
Getty Images/Seksan Mongkhonkhamsao
Hooded hacker with bad posture hunches over their Matrix-code workstation.
Hooded hacker with bad posture hunches over their Matrix-code workstation.
<![CDATA[ I have the most luxurious standing desk imaginable, but I can't quite bring myself to game while standing ]]>
<p>For more than two-and-a-half years, I've been blessed to call the inimitable <a data-analytics-id="inline-link" href="https://secretlab.co/pages/magnus-pro">Secretlab Magnus Pro</a> my main workspace. A colossal sit-stand desk, the hulking slab of metal comes with a nifty integrated sit-stand controller, two gargantuan legs, extensive cable management, and an eye-wateringly expensive magnetic ecosystem — the entire surface is magnetic, and can be used with a range of metal cable management devices, headphone stands, and cable sleeves. It is truly the Rolls-Royce of office furniture, vast and expensive, the purview of only the most hardcore work surface connoisseurs.</p><p>Yet on reflection of my time behind Secretlab's (probably) bombproof behemoth, I've come to the awful realization that I might have played myself. Somewhere along the way these last two years, I simply stopped standing up to use this desk, a full 50% of this desk's potential left on the table, if you'll pardon the pun. More specifically, I've realized that I own one of the world's most luxurious standing desks for gaming, yet I never stand up to game.</p><p>The benefits of using a standing desk are well-documented and numerous. Did you know that <a data-analytics-id="inline-link" href="https://pubmed.ncbi.nlm.nih.gov/24297826/" target="_blank">standing up at your desk</a> after lunch can reduce blood sugar spikes by up to 43%?<a data-analytics-id="inline-link" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10973891/" target="_blank"> Reducing your time spent</a> sitting can lower blood pressure in older adults, reduce <a data-analytics-id="inline-link" href="https://www.tandfonline.com/doi/10.1080/00140139.2024.2414197?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed" target="_blank">pain and fatigue</a>, and, of course, help your posture. There's even some evidence to suggest that moving between <a data-analytics-id="inline-link" href="https://onlinelibrary.wiley.com/doi/10.1111/psyp.14634" target="_blank">sitting and standing makes you more productive at work</a>.</p><p>But it's not productivity I'm after. While I've historically been happy to stand up at my desk for work, it's for play where I've generally neglected standing up altogether, leading to my realisation that I've stopped standing up completely while at my desk and probably need to take a long, hard look at myself. But it got me thinking: Is there any market at all for a desk that lets you stand whilst gaming? The Magnus Pro is dynamite for productivity, but would I <em>ever</em> want to stand up to game behind it?</p><h2 id="a-place-called-vertigo-2">A place called vertigo</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:6000px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="aGLyaVyf4a9jaAw8gnKhaZ" name="IMG_1184" alt="Secretlab Magnus Pro" src="https://cdn.mos.cms.futurecdn.net/aGLyaVyf4a9jaAw8gnKhaZ.jpg" mos="" align="middle" fullscreen="" width="6000" height="3375" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>There are numerous reasons why someone might choose to sit down while gaming. Notably, gaming is generally a relaxing thing you do to unwind. The bash marks on my Xbox controller and that Samsung monitor I smashed in 2017 might disagree with me, but for inexplicable reasons, I feel like anyone standing up to game would generally be untrustworthy. <br><br>I associate standing up to game with those old Xbox 360 arcade machines you'd find at GameStop (or your local equivalent), or with going to the Casino. If you dialled into an Xbox Live party or your usual Discord haunt and found out that one of your friends was standing up to play during that particular session, you'd probably ask them if everything was okay, or perhaps if there was something wrong with their chair.</p><p>Either gaming is relaxing (like a nice leisurely <em>Rome Total War </em>conquest), or it is quite intense (the throngs of the new <em>Battlefield 6</em>). For me, both call for the planted assurance of being sat in a sturdy chair (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/secretlab-titan-evo-2022-review-superior-gaming-chair">Secretlab's Titan Evo</a> if you were wondering). I'm pretty sure doing any kind of FPS or fast-paced gaming standing up would make most people feel a bit nauseous or dizzy.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:6000px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="r2kacY8864oNBzgu4cjSaZ" name="IMG_1182" alt="Secretlab Magnus Pro" src="https://cdn.mos.cms.futurecdn.net/r2kacY8864oNBzgu4cjSaZ.jpg" mos="" align="middle" fullscreen="" width="6000" height="3375" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>I'm not really sure why I've never realized it before, but I own a $1,000 sit-stand gaming desk that I never use for gaming, and now that I've figured that out, I'm quite sad. Perhaps then, the beauty of a sit-stand desk dedicated to gamers is to be found in the work half of your work-life balance. Stand up during the day and get the posture and circulatory benefits while you're at work, then put your feet up at the end of the day.
I might try my hand at a little upright recreation going forward, but I can't help thinking that I'll last maybe five minutes before I decide I'd much rather sit down. <br><br>Have you gamed while standing up since the heyday of arcades? Let me know in the comments.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/peripherals/desks/i-have-the-most-luxurious-standing-desk-imaginable-but-i-cant-quite-bring-myself-to-game-while-standing
I have a $1,000 sit-stand desk designed for gaming, so why do I never stand up to game?
e6MADYm8BNDJeD44b6Dy7N
Fri, 19 Sep 2025 11:00:00 +0000 Desks
Peripherals
stephen.warwick@futurenet.com (Stephen Warwick)
Stephen Warwick
Future
Secretlab Magnus Pro
Secretlab Magnus Pro
<![CDATA[ Nvidia drops a cool $900 million on Enfabrica tech and hiring its CEO, report claims — AI networking chip company boasts capacity to connect 100,000 GPUs together ]]>
<p>Nvidia has reportedly paid out more than $900 million to hire the CEO of AI networking hardware company, Enfabrica. Rochan Sankar and a number of his colleagues at Enfabrica will become Nvidia staff members as part of the deal, and Nvidia will get the option to license the company's hardware, <a data-analytics-id="inline-link" href="https://www.cnbc.com/2025/09/18/nvidia-spent-over-900-million-on-enfabrica-ceo-ai-startup-technology.html" target="_blank">according to CNBC. </a></p><p>The deal will reportedly involve cash and stock options, and comes just two years after Nvidia invested part of a $125 million fund into Enfabrica. However, while the company was valued in the high hundreds of millions, paying close to a billion dollars for a handful of staff and a license would potentially value the company much higher. Clearly, Enfabrica has talent and technology that Nvidia feels is crucial to its ongoing development. An Nvidia spokesperson declined to comment on the report.</p><p>Indeed, it's not hard to see why. Nvidia's meteoric rise in value and prominence in recent years has been on the back of its AI hardware developments. Today, its<a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-gb200-production-ramps-up-after-suppliers-tackle-ai-server-overheating-and-liquid-cooling-leaks"> GPUs power many of the world's data centers </a>(and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-is-building-100-ai-factories-jensens-50-year-gambit-begins">"AI factories"</a>) with 72 within each of the latest Blackwell GPU stacks. In some of these data centers, though, 10s or even hundreds of thousands of GPUs can be working in conjunction to power the latest AI technologies, and it's the connections between those chips that Enfabrica may be able to help improve.</p><p>Nvidia has touted its NVLink technology for allowing high-speed communication between its GPUs, allowing them to work on the same task in parallel without bottlenecking one another or causing synchronization issues. Enfabrica's tech may go beyond that, though, with the company claiming it can help connect more than 100,000 GPUs. That's the kind of scalability that Nvidia is looking for as it continues its global expansion.</p><p>This major (and expensive) hire for Nvidia mirrors those announced by other tech firms in recent months. Meta famously spent tens of billions hiring the CEO of ScaleAI and other AI experts from various companies, and many of the top tech firms like OpenAI, Microsoft, Anthropic, and Google have been <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-says-meta-is-offering-obscene-usd100m-bonuses-to-poach-ai-employees-and-even-bigger-salaries-openai-ceo-says-none-of-our-best-people-decided-to-take-them-up-on-that">trying to poach each other's top talent</a> for many months now. More recently, however, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/scale-ai-lays-off-200-employees-one-month-after-metas-usd14-billion-investment-says-it-scaled-up-too-quickly">Meta announced a freeze</a> to its hiring and spending spree, and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/google-terminates-200-ai-contractors-ramp-down-blamed-but-workers-claim-questions-over-pay-and-job-insecurity-are-the-real-reason-behind-layoffs">Google fired several hundred of its AI contractors</a>.</p><p>Nvidia isn't slowing down, though. Alongside this new hire, it recently announced its purchase of a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">$5 billion stake in CPU giant, Intel,</a> and that it was <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/uk-cosies-up-to-big-tech-with-usd42-billion-data-center-and-ai-investment-deal">investing over $700 million into the development of a new data center</a> in the UK in partnership with startup Nscale.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/nvidia-drops-a-cool-usd900-million-on-enfabrica-tech-and-hiring-its-ceo-report-claims-ai-networking-chip-company-boasts-capacity-to-connect-100-000-gpus-together
Nvidia has hired the CEO of AI networking chip company, Enfabrica, alongside a number of other staff for $900 million. The deal will also give Nvidia a license to use the unique technology, which the developers claim can link up 100,000 GPUs.
6pkJHEdwsnDJU6mPAmyF3h
Fri, 19 Sep 2025 10:56:04 +0000 Tech Industry
Jon Martindale
Getty Images/VCG
Nvidia CEO Jensen Huang looking smug during a
Thematic Event on Advanced Manufacturing in China.
Nvidia CEO Jensen Huang looking smug during a
Thematic Event on Advanced Manufacturing in China.
<![CDATA[ Here's how I installed Windows 11 using a nano-sized 2.4GB ISO — final install weighed in at a mere 8.36GB, two-thirds the size of a normal Windows install, courtesy of Nano11 Builder ]]>
<p>I’ve yet to make the leap to Windows 11; I’ve got an IT request form to file, and I’ll probably need a weekend to migrate all of my files. I’m probably best known for being a Linux and Raspberry Pi advocate, so why should I want to move to Windows 11? Well, firstly, I have to for my work. However, I also edit videos using DaVinci Resolve, and yes, you can install it on Linux, but I’ve yet to be successful.</p><p>Recently, we <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/windows/nano11-compresses-windows-11-install-footprint-to-as-little-as-2-8gb-extreme-experimental-script-is-3-5-times-smaller-than-tiny11-and-comes-with-none-of-the-fluff"><u>covered a story</u></a> on NTDEV’s latest project to shrink the Windows 11 installation ISO image. The project, Nano11 Builder, aims to create the smallest ISO possible, but it is “an extreme experimental script designed for creating a quick and dirty development testbed,” according to the pseudonymous developer.</p><p>Why would you use such a tool to make smaller ISOs? Because you can, and they could be useful for developing your own appliances based on Windows. Should you use it for mission-critical applications? Absolutely not, but am I going to give it a go and see how small an installation I can create? Of course!</p><h2 id="creating-the-smallest-windows-11-isos-possible-2">Creating the smallest Windows 11 ISOs possible</h2><div ><table><caption>ISO Sizes</caption><thead><tr><th class="firstcol empty" ></th><th
><p>Size KB</p></th><th
><p>Difference to Microsoft</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Direct from Microsoft</p></td><td
><p>5,695,402 (5.69GB)</p></td><td
></td></tr><tr><td class="firstcol " ><p>Tiny</p></td><td
><p>2,812,798 (2.81GB)</p></td><td
><p>50.6% Smaller</p></td></tr><tr><td class="firstcol " ><p>Nano</p></td><td
><p>2,145,104 (2.14GB)</p></td><td
><p>62.30%</p></td></tr></tbody></table></div><p>Using the Nano and Tiny scripts to tweak the official ISO image was easy. I just followed the instructions and waited. No, seriously, I waited a long time for the scripts to complete, around 90 minutes on my Lenovo X390 with an eighth-generation Intel Core i5.</p><p>The base ISO is direct from Microsoft, and it weighs in at 5,695,402KB (5.69GB), which isn’t huge by today's standards, but it is bulging with applications and tools that some users may not want or need. The Tiny version of the ISO is half the size of the base ISO, and the nano is nearly two-thirds the size of the base image. A smaller ISO means that plenty of apps and features have been removed, leaving us with a core Windows 11 installation.</p><p>With that in mind, how small is the final installation for each of the ISOs? There is only one way to find out, and it is going to take a little more time.</p><h2 id="installing-windows-11-three-times-for-science-2">Installing Windows 11 Three Times…for Science?</h2><div ><table><caption>ISO Sizes</caption><thead><tr><th class="firstcol empty" ></th><th
><p>Size KB</p></th><th
><p>Difference to Microsoft</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Direct from Microsoft</p></td><td
><p>26,445,119,488 (24.6GB)</p></td><td
></td></tr><tr><td class="firstcol " ><p>Tiny</p></td><td
><p>10,865,098,752 (10.1GB)</p></td><td
><p>59.1% Smaller</p></td></tr><tr><td class="firstcol " ><p>Nano</p></td><td
><p>8,984,236,032 (8.36GB)</p></td><td
><p>66.2% Smaller</p></td></tr></tbody></table></div><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1366px;"><p class="vanilla-image-block" style="padding-top:56.22%;"><img id="tx7RFNjXEJwE8ULk4rDAsE" name="Win11-Full1" alt="Smol Windows" src="https://cdn.mos.cms.futurecdn.net/tx7RFNjXEJwE8ULk4rDAsE.jpg" mos="" align="middle" fullscreen="" width="1366" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>For the Nano and Tiny installs, I used a VirtualBox virtual machine with 8GB of RAM, four cores, and a 40GB virtual hard drive. For the full Windows 11 installation, I used a Lenovo X220, a machine that is clearly not intended to run Windows 11. I did need to use Rufus to write the ISO images to a USB flash drive, including a quick hack to bypass RAM, TPM, and Secure Boot requirements.</p><p>Why the difference? Because after successfully installing Windows 11 Nano and Tiny ISOs in the VM, the full Windows 11 ISO would not install in the VM, despite meeting the requirements. As the focus is on the installation size, the tests are valid.</p><p>Installing Nano started off simple. A few steps were skipped (no reg key and no choice of Windows 11 version), and the installation started. I got to 75% and then the installer crashed with an error, and the laptop rebooted. I tried again. Same issue. How did I bypass it? Well, it took raising a GitHub issue, receiving a suggestion to try the older setup process, and yes, it worked!</p><p>Installation was slow on my VM, and at times I had to reboot, but I got there in the end. I had to follow <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/install-windows-11-without-microsoft-account" target="_blank"><u>our guide</u></a> to bypass the Windows sign-in and use a local account. Hat tip to our old Editor in Chief for writing that guide.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1366px;"><p class="vanilla-image-block" style="padding-top:56.22%;"><img id="tx7RFNjXEJwE8ULk4rDAsE" name="Win11-Full1" alt="Smol Windows" src="https://cdn.mos.cms.futurecdn.net/tx7RFNjXEJwE8ULk4rDAsE.jpg" mos="" align="middle" fullscreen="" width="1366" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>After installation and setup, the nano-sized Windows 11 took up 8.36GB of my virtual hard disk, which is two-thirds smaller than a typical Windows 11 installation.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:74.02%;"><img id="n4ChEh2yGhPvWDxTnBBMzE" name="app-list" alt="Smol Windows" src="https://cdn.mos.cms.futurecdn.net/n4ChEh2yGhPvWDxTnBBMzE.gif" mos="" align="middle" fullscreen="" width="1024" height="758" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The OS was intentionally light. Not even a web browser. Sure I had the Microsoft Store, but if I didn’t want to use that, then the only way to get apps would be downloading the executables on another machine and copying them across via USB.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="ejBGaspk7tD5MrLrrEivqE" name="Tiny11-Install" alt="Smol Windows" src="https://cdn.mos.cms.futurecdn.net/ejBGaspk7tD5MrLrrEivqE.jpg" mos="" align="middle" fullscreen="" width="1024" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>I also test installed the previous smallest Windows 11 installation, a “Tiny” version from the same developer. The installation process followed the exact same steps as nano, falling down at the same issues. Luckily the older setup process got me through the install, but I was able to go out and get lunch while it did its thing. Installation complete and it claims 10.1GB of my hard drive, 1.5GB more than the Nano install. The lack of apps was the same as Nano, so it looks like Nano has a lot more cut from behind the scenes.</p><h2 id="so-what-can-i-do-with-the-nano-sized-windows-11-2">So what can I do with the nano-sized Windows 11? </h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:75.00%;"><img id="oiGwYq23BY5bhMjDXHENqE" name="nano-apps" alt="Smol Windows" src="https://cdn.mos.cms.futurecdn.net/oiGwYq23BY5bhMjDXHENqE.jpg" mos="" align="middle" fullscreen="" width="1024" height="768" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>What can I do with the nano-sized Windows 11? Well, not much, unless I install apps, but I could use this as a base to build appliances (machines with a limited purpose) that I have to use Windows for. The small install base means that performance, even on constrained hardware, should be adequate. That said, my VM had four cores and 8GB of RAM; it felt “OK,” but not great.</p><p>Personally, I would install an emulation system like LaunchBox onto an older gaming rig. Say something around Intel eighth to tenth-generation (AMD Ryzen 3000 series too) and using an older GPU, like an Nvidia RTX 2000 series, to give me the grunt that I need to emulate Nintendo Gamecube, Sony PlayStation 2, and OG XBOX era machines. Throw all of this into a DIY arcade cabinet build and never update it, or connect it to the Internet, and you’ve got yourself a great gaming setup from recycled kit.</p><p>Now, I need to send that form to IT before October 14…</p>
https://www.tomshardware.com/software/operating-systems/heres-how-i-installed-windows-11-using-a-nano-sized-2-4gb-iso-final-install-weighed-in-at-a-mere-8-36gb-two-thirds-the-size-of-a-normal-windows-install-courtesy-of-nano11-builder
Using some custom scripts and a base Windows 11 ISO install image, just how much can you remove and keep a functional operating system?
AWWEp84qZZ6PpEF8aq4a68
Fri, 19 Sep 2025 10:00:00 +0000 Operating Systems
Software
Les Pounder
Future / OpenClipArt
Smol Windows
Smol Windows
<![CDATA[ Huawei unveils Atlas 950 SuperCluster — promises 1 ZettaFLOPS FP4 performance and features hundreds of thousands of 950DT APUs ]]>
<p>Huawei has <a data-analytics-id="inline-link" href="https://www.huawei.com/en/news/2025/9/hc-xu-keynote-speech">unveiled</a> its next-generation data-center scale AI solution that can offer 1 FP4 ZettaFLOPS performance for AI inference and 524 FP8 ExaFLOPS for AI training at its Huawei Connect 2025 conference on Thursday. The new SuperCluster 950 system runs hundreds of thousands of the company's Ascend 950DT neural processing units (NPUs) and promises to be one of the most powerful supercomputers for artificial intelligence on the planet. Huawei expects its SuperCluster to compete with Nvidia's Rubin-based systems in late 2026.</p><h2 id="massive-performance-2">Massive performance</h2><p>Huawei's Atlas 950 SuperCluster will consist of 64 Atlas 950 SuperPoDs, which are the company's rack-scale AI solutions akin to Nvidia's GB300 NVL72 or the next-generation Vera Rubin NVL144. The Atlas 950 SuperCluster will be built upon 524,288 Ascend 950DT AI accelerators distributed across over 10,240 optically interconnected cabinets.</p><p>The supercomputer purportedly offers up to 524 FP8 ExaFLOPS for AI training and up to 1 FP4 ZettaFLOPS for AI inference (<a data-analytics-id="inline-link" href="https://huggingface.co/blog/RakshitAralimatti/learn-ai-with-me">MXFP4</a> to be more specific), which puts it just behind leading-edge AI supercomputers, such as Oracle's OCI Supercluster running 131,072 B200 GPUs and offering peak performance of up to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-and-oracle-team-up-for-zettascale-cluster-available-with-up-to-131072-blackwell-gpus">2.4 FP4 ZettaFLOPS for inference introduced last year</a>. Keep in mind these figures pertain to peak performance numbers, so it remains to be seen whether they can be achieved in real life.</p><p>This SuperCluster is designed to support both RoCE (Remote Direct Memory Access over Converged Ethernet) and Huawei's proprietary UBoE (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/huawei-to-open-source-its-ub-mesh-data-center-scale-interconnect-soon-details-technical-aspects-one-interconnect-to-rule-them-all-is-designed-to-replace-everything-from-pcie-to-tcp-ip">UnifiedBus</a> over Ethernet) protocols, though it remains to be seen how fast the latter will be adopted. According to Huawei, UBoE offers lower idle-state latency, higher hardware reliability, and requires fewer switches and optical modules than traditional RoCE setups.</p><p>Huawei positions its Atlas 950 SuperCluster to support training and inference workloads for AI models with hundreds of billions to tens of trillions of parameters. Huawei believes this platform is well-suited for the next wave of large-scale dense and sparse models, thanks to its combination of compute throughput, interconnect bandwidth, and system stability. Though given its size, it is unclear how many companies will be able to accommodate the system.</p><h2 id="massive-footprint-2">Massive footprint</h2><p>Huawei admits that it cannot build processors that would challenge Nvidia's GPUs in terms of performance. Therefore, to achieve 1 ZettaFLOPS with the Atlas 950 SuperCluster, it intends to use a brute force approach, utilizing hundreds of thousands of AI accelerators to compete against Nvidia Rubin-based clusters in 2026–2027.</p><p>A common building block of Huawei's Atlas 950 SuperCluster is the Atlas 950 SuperPoD that integrates 8,192 Ascend 950DT chips, representing a 20-fold increase in processing units compared to the Atlas 900 A3 SuperPoD (also known as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/huaweis-new-ai-cloudmatrix-cluster-beats-nvidias-gb200-by-brute-force-uses-4x-the-power">CloudMatrix 384</a>) and a massive increase in compute performance — 8 FP8 ExaFLOPS and 16 FP4 ExaFLOPS.</p><p>Performance of the Atlas 950 SuperCluster is truly impressive on paper; it is said to be massively higher compared to Nvidia's Vera Rubin NVL144 (1.2 FP8 ExaFLOPS, 3.6 NVFP4 ExaFLOPS), a product that the company compares it to. However, that performance comes at a price, namely size. The Atlas 950 SuperCluster setup includes 160 total cabinets — 128 for computation and 32 for communications — spread across 1,000 square meters, which is about the size of two basketball courts. By contrast, Nvidia's Vera Rubin NVL144 is a rack-scale solution that consists of one compute rack and one cable and switch rack that requires just several square meters of space.</p><p>As for Huawei's Atlas 950 SuperCluster — which consists of 64 Atlas 950 SuperPoDs and should measure around 64,000 m2 — its size is comparable to 150 basketball courts, or nine regulation soccer fields. Keep in mind, though, that a real campus would likely require additional space for power rooms, chillers/cooling towers, battery/UPS systems, and support offices, so the total site footprint could be significantly larger than 64,000 m².</p><h2 id="the-road-ahead-2">The road ahead</h2><p>One of the things about selling server hardware is that customers always want to know what is next, so in addition to having a good product, it is vital to have a roadmap. So at the Huawei Connect, the company disclosed plans to launch the Atlas 960 SuperCluster alongside the Atlas 960 SuperPoD in the fourth quarter of 2027.</p><p>This next-generation system will scale up to more than 1 million Ascend 960 NPUs and will provide 2 FP8 ZettaFLOPS and 4 MXFP4 ZettaFLOPS of performance. It will also support both UBoE and RoCE, with the former expected to deliver improved latency and uptime metrics while continuing to rely on Ethernet.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/huawei-unveils-atlas-950-supercluster-touting-1-fp4-zettaflops-performance-for-ai-inference-and-524-fp8-exaflops-for-ai-training-features-hundreds-of-thousands-of-950dt-apus
Huawei has outlined its roadmap for Zettascale AI systems that will rely on hundreds of thousands and then millions of AI accelerators.
2p3W7W4U5pS6Kt2ja9wLdS
Fri, 19 Sep 2025 09:53:59 +0000 Artificial Intelligence
Tech Industry
ashilov@gmail.com (Anton Shilov)
Anton Shilov
Huawei
Huawei
Huawei
<![CDATA[ Ditching Windows 10? Here's how I installed Windows 11, removed AI, and stripped out unnecessary features using Flyoobe ]]>
<p>Windows 10 is set to go end-of-life (EOL) on October 14, and while you can purchase an extended service license to keep your machine updated for a little longer, some will ultimately choose to upgrade to Windows 11. But what if your machine doesn’t meet Microsoft’s requirements? You can get around them quite easily with <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/clean-install-windows-11"><u>Rufus</u></a> or the focus of this how-to, <a data-analytics-id="inline-link" href="https://github.com/builtbybel/Flyoobe"><u>Flyoobe</u></a>.</p><p>Originally known as Flyby11, <a data-analytics-id="inline-link" href="https://github.com/builtbybel/Flyoobe" target="_blank"><u>Flyoobe</u></a> touts itself as “A better way to set up Windows,” and it has an expansive set of features that should make Windows 11 a much better experience on more modest hardware.</p><ul><li>Upgrade a Windows 10 machine to Windows 11</li><li>Remove AI components</li><li>Slim down Windows 11</li><li>Enhance the Windows 11 experience</li><li>Install commonly used applications</li></ul><p>In this how-to, I’ll be updating a laptop that is over a decade old to run Windows 11. This process can be applied to any machine running Windows 10. For those already running Windows 11, skip the upgrade and go right to the tweaks.</p><p>No matter what, though, you will need to install Flyoobe.</p><h2 id="download-and-install-flyoobe-2">Download and Install Flyoobe</h2><p><strong>1. Download the latest </strong><a data-analytics-id="inline-link" href="https://github.com/builtbybel/Flyoobe/releases"><u><strong>Flyoobe release</strong></u></a><strong> from the official GitHub repository.</strong> At the time of writing <a data-analytics-id="inline-link" href="https://github.com/builtbybel/Flyoobe/releases/tag/1.10.340"><u>this was 1.10.</u></a></p><p><strong>2. Extract the files to a folder on your desktop.</strong></p><p><strong>3. Double click on the Flyoobe application.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:875px;"><p class="vanilla-image-block" style="padding-top:46.86%;"><img id="65CLRJgUmrdr4CaEGNCB9E" name="Flyoobe-app" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/65CLRJgUmrdr4CaEGNCB9E.jpg" mos="" align="middle" fullscreen="" width="875" height="410" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><h2 id="upgrading-from-windows-10-to-windows-11-2">Upgrading from Windows 10 to Windows 11</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:666px;"><p class="vanilla-image-block" style="padding-top:89.04%;"><img id="xENShsSv9ayjyrsfa5da9E" name="Win10.JPG" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/xENShsSv9ayjyrsfa5da9E.jpg" mos="" align="middle" fullscreen="" width="666" height="593" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>On my test machine, an ancient Lenovo X220 with a paltry 2nd-Gen Intel Core i5, I have Windows 10, freshly installed and ready for this how-to. But I want to install Windows 11. I could use <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/clean-install-windows-11"><u>Rufus to make an installation USB drive</u></a>, but I wanted to use Flyoobe, which will download the latest Windows 11 ISO and install it using a Windows Server variant of the setup tools to skip hardware checks for TPM 2.0, Secure Boot, and, of course, the 2nd-Gen i5, which is most certainly not supported.</p><p>I’ll assume that you have an older machine running Windows 10, a machine that does not meet Microsoft’s Windows 11 requirements.</p><p><strong>1. Open Flyoobe.</strong></p><p><strong>2. Click on Get Windows 11.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="AHHs5pBTrk6qDwcWEADnAE" name="Win11Install1.JPG" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/AHHs5pBTrk6qDwcWEADnAE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Click on Download ISO from Microsoft website</strong>, this will trigger a browser to open.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="Ar7KTk2Y5V2G6WFswdiFAE" name="Win11Install2.JPG" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/Ar7KTk2Y5V2G6WFswdiFAE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Download the Windows 11 multi-edition ISO and set your preferred language. Click Confirm to move on.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1366px;"><p class="vanilla-image-block" style="padding-top:53.29%;"><img id="uXq7RwozMBSKUP7MSvq3CE" name="Win11Install3.JPG" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/uXq7RwozMBSKUP7MSvq3CE.jpg" mos="" align="middle" fullscreen="" width="1366" height="728" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>5. Click on the generated link to download the Windows 11 ISO.</strong></p><p><strong>6. Drag the downloaded ISO from the file manager into Flyoobe</strong>, this will trigger the installer to start.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:702px;"><p class="vanilla-image-block" style="padding-top:79.06%;"><img id="pDycnNr2KYwaJrdtiK2j8E" name="Win11Install4.JPG" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/pDycnNr2KYwaJrdtiK2j8E.jpg" mos="" align="middle" fullscreen="" width="702" height="555" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>7. Follow the instructions to run the install.</strong> When prompted to keep files, settings, apps or just personal files, make the choice most relevant to your requirements.</p><p><strong>8. When ready, click on Install to install Windows 11 over the Windows 10 installation. The installer will reboot automatically and run the Windows 11 post installation setup script.</strong></p><h2 id="tweaking-windows-11-with-flyoobe-2">Tweaking Windows 11 with Flyoobe</h2><p>We’ve got Windows 11 on an unsupported machine, so now lets spend a little time tweaking it for better performance. The first task is to remove the AI features. I don’t need them, nor do I want them. So let's use Flyoobe to remove them all.</p><p><strong>1. In the Flyoobe app, select the AI tab.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="2xkqqWK7yhyWfECwU83TCE" name="flyoobe-ai1" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/2xkqqWK7yhyWfECwU83TCE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Click on Check to search for all of the possible AI components.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="jrcCHZRx32CRUa5osxJTCE" name="flyoobe-ai2" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/jrcCHZRx32CRUa5osxJTCE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Select all of the components that you wish to turn off.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="LmJpGjvQpw7H9cwKZeZVCE" name="flyoobe-ai3" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/LmJpGjvQpw7H9cwKZeZVCE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Click on “Turn off selected”.</strong> This will trigger Windows Powershell to run a script to deactivate the AI elements.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="NLmdVXBMnaLCk2JWZimsBE" name="flyoobe-ai4" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/NLmdVXBMnaLCk2JWZimsBE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>5. Reboot for the changes to take effect.</strong></p><h2 id="install-updates-and-driver-with-flyoobe-2">Install Updates and Driver with Flyoobe</h2><p>My old Lenovo X220 worked out of the box with Windows 11, but the trackpad scroll button did not work and I love scrolling with that! So I needed to install the correct drivers and updates.</p><p><strong>1. Right click on the Flyoobe app and select Run as Administrator. </strong>We need to do this so that we can install the drivers / updates. I tried without, and the process just hung.</p><p><strong>2. Select the Updates tab and click on “Check for Updates”.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:957px;"><p class="vanilla-image-block" style="padding-top:64.47%;"><img id="ReiwVKtYUqRQDvJ4bmDBAE" name="Flyoobe-update1" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/ReiwVKtYUqRQDvJ4bmDBAE.jpg" mos="" align="middle" fullscreen="" width="957" height="617" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Select all of the relevant updates and click on “Install Updates”.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:963px;"><p class="vanilla-image-block" style="padding-top:64.07%;"><img id="irVkktdXgQCBiYH8R73wEE" name="Flyoobe-update2" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/irVkktdXgQCBiYH8R73wEE.jpg" mos="" align="middle" fullscreen="" width="963" height="617" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Wait for the process to finish</strong>. You may see some errors, these can be ignored.</p><p><strong>5. Reboot for the changes to take effect.</strong></p><h2 id="improve-the-windows-11-experience-with-flyoobe-2">Improve the Windows 11 Experience with Flyoobe</h2><p>Windows 11 is a different experience from Windows 10. For one thing, the Start menu is now in the center, and my muscle memory reminds me of this every time I use it. I wanted to improve my Windows 11 experience, and so I turned to Flyoobe, which has a quick list of changes that I can make to speed up and improve the overall features of Windows 11.</p><p><strong>1. Open Flyoobe and go to the Experience tab.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:964px;"><p class="vanilla-image-block" style="padding-top:64.32%;"><img id="Rb8Z8Zc4cAjr4YoEfMSVAE" name="Flyoobe-Extension" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/Rb8Z8Zc4cAjr4YoEfMSVAE.jpg" mos="" align="middle" fullscreen="" width="964" height="620" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Using the dropdown menu, select “Use quick settings”</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:960px;"><p class="vanilla-image-block" style="padding-top:64.06%;"><img id="uj23uMdidR4qMFoiqQnvBE" name="Flyoobe Exp2" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/uj23uMdidR4qMFoiqQnvBE.jpg" mos="" align="middle" fullscreen="" width="960" height="615" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Click on “Toggle All” and click Apply to action.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:542px;"><p class="vanilla-image-block" style="padding-top:60.33%;"><img id="q7j5izqHa4c29wt3Cd4v9E" name="Flyoobe Exp3" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/q7j5izqHa4c29wt3Cd4v9E.jpg" mos="" align="middle" fullscreen="" width="542" height="327" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Wait for the process to finish</strong>. You may see some errors, these can be ignored.</p><p><strong>5. Reboot for the changes to take effect.</strong></p><h2 id="remove-unwanted-apps-with-flyoobe-2">Remove Unwanted Apps with Flyoobe</h2><p>Windows 11, like other Windows before it, comes with a plethora of applications. Some useful, some not so. I wanted a clean start and for that I turned to Flyoobe to remove all of the unwanted applications.</p><p><strong>1. Open Flyoobe and go to the Apps tab.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="opkSqhn4NY4DMBbQ25AGCE" name="flyoobe-apps" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/opkSqhn4NY4DMBbQ25AGCE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Using the dropdown menu, select “Minimal Windows” Of course you can change this to meet your needs. The “Balanced” option provides all of the essential apps and that is why it is recommended.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="DG8YdPaTwt44TWAdDg3qCE" name="flyoobe-apps-select" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/DG8YdPaTwt44TWAdDg3qCE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Click on “Remove Selected Apps” to start the uninstall process.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:947px;"><p class="vanilla-image-block" style="padding-top:63.04%;"><img id="zVo46VH5w5m2cF2xUb5tEE" name="flyoobe-apps3" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/zVo46VH5w5m2cF2xUb5tEE.jpg" mos="" align="middle" fullscreen="" width="947" height="597" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Wait for the process to finish</strong>. You may see some errors, these can be ignored.</p><p><strong>5. Reboot for the changes to take effect.</strong></p><h2 id="install-apps-with-flyoobe-2">Install Apps with Flyoobe</h2><p>The complete opposite now! Whenever I install any OS, there are always a number of applications that I want / need to install. Flyoobe has an Installer tab that I can use to install a few of these, making it a little easier to get started.</p><p><strong>1. Open Flyoobe and go to the Installer tab.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:980px;"><p class="vanilla-image-block" style="padding-top:64.80%;"><img id="dHmbsnwopjymXx99haGqCE" name="Flyoobe-Install" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/dHmbsnwopjymXx99haGqCE.jpg" mos="" align="middle" fullscreen="" width="980" height="635" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Scroll down the list and select the apps that you want to install.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:564px;"><p class="vanilla-image-block" style="padding-top:52.84%;"><img id="aDg3MCoqHsW5G4MHmEwR9E" name="Flyoobe-Install2" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/aDg3MCoqHsW5G4MHmEwR9E.jpg" mos="" align="middle" fullscreen="" width="564" height="298" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Click on “Install Apps” to start the install process.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:980px;"><p class="vanilla-image-block" style="padding-top:64.80%;"><img id="dKaigkgxqS3oP8p8WZmpBE" name="Flyoobe-Install3" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/dKaigkgxqS3oP8p8WZmpBE.jpg" mos="" align="middle" fullscreen="" width="980" height="635" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Wait for the process to finish</strong>. You may see some errors, these can be ignored.</p><p><strong>5. Go to the start menu and your new apps are ready to use.</strong></p><h2 id="flyoobe-extensions-2">Flyoobe Extensions</h2><p>This section covers extensions, written for Flyoobe, and designed to further improve your experience. There are extensions for</p><ul><li>Post-setup cleanup</li><li>Restoring deleted apps</li><li>View telemetry settings</li><li>Tweak the File Explorer</li></ul><p>and many more.</p><p>I’m going to show you how to run Disk Cleanup via Flyoobe, but the process is the same for the other extensions.</p><p><strong>1. Open Flyoobe and go to the Extensions tab.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:964px;"><p class="vanilla-image-block" style="padding-top:64.32%;"><img id="Rb8Z8Zc4cAjr4YoEfMSVAE" name="Flyoobe-Extension" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/Rb8Z8Zc4cAjr4YoEfMSVAE.jpg" mos="" align="middle" fullscreen="" width="964" height="620" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Scroll down the list and select the Post-setup cleanup option.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:964px;"><p class="vanilla-image-block" style="padding-top:64.32%;"><img id="ykbYuTGCsLqeGBKcEjkKBE" name="Flyoobe-Extension2" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/ykbYuTGCsLqeGBKcEjkKBE.jpg" mos="" align="middle" fullscreen="" width="964" height="620" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Click on “Run” to start the cleanup process.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:964px;"><p class="vanilla-image-block" style="padding-top:64.32%;"><img id="dwFNbyA3x6QuBkhz9aVSAE" name="Flyoobe-Extension3" alt="Flyoobe" src="https://cdn.mos.cms.futurecdn.net/dwFNbyA3x6QuBkhz9aVSAE.jpg" mos="" align="middle" fullscreen="" width="964" height="620" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4.Wait for the process to finish</strong>. You may see some errors, these can be ignored.</p><p>5. The disk cleanup dropdown has other options for cleaning up the system. <strong>Choose any that are relevant to your needs and setup. Just remember to take backups before clicking the button.</strong></p>
https://www.tomshardware.com/software/windows/ditching-windows-10-heres-how-i-installed-windows-11-removed-ai-and-stripped-out-unnecessary-options-using-flyoobe
Flyoobe offers a convenient way to install Windows 11 on computers that don’t meet Microsoft’s requirements. But that’s not all. Flyoobe can also be used to tweak Windows 11, remove AI components, and get the best Windows 11 experience on even elderly hardware.
8QGggyPYzHKRxxy7vE4h9D
Fri, 19 Sep 2025 09:00:00 +0000 Windows
Software
Operating Systems
Les Pounder
Tom&#039;s Hardware
Flyoobe
Flyoobe
<![CDATA[ Teams at Nvidia and Intel have been working in secret on jointly developed processors for a year — 'The Trump administration has no involvement in this partnership at all' ]]>
<p>Intel and Nvidia have been working on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">jointly developed processors for client and data center products</a> for about a year now as both companies see huge opportunities behind their Intel x86 RTX SoCs and custom Nvidia x86 data center processors. Although the Nvidia CEO Jensen Huang said in a press call that the Trump administration was pleased with the collaboration between two leading U.S. companies, it had nothing to do with it.</p><h2 id="trump-not-involved-2">Trump not involved</h2><p>"The Trump administration had had no involvement in this partnership at all," said Nvidia's Huang said, during the joint press conference with Nvidia on Thursday. "They would have been very supportive, of course. Today I had the opportunity to tell Secretary [of Commerce Howard] Lutnick and he was very excited and very supportive of seeing two American technology companies working together."</p><p>The work began around a year ago, and preliminary agreements were reached by Intel's then-CEO Pat Gelsinger and Nvidia's Jensen Huang even before that. (A year ago, Joe Biden was president, though no one suggested his administration was involved, either.) Intel and Nvidia are working on custom data center CPUs that Nvidia will integrate into its AI platforms as well as GPU tiles that Intel will integrate into its upcoming client processors. In both cases CPUs and GPUs will use Nvidia's NVLink technology as an I/O interface. By now, there are three teams working together on the joint projects.</p><h2 id="the-work-is-ongoing-2">The work is ongoing</h2><p> "The two technology teams have been discussing and architecting solutions now for probably coming out to a year," said Jensen Huang, chief executive of Nvidia. "The two architecture teams… Well, it is three architecture teams are working across... the CPU architecture, as well as product lines for server and PCs. The architecture work is fairly extensive, and the teams are really excited about the new architecture. The teams have been working for a while and we are excited about the announcement today."</p><p>As Huang mentioned teams working on a CPU architecture as well as client and data center product lines, we figure out that Nvidia wants rather deep customizations of Intel's Xeons to meet the needs of its AI platforms.</p><p>The involvement of a CPU architecture team highlights the depth of the partnership between Intel and Nvidia as well as indicates that the CPU company is implementing rather deep optimizations required by next-generation AI platforms. Given Nvidia's history with Grace and Vera CPUs (custom Arm) and the high bandwidth needs of its next-gen GPUs (e.g. Rubin, Feynman, post-Feynman, etc.), it is reasonable to expect tailored cache structures, memory IO, and coherency protocols on these x86 CPUs.<br><br>Such a deep collaboration probably means that custom Intel processors will be used by Nvidia sometimes in the post-Vera Rubin platform era. We would certainly expect Nvidia's data center GPU team to work with Intel as well, but Huang never mentioned one during the call, probably because Feynman GPUs have already been defined by now.</p><p>Yet, he mentioned that there are two more teams working on product lines for server and PC products, which probably points to data center system level architecture team on Nvidia's side as well as client CPU/system level architecture team on Intel's side.</p><p>While the collaboration between Intel and Nvidia on the data center front is a multi-faceted cross-organizational effort, the timing to its fruition is tied to emergence of Intel's custom CPUs for Nvidia.</p><p>As for the joint work on client project (or projects), developing an Intel CPU with Nvidia GPU chiplet will take at least three to four years from drawing board to volume production. The collaboration requires deep integration across SoC fabrics, dimensions, performance/power consumption targets, packaging technologies (Foveros, EMIB), and software stacks from both companies. The collaboration likely began in 2024, so the first products could hit the market in late 2027 or early 2028.</p><h2 id="hundreds-of-millions-of-pcs-2">Hundreds of millions of PCs</h2><p>While we do not know for sure when Intel and Nvidia plan to come up with jointly developed products, it looks like they intend to address a broad range of applications. At least, Jensen Huang said that that the two companies plan to build CPUs that could address the vast majority of notebooks, which points to hundreds of millions of devices.</p><p>"Just the notebook market is 150 million notebooks sold each year," said Huang. "So that kind of gives you a sense of the scale of the work that we are going to do here. We are going to address the consumer market, we are going to address a vast majority of that consumer PC market, consumer PC notebook market."</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/teams-at-nvidia-and-intel-have-been-working-in-secret-on-jointly-developed-processors-for-a-year-the-trump-administration-has-no-involvement-in-this-partnership-at-all
Intel and Nvidia have quietly spent the past year co-developing custom x86 processors and SoCs for data center and client PCs with deep architectural collaboration across three joint teams.
GshJWp2g4cucSdaspaEZfa
Thu, 18 Sep 2025 20:41:38 +0000 CPUs
PC Components
ashilov@gmail.com (Anton Shilov)
Anton Shilov
intel, Nvidia
Intel, Nvidia
Intel, Nvidia
<![CDATA[ xAI's new gas turbine facility gets halfway to Elon Musk's 1-gigawatt 'AI factory' goal ]]>
<p>xAI is moving faster than anyone expected on its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musk-xai-power-plant-overseas-to-power-1-million-gpus"><u>power strategy</u></a>. According to a new report from <a data-analytics-id="inline-link" href="https://semianalysis.com/2025/09/16/xais-colossus-2-first-gigawatt-datacenter/" target="_blank"><u><em>SemiAnalysis</em></u></a>, Elon Musk’s AI startup already has 460MW of natural gas generation either installed or under construction, split between its Memphis campus and a new site across the border in Southaven, Mississippi.</p><p>The numbers check out against state filings and local reporting. Shelby County granted xAI a permit in July for 15 stationary gas turbines at its Paul R. Lowry Road facility in Memphis after months of wrangling with environmental groups, which alleged that dozens of turbines had been running without proper approval. In Mississippi, regulators issued a 12-month authorization to operate gas turbines at 2875 Stanton Road, a property xAI acquired from Duke Energy this summer, while the company builds out a permanent plant.</p><p>Equipment lists emerging from legal disclosures align with SemiAnalysis’s reporting of 12 SMT-130 turbines, rated at roughly 16MW apiece, on the Memphis side, and seven Titan 350 units in Southaven, each capable of more than 35MW. Together, that brings xAI’s on-site capacity close to half a gigawatt, or roughly the output of a midsize utility plant, stood up in less than a year.</p><div class="see-more see-more--clipped"><blockquote class="twitter-tweet hawk-ignore" data-lang="en"><p lang="en" dir="ltr">xAI now has 460 MW of natural gas turbines installed and either operating or under construction. This includes 12 SMT-130 turbines at Colossus-1 and 7 Titan-350 turbines in Mississippi, located right across from Colossus-2. @elonmusk and @BrentM_SpaceX chose Mississippi due to… pic.twitter.com/dCTYjfK7oQ<a href="https://twitter.com/cantworkitout/status/1968420084214571512">September 17, 2025</a></p></blockquote><div class="see-more__filter"></div></div><p>A single Nvidia GB200 NVL72 rack is modeled at around 120-1302kW. Even after factoring in cooling and overhead, 460MW of generation translates to headroom for nearly 3,000 NVL72 racks, which is more than 200,000 GPUs in total. If xAI succeeds in doubling that to a full gigawatt, it would dwarf most hyperscale campuses in terms of concentrated GPU capacity.</p><p>xAI’s choice of turbine suggests a sense of urgency behind the project. Solar’s SMT-130 and Titan 350 packages are containerized modules designed for rapid deployment, essentially acting as bridge power while xAI transitions to the larger Southaven site. That speed helps sidestep the years-long queue for new grid interconnects, but also explains the geographic shuffle — Tennessee pushback slowed the Memphis approvals, so xAI pivoted to Mississippi, where regulators moved faster.</p><p>Compared with Microsoft and Amazon, which are experimenting with 100-200MW on-site projects, xAI’s goal of leaping straight to a gigawatt sets its power strategy apart from anything else. None of this means xAI is home free. The Memphis permit is already under appeal, and the Southaven authorization is temporary. Critics argue the company is prioritizing speed over compliance, which is a familiar criticism for Musk’s ventures.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> </em><u><em>Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> </em><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/xai-pushes-power-strategy-towards-1gw-ai-factory
According to a new report from SemiAnalysis, Elon Musk’s AI startup already has 460MW of natural gas generation either installed or under construction.
2QteaApzqNubxMRupgrKp9
Thu, 18 Sep 2025 18:23:23 +0000 Tech Industry
lukejamesalden@gmail.com (Luke James)
Luke James
Getty Images
Elon Musk&#039;s face imposed over a phone displaying xAI&#039;s logo.
Elon Musk&#039;s face imposed over a phone displaying xAI&#039;s logo.
<![CDATA[ I managed to snag a Core i5 CPU for $10, because someone scammed Amazon out of an i7-14700 ]]>
<p>The other day, I stopped into a local Amazon returns store on my lunch break. You probably know the type: chaotically overflowing other people's returned orders, with half-open boxes scattered about in huge bins. It's like some kind of post-apocalyptic ball pit game show, where you might find something worthwhile if you wade through enough discarded shelving kits, no-name iPhone cases, and shoe insoles. This particular store is only a few months old, and I'd visited a few times without finding much (other than a pair of insoles, which I needed because I walk 9-10 miles a day).</p><p>On this trip, the first day after a weekly restock, when everything in the bins costs $10, I managed to find a roll of Creality 3D printer PLA filament. That's not a huge discount over its typical Amazon sale price, but I happened to need a new spool for my Anycubic printer, and I was a few blocks from home, so this saved me the hassle of ordering. After a few more minutes pawing through returns, I hadn't found anything else and went up to pay. But there was a line, and I wound up waiting at the corner of one of the closest bins to the register. Killing time, I idly dug around while I waited, and soon spotted the familiar blue of an Intel CPU box. I flipped it over and saw an i7-14700 sticker!<br><br>Could I really have just found the frequency-locked version of Intel's last-generation flagship for $10? And if so, had someone returned it because of the<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/raptor-lake-instability-saga-continues-as-intel-releases-0x12f-update-to-fix-vmin-instability"> <u>notorious instability issues</u></a>? Something else? I could see the CPU in its plastic clamshell through the cardboard window. The back looked OK, but the top was covered in thermal paste.</p><p>I was suspicious, but by this time, I was next in line, curious, and decided to gamble $10 on Intel. That’s maybe not the smartest wager I could make in 2025, but I was curious, and figured this would at least be more interesting than wasting money on a scratch-off ticket. I checked out with three items: the filament, the CPU, and another pair of shoe insoles – seriously, I wear those things out and can never have enough.</p><p>After paying my $32.25 after tax, I stepped outside, wishing I had a napkin to immediately wipe the thermal paste off with. Instead, I slacked my coworkers about what I had found, while I marched back to my apartment. When I got home, I immediately opened the CPU box, grabbed a paper towel, and wiped the used thermal paste off the CPU's IHS, to be met with immediate disappointment. This wasn't a 14th Gen Core i7 after all!</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:4000px;"><p class="vanilla-image-block" style="padding-top:56.30%;"><img id="ekkPrJ5PeuWEPmFpbVhhGY" name="Intel Core i5 return clamshell" alt="Tom's Hardware" src="https://cdn.mos.cms.futurecdn.net/ekkPrJ5PeuWEPmFpbVhhGY.jpg" mos="" align="middle" fullscreen="" width="4000" height="2252" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>But it was a 13th Gen Core i5 – a<a data-analytics-id="inline-link" href="https://www.intel.com/content/www/us/en/products/sku/230580/intel-core-i513500-processor-24m-cache-up-to-4-80-ghz/specifications.html"> <u>Core i5 13500</u></a>, to be specific. Not quite one of the<a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-cpus,3986.html"><u> best CPUs</u></a>, and a generation older than what the box promised, but still a very usable chip, with 14 cores, 20 threads, and a Turbo Frequency of 4.8 GHz. It's not the fastest chip, but it currently sells for $264 at Newegg – not a bad pickup for $10. If it works, anyway.</p><p>So why was a 13th Gen Core i5 returned in a Core i7-14700 box? For those who haven't already connected the dots, it's likely that someone scammed Amazon by buying a new, higher-end chip than what they had, put the old one back in the box (helpfully obscured by thermal paste), and returned it for a refund. And Amazon, dealing as it does with millions of packages a day, seemingly accepted the return without checking that the returned product was actually what was returned, eventually selling it as part of a lot of liquidated returns.</p><p>I have no way to verify any of this, of course, but it seems the most likely scenario. And it's certainly unsurprising that Amazon would just accept a return without paying someone to open the box, wipe off the thermal paste, and confirm they had received the Core i7-14700 the customer had ordered. There's no way Amazon could continue to run its business if it had to do something like that with even half of its returns.</p><p>The only lingering question I had was whether my $10 13th Gen Core i5 CPU actually works. So I grabbed my trusty Hoto screwdriver, removed the AIO waterblock on the system that previously served as our external SSD storage testbed, and removed the 12th Gen Core i5 CPU that previously resided in the LGA 1700 socket. I then dropped my 13th Gen Core i5 into the motherboard, applied<a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/apply-thermal-paste-to-your-cpu"> <u>five small drops of thermal paste</u></a>, re-attached the cooler, and plugged the system back in.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3120px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="U76oWNfzfjLJE5jSZm7cSW" name="Intel Core i5 return in socket" alt="Tom's Hardware" src="https://cdn.mos.cms.futurecdn.net/U76oWNfzfjLJE5jSZm7cSW.jpg" mos="" align="middle" fullscreen="" width="3120" height="1755" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>I pressed the power button and stared at the blackness of my test bench monitor for what felt like too many seconds, but eventually I saw the spinning circle and soon the familiar Windows 11 login screen. The old system booted up without an issue, and after running a few benchmarks, it looks like my $10 chip performs as expected.</p><p>Now the only question is, what should I do with it? I don't need another gaming rig – I'm writing this on an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/amd-ryzen-9-7950x-ryzen-5-7600x-cpu-review"><u>AMD Ryzen 7950X</u></a> / <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/nvidia-geforce-rtx-4090-review"><u>Nvidia RTX 4090</u></a> PC I built back in 2023, and I already have a few other systems and CPUs for testing PC cases and accessories. Maybe I'll build a system for a family member or friend.</p><p>All I know is, while it didn't turn out to be a 14th Gen Core i7 promised on the box, I'm happy with the results of my $10 CPU gamble, and I wonder what I'll find at the returns store next week. I don't really need any more PC hardware, but if I could pass up enticing tech that I don't really need, I probably wouldn't have gotten into this crazy business in the first place.</p>
https://www.tomshardware.com/pc-components/cpus/i-managed-to-snag-a-core-i5-cpu-for-usd10-because-someone-scammed-amazon-out-of-an-i7-14700
Someone scammed Amazon out of a Core i7-14700, but I got a 13th Gen Core i5 for $10 as a result.
ec7uNwubEj6hmkGR89oy3T
Thu, 18 Sep 2025 18:23:22 +0000 CPUs
PC Components
Matt Safford
Tom&#039;s Hardware
An Intel Core i7-14700 CPU box, along with a cleaned-off version of the Core i5-13500 that was inside, next to a syringe of thermal paste
An Intel Core i7-14700 CPU box, along with a cleaned-off version of the Core i5-13500 that was inside, next to a syringe of thermal paste
<![CDATA[ Snapdragon 8 Elite Gen 5 shows up in Geekbench with a score of 3,831 — upcoming chip catches Apple's just-launched A19 Pro, beats desktop chips on single-core perf ]]>
<p>The upcoming Snapdragon 8 Elite Gen 5 was formally announced a few days ago, but now Geekbench leaks are rolling in. <br><br>The keen-eyed X leaker <a data-analytics-id="inline-link" href="https://x.com/never_released/status/1968418182961496131?s=31" target="_blank">Longhorn</a> noticed an unnamed Xiaomi 25113PN0EC device (possibly a Xiaomi 17 Pro) with the Qualcomm SoC inside, posting a <a data-analytics-id="inline-link" href="https://browser.geekbench.com/v6/cpu/13864869" target="_blank">whopping 3,831-point</a> single-threaded score, a value that should put it head-to-head with<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/apples-a19-pro-beats-ryzen-9-9950x-in-single-thread-geekbench-tests-iphone-17-pro-chip-packs-11-12-percent-cpu-performance-bump-gpu-performance-up-37-percent-over-predecessor"> Apple's A19 Pro SoC</a> inside the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/apple-debuts-a19-and-a19-pro-processors-for-iphone-17-iphone-air-and-iphone-17-pro">iPhone 17 Pro</a>.</p><p>If that figure is reflective of shipping products, that would be quite the leap for Qualcomm's chips. The company's SoCs have historically trailed Apple's designs by some margin in both performance and efficiency, so catching up would be quite the feat. The A19 Pro <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/apples-a19-pro-beats-ryzen-9-9950x-in-single-thread-geekbench-tests-iphone-17-pro-chip-packs-11-12-percent-cpu-performance-bump-gpu-performance-up-37-percent-over-predecessor">rings in at close to 3,900</a> points in Geekbench. To put this into perspective, even the mighty Ryzen 7 9800X3D and Ryzen 9 9950X3D post scores of about 3,400 and 3,500, respectively. That's by no means an ultimate measure of real-world performance, but it does display the might of contemporary Arm-based chips, at least in power-constrained scenarios.</p><p>The 3,831-point figure for the Snapdragon 8 Elite Gen 5 might sound a little too good to be true — particularly as it would mean a generational uplift of over 34% — but it is at least consistent with leaks that showed a purported Samsung handset <a data-analytics-id="inline-link" href="https://hothardware.com/news/snapdragon-8-elite-gen-2-impresses-in-early-benchmarks-running-at-474ghz" target="_blank">displaying a score of nearly 3,400</a> at only a 4 GHz boost clock speed. Per Qualcomm's recent announcement, the chip uses two performance cores and six efficiency cores, with the performance cores hitting 4.61 GHz in the standard configuration, or 4.74 GHz in a Samsung Galaxy-specific flavor. This makes 3,831 points at least plausible, as the recent score post shows 4.6 GHz for the performance cores. The new SoC is manufactured in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track.">TSMC's 3-nm N3P node</a>, an evolution of the previous N3E.</p><div ><table><caption>Geekbench scores</caption><tbody><tr><td class="firstcol " ><p>Snapdragon 8 Elite Gen 5 @ 4.6 GHz</p></td><td
><p>3,831 (unconfirmed)</p></td></tr><tr><td class="firstcol " ><p>Apple A19 Pro</p></td><td
><p>3,895 (unconfirmed)</p></td></tr><tr><td class="firstcol " ><p>Ryzen 9 9950X3D</p></td><td
><p>~3,500</p></td></tr><tr><td class="firstcol " ><p>Ryzen 7 9800X3D</p></td><td
><p>~3,400</p></td></tr><tr><td class="firstcol " ><p>Snapdragon 8 Elite</p></td><td
><p>~2,850</p></td></tr></tbody></table></div><p>That's not the only noteworthy difference, though. Longhorn points out in their X post that "SVE2 and SME say hello", implying that the new chip ought to support the newer versions of Arm's Scalable Vector Instructions and Scalable Matrix Instructions. Both
of these CPU instruction sets are called "SIMD" (Single Instruction Multiple Operation), making it easy for developers to efficiently process chunks of data at a time with few instructions.</p><p>That means that applications that can make use of those instructions should see quite a significant speed boost. The original SVE was designed for AI-related data processing, but <a data-analytics-id="inline-link" href="https://developer.arm.com/documentation/102340/0100/Introducing-SVE2" target="_blank">Arm says that SVE2</a> should cover more broad uses cases, and calls out general-purpose software, multimedia, computer vision, and in-memory databases. <a data-analytics-id="inline-link" href="https://www.geekbench.com/blog/2024/04/geekbench-63/" target="_blank">Geekbench does use SME</a> (which in turn apparently needs a subset of SVE2), so the posted scores should reflect the use of these optimizations.</p><p>By the way, if the "Gen 5" name in this report is throwing you off, know that you're not alone. Many people thought the new Snapdragon 8 Elite SoC would be called "Gen 2", but Qualcomm has decided that the "Gen" suffix now applies to its series of Snapdragon products, making this chip the fifth generation, across Snapdragon 8 Gen 1, Gen 2, Gen 3, with the original Snapdragon 8 Elite counting as "Gen 4".</p><p>Of course, consider that these recently-posted figures originate from leakers around the globe and may not reflect production silicon, clock-speed targets, or power envelopes of their final devices. Second, although Geekbench single-core results mostly track with general application performance, that may not be true of every scenario. Regardless, even if figures for production Snapdragon 8 Elite Gen 5 devices are somewhat lower, that would still be an impressive showing.</p>
https://www.tomshardware.com/phones/snapdragon-8-elite-gen-5-shows-up-in-geekbench-with-a-score-of-3-831-upcoming-chip-catches-to-apples-just-launched-a19-pro-beats-desktop-chips-on-single-core-perf
A Xiaomi device in Geekbench packing a Snapdragon 8 Elite Gen 5 chip posted a single-thread score of 3831 points.
Y5D5pgbEnMUqCNao2Qmkwk
Thu, 18 Sep 2025 18:01:41 +0000 CPUs
PC Components
Bruno Ferreira
Qualcomm
Snapdragon 8 Elite handset
Snapdragon 8 Elite handset
<![CDATA[ Silverstone's retro PC FLP02 case launches
— throw-back 5.25-inch expansion bays meet modern 360mm radiator support, likely to be $240 in the US ]]>
<p><strong>09/18/25 update: </strong><em>A few hours after initial publication, a representative from Silverstone reached out to say that the U.S. price for the FLP02 had not yet been set, but that it will likely be around
$240. We changed the headline and removed a reference to a $220 U.S. MSRP at the end of the article.</em><br><br><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/pc-cases/silverstone-reveals-the-flp02-late-80s-style-tower-pc-case-proudly-beige-but-thoroughly-modern-inside">Initially presented</a> at Computex 2025, Silverstone has launched its new FLP02 (SST-FLP02), a retro-inspired PC case designed to house modern hardware components. The FLP02 is the second old-school-looking case introduced by Silverstone, following the successful <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/pc-cases/retro-pc-case-flaunts-floppy-disk-style-bay-cover-silverstone-flp01-will-sell-for-around-usd130">FLP01</a>.</p><p>Silverstone manufactures the FLP02 using steel and plastic, and it is finished with the characteristic beige color reminiscent of the good old days. The case dimensions are 9.13 x 19.45 x 18.58 inches (232 x 494 x 472mm), a moderate size by today's standards. With a weight of 21.6 pounds (9.79 kg), the FLP02 is not overly heavy, allowing it to be conveniently placed on a desk, which is an appropriate location for display purposes.</p><p>The FLP02 accommodates mini-ATX, microATX, ATX, and SSI-CEB motherboards. Considering the dimensions of the FLP02, a mini-ATX motherboard may appear disproportionate within the case; however, individual preferences vary. The FLP02 is equipped with a total of seven expansion slots, with an additional two slots available should you choose to install the graphics card in a vertical orientation.</p><p>Regarding the dimensions of graphics cards, the maximum length is 15.2 inches (386 mm) when the front 3.5-inch/2.5-inch combo bay drive cage is not installed, and 11 inches (279 mm) when it is installed. In terms of width, the maximum is 7.4 inches (188 mm) without the GPU brace, and 6 inches with it installed. Consequently, even large graphics cards such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5090-review">GeForce RTX 5090</a> are compatible with the FLP02 case. The case accepts power supplies with a length of up to 9.8 inches (250mm), so even monstrous power supplies, such as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/power-supplies/corsair-launches-gargantuan-3-000w-power-supply-for-usd599-99-comes-with-four-native-12v-2x6-600w-gpu-cables">Corsair's 3,000W unit,</a> can fit easily into the FLP02.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="LKmBRft6E6LxSVG3NhXUJ4" name="flp02-1" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/LKmBRft6E6LxSVG3NhXUJ4.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="H3TW6ari6WPFuCiA5hZcHC" name="flp02-26" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/H3TW6ari6WPFuCiA5hZcHC.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="frNc5GMW7Vd6D77aCj6WZB" name="flp02-28" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/frNc5GMW7Vd6D77aCj6WZB.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="vtUXch3ZRSyGuAynJv6tvE" name="flp02-4" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/vtUXch3ZRSyGuAynJv6tvE.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div></div></div><p>The FLP02 provides a variety of cooling options. The case comes with two front 120mm fan mounts and one rear fan mount accommodating either a 120mm or 140mm fan. Silverstone supplies three black cooling fans with the FLP02. The top fan mounts offer greater flexibility, allowing for configurations of either three 120mm fans, two 140mm fans, or two 160mm fans. Regarding radiator support, 120mm and 140mm radiators are compatible at the front, while the top accommodates radiators ranging from 120mm to 360mm in size.</p><p>If you prefer air cooling, the FLP02 has you covered as well. The case measures 9.13 inches wide, with clearance space for CPU air coolers up to a maximum height of 7.2 inches (182mm). You'll have no issues slipping something like a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/air-cooling/noctua-nh-d15-g2-review">Noctua NH-D15 G2</a> or a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/air-cooling/jiushark-jf15k-review">Jiushark JF15K </a>in the FLP02.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="fRYvybdFLw3JNUkZ9zyxVb" name="flp02-10" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/fRYvybdFLw3JNUkZ9zyxVb.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="PSVhRFUGqjMYvddtuuDnrW" name="flp02-8" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/PSVhRFUGqjMYvddtuuDnrW.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="aM2CRwaNxbt364EzWSa2Vh" name="flp02-19" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/aM2CRwaNxbt364EzWSa2Vh.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="zUBtYZjzV4MLp3TtRhahZQ" name="flp02-20" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/zUBtYZjzV4MLp3TtRhahZQ.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div></div></div><p>The FLP02 features three external 5.25-inch expansion bays, which are suitable for optical drives, fan controllers, USB hubs, or hot-swap bays. Regarding storage options, it provides one 3.5-inch bay and one 2.5-inch bay, each capable of housing up to two drives of their respective sizes. Additionally, there is a third bay that supports either two 3.5-inch or two 2.5-inch drives.</p><p>The front panel of the FLP02 houses a flickable power switch, a reset button, and a turbo button. There's even a special key lock that prevents accidental actuations on the power and reset buttons. Silverstone has incorporated a fan controller on the FLP02, where you can connect up to eight PWM fans. The turbo button essentially cranks these fans up to full speed. There's a cable that connects to one of your motherboard's PWM fan headers, allowing you to fine-tune the fans.</p><p>As for indicators, you'll find a green power LED, an orange turbo LED, a red hard drive activity LED, and a large digital display that shows the duty cycle for the fans that are connected to the fan controller. Front I/O ports include one USB Type-C port, two USB 3.0 Type-A ports, and a combo 3.5mm audio port.</p><p>The FLP02 will launch in the U.S. market in the fourth quarter of the year.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/pc-cases/silverstones-retro-pc-flp02-case-launches-for-usd220-throw-back-5-25-inch-expansion-bays-meet-modern-360mm-radiator-support
Silverstone has launched the FLP02, a retro-inspired PC case with high-performance cooling and expansion support.
Tr9ZTqTJxLWoGBi5qzpv5B
Thu, 18 Sep 2025 16:44:44 +0000 PC Cases
PC Components
Zhiye Liu
Silverstone
Silverstone FLP02
Silverstone FLP02
<![CDATA[ Gigabyte's upgradeable Aero X16 gaming laptop is on sale for just $1,349 on Best Buy — Get $300 off on this device featuring an RTX 5070, Ryzen 7 AI CPU, and 32 GB RAM ]]>
<p>If you're looking for a performant laptop that doesn't cost an arm and a leg, looks sleek, and has all the latest features — we've got the perfect deal for you. It's not easy to find Nvidia's 50-series machines for a low price, especially if you're after a slightly upscale device that doesn't scream "gamer." Fortunately, <a data-analytics-id="inline-link" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank">Gigabyte's Aero X16 is on sale for just $1,349 on Best Buy</a> right now, offering you an insane $300 discount from its list price of $1,649. It's a great deal on a laptop that can work and play comfortably.</p><ul><li><a href="https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&qp=category_facet%3DLaptops%7Eabcat0502000&st=RTX+5070+" target="_blank">Check out RTX 5070 gaming laptops on Best Buy</a></li></ul><div class="product star-deal"><a data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension48="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension25="$1349" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:771px;"><p class="vanilla-image-block" style="padding-top:62.91%;"><img id="Vdt5RgtC5Ucxa2RSSJ5ohb" name="Gigabyte Aero X16" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/Vdt5RgtC5Ucxa2RSSJ5ohb.jpg" mos="" align="middle" fullscreen="" width="771" height="485" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price.<a class="view-deal button" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank" rel="nofollow" data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension48="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension25="$1349">View Deal</a></p></div><p>The Aero X16 we're taking a look at today is the RTX 5070 variant, and it's paired up with an 8-core Ryzen Al 7 350 processor. You get an 85W TDP on that 5070, offering you a decent balance between battery and power. More importantly, this model comes with 32 GB of DDR5 RAM — which is plenty already — but you can upgrade the vacant SODIMM slots to add even more. The same goes for the storage, you get 1TB PCIe 4.0 storage that can be expanded with the extra M.2 NVMe slot. In terms of flexibility, this is a clear standout among current-gen mainstream offerings.</p><p>As the name suggests, the Aero X16 has a gorgeous 16" screen, featuring a 165 Hz IPS panel with 100% coverage of the sRGB color space. It's no OLED, but the 16:10 aspect ratio offers a bit more screen real estate, and the 1440p resolution keeps things looking sharp no matter what. That screen is accompanied by a solid RGB-lit keyboard, trackpad, and 1080p webcam with Windows Hello. The Aero X16 is healthy in terms of connectivity, too, offering multiple USB-C and USB-A ports with fast speeds, along with HDMI 2.1, headphone jack, and a gigabit Ethernet port.</p><p><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-deals-on-ssds"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-computer-monitor-deals"><em>Gaming Monitor Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/gigabytes-ultra-upgradeable-aero-x16-gaming-laptop-is-on-sale-for-just-usd1-349-on-best-buy-get-usd300-off-on-this-device-featuring-an-rtx-5070-ryzen-7-ai-cpu-and-32-gb-ram
Gigabyte is a reliable name when it comes to PC hardware, and this laptop deal is no different. The Aero X16 features excellent specs, like an RTX 5070 and Ryzen AI 7 350 CPU, along with 32 GB of memory and 1 TB SSD — all while costing less than $1,500. It features a clean design and weighs under 2KG.
3BStm2XEwurDDwE6KnS9Vk
Thu, 18 Sep 2025 16:36:42 +0000 PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Tom&#039;s Hardware / Gigabyte
Gigabyte Aero X16
Gigabyte Aero X16
<![CDATA[ Microsoft announces 'world's most powerful' AI data center — 315-acre site to house 'hundreds of thousands' of Nvidia GPUs and enough fiber to circle the Earth 4.5 times ]]>
<p>Microsoft is planning to bring the "world's most powerful"
AI datacenter online in early 2026, the <a data-analytics-id="inline-link" href="https://blogs.microsoft.com/blog/2025/09/18/inside-the-worlds-most-powerful-ai-datacenter/" target="_blank">company announced</a> today. The Pleasantville, Wisconsin-based datacenter,
dubbed Fairwater, is. meant specifically for training AI models as well as running large-scale models. The datacenter will be housed on 315 acres of land, with 1.2 million square feet in three buildings to house "hundreds of thousands" of Nvidia GB200 and GB300 GPUs.</p><div class="see-more see-more--clipped"><blockquote class="twitter-tweet hawk-ignore" data-lang="en"><p lang="en" dir="ltr">If intelligence is the log of compute… it starts with a lot of compute! And that’s why we’re scaling our GPU fleet faster than anyone else.Just last year, we added over 2 gigawatts of new capacity – roughly the output of 2 nuclear power plants.And today we’re going further,… pic.twitter.com/cZJ3pdN1rX<a href="https://twitter.com/cantworkitout/status/1968677244861379012">September 18, 2025</a></p></blockquote><div class="see-more__filter"></div></div><p>On X, Microsoft CEO Satya Nadella wrote that these GPUs will be "connected by enough fiber to circle the Earth 4.5 times" and said that they will deliver ten times more performance than today's fastest supercomputer. This is likely a comparison to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/musks-colossus-is-fully-operational-with-200-000-gpus-backed-by-tesla-batteries-phase-2-to-consume-300-mw-enough-to-power-300-000-homes">xAI's Colossus</a>, which uses over 200,000 GPUs and 300 megawatts of power. Microsoft didn't specify its exact number of GPUs nor the expected power consumption.<br><br>Fairwater uses closed-loop water cooling, which the company suggests will have "zero water waste," with all of the water supplied once, at construction. In fact, Microsoft says it's the second-largest water-cooled chiller plant on Earth. Hot water will be sent out to cooling fins on each side of Fairwater, and then cooled with 172 20-foot fans before being sent back in to cool the GPUs again.<br><br>The other 10% will be traditional servers using outside air for cooling, and will move to water "only during the hottest days."<br><br>In a <a data-analytics-id="inline-link" href="https://blogs.microsoft.com/on-the-issues/2025/09/18/made-in-wisconsin-the-worlds-most-powerful-ai-datacenter/">separate blog post</a>, Microsoft president Brad Smith wrote that the company is working to avoid driving up electricity costs for surrounding communities.<br><br>The construction sounds immense. Executive vice president of Cloud and AI, Scott Guthrie, wrote that the new datacenter uses "46.6 miles of deep foundation piles, 26.5 million pounds of structural steel, 120 miles of medium-voltage underground cable and 72.6 miles of mechanical piping." The datacenter's storage systems alone are "five football fields" long. <br><br>Beyond the Mount Pleasant facility, Guthrie adds that several identical Fairwater data centers are under construction elsewhere in the United States.<br></p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/microsoft-announces-worlds-most-powerful-ai-data-center-315-acre-site-to-house-hundreds-of-thousands-of-nvidia-gpus-and-enough-fiber-to-circle-the-earth-4-5-times
Microsoft's new datacenter in Wisconsin will use "hundreds of thousands of NVIDIA GB200s, connected by enough fiber to circle the Earth 4.5 times," CEO Satya Nadella wrote.
GARcWuzpDDF55cKiUmcdgi
Thu, 18 Sep 2025 16:26:37 +0000 Artificial Intelligence
Tech Industry
Andrew E. Freedman
Microsoft
Microsoft Wisconsin data center
Microsoft Wisconsin data center
<![CDATA[ My favorite SSD stick, SK hynix's speedy 2TB Tube T31, is down to $118, an all-time low price ]]>
<p>My personal favorite flash drive, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-flash-drives" target="_blank" rel="nofollow">best flash drive</a> we've tested, SK hynix's Tube T31, is down to <a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1">a new low price of $118 at Amazon</a>. It's one of the fastest USB-A flash drives we've ever tested and a great choice for people looking for a convenient storage solution that is compact and plugs right into any USB Type-A port, while delivering great performance. It's technically a tiny external SSD, but the Tube T31 keeps the familiar (and handily cable-free) USB flash drive form factor.</p><p>The<a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow"> Tube T31 2TB flash drive is 26% off, taking it down to a new low price of $118</a>. And if you want to spend less and don't need 2TB, the <a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0CQZCWHTQ/?th=1">1TB Tube T31 is also down to $59, </a>although that's not a new low price.
<br><br>While some will prefer USB-C, the Tube T31's ultra-prevalent USB Type-A connector makes it easy to use with a host of everyday computing devices, as well as consoles, and it's a great backup or offline data transfer tool.</p><div class="product star-deal"><a data-dimension112="9433ea31-d205-4fb6-9253-99b12e9a2c43" data-action="Star Deal Block" data-label="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension48="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension25="$118" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:209px;"><p class="vanilla-image-block" style="padding-top:208.13%;"><img id="C7F2SwynPwDj6ZBz8ucQpD" name="SK hynix Tube T31 1TB.PNG" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/C7F2SwynPwDj6ZBz8ucQpD.png" mos="" align="middle" fullscreen="" width="209" height="435" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC & Mac. Perfect for your portable storage needs.<a class="view-deal button" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow" data-dimension112="9433ea31-d205-4fb6-9253-99b12e9a2c43" data-action="Star Deal Block" data-label="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension48="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension25="$118">View Deal</a></p></div><p>I <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/external-ssds/sk-hynix-tube-t31-review">reviewed the 1TB Tube T31</a> in 2024 and gave it an Editor's Choice award for its performance. It's a little bulky for a flash drive, and could block neighboring USB ports, particularly on a desktop. But most laptops should have adjacent USB-C ports spaced far enough apart. And the Tube T31 did deliver class-leading performance compared to its peers and, turning in some of the fastest transfer speeds available for a 10 Gbps USB-A drive.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3643px;"><p class="vanilla-image-block" style="padding-top:56.27%;"><img id="djePcXx4bEXoUaxS9PGfyM" name="SK hynix Tube T31 Comparison" alt="The SK hynix Tube T31 is a bit bulky compared to competing drives from Kingston and Teamgroup." src="https://cdn.mos.cms.futurecdn.net/djePcXx4bEXoUaxS9PGfyM.jpg" mos="" align="middle" fullscreen="" width="3643" height="2050" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The SK hynix Tube T31 2TB uses its impressive performance to blur the lines between external SSDs and flash drives in terms of both speed and capacity. It uses a USB 3.2 Gen 2 Type-A connector, making it compatible with basically any recent device with a USB-A port, and utilizes the full 10 Gbps data bandwidth available, providing speeds of up to 1,000MB/s when transferring your data. <br><br><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-ssd-deals"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/hdds/best-hard-drive-deals-amazon-prime-day-2025"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/monitors/best-computer-monitor-deals"><u><em>Gaming Monitor Deals</em></u></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/usb-flash-drives/my-favorite-ssd-stick-sk-hynixs-speedy-2tb-tube-t31-is-down-to-usd118-an-all-time-low-price
The Tube T31 is an M.2 SSD on a USB-A stick, with the best performance we've seen from a USB-A drive.
LdJsm6U8bXL9ZymMUSuqai
Thu, 18 Sep 2025 15:48:23 +0000 USB Flash Drives
PC Components
Storage
Matt Safford
Tom&#039;s Hardware
SK hynix Tube T31 in hand
SK hynix Tube T31 in hand
<![CDATA[ Valve to drop Steam support for 32-bit Windows versions next year — says it's no longer compatible with core client features, only 0.01% of players actually used it ]]>
<p>Valve is dropping support for Steam running on 32-bit versions of Windows, <a data-analytics-id="inline-link" href="https://help.steampowered.com/en/faqs/view/49A1-B944-48B8-FF00" target="_blank">starting January 1, 2026</a>. Steam has been available on Windows for more than two decades and, therefore, was built with 32-bit systems in mind. Today, every modern computer is 64-bit, with compatibility layers built in to support older 32-bit apps. So, even though 32-bit apps have carried forward, there's really no place for 32-bit operating systems anymore — which is why Valve is axing support for them.</p><p>It's important to understand the distinction between 32-bit apps and operating systems. Steam itself is 32-bit, partly because it's from that era, but mostly because it doesn't need to be updated to a 64-bit instruction set, given its lightweight nature. A lot of games on Steam are also 32-bit. None of that will be affected by the sunsetting of 32-bit Windows support, since it's only support for the operating system itself that's being phased out. Windows 10 32-bit is the only version Steam currently supports anyways, and Valve says just 0.01% of players are still using it today.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3000px;"><p class="vanilla-image-block" style="padding-top:61.70%;"><img id="wcQFM25CBLa32ErQkMP8jP" name="GettyImages-1169550" alt="Bill Gates on stage announcing Windows XP 64-bit" src="https://cdn.mos.cms.futurecdn.net/wcQFM25CBLa32ErQkMP8jP.jpg" mos="" align="middle" fullscreen="" width="3000" height="1851" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">What once was. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Getty Images)</span></figcaption></figure><p>Windows 11 is exclusively 64-bit, on the other hand, and now holds more than 60% of the OS share, according to <a data-analytics-id="inline-link" href="https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam" target="_blank">Steam's August 2025 Hardware Survey</a>. Valve has made it clear that 32-bit Windows is no longer compatible with drivers and libraries required for the core features of the Steam client, rendering continued support for it unfeasible.</p><p>Come January 1st, the client itself will still work for a while, but will stop receiving security updates, and Valve won't entertain support requests for it. The company advises gamers to upgrade to 64-bit Windows to keep receiving timely updates and assistance. This move somewhat aligns with Microsoft's own plans for Windows 10, which will completely lose official support next month. Steam should still continue to run on 64-bit versions of Windows 10, however, which makes up 35% of all Steam users right now.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/pc-gaming/valve-to-drop-steam-support-for-32-bit-windows-versions-next-year-says-its-no-longer-compatible-with-core-client-features-only-0-01-percent-of-players-actually-used-it
Steam will stop supporting 32-bit versions of Windows from next year. Valve says the libraries and drivers needed for core client features are no longer compatible with 32-bit architecture, and therefore need to be left behind. Upgrade to Windows 10 64-bit, or Windows 11, to keep receiving timely updates and assistance.
b2Y5WmQSxWSDonz3va7X5R
Thu, 18 Sep 2025 15:42:37 +0000 PC Gaming
Video Games
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Future
Steam Library shot
Steam Library shot
<![CDATA[ A wireless device exploit uncovered 11 years ago still hasn't been fixed by some manufacturers — six vendors and 24 devices found harbouring vulnerable firmware across routers, range extenders, and more ]]>
<p>NetRise has <a data-analytics-id="inline-link" href="https://www.netrise.io/hubfs/Pixie-Dust-Report.pdf" target="_blank">revealed</a> (PDF) that wireless devices from several manufacturers remain vulnerable to the Pixie Dust exploit disclosed in 2014, even though companies have had over a decade to harden their products against the well-known security flaw.</p><p>"Across six vendors, we found 24 devices, including routers, range extenders, access points, and hybrid Wi-Fi/powerline products, with firmware that was released vulnerable to Pixie Dust," NetRise said. "The oldest vulnerable firmware in the set dates to Sept. 2017, nearly three years after public disclosure of the Pixie Dust exploit. On average, vulnerable releases occurred 7.7 years after the exploit was first published."</p><p><em>SecurityWeek </em><a data-analytics-id="inline-link" href="https://www.securityweek.com/decade-old-pixie-dust-wi-fi-hack-still-impacts-many-devices/" target="_blank">reported</a> that Pixie Dust can be "exploited to obtain a router’s [Wi-Fi Protected Setup] PIN and connect to the targeted wireless network without needing its password." All someone has to do to take advantage of this exploit is make sure they're within range of the network they want to access, capture the initial WPS handshake between the network and a client device, and then crack the PIN offline.</p><p>Pixie Dust is so well-known that numerous resources use it to demonstrate introductory wireless network hacking techniques. Researchers have also developed <a data-analytics-id="inline-link" href="https://github.com/wiire-a/pixiewps" target="_blank">several</a> open source <a data-analytics-id="inline-link" href="https://github.com/t6x/reaver-wps-fork-t6x" target="_blank">tools</a> capable of exploiting Pixie Dust—one of which is <a data-analytics-id="inline-link" href="https://www.kali.org/tools/reaver/#reaver-1" target="_blank">highlighted</a> by the security-focused Kali Linux distribution—so manufacturers can't really feign ignorance about the ease with which vulnerable devices can be hacked.</p><p>An exploit this old remaining viable on dated hardware wouldn't necessarily come as a surprise; most companies release enough products each year that it would be unreasonable to expect all of them to be fully supported in perpetuity. (Even if there <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/linux/windows-10-support-is-ending-but-end-of-10-wants-you-to-switch-to-linux" target="_blank">are many people</a> who don't want to upgrade to a newer gizmo.) But that doesn't seem to be what's happening with the devices NetRise scrutinized for its report.</p><p>"Of the 24 devices, only four were ever patched, and these patches arrived late," NetRise said. "As of this writing, thirteen devices remain actively supported but unpatched. Another seven reached end of life without ever receiving fixes. In some cases, vendors described fixes vaguely in changelogs as, 'Fixed some security vulnerability,' with no acknowledgement of Pixie Dust."</p><p>This means six manufacturers released products with known vulnerabilities and, in many cases, have neglected to update the relevant firmware even though their customers have been assured the products are still being supported. Even the products that received patches did so long after the fact—NetRise said on average Pixie Dust patches arrived 9.6 years after the exploit's public disclosure.</p><p>"The Pixie Dust exploit is not an isolated case but a symptom of systemic issues in firmware supply chains, from weak cryptography and poor entropy generation to opaque vendor patch practices," NetRise said. "The lesson is clear: without consistent visibility into firmware, organizations cannot assume that old exploits are gone."</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/a-wireless-device-exploit-uncovered-11-years-ago-still-hasnt-been-fixed-by-some-manufacturers-six-vendors-and-24-devices-found-harbouring-vulnerable-firmware-across-routers-range-extenders-and-more
NetRise has revealed that wireless devices from several manufacturers remain vulnerable to the Pixie Dust exploit disclosed in 2014, even though companies have had over a decade to harden their products against the well-known security flaw.
fasgjLhZwm8j8tBJfqjovK
Thu, 18 Sep 2025 13:49:43 +0000 Cybersecurity
Tech Industry
Nathaniel Mott
Shutterstock
A broken lock on a PCB.
A broken lock on a PCB.
<![CDATA[ China foes get worse results using DeepSeek, research suggests — CrowdStrike finds nearly twice as many flaws in AI-generated code for IS, Falun Gong, Tibet, and Taiwan ]]>
<p>Research suggests that your DeepSeek AI results can be of drastically lower quality if you trigger topics that are geopolitically sensitive or banned in China. During tests undertaken by U.S. security firm CrowdStrike, it was observed that code generated for a professed Islamic State militant group computer system contained nearly twice as many flaws as it would otherwise have had. Other potential topics included: Falun Gong, Tibet, and Taiwan, according to a new <a data-analytics-id="inline-link" href="https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/" target="_blank">Washington Post</a> report.</p><p>One of the key findings, highlighted by the source, is that DeepSeek AI-generated code for a program to run an industrial control system would typically result in 22.8% of the code featuring flaws. If requested on behalf of an Islamic State project, a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/microsoft-and-open-ai-investigate-whether-deepseek-illicitly-obtained-data-from-chatgpt">DeepSeek user</a> could see that the flaw percentage rises sharply, to 42.1%.</p><p>Rather than delivering faulty code, DeepSeek would sometimes refuse to generate code for the likes of professed Islamic State backers or devotees of the spiritual movement Falun Gong. Refusals to aid those groups would occur 61% and 45% of the time, respectively. Notably, both movements are banned in China.</p><p>However, DeepSeek’s perceived reduction of the quality of code, when it is generated for such organizations and others, has surprised some. “That is something people have worried about — largely without evidence,” Helen Toner, from the Center for Security and Emerging Technology at Georgetown University, told the Washington Post.</p><p>DeepSeek’s reasons behind the downgrading of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/linux/linux-distros-ban-tainted-ai-generated-code">AI-generated code</a> for purported use in places like Tibet and Taiwan may be less clear-cut. But such code was also less flawed than that generated for the Islamic State, for example.</p><h2 id="what-is-happening-a-few-theories-2">What is happening? A few theories.</h2><p>The Washington Post has sought comment from the makers of DeepSeek regarding <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/windows/crowdstrikes-market-cap-falls-dollar125-billion-in-wake-of-global-outage">CrowdStrike</a>’s research findings, but has yet to get a response. It has a few theories about what might be happening, though…</p><p>One of the possibilities the source muses over is that sneakily producing <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/firmware-flaw-affects-numerous-generations-of-intel-cpus-uefi-code-execution-vulnerability-found-for-intel-cpus-from-14th-gen-raptor-lake-to-6th-gen-skylake-cpus">flawed code</a> is a less obvious sabotage technique, used to blunt the energies of foes. It could also provide a wider attack surface for subsequent hacking.</p><p>Another possibility is that, as the most secure code found during testing was for projects destined for American clients, DeepSeek is trying harder to penetrate this market.</p><p>The source also ponders whether code quality is impacted by its target market due to training on the existing regional material. It expects many more relevant training resources for coders working in the U.S. than in Tibet, for example.</p><p>Last but not least, perhaps DeepSeek is working ‘off its own initiative’ to supply more error-prone code to entities and regions governed by what it has learned are ‘rebels.’ All of these are mere hypotheticals, but the AI outfit is not without attachments to Beijing. In August, it was reported that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseek-reportedly-urged-by-chinese-authorities-to-train-new-model-on-huawei-hardware-after-multiple-failures-r2-training-to-switch-back-to-nvidia-hardware-while-ascend-gpus-handle-inference">DeepSeek switched to training its models on Huawei hardware instead of Nvidia at the behest of China</a>, leading to delays caused by hardware failures.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/china-foes-get-worse-results-using-deepseek-research-suggests-crowdstrike-finds-nearly-twice-as-many-flaws-in-ai-generated-code-for-is-falun-gong-tibet-and-taiwan
Research suggests that your DeepSeek AI results can be of drastically lower quality if you trigger China’s geopolitically sensitive tripwires.
idLipufznFPQ8EcLWgYK8d
Thu, 18 Sep 2025 12:51:06 +0000 Artificial Intelligence
Tech Industry
Mark Tyson
Getty / Herstockart
Deepseek logo on an iPhone
Deepseek logo on an iPhone
<![CDATA[ Huawei reveals long-range Ascend chip roadmap — three-year plan includes ambitious provision for in-house HBM with up to 1.6 TB/s bandwidth ]]>
<p>Huawei’s AI silicon roadmap is no longer a state secret. Speaking at the Huawei Connect conference on September 18, rotating chairman Xu Zhijun outlined the company’s first official long-range <a data-analytics-id="inline-link" href="https://www.reuters.com/business/media-telecom/chinas-huawei-hypes-up-chip-computing-power-plans-fresh-challenge-nvidia-2025-09-18">Ascend chip strategy</a>, with four new parts scheduled across the next three years: Ascend 950PR and 950DT in early 2026, followed by Ascend 960 and 970 in 2027 and 2028, respectively.</p><p>Huawei says its upcoming 950PR chip will ship in Q1 next year with in-house HBM designed to compete with the likes of SK hynix and Samsung. That’s a pretty bold claim considering HBM supply and factors like packaging and bandwidth efficiency have arguably become the single biggest constraint on AI accelerator performance at scale.</p><p>According to Huawei, the 950PR will feature 128GB of its in-house HBM delivering up to 1.6 TB/s of bandwidth, while the 950DT increases those figures to 144GB and 4 TB/s, but Huawei hasn’t disclosed how its in-house HBM is manufactured, what packaging is used, or which foundry is producing the chip itself.</p><p>Under U.S. sanction rules, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/taiwan-bans-chip-exports-to-huawei-smic-ban-comes-after-huawei-tricked-tsmc-into-making-one-million-ai-processors-despite-us-restrictions">Huawei is barred from accessing TSMC’s advanced nodes</a> and CoWoS packaging lines, both of which Nvidia uses to stack HBM around its top-end Hopper and Blackwell GPUs. If Huawei is working with SMIC or other domestic fabs, yields and bandwidth may prove to be hugely limiting factors.</p><p>That hasn’t stopped the company from talking scale, though. Alongside its chip roadmap, Huawei teased new so-called “supernodes” that will house thousands of Ascend chips. The Atlas 950 and 960 systems are positioned as next-gen AI compute clusters that, on paper, rival Nvidia’s GB200 NVL72 configurations in deployment scale, with up to 15,488 Ascend accelerators in a single system. Huawei says Atlas 950 will debut in Q4 this year.</p><p>But big numbers don’t necessarily translate into performance. Nvidia’s big advantage isn’t just its silicon but NVLink and a tightly optimized software stack that keeps its clusters saturated across large model workloads. To challenge that, Huawei is going to need more than a boastful chip roadmap — a roadmap that has landed conveniently alongside demands from the Chinese government to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-mandates-domestic-firms-source-50-percent-of-chips-from-chinese-producers-beijing-continues-to-squeeze-companies-over-reliance-on-foreign-semiconductors">produce more domestic silicon</a> and a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d">ban on procuring Nvidia parts</a>.</p><p>Huawei will need a proven end-to-end platform that can match Nvidia in training, efficiency, and model throughput for its roadmap to succeed. Right now, it doesn’t, and plans alone don’t break bottlenecks.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/huawei-unveils-ascend-roadmap-backed-by-in-house-hbm
Speaking at the Huawei Connect conference on September 18, rotating chairman Xu Zhijun outlined the company’s first official long-range Ascend chip strategy.
zZAnWKX8VdPmt3aqKAK95Q
Thu, 18 Sep 2025 12:33:07 +0000 Semiconductors
Tech Industry
Manufacturing
lukejamesalden@gmail.com (Luke James)
Luke James
Shutterstock
Huawei
Huawei
<![CDATA[ Save $700 on the LG Ultragear 39-Inch OLED curved ultrawide monitor — now available at its cheapest price of $899, with a massive 44% off ]]>
<p>A premium ultrawide monitor not only gives you more screen real estate but also makes multitasking and productivity easier without switching between multiple displays. Additionally, it enhances your gaming or movie-watching experience with immersive visuals. One such offering is the 39-inch LG Ultragear OLED, which is currently on sale and is available at its <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank">lowest price ever on Amazon</a>.</p><p>Launched earlier this year in April, the LG Ultragear 39GX90SA is currently priced at $899, which is a significant saving considering its launch price of $1,599. While the monitor has seen price drops in recent months, this is the lowest we’ve ever seen it go.</p><ul><li><a href="https://www.amazon.com/dp/B0F15C7JL2?th=1">Check out this deal on Amazon</a></li></ul><div class="product star-deal"><a data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension48="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension25="$899.99" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1143px;"><p class="vanilla-image-block" style="padding-top:69.29%;"><img id="ssx8ttEFP2QKfp3xYctoMU" name="LG-39GX90SA Ultrawide OLED monitor" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/ssx8ttEFP2QKfp3xYctoMU.jpg" mos="" align="middle" fullscreen="" width="1143" height="792" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p><em>All-time low price </em></p><p>The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows. <a class="view-deal button" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank" rel="nofollow" data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension48="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension25="$899.99">View Deal</a></p></div><p>The monitor features a 39-inch curved WOLED panel built around an 800R curvature with a resolution of 3440 x 1440. According to LG, the monitor offers an ultra-fast response time of 0.03ms (grey-to-grey) along with a 240 Hz refresh rate. The panel is rated to offer up to 1300 nits peak brightness and has been tuned to deliver standard luminosity of 275 nits in SDR at a 100% APL (Average Picture Level). It also supports 10-bit colour depth and a wide colour gamut covering 98.5% DCI-P3. Additionally, it is VESA DisplayHDR 400 True Black certified for an enhanced HDR experience along with high contrast and deep blacks.</p><p>The LG Ultragear 39 OLED also doubles as an entertainment hub, as it comes with LG’s WebOS interface, which is usually seen on its smart TV range. You get access to video streaming apps like Netflix, Prime Video, Hulu, Disney+, over 300 free LG Channels, as well as access to cloud gaming services like Nvidia GeForce Now, Amazon Luna, and Blacknut.</p><p>As for ports, the monitor comes with a USB Type-C with DisplayPort Alt mode and 65W power delivery, DisplayPort 1.4, two HDMI 2., two USB Type-A, Ethernet jack, and a 3.5mm headphone jack. The monitor also comes with built-in stereo speakers, each rated at 7W. For the ones who care about their gamer aesthetics, the
LG Ultragear 39GX90SA-W also features RGB lighting at the back in a hexagonal pattern.</p><p><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-deals-on-ssds"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-computer-monitor-deals"><em>Gaming Monitor Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/save-usd700-on-the-lg-ultragear-39-inch-oled-curved-ultrawide-monitor-now-available-at-its-cheapest-price-of-usd899-with-a-massive-44-percent-off
The 39-inch curved WOLED monitor is great for gamers and comes with smart TV features courtesy of LG's WebOS interface.
riVG6jcgn8gJiuKW9f3DWh
Thu, 18 Sep 2025 12:00:07 +0000 PC Components
editors@tomshardware.com (Kunal Khullar)
Kunal Khullar
Tom&#039;s Hardware / LG
LG monitor
LG monitor
<![CDATA[ Kioxia Exceria Plus G4 2TB SSD Review: A safe but unexceptional drive ]]>
<p>The Kioxia Exceria Plus G4 is a solid all-around SSD. Kioxia is probably best known for its OEM and enterprise drives, and to some extent, its Exceria line of consumer drives has flown under the radar. This is a misjustice because these drives have a solid reputation for reliability, with generally few downsides with the hardware. Performance and power efficiency are acceptable to good, and there’s no unusual switching of the type of flash, unlike what we see with some other vendors. You can pick up one of these, including the Plus G4, and expect a straightforward experience. What’s not to love?</p><p>If there’s a cost for this experience, it’s probably found in the limited capacity range, middle-of-the-road performance, and, to some extent, availability and pricing concerns. This isn’t the drive for maximum performance or power efficiency; it’s not going to solve your needs for a super small drive or a large drive, and it might not always be a sensible option economically. But there are sales, and perhaps more importantly, some regions of the world have fewer reliable drive choices, and Kioxia’s Exceria drives might be more competitive. The Plus G4, in particular, also demonstrates what’s good about this class of drives – they can be used for any purpose while delivering a decent experience.</p><p>This makes it a safe drive to pick up if you’re just trying to put the last-minute final touches on a build. Maybe you’re not sure what to get, or maybe this drive catches your eye on a sale. Whatever the case, its greatest strength is that you can buy it without worry. Peace of mind is a value of its own. Kioxia’s SSDs are not fancy, and that, in our opinion, is to their benefit. We believe Crucial has a stronger hold in this market segment with the P510, but the Plus G4 is a good alternative, and it surpasses the P510 in enough areas to remain competitive.</p><h2 id="kioxia-exceria-plus-g4-specifications-2">Kioxia Exceria Plus G4 Specifications</h2><div ><table><thead><tr><th class="firstcol " ><p>Product</p></th><th
><p>1TB</p></th><th
><p>2TB</p></th></tr></thead><tbody><tr><td class="firstcol " ><p><strong>Pricing</strong></p></td><td
><p><a href="https://www.amazon.com/KIOXIA-EXCERIA-PLUS-NVMe-Gen5/dp/B0DW52LDPD">$142.99</a></p></td><td
><p><a href="https://www.amazon.com/KIOXIA-EXCERIA-PLUS-NVMe-Gen5/dp/B0DW52LDPD">$209.99</a></p></td></tr><tr><td class="firstcol " ><p><strong>Form Factor</strong></p></td><td
><p>M.2 2280</p></td><td
><p>M.2 2280</p></td></tr><tr><td class="firstcol " ><p><strong>Interface / Protocol</strong></p></td><td
><p>PCIe 5.0 x4
</p><p>NVMe 2.0c</p></td><td
><p>PCIe 5.0 x4
</p><p>NVMe 2.0c</p></td></tr><tr><td class="firstcol " ><p><strong>Controller</strong></p></td><td
><p>Phison E31T</p></td><td
><p>Phison E31T</p></td></tr><tr><td class="firstcol " ><p><strong>DRAM</strong></p></td><td
><p>N/A (HMB)</p></td><td
><p>N/A (HMB)</p></td></tr><tr><td class="firstcol " ><p><strong>Flash Memory</strong></p></td><td
><p>Kioxia 218-Layer (BiCS8) TLC</p></td><td
><p>Kioxia 218-Layer (BiCS8) TLC</p></td></tr><tr><td class="firstcol " ><p><strong>Sequential Read</strong></p></td><td
><p>10,000 MB/s</p></td><td
><p>10,000 MB/s</p></td></tr><tr><td class="firstcol " ><p><strong>Sequential Write</strong></p></td><td
><p>7,900 MB/s</p></td><td
><p>8,200 MB/s</p></td></tr><tr><td class="firstcol " ><p><strong>Random Read (IOPS)</strong></p></td><td
><p>1,300K</p></td><td
><p>1,300K</p></td></tr><tr><td class="firstcol " ><p><strong>Random Write (IOPS)</strong></p></td><td
><p>1,400K</p></td><td
><p>1,400K</p></td></tr><tr><td class="firstcol " ><p><strong>Security</strong></p></td><td
><p>N/A</p></td><td
><p>N/A</p></td></tr><tr><td class="firstcol " ><p><strong>Endurance (TBW)</strong></p></td><td
><p>600TB</p></td><td
><p>1,200TB</p></td></tr><tr><td class="firstcol " ><p><strong>Part Number</strong></p></td><td
><p>LVD10Z001TG8</p></td><td
><p>LVD10Z002TG8</p></td></tr><tr><td class="firstcol " ><p><strong>Warranty</strong></p></td><td
><p>5-Year</p></td><td
><p>5-Year</p></td></tr></tbody></table></div><p>The Kioxia Exceria Plus G4 is only available in 1TB and 2TB capacities, which might sound crazy to some. There is a market for smaller drives, especially 512GB, and larger drives of 4TB or more. However, the statistics don’t lie – 1TB and 2TB remain the most popular capacities, and these provide plenty of space for most people. Crucial’s <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/the-crucial-p510-2tb-ssd-review"><u>P510</u></a> has the same capacities on offer for good reason. The Exceria Plus G4 directly competes with that drive, so we can’t act too surprised by this turn of events. With costs getting tighter in the NAND flash and SSD storage markets, it’s safer to focus on high-volume SKUs.</p><p>Currently, the Exceria Plus G4, or Plus G4 for short, sells at $142.99 and $209.99 on Amazon. This is way too high for the 1TB, and the 2TB is more expensive than the competition, including the P510. However, the Plus G4 is likely to be more widely available in other regions and probably at a more competitive price. The drive has modest performance levels of up to 10,000 / 8,200 MB/s for sequential reads and writes with up to 1,300 K / 1,400K random read and write IOPS. The warranty is standard at five years, with up to 600TB of write endurance per TB of capacity</p><h2 id="kioxia-exceria-plus-g4-software-and-accessories-2">Kioxia Exceria Plus G4 Software and Accessories</h2><p>Kioxia offers its <a data-analytics-id="inline-link" href="https://apac.kioxia.com/en-apac/personal/software/ssd-utility.html"><u>SSD Utility management software</u></a> for its SSDs. This is an SSD toolbox that gives a health summary of the drive and also lets you monitor the SSD in real time. The application also helps with firmware updates, password protection, and enables functions such as secure erase. The program works for Windows 10 and up, and it works on all of Kioxia’s recent SSDs.</p><p>It’s nice to see such software being offered for what are essentially client or OEM drives, but Kioxia has been pushing deeper into the retail space with its Plus line of drives. Most users are on Windows, and the software covers the most common functions, so it’s a respectable attempt.</p><h2 id="kioxia-exceria-plus-g4-a-closer-look-2">Kioxia Exceria Plus G4: A Closer Look</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="3WY3PhbjGr95kiogiATJvH" name="02" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/3WY3PhbjGr95kiogiATJvH.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Xm22ZjL2ZyeRrCKsLqMCmH" name="03" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Xm22ZjL2ZyeRrCKsLqMCmH.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Even without removing the top label, which Kioxia states does help spread and dissipate heat, we can tell this is a Phison drive from the power management IC (PMIC). The label states it’s a PCIe 5.0 drive, so that narrows things down considerably.</p><p>This is a single-sided drive at all capacities, so the back has no components. The drive lists its Physical Security ID (PSID), which means this drive supports TCG Opal. Phison controllers can and do support hardware encryption, but that feature likely adds to the manufacturer’s cost, which is why many brands omit SED support. It’s more common as at least an option on client and OEM drives due to business requirements, although typically you will have two separate SKUs for it, as is common with Micron drives.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Ew2mHdrtJNsEniGiPNfisS" name="04" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Ew2mHdrtJNsEniGiPNfisS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="CLN6oGrmmpt34pZPPzTRsS" name="06" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/CLN6oGrmmpt34pZPPzTRsS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="8M5RSBpTSth2mfJM8pPzeS" name="05" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/8M5RSBpTSth2mfJM8pPzeS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 uses Phison’s E31T controller. For more details, see our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/phison-e31t-es-2tb-review"><u>E31T preview</u></a>. For a brief reminder of the <a data-analytics-id="inline-link" href="https://www.phison.com/images/products_datasheet/ProductBrochure_Consumer_PS5031-E31T_040825.pdf"><u>specifications</u></a>: this is a four-channel DRAM-less PCIe 5.0 solution that can support an I/O rate up to 3,600 MT/s per NAND channel. 3,600 MT/s can be assumed to be 3,600 MB/s as consumer NAND flash transfers 8 bits, or one byte, at a time. There is overhead on these transfers, so the maximum bandwidth will be less than the channel count times this number. In this case, Phison rates the E31T for up to 10,600 MB/s. Eventually, these drives will be surpassed by 4,800 MT/s capable controllers and flash.</p><p>With four chip enable (CE) signals per channel, this drive can normally handle up to 32 dies without a problem, which is 4TB with current flash, although 2Tb dies would bump this up to 8TB. This is unlikely to ever happen, and for the most part, we’ve really only seen drives up to 2TB with this controller. This is something that frustrates the storage community, who see no reason for 4TB not to be commonplace. The reality is that it’s not cost-effective to run fast flash at that capacity when most of the market is selling smaller drives. 4TB drives can be found in other segments – on higher-end drives or with YMTC flash – and sticking to 2TB or less streamlines the production process for the third-party vendors. Flash availability is also a direct influence here, as QLC is in high demand in the enterprise.</p><p>Kioxia has an easier time with that since it manufactures its own flash. The NAND flash packages here are labeled TH58LKT3T488A8S, which are still using the old Toshiba coding. We already know these are 1TB packages with eight 1Tb dies each, using 218-Layer BiCS8 TLC flash. We’ve only had good results with this flash – see the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sandisk-wd-black-sn8100-2tb-ssd-review"><u>Sandisk WD_Black SN8100</u></a> – and it’s proven to be power-efficient with low 4K latency. Combined with Kioxia’s usually reliable custom firmware, we expect only good things.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-ssds,3891.html"><strong>Best SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-external-hard-drive-ssd,5987.html"><strong>Best External SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ssd-for-steam-deck"><strong>Best SSD for the Steam Deck</strong></a></p><h2 id="comparison-products-2">Comparison Products</h2><p>The Kioxia Exceria Plus G4 is directly positioned to compete with mid-range PCIe 5.0 SSDs, so we arranged our test pool accordingly. Some popular ones include the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/corsair-mp700-elite-ssd-review"><u>Corsair MP700 Elite</u></a>, which uses the same hardware, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/the-crucial-p510-2tb-ssd-review"><u>Crucial P510</u></a>, which has the same controller but Micron rather than Kioxia TLC flash. This performance line was once fulfilled by early E26-based drives like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/corsair-mp700-ssd-review"><u>Corsair MP700</u></a>. Those have eight channels and DRAM, but older flash.</p><p>Higher-end options include the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sandisk-wd-black-sn8100-2tb-ssd-review"><u>Sandisk WD_Black SN8100</u></a>, which is the best of the best right now, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/acer-predator-gm9000-2tb-ssd-review"><u>Acer Predator GM9000</u></a>, a drive that represents less expensive high-end options that cut cost by using older flash or, in the case of the Biwin Black Opal X570, no DRAM.</p><p>We are also comparing the three musketeers of high-end Gen 4 DRAM-less: the Maxio MAP1602-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/silicon-power-us75-2tb-review-a-practical-choice-for-the-everyday-gamer"><u>Silicon Power US75</u></a>, the Phison E27T-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sabrent-rocket-4-2tb-ssd-review"><u>Sabrent Rocket 4</u></a>, and the SMI SM2268XT2-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/kingston-nv3-ssd-review"><u>Kingston NV3</u></a>. The NV3 and US75 are known for hardware revisions but we want to cover all potential competitors. If you’re looking at a drive like the Plus G4, then there’s the sparkle of some money saved by dropping down to PCIe 4.0, especially given that you might be running your next drive at least temporarily at that speed.</p><h2 id="trace-testing-3dmark-storage-benchmark-2">Trace Testing — 3DMark Storage Benchmark</h2><p>Built for gamers, 3DMark’s Storage Benchmark focuses on real-world gaming performance. Each round in this benchmark stresses storage based on gaming activities including loading games, saving progress, installing game files, and recording gameplay video streams. Future gaming benchmarks will be DirectStorage-inclusive and we also include notes about which drives may be future-proofed.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="reukbhkaM66GSUCNebbN9b" name="ALLSSD-3DMMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/reukbhkaM66GSUCNebbN9b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="342Tz2K9WwBeemdhihQR8b" name="ALLSSD-3DMLatency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/342Tz2K9WwBeemdhihQR8b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="QYfadk83we7Vz5Z62HVL8b" name="ALLSSD-3DMPoints" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/QYfadk83we7Vz5Z62HVL8b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Hey, the Plus G4 scores pretty well here! It edges out the Micron-fuelled P510 and lines up nicely with the MP700 Elite. The drive is performing exactly as expected, which, for games, is exceptional. High-end PCIe 5.0 drives are still better, but the Plus G4 is more than fast enough for a primary drive where you also keep all of your games. The only downside is that it only goes up to 2TB.</p><h2 id="trace-testing-pcmark-10-storage-benchmark-2">Trace Testing — PCMark 10 Storage Benchmark</h2><p>PCMark 10 is a trace-based benchmark that uses a wide-ranging set of real-world traces from popular applications and everyday tasks to measure the performance of storage devices. The results are particularly useful when analyzing drives for their use as primary/boot storage devices and in work environments.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="PtV6HB3ggnMQimEY9Yfopi" name="ALLSSD-PCM10Latency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/PtV6HB3ggnMQimEY9Yfopi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="uVBxXZBtui39RdUwc6Nmpi" name="ALLSSD-PCM10BW" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/uVBxXZBtui39RdUwc6Nmpi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="4Yzis4gmpin6EQxmgfdkpi" name="ALLSSD-PCM10Score" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/4Yzis4gmpin6EQxmgfdkpi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 also has relatively good application performance, but here it is closer to the P510, and it falls behind the similarly equipped MP700 Elite. Why is this so? Well, PCMark is one of those benchmarks that we know some manufacturers have optimized for in firmware. This also goes the other way in that optimized firmware could hurt a drive in this benchmark.</p><p>Client drives, with one application being for use in standardized prebuilt PCs for small businesses, have different requirements than retail. Getting fully specced for Dell or HP is actually a long and potentially grueling process. Client drives usually have a tighter performance envelope based on thermals, and reliability is a higher priority. This is one reason Kioxia drives have proven to be more reliable than analogous retail drives, even with spotty controllers like the InnoGrit IG5236 on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/kioxia-xg8-review"><u>XG8</u></a>.</p><p>We’re pointing that out because a lot of the time, SSD buyers have a single priority in mind: reliability. This is a very difficult thing to quantify. Most of the time, it comes down to a battle of anecdotes. Well, Kioxia drives have a decent track record for reliability, and if a slight decrease in PCMark 10 performance isn’t concerning to you, then you should consider a drive like the Plus G4 if you’re weighing various options. Kioxia makes the flash on this drive, which gives them a leg up on understanding how to optimize for a consistent, reliable experience.</p><h2 id="console-testing-playstation-5-transfers-2">Console Testing — PlayStation 5 Transfers</h2><p>The PlayStation 5 is capable of taking one additional PCIe 4.0 or faster SSD for extra game storage. While any 4.0 drive will technically work, Sony recommends drives that can deliver at least 5,500 MB/s of sequential read bandwidth for optimal performance. In our testing, PCIe 5.0 SSDs don’t bring much to the table and generally shouldn’t be used in the PS5, especially as they may require additional cooling. Check our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ps5-ssds"><u>Best PS5 SSDs</u></a> article for more information.</p><p>Our testing utilizes the PS5’s internal storage test and manual read/write tests with over 192GB of data both from and to the internal storage. Throttling is prevented where possible to see how each drive operates under ideal conditions. While game load times should not deviate much from drive to drive, our results can indicate which drives may be more responsive in long-term use.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="yx4MnxoRC6seU92eEAEU95" name="PS5E28-PS5ReadTest" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/yx4MnxoRC6seU92eEAEU95.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="zCWUrRZ2jKAeSeAeZiUZ85" name="PS5E28-CopyToMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/zCWUrRZ2jKAeSeAeZiUZ85.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="ZqjfHo8gKHWMzv6k79rY85" name="PS5E28-CopyFromMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/ZqjfHo8gKHWMzv6k79rY85.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>We don’t see any reason to particularly recommend the Plus G4 for the PS5. It <em>could</em> be a good choice if you want something that might last a long time in a predictable role, but usually it’s better to go with something less expensive that has a full five-year warranty from a known name brand. If such a drive goes bad, you can often get an equivalent or superior replacement. In some regions, this is more difficult, and Kioxia drives can be a safer bet than alternatives with unknown hardware.</p><h2 id="transfer-rates-diskbench-2">Transfer Rates — DiskBench</h2><p>We use the DiskBench storage benchmarking tool to test file transfer performance with a custom, 50GB dataset. We write 31,227 files of various types, such as pictures, PDFs, and videos to the test drive, then make a copy of that data to a new folder, and follow up with a reading test of a newly-written 6.5GB zip file. This is a real world type workload that fits into the cache of most drives.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="eSBVdN27sUtdSVSyiKAusC" name="ALLSSD-DiskBench50Copy" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/eSBVdN27sUtdSVSyiKAusC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="XJeNFC8XrdMMiUBKan5wsC" name="ALLSSD-DiskBench50Write" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/XJeNFC8XrdMMiUBKan5wsC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="DHcWda8snvUu4RjZjVakrC" name="ALLSSD-DiskBench65Read" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/DHcWda8snvUu4RjZjVakrC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 is back to scoring where it should be on our DiskBench copy test: above the P510 and close to the MP700 Elite. It should match the MP700 Elite, and instead, it’s a little behind, but this is expected. The Plus G4 may be optimized differently for sustained writes, which will impact its write performance in this test. Additionally, it has a different firmware revision than the one we tested on the Corsair. Also expected is the P510 falling even more behind, but this perhaps warrants more discussion.</p><p>Careful readers will recall that in our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/crucial-t710-2tb-ssd-review" target="_blank"><u>Crucial T710</u></a> review, we mentioned that the T710, with its six-plane Micron TLC flash, can actually be faster at the lowest 1TB capacity. Likewise, the P510 is rated higher for sequential writes at 1TB – with the same flash as the T710 – in comparison to the four-plane BiCS8 on the MP700 Elite and Plus G4. This means that bandwidth-hungry buyers should lean towards the P510 at 1TB, while BiCS8 is potentially better at 2TB. If you care less about throughput – and if so, why are you looking at a mid-range PCIe 5.0 drive? – then we have typically found BiCS8 to have better latency.</p><h2 id="synthetic-testing-atto-crystaldiskmark-2">Synthetic Testing — ATTO / CrystalDiskMark</h2><p>ATTO and CrystalDiskMark (CDM) are free and easy-to-use storage benchmarking tools that SSD vendors commonly use to assign performance specifications to their products. Both of these tools give us insight into how each device handles different file sizes and at different queue depths for both sequential and random workloads.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="BpcfGsLG4u7PbEbjEUUq9L" name="ALLSSD-ATTOLinWrite" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/BpcfGsLG4u7PbEbjEUUq9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="dpz8uLfbgqMvjLRg2aoo9L" name="ALLSSD-ATTOLogRead" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/dpz8uLfbgqMvjLRg2aoo9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="uxd2HggaqUBJ2TGAXssn9L" name="ALLSSD-ATTOLinRead" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/uxd2HggaqUBJ2TGAXssn9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="bnp8MnjDrjPZ2DRr3WA99L" name="ALLSSD-ATTOLogWrite" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/bnp8MnjDrjPZ2DRr3WA99L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="rKXUbeP5QAuMg5qVqixR7L" name="ALLSSD-CDMSeqWriteQD8" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/rKXUbeP5QAuMg5qVqixR7L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="iX2NHQDpKgGibpWtjva77L" name="ALLSSD-CDMSeqReadQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/iX2NHQDpKgGibpWtjva77L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 7 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Jnbx6nr7zwutq2oafXDy6L" name="ALLSSD-CDMSeqReadQD8" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Jnbx6nr7zwutq2oafXDy6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 8 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="W7pedFJtwTAnL5i7RMfz6L" name="ALLSSD-CDMRandWriteIOPSQD256" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/W7pedFJtwTAnL5i7RMfz6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 9 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="2ChpF9ZnGAKLUdkEmj7x6L" name="ALLSSD-CDMRandWriteIOPSQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/2ChpF9ZnGAKLUdkEmj7x6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 10 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="fUbiaor7SCHNTKibFBzd6L" name="ALLSSD-CDMRandReadIOPSQD256" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/fUbiaor7SCHNTKibFBzd6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 11 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="b95RWoqMNcFCEQEK6ePv6L" name="ALLSSD-CDMRandWriteLatencyQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/b95RWoqMNcFCEQEK6ePv6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 12 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="AYMXfAoB4HWypWVLrchn6L" name="ALLSSD-CDMRandReadIOPSQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/AYMXfAoB4HWypWVLrchn6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 13 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="g4DzbBzXLLXtNA2bPG9R6L" name="ALLSSD-CDMSeqWriteQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/g4DzbBzXLLXtNA2bPG9R6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 14 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="JHFU7piYTSTP7g3LdPmz4L" name="ALLSSD-CDMRandReadLatencyQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/JHFU7piYTSTP7g3LdPmz4L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 tracks closely with the MP700 Elite in ATTO, with a slight deviation at the largest block size for reads. These two drives are close to the P510 in writes, but they fall behind on reads starting at 256KiB. As all three are using the same controller, this is probably due to the difference in flash. If we had 1TB versions of these drives to compare, we could make a better guess as to why the drives perform this way. Most likely, it’s due to the plane count difference, as this can influence interleaving with superpages, probably explaining why the P510 dips at 128KiB as well.</p><p>This is reinforced to some extent by the sequential CDM results. QD1 sequential reads favor the P510, suggesting the higher plane count can be useful here. That lines up with our thoughts in the T710 review. As QD1 is a more realistic workload, going with Micron’s newest TLC flash has potential real-world advantages. This advantage disappears with queue depth. Also obvious here is that PCIe 4.0 drives have no chance of keeping up in bandwidth, and the fastest PCIe 5.0 drives are in a class of their own. The mid-range drives like the Plus G4 are still worth a look as they are less expensive and will perform well in a PCIe 4.0 M.2 slot if so required.</p><p>We then look at 4K random I/O performance with a specific emphasis on low queue depth latency. Yes, the ability of the Plus G4 to push over a million IOPS is incredible, but this class of drive is less likely to encounter such workloads. Luckily, the BiCS8 TLC flash does good work on this drive with top-notch 4K QD1 read and write latencies. It can’t match the Black SN8100, but it beats the rest. We’ve come to expect good things out of BiCS8 flash, and the Plus G4 doesn’t disappoint.</p><p>If you did need to use this drive for more powerful things, for AI or otherwise, it is certainly up to the task, but we’re not convinced it’s the best option for that.</p><h2 id="sustained-write-performance-and-cache-recovery-2">Sustained Write Performance and Cache Recovery</h2><p>Official write specifications are only part of the performance picture. Most SSDs implement a write cache, which is a fast area of pseudo-SLC (single-bit) programmed flash that absorbs incoming data. Sustained write speeds can suffer tremendously once the workload spills outside of the cache and into the "native" TLC (three-bit) or QLC (four-bit) flash. Performance can suffer even more if the drive is forced to fold, which is the process of migrating data out of the cache in order to free up space for further incoming data.</p><p>We use Iometer to hammer the SSD with sequential writes for 15 minutes to measure both the size of the write cache and performance after the cache is saturated. We also monitor cache recovery via multiple idle rounds. This process shows the performance of the drive in various states as well as the steady state write performance.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="T63ffo62xr5TKR4ZmRjM2X" name="ALLSSD-WriteSaturation-900s" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/T63ffo62xr5TKR4ZmRjM2X.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="qZ5tVCmQAUycd72ojiuQzW" name="ALLSSD-WriteSaturation-150s" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/qZ5tVCmQAUycd72ojiuQzW.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="3oCV8UN5T9uan28yFd7qtW" name="ALLSSD-WriteSaturation-AvgMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/3oCV8UN5T9uan28yFd7qtW.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The 2TB Plus G4 first writes in its fastest, single-bit mode at over 8.9 GB/s. This is a temporary mode designed to trade capacity for speed. The cache size will vary with how full the drive is, but when empty, as in our testing, the cache extends to over 435GB. When converting 3-bit TLC flash to this pSLC mode, you can have up to almost 700GB, so this cache is of a more moderate size.</p><p>Larger, full-drive caches are typical for low-end DRAM-less drives, while small caches are rarer. The P510 is an example of the latter, and such a scheme allows it to write more consistently, which is particularly good for external use in an enclosure or for certain workloads such as NAS caching. The Plus G4 takes the more common course of somewhere in between.</p><p>Once the Plus G4 fills the cache, it falls to a direct-to-TLC mode at 1.5 GB/s, which is a pretty good speed and matches the MP700 Elite’s steady state write speed. Both it and the MP700 Elite write in TLC for quite a while before finally hitting a folding state. This occurs when the drive is forced to wait for data to be moved over from the cache to the native flash before it can accept incoming writes. The drive can and will move some data over while in TLC mode, but depending on the cache size and drive speed, this may be unsustainable. Folding is an undesirable state as it’s slower with higher latency, which can also impact reads for mixed workloads.</p><p>Generally, any given drive is limited to the base speed of its native flash. This is why QLC flash inevitably gets very slow. The pSLC write state is so much faster than the QLC flash – and QLC can be just as fast as TLC in that mode with the same plane count – that the drive hits a wall more quickly and more drastically, especially as QLC is going from 4-bit to 1-bit instead of 3-bit to 1-bit. The relevance here is that the Plus G4 looks worse than the MP700 Elite in the long run in this test despite having the same flash, but that’s likely because the Plus G4 is optimized differently. Client and OEM drives aren’t designed for sustained writes and often have a tighter power-thermal envelope. The performance here in pSLC and TLC is perfectly consistent, though.</p><p>The one standout here is the P510, which, as we mentioned above, has a smaller cache. The TLC state is then <em>fast</em> in comparison to the Plus G4, but is actually <em>slow</em> in terms of what the drive can do – the P510 has no trouble recovering to 4 GB/s with enough writes. Take into consideration that it’s not realistic to write the entire drive and that interpolation can get messy when we do this level of writes, but the results still suggest that Crucial is being conservative with the P510. We previously pointed out that this might be intended to improve the “quality of service” that was an issue on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/crucial-2tb-t500-ssd-review" target="_blank"><u>T500,</u></a> or it could be hinting at future external drive products.</p><p>This type of write behavior would be perfect for an enclosure where bottlenecks don’t benefit much from pSLC anyway, and a consistent write speed is desirable. However, for desktop use and moving back to the drive under review, the Plus G4 is adequate for even fairly heavy use.</p><h2 id="power-consumption-and-temperature-2">Power Consumption and Temperature</h2><p>We use the Quarch HD Programmable Power Module to gain a deeper understanding of power characteristics. Idle power consumption is an important aspect to consider, especially if you're looking for a laptop upgrade as even the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ultrabooks-premium-laptops"><u>best ultrabooks</u></a> can have mediocre stock storage. Desktops may be more performance-oriented with less support for power-saving features, so we show the worst-case.</p><p>Some SSDs can consume watts of power at idle while better-suited ones sip just milliwatts. Average workload power consumption and max consumption are two other aspects of power consumption but performance-per-watt, or efficiency, is more important. A drive might consume more power during any given workload, but accomplishing a task faster allows the drive to drop into an idle state more quickly, ultimately saving energy.</p><p>For temperature recording we currently poll the drive’s primary composite sensor during testing with a ~22°C ambient. Our testing is rigorous enough to heat the drive to a realistic ceiling temperature.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="NT6DfieZo5tbUpYY9bWc7h" name="ALLSSD-QuarchEfficiency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/NT6DfieZo5tbUpYY9bWc7h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="jgRRxLY9hGo4V7z67rE47h" name="ALLSSD-QuarchMaxPower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/jgRRxLY9hGo4V7z67rE47h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="DVUpyQYhpiHb2rHsvDy68h" name="ALLSSD-QuarchIdlePower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/DVUpyQYhpiHb2rHsvDy68h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="ZogyLWW3sojjFhYrUFh78h" name="ALLSSD-QuarchAvgPower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/ZogyLWW3sojjFhYrUFh78h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 is quite efficient, more efficient than the P510 but less than the MP700 Elite. We already know from experience that BiCS8 flash is more efficient than Micron’s, with appropriate trade-offs, but the Plus G4 still falls behind the fastest BiCS8-based drives. This might again be due to optimization.</p><p>We would like to point out that among the E31T-based drives we’ve tested, some – specifically the P510 and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/pny-cs2150-2tb-ssd-review" target="_blank"><u>PNY CS2150</u></a> – have custom firmware strings, while others, including the MP700 Elite and Plus G4, have utilized standard Phison versioning. That does not mean there is or isn’t a lack of custom implementation, not least because Sandisk and Kioxia BiCS8 actually do not perform the same in all cases. That sounds unusual, as the flash should be identical; however, there are performance differences on some Phison controllers, such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/phison-e28-2tb-ssd-review" target="_blank"><u>E28,</u></a> based on early reports.</p><p>Regardless of the specific reasons for any differences, while the Plus G4 is less efficient on paper and has some minor performance quirks, the result is a more reliable experience. Our temperature testing backs this up as we hit a maximum temperature of 51°C, which is more than 30°C below the throttling point. This is an excellent result, making this a fantastic drive for laptops and other hot or confined environments. This is sensible since client and OEM drives often need to survive in low-airflow cases and warmer ambients.</p><h2 id="test-bench-and-testing-notes-2">Test Bench and Testing Notes</h2><div ><table><caption>Test Bench and Testing Notes</caption><tbody><tr><td class="firstcol " ><p><strong>CPU</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B09FXDLX95">Intel Core i9-12900K</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Motherboard</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BG6M53DG/">Asus ROG Maximus Z790 Hero</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Memory</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BJ1892HJ">2x16GB G.Skill DDR5-5600 CL28</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Graphics</strong></p></td><td
><p>Intel Iris Xe UHD Graphics 770</p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>CPU Cooling</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B07PB24DN2">Enermax Aquafusion 240</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Case</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B08412JPCH">Cooler Master TD500 Mesh V2</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Power Supply</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BXFQ6XPB">Cooler Master V850 i Gold</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>OS Storage</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BJ116VV2">Sabrent Rocket 4 Plus-G 2TB</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Operating System</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B09V71FYGS">Windows 11 Pro</a></p></td><td
></td></tr></tbody></table></div><p>We use an Alder Lake platform with most background applications such as indexing, Windows updates, and anti-virus disabled in the OS to reduce run-to-run variability. Each SSD is prefilled to 50% capacity and tested as a secondary device. Unless noted, we use active cooling for all SSDs.</p><h2 id="lexar-nm1090-pro-bottom-line-2">Lexar NM1090 Pro Bottom Line</h2><p>The Kioxia Exceria Plus G4 isn’t a drive we expected to be excited about, but Kioxia’s Exceria line has gained popularity, especially in non-U.S. regions, and the drives have at least a neutral reputation, and usually a positive one. Contrast this with the infamous problems that we see with some drives that have changing hardware, which includes not only going from TLC to QLC flash but also swaps to hotter or less reliable controllers, and the usual rumor mill of “broken” drives, like with the Phison E18 performance issue. Kioxia has effectively dodged all of this and has also managed to maintain respectable levels of performance and power efficiency.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="8M5RSBpTSth2mfJM8pPzeS" name="05" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/8M5RSBpTSth2mfJM8pPzeS.jpg" mos="" align="middle" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>That’s both a positive and a negative. The Plus G4 doesn’t stand out in any way, but its baseline characteristics are sufficient. We would like better availability and maybe a wider capacity range. However, for many markets, the Plus G4 could be a diamond in the rough, and we feel it’s worth covering this drive for that reason alone.</p><p>Pricing right now in the U.S. isn’t competitive, but at least we get to see where this drive falls against the competition, most notably the Crucial P510. You can get higher bandwidth, decent power efficiency, good all-around and sustained performance, and reasonable pricing at the most popular capacities. You can’t go wrong buying any of these drives, including the Plus G4, and that's a good thing.</p><p>If you want something less expensive, there are plenty of PCIe 4.0 drives, and if you want something faster, there are high-end PCIe 5.0 drives available. If you need more capacity, well, there are many affordable 4TB drives, and the 8TB WD Black SN850X remains a good choice. Nothing much changes here, but the Plus G4 has its place.</p><p>We have the feeling that the Plus G4 would be a reliable drive that runs cool and has at least halfway decent software support. This isn’t a no-name brand slapping its name on random hardware. It’s a viable alternative and is a safe pick for a last-minute build or project. At the end of the day, the Plus G4 is not terribly exciting, but it’s a good SSD – not everything has to be covered in liquid cooling and RGB – and we can readily recommend it.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-ssds,3891.html"><strong>Best SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-external-hard-drive-ssd,5987.html"><strong>Best External SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ssd-for-steam-deck"><strong>Best SSD for the Steam Deck</strong></a></p>
https://www.tomshardware.com/pc-components/ssds/kioxia-exceria-plus-g4-2tb-ssd-review
Kioxia’s Exceria Plus G4 is a mid-range PCIe 5.0 drive that is a safe choice.
s9HjrRCbdngU2443r5r5Ko
Thu, 18 Sep 2025 12:00:00 +0000 SSDs
PC Components
Storage
Shane Downing
Tom&#039;s Hardware
Kioxia Exceria Plus G4 2TB SSD
Kioxia Exceria Plus G4 2TB SSD
<![CDATA[ Microsoft's new handheld gaming mode, exclusive to ROG Xbox Ally, has just leaked for every handheld running Windows 11 — all you need is the 25H2 update and a few registry tweaks ]]>
<p>Earlier this year, Microsoft and Asus announced the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/xbox/asus-partners-with-microsoft-launch-first-xbox-gaming-handhelds-the-rog-xbox-ally-and-ally-x">ROG Xbox Ally</a>, an update to the first-gen ROG Ally, now adorned with Xbox branding. Along with the new name came new specs, but more importantly, the Xbox partnership wasn’t just a token collaboration — rather, a deeply integrated experience finally meant to "fix" Windows on handheld devices. Hence, the ROG Xbox Ally would ship with a new full-screen Xbox app that it would boot into by default, superseding Windows 11 entirely. This was exclusive to the ROG Xbox Ally and was supposed to come to other handhelds later; however, it seems the opposite has happened.</p><p><a data-analytics-id="inline-link" href="https://x.com/tomwarren/status/1968436869982503121" target="_blank">The Verge's Tom Warren</a> reports that the full-screen Xbox experience meant for ROG Xbox Ally devices has leaked early, and can now be installed on any handheld running Windows. The actual ROG Xbox Ally is set to launch next month, which means that everyone else will get to enjoy Microsoft's new handheld gaming mode before the very device that was set to debut it. There's a <a data-analytics-id="inline-link" href="https://www.reddit.com/r/ROGAlly/comments/1niwsfi/guide_for_enabling_the_full_screen_experience_on/" target="_blank">full guide on Reddit</a> that breaks down how to install it, and it's relatively easy to follow as long as you know your way around Windows. If you have a regular Ally, Lenovo's Legion Go, or the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/best-pc-gaming-handhelds">myriad of PC handhelds out there</a>, you should be eligible.</p><blockquote class="reddit-card"
><a href="https://www.reddit.com/r/ROGAlly/comments/1niwsfi/guide_for_enabling_the_full_screen_experience_on">Guide for enabling the Full screen experience on the ROG Ally (and other handhelds)</a> from <a href="https://www.reddit.com/r/ROGAlly">r/ROGAlly</a></blockquote><script async src="//embed.redditmedia.com/widgets/platform.js" charset="UTF-8"></script><p>The only requirement for this to work is the 25H2 Windows 11 update, for which you need to be part of the Windows Insider program. The build resides in the Release Preview channel, so once you've got that going, there are a few registry edits you need to make <em>if </em>you don't see the "Enter full-screen experience on start up" toggle right away. After all is said and done, a restart should boot you directly into the new Xbox experience, where all your favorite games should be consolidated into one place — including stores like Steam, Epic Games, and Battle.net.</p><p>The full-screen handheld mode is still based on Windows 11, just running without any of the extra stuff that hogs up resources in the background. It should use less memory, and the UI should be a lot more handheld-friendly. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/hardware/handheld-gaming-pc/we-hacked-the-new-windows-11-xbox-mode-onto-the-old-rog-ally-how-does-it-optimize-performance" target="_blank">Windows Central tested the update</a> on an original ROG Ally and saw marked improvements across the board in terms of performance—going from 29 FPS in Shadow of the Tomb Raider to 38 FPS—and even an extra hour gained in battery life. These upgrades mostly come courtesy of disabling unnecessary background processing and startup apps, not some significant underlying change to the Windows kernel itself.</p><div class="youtube-video" data-nosnippet ><div class="video-aspect-box"><iframe data-lazy-priority="low" data-lazy-src="https://www.youtube.com/embed/8cD4krJLndY" allowfullscreen></iframe></div></div><p>It's important to note that you're not locked into the full-screen experience either; the ability to Alt-Tab out of apps and go into the regular desktop environment is still there. That being said, Microsoft itself recommends using mouse and keyboard for that since it's not designed for handhelds.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/handheld-gaming/microsofts-new-handheld-gaming-mode-exclusive-to-rog-xbox-ally-has-just-leaked-for-every-handheld-running-windows-11-all-you-need-is-the-25h2-update-and-a-few-registry-tweaks
If you've been waiting for the new Xbox handheld experience Microsoft has cooked up with Asus for the Xbox ROG Ally, your wishes have been answered. A leaked version of the full-screen Xbox experience has surfaced online, allowing any Windows handheld to gain the USP of the ROG Xbox Ally.
eWP6oMJPsppDxxt42ab2p9
Thu, 18 Sep 2025 11:55:25 +0000 Handheld Gaming
Video Games
Console Gaming
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Asus
Asus ROG Xbox Ally
Asus ROG Xbox Ally
<![CDATA[ China targets brain computer interface race with new standard — groundwork could lead to breakthroughs as soon as 2027 ]]>
<p>China appears to be moving fast to establish itself as a brain computer interface (BCI) leader. The nation's latest advance is the release of a medical device industry standard, the ‘Medical Device Terminology Using Brain-Computer Interface Technology,’ reports <a data-analytics-id="inline-link" href="https://www.ithome.com/0/883/421.htm" target="_blank">IT Home</a> (machine translation).</p><p>While many BCI headlines revolve around the Elon Musk-backed <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/wearable-tech/brain-interface-used-to-edit-youtube-video-paralyzed-neuralink-patient-also-uses-ai-to-narrate-with-his-own-voice">Neuralink </a>system, increasingly, we are seeing breakthrough advances from China. Earlier this year, we reported on a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/wearable-tech/china-launches-first-ever-invasive-brain-computer-interface-clinical-trial-tetraplegic-patient-could-skillfully-operate-racing-games-after-just-three-weeks">tetraplegic patient</a> skillfully playing racing games, and another subject <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/controllers-gamepads/chinese-brain-computer-interface-user-reportedly-plays-black-myth-wukong-other-games">enjoying complex PC games</a> such as <em>Black Myth: Wukong </em>and<em> Honor of Kings</em>.</p><p>For a while now, China has let it be known that it will try to coordinate its broad range of BCI research and development talents and commercial enterprises for the benefit of its industry on a wider scale. Earlier this month, we reported on a significant milestone towards this goal, with <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/china-bci-blueprint">a state-backed blitz</a> coordinating ministries, planners, and regulatory bodies, and laying out 17 steps from R&D to commercialization.</p><p>Returning to our headline news, and another piece of the jigsaw puzzle has been slotted into place. According to the source, China’s equivalent of the FDA will implement the new BCI standard from January 1, 2026.</p><p>An excerpt from the official release shows that the new standard, number YY/T 1987 – 2025, concerns “Medical devices using brain computer interface technology.” As with computing, setting a standard in any field can be a vital advantage to early adopters in the market. Standards setters can earn a range of advantages, and widespread use and adoption of useful early standards can establish an entity (company, country) as a clear leader.</p><h2 id="the-long-walk-from-what-if-to-what-is-2">The long walk from 'what if' to 'what is'</h2><p>There’s still a way to go, from planning to execution of this broad strategy. And, of course, the acceptance of China-established standards outside the country. However, should everything fall into place, China predicts most technical hurdles to be cleared by 2027. Moreover, it expects to incubate two to three leading BCI enterprises with global reach by 2030.</p><p>In the real world, we know there can be a technological or performance gulf between China's claims and China's results. So it seems healthy to throw a year or two onto the stated projections to account for that, and the typical extra distance between inspiration and implementation. Having healthy BCI competition from China could make such devices more accessible to patients in the rest of the world.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/peripherals/wearable-tech/china-targets-brain-computer-interface-race-with-new-standard-new-bci-standard-could-lead-to-breakthroughs-as-soon-as-2027
China appears to be moving fast to establish itself as a brain computer interface (BCI) leader, and officials have just published a new medical industry standard.
N7pRx4Kxgcj9dyGzAtULd8
Thu, 18 Sep 2025 11:39:00 +0000 Wearable Tech
Peripherals
Mark Tyson
Getty / Yuichiro Chino
Brain computer illustration
Brain computer illustration
<![CDATA[ Asus is 'actively investigating' ROG gaming laptop stuttering woes — Models released / sold between 2021 - 2024 affected by 'performance interruptions' ]]>
<p>Asus has acknowledged reports about an ongoing stuttering issue with some of its ROG <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/best-gaming-laptops">gaming laptops</a> that users claim has been negatively affecting performance and the user experience, according to <a data-analytics-id="inline-link" href="https://www.techpowerup.com/341121/asus-promises-fix-of-the-stuttering-issues-actively-investigates-the-problem" target="_blank">TechPowerUp</a>. The company stopped short of agreeing that an issue exists, but said that it was "actively investigating these cases," all the same.</p><p>"We've seen recent reports about performance interruptions on some ROG laptops, and we want you to know our team is actively investigating these cases," the Asus Statement reads. "We understand that smooth and reliable performance is crucial to high-performance machines like these, and we're dedicated to delivering that. Your feedback and detailed reports are invaluable, and we'll continue to provide updates and support through our official channels." It then thanked users for their patience.</p><p>The first reports of this problem started popping up earlier this week when GitHub user, Mohamed "Zephkek" Maatallah <a data-analytics-id="inline-link" href="https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive">created a new repository</a> collecting evidence about the problem. They claim it seems to be affecting a range of users on various Asus ROG gaming laptops, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/asus-rog-strix-g16-2025-review">including Strix</a>, Scar, and Zephyrus lines.</p><p>The issues include stuttering while watching YouTube videos, audio crackling and pops on Discord, and random mouse cursor freezes. After attempting a range of more generic fixes, Zephkek claims they narrowed the issue down to a problem with Asus' BIOS firmware.</p><p>Using the latency monitoring tool LatencyMon, Zephkek discovered that a single CPU core was being bottlenecked by interrupt requests, in some cases for as long as 90 seconds, hamstringing performance and making time-sensitive tasks stutter while core priority is juggled.</p><p>Deeper into their investigation, Zephkek also discovered strange power cycling of the dedicated GPU, turning it off and on again repeatedly every 15-30 seconds, even when it's supposed to be consistently active performing specific tasks.</p><p>Zephkek concluded that there are actually three problems affecting ROG laptops:</p><ul><li>Misunderstanding of interrupt context introducing unnecessary delays.</li><li>Mishandling of interrupt requests that aren't properly cleared, leading to looping interrupts.</li><li>GPU power cycling that doesn't check which GPU is currently in-use.</li></ul><p>They even did the legwork to track down the first public reports of these issues and <a data-analytics-id="inline-link" href="https://rog-forum.asus.com/t5/rog-strix-series/g15-advantage-edition-g513qy-severe-dpc-latency-audio-dropouts/m-p/809512" target="_blank">found one in August 2021</a> for a G15 Advantage Edition Asus laptop. They then discovered further reports of problems persisting with laptops through 2021 and into 2024, suggesting it affects multiple generations of Asus gaming laptops.</p><p>Asus has said it's looking into this issue, so hopefully it won't be long until it releases a firmware patch that fixes this for the majority of users.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/laptops/asus-is-actively-investigating-rog-gaming-laptop-stuttering-issue-2021-2024-models-affected-by-performance-interruptions
Asus has acknowledged reports of stuttering issues with its ROG gaming laptops from 2021 through 2024 and has confirmed it is investigating the issue, though stopped short of admitting any kind of fault.
T49knL6fZwC9VzL5ywWd7o
Thu, 18 Sep 2025 11:30:00 +0000 Laptops
Jon Martindale
Tom&#039;s Hardware
ASUS ROG Zephyr
ASUS ROG Zephyr
<![CDATA[ Nvidia and Intel announce jointly developed 'Intel x86 RTX SOCs' for PCs with Nvidia graphics, also custom Nvidia data center x86 processors — Nvidia buys $5 billion in Intel stock in seismic deal ]]>
<p>In a surprise announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies will jointly develop multiple new generations of x86 products together — a seismic shift with profound implications for the entire world of technology. Before the news broke, Tom's Hardware spoke with Nvidia representatives to learn more details about the company’s plans.</p><p>The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.)</p><p>The partnership between the two companies is in the very early stages, Nvidia told us, so the timeline for product releases, along with any product specifications, will be disclosed at a later, unspecified date. (Given the traditionally long lead-times for new processors, it is rational to expect these products will take at least a year, and likely longer, to come to market.)</p><p>Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. But Nvidia representatives tell us it also remains fully committed to other announced product roadmaps and architectures, including the company's Arm-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-project-digits-desktop-ai-supercomputer-fits-in-the-palm-of-your-hand-usd3-000-to-bring-1-pflops-of-performance-home">GB10 Grace Blackwell processors for workstations</a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-unveils-144-core-grace-cpu-superchip-claims-arm-chip-15x-faster-than-amds-epyc-rome">Nvidia Grace</a> <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process">CPUs for data centers</a>, as well as the next-gen <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-rubin-gpu-and-vera-cpu-data-center-ai-platforms-begin-tape-out-both-chips-in-fab-and-on-track-for-2026">Vera CPUs</a>. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to existing initiatives.</p><p>Nvidia hasn’t disclosed whether it will use Intel Foundry to produce any of these products yet. However, while Intel has used TSMC to manufacture some of its own recent products, its goal is to bring production of most high-performance products back into its own foundries.</p><p>Some products never left. For instance, Intel’s existing <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-launches-granite-rapids-xeon-6900p-series-with-120-cores-matches-amd-epycs-core-counts-for-the-first-time-since-2017">Granite Rapids</a> data center processors use the ‘Intel 3’ node, and the upcoming <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/servers/intel-reveals-288-core-xeon">Clearwater Forest Xeons</a> will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good">could be fabbed on Intel nodes</a>. Intel also uses TSMC to fabricate many of its client x86 processors, however, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.</p><p>In either case, Nvidia has been <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-in-talks-with-intel-foundry-intel-and-amd-know-all-our-secrets">mulling using Intel Foundry since 2022</a>, has <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good">fabbed test chips</a> there, and participates in the U.S. Defense Dept.'s <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-foundry-services-wins-us-defense-contract-for-chips-with-18a-node">RAMP-C project</a> with Intel. The DoD project involves Nvidia already <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-foundry-head-stu-pann-explains-companys-plan-to-build-arm-chips-move-more-manufacturing-to-the-us">making chips on Intel's 18A process node</a>, so it wouldn't be a total surprise.</p><p>While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. The PCIe interface has long been used to connect Intel CPUs and Nvidia GPUs. The new partnership will find tighter integration using the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products">NVLink interface for CPU-to-GPU communication</a>, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. That's a strategic advantage. Let’s dive into the details we’ve learned so far.</p><h2 id="intel-x86-rtx-socs-for-the-pc-gaming-market-2">Intel x86 RTX SOCs for the PC gaming market</h2><p>For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.</p><p>Intel's new x86 RTX CPUs will compete directly with AMD's APUs. For AMD, that means it faces intensifying competition from a company with the leading market share in notebook CPUs (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago" target="_blank">Intel ships ~79% of laptop chips worldwide</a>) that's now armed with GPU tech from Nvidia, which <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-discrete-desktop-gpu-market-share-hits-all-time-low-as-nvidia-extends-its-lead" target="_blank">ships 92% of the world's gaming GPUs</a>.</p><p>This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, we're told these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.</p><p>Intel has worked on a similar type of chip before with AMD; there is at least one significant technical difference between these initiatives, however. Intel launched its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html">Kaby Lake-G chip in 2017</a> with an Intel processor fused into the same package as an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:65.70%;"><img id="v86mjFRvYe7QGC7NP6gPLm" name="8th-Gen-Intel-Core-processor.jpg" alt="sdf" src="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" mos="" link="" align="" fullscreen="" width="1280" height="841" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="caption-text">An RTX GPU chiplet connected to an Intel CPU chiplet via the fast and efficient NVLink interface.
</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:60.20%;"><img id="5Hd4zpDFEkftzMEPoMmF99" name="05.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/5Hd4zpDFEkftzMEPoMmF99.jpg" mos="" link="" align="" fullscreen="" width="1510" height="909" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:60.33%;"><img id="8RHXSnZBY8Zjvh8vzTPeuY" name="09.JPG" alt="afd" src="https://cdn.mos.cms.futurecdn.net/8RHXSnZBY8Zjvh8vzTPeuY.jpg" mos="" link="" align="" fullscreen="" width="1510" height="911" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:59.34%;"><img id="y3y8rMjRgDiJ8hD326woUR" name="02.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/y3y8rMjRgDiJ8hD326woUR.jpg" mos="" link="" align="" fullscreen="" width="1510" height="896" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:59.93%;"><img id="Cqhf8QvkAC9x6sHChgeeCm" name="07.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/Cqhf8QvkAC9x6sHChgeeCm.jpg" mos="" link="" align="" fullscreen="" width="1510" height="905" attribution="" endorsement="" class=""></p></div></div></figure></div></div></div><p>This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory. Given the particulars of Nvidia's NVLink Fusion architecture, we can expect the chips to communicate via a refined interface, but it is unlikely that it will leverage Nvidia's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process">C2C (Chip-to-Chip) technology</a>, an inter-die/inter-chip interconnect that's based on Arm protocols that aren't likely optimized for x86.</p><p>Intel notoriously <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-discontinue-kaby-lake-g-amd-graphics,40577.html">axed the Kaby Lake-G products in 2019</a>, and the existing systems were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-graphics-driver-update-hades-canyon-amd-12-month-delay">left without proper driver support</a> for <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/windows-11-kaby-lake-g-drivers">quite some time</a>, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.</p><p>We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.</p><h2 id="nvidia-s-first-x86-data-center-cpus-2">Nvidia's first x86 data center CPUs</h2><p>Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modifications are currently unknown. We know that Nvidia will employ its NVLink interface, which suggests that the chips could leverage Nvidia’s new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products" target="_blank">NVLink Fusion</a> technology for custom CPUs and accelerators, enabling faster and more efficient communication with Nvidia’s GPUs than is possible with the PCIe interface.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3485px;"><p class="vanilla-image-block" style="padding-top:52.88%;"><img id="MftMZVxs3dkte2VoNsxtMi" name="Screenshot 2025-05-19 115749.png" alt="NVLink Fusion" src="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi.png" mos="" align="middle" fullscreen="" width="3485" height="1843" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Nvidia)</span></figcaption></figure><p>Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.</p><p>Intel has<a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-announces-idm-20-foundry"> said that it will design completely custom x86 chips for customers</a> as part of its IDM 2.0 strategy. However, aside from a recent announcement of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-outlines-a-plan-to-get-back-in-the-game-pause-fab-projects-in-europe-make-the-foundry-unit-an-independent-subsidiary-and-streamline-the-x86-portfolio">custom AWS chips</a> that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/intel-ousts-ceo-of-products-as-part-of-the-latest-executive-shake-up-ending-30-year-career-company-also-establishes-new-custom-chip-design-unit">announced a new custom chip design unit just two weeks ago</a>, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.</p><p>Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it.</p><p>The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/amd-infinity-fabric-cpu-to-gpu">Infinity Fabric (XGMI)</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/ualink-has-nvidias-nvlink-in-the-crosshairs-final-specs-support-up-to-1-024-gpus-with-200-gt-s-bandwidth">Ultra Accelerator Link (UALink)</a> initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.</p><h2 id="dollar-and-cents-geopolitics-2">Dollar and Cents, Geopolitics</h2><p>Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.</p><p>Nvidia’s buy-in comes on the heels of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/big-tech/trump-says-u-s-govt-will-take-a-10-percent-ownership-stake-in-intel-lip-bu-tan-reportedly-agreed-to-unprecedented-arrangement-for-a-domestic-chipmaker">U.S government buying $10 billion of newly-created Intel stock</a>, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/softbank-to-buy-usd2-billion-in-intel-shares-at-usd23-each-firm-still-owns-majority-share-of-arm">Softbank has also recently purchased $2 billion worth of primary issuance of Intel stock</a> at $23 per share.</p><div ><table><caption>Purchases of Intel Stock</caption><tbody><tr><td class="firstcol empty" ></td><td
><p>Total</p></td><td
><p>Share Price</p></td><td
><p>Stake in Intel</p></td></tr><tr><td class="firstcol " ><p>Nvidia</p></td><td
><p>$5 Billion</p></td><td
><p>$23.28</p></td><td
><p>~5%</p></td></tr><tr><td class="firstcol " ><p>U.S. Government</p></td><td
><p>$9 Billion</p></td><td
><p>$20.47</p></td><td
><p>~9.9%</p></td></tr><tr><td class="firstcol " ><p>Softbank</p></td><td
><p>$2 Billion</p></td><td
><p>$23</p></td><td
></td></tr></tbody></table></div><p>The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.</p><p>“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”</p><p>“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”</p><p>We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a <a data-analytics-id="inline-link" href="https://events.q4inc.com/attendee/108505485">webcast press conference at 10 am PT</a>.
{<strong>EDIT</strong>: you can <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/teams-at-nvidia-and-intel-have-been-working-in-secret-on-jointly-developed-processors-for-a-year-the-trump-administration-has-no-involvement-in-this-partnership-at-all">read our futher coverage of that press event here</a>.}</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal
Nvidia and Intel announced today that the companies would jointly develop multiple new generations of products together. The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, and custom-built Intel x86 CPUs for Nvidia’s AI products for hyperscale and enterprise customers.
6S7ZPUsULrjZhoioYnhg6Z
Thu, 18 Sep 2025 11:00:11 +0000 CPUs
PC Components
palcorn@outlook.com (Paul Alcorn)
Paul Alcorn
Nvidia
asdf
asdf
<![CDATA[ Shai-Hulud malware campaign dubbed 'the largest and most dangerous npm supply-chain compromise in history' — 'hundreds' of JavaScript packages affected ]]>
<p>It's a bad time to be a JavaScript developer, after Koi Security <a data-analytics-id="inline-link" href="https://www.koi.security/blog/shai-hulud-npm-supply-chain-attack-crowdstrike-tinycolor" target="_blank">revealed</a> yesterday that it is tracking "the largest and most dangerous npm supply-chain compromise in history."<br><br>The security firm said the Shai-Hulud malware campaign "has now impacted <strong>hundreds of packages</strong> across multiple maintainers," including "popular libraries such as <strong>@ctrl/tinycolor</strong> as well as packages maintained by <strong>CrowdStrike</strong>." (Emphasis theirs.) And the problem is probably going to get worse before it gets better, because the malware in question is a worm that autonomously spreads from package to package.<br><br>"Attackers published malicious versions of @ctrl/tinycolor and other npm packages, injecting a large obfuscated script (bundle.js) that executes automatically during installation," Koi Security said in the blog post revealing this campaign. "This payload repackages and republishes maintainer projects, enabling the malware to <strong>spread laterally across related packages</strong> without direct developer involvement."<br><br>To be clear: This campaign is distinct from the incident that we covered on Sept. 9, which saw <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/cyber-security/javascript-packages-with-billions-of-downloads-were-injected-with-malicious-code-in-worlds-largest-supply-chain-hack-geared-to-steal-crypto-a-phishing-email-is-all-it-took-to-undermine-npm-packages" target="_blank">multiple npm packages</a> with billions of weekly downloads compromised in a bid to steal cryptocurrency. The ecosystem is the same — attackers have clearly realized the GitHub-owned npm package registry for the Node.js ecosystem is a valuable target—but whoever's behind the Shai-Hulud campaign is after more than just some Bitcoin.<br><br>"The injected script performs <strong>credential harvesting and persistence operations</strong>," Koi Security said. "It runs TruffleHog to scan local filesystems and repositories for secrets, including npm tokens, GitHub credentials, and cloud access keys for [Amazon Web Services], [Google Cloud Platform], and Azure. It also writes a hidden GitHub Actions workflow file (.github/workflows/shai-hulud-workflow.yml) that exfiltrates secrets during CI/CD runs, ensuring long-term access even after the initial infection. This dual focus on <strong>endpoint secret theft and backdoors</strong> makes Shai-Hulud one of the most dangerous campaigns ever compared to previous compromises."<br><br>That might be confusing to anyone who doesn't have to worry about developing and distributing Node.js software. But the long and short of it is that Shai-Hulud is using a well-known offensive security tool (TruffleHog) alongside developer tooling (GitHub Actions) in an environment that is designed specifically to help distribute software without much developer involvement (npm).<br><br>We suggested in our previous report that whoever compromised the npm packages to steal cryptocurrency did us a favor, because they could have used their access to those packages to accomplish far worse attacks. Now it seems that someone is looking to do just that — and it's hard to feign surprise when the Node.js ecosystem and the tooling built around it were practically built to enable widespread attacks like this.<br><br>Koi Security is updating its blog post with a list of npm packages known to have been compromised via the Shai-Hulud campaign. StepSecurity has also <a data-analytics-id="inline-link" href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised#immediate-actions-required" target="_blank">published</a> indicators of compromise alongside a technical breakdown of how the malware spreads, what it does, and how organizations should respond if they discover that a compromised package has been used somewhere in their infrastructure.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/shai-hulud-malware-campaign-dubbed-the-largest-and-most-dangerous-npm-supply-chain-compromise-in-history-hundreds-of-javascript-packages-affected
Security researchers are tracking a malware campaign that has compromised hundreds of packages distributed via the npm ecosystem.
daBToSYGWTwwSJGUupnwkm
Thu, 18 Sep 2025 09:42:31 +0000 Cybersecurity
Tech Industry
Nathaniel Mott
Curly_photo / Getty
Cyberattack concept
Cyberattack concept
<![CDATA[ Logitech's next gaming mouse will have haptic-based clicks, adjustable actuation, and rapid trigger — new G Pro X2 Superstrike will land at $180 ]]>
<p>Logitech's latest addition to its ultra-light wireless gaming mouse lineup has something no other mouse has: an "innovative blend of inductive analog sensing and real-time click haptics." What this means is that Logitech's new G Pro X2 Superstrike mouse will feature an analog system that allows you to adjust the point at which your mouse switches actuate <em>and </em>still get the real-time feedback of a physical "click," thanks to haptics. <br><br>Logitech's new mouse will feature its new "Superstrike" technology, which involves a "bespoke Haptic Inductive Trigger System (HITS)" that "combines adjustable actuation point and rapid trigger capabilities with an innovative haptics system." <br><br>While we've seen analog switches and adjustable actuation a lot in the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/best-gaming-keyboards">best gaming keyboards</a>, this is the first time we'll see it in a gaming mouse. This is because gamers <em>want </em>their mice to click exactly when the button actuates, and you can't have this without something like HITS, which changes the "click" to match the new actuation point. <br><br>According to Logitech, the mouse will feature 10 selectable actuation steps and five rapid trigger reset levels over 0.6mm of click travel, which seems like a fairly detailed amount of fine-tuning. But the brand does point out that the mouse is designed for professional athletes, so this makes sense. <br><br>Haptic Inductive Trigger System aside, the Pro X2 Superstrike will feature Logitech's latest Hero 2 optical sensor, which has a maximum resolution of 44,000 DPI and a maximum speed of 888 IPS and can handle up to 88 G's of acceleration. (This is the same sensor found in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-mice/logitech-g-pro-x-superlight-2-dex-review">Logitech's G Pro X
Superlight 2 Dex</a>.) <br><br>It will also feature up to an 8,000 Hz polling rate and offer up to 90 hours of battery life, albeit not at the same time. Specs-wise, the mouse will weigh 65g (2.29oz) and measure 4.92 x 2.5 x 1.57 inches (125 x 63.5 x 40mm), which makes it identical to the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/logitech-g-pro-x-superlight-2">Logitech G Pro X Superlight 2</a> in size and shape and about 5g heavier in weight. <br><br>Logitech says the Pro X2 Superstrike will hit shelves in Q1 of 2026 for $179.99, so we'll probably see it showcased at CES 2026.</p>
https://www.tomshardware.com/peripherals/gaming-mice/logitechs-next-gaming-mouse-will-have-haptic-based-clicks-adjustable-actuation-and-rapid-trigger-new-g-pro-x2-superstrike-will-land-at-usd180
Logitech's latest addition to its ultra-light wireless gaming mouse lineup has something no other mouse has: an "innovative blend of inductive analog sensing and real-time click haptics."
URHBBM5rMRtWw9JhRL7kDi
Thu, 18 Sep 2025 09:00:00 +0000 Gaming Mice
Peripherals
Mice
Sarah Jacobsson Purewal
Logitech
logitech superstrike mouse
logitech superstrike mouse
<![CDATA[ PlayStation 5 Digital Edition with 1TB SSD downgraded to 825GB listed at the same price — CFI-2116 revision emerges overseas on Amazon ]]>
<p>The rumors (via <a data-analytics-id="inline-link" href="https://x.com/billbil_kun/status/1967899998382952566?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1967899998382952566%7Ctwgr%5E85f97f0baf6bbfa74ab49ae3fe800cdd6accf54c%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fkotaku.com%2Fsony-playstation-5-digital-slim-storage-price-2000622679">billbil-kun</a>) regarding Sony's revision of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/playstation-5-sony-ps5">PlayStation 5 </a>Digital Edition have been confirmed through various overseas Amazon listings. Although the new CFI-2116 revision maintains the original pricing, Sony has decreased the internal storage from 1TB to 825GB, representing a 17.5% reduction in capacity. This change further emphasizes the importance for owners to upgrade their consoles with one of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ps5-ssds">best PS5 SSDs.<br><br></a>The PlayStation 5 has been on the market for nearly five years, during which time the console has undergone multiple hardware revisions. These modifications have varied from minor optimizations, such as improvements in heatsink design, to substantial hardware alterations, including a die shrink, culminating in the transition to the PlayStation 5 Slim models. Consequently, the most recent CFI-2116 revision feels like a regression.<br><br>The original PlayStation 5, colloquially referred to now as the PlayStation 5 Fat, was equipped with 825GB of internal storage, of which approximately 650GB was accessible to the user, depending on system updates and other variables. The shift to the PlayStation 5 Slim introduced numerous enhancements, including an increase to 1TB of storage, providing the user with approximately 850GB of available space (subject to similar factors).<br><br>The CFI-2116 revision, also known as "Chassis E," marks the return of the 825GB SSD, which Sony advertises on the new packaging. Consumers are losing close to 200GB, or 24%, of usable, high-speed storage with the latest revision. You could argue that 200GB isn't a lot, and that's true in a way since some AAA titles — specifically, <em>Call of Duty: Black Ops Cold War —</em> are pushing over 300GB of installed size. But under normal circumstances, 200GB should be enough for one or two games.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1500px;"><p class="vanilla-image-block" style="padding-top:48.20%;"><img id="Mvu4VYuhS7ziGvZMKZKXb5" name="61h7VjYt-fL._AC_SL1500_ (1)" alt="PlayStation 5 Digital Edition" src="https://cdn.mos.cms.futurecdn.net/Mvu4VYuhS7ziGvZMKZKXb5.jpg" mos="" link="" align="" fullscreen="" width="1500" height="723" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Amazon Italy)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:459px;"><p class="vanilla-image-block" style="padding-top:154.68%;"><img id="8ZhCgh6rkTRDJ4ucLrcMGF" name="41gEsV574zL" alt="PlayStation 5 Digital Edition" src="https://cdn.mos.cms.futurecdn.net/8ZhCgh6rkTRDJ4ucLrcMGF.jpg" mos="" link="" align="" fullscreen="" width="459" height="710" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Amazon Italy)</span></figcaption></figure></div></div></div><p>The reason Sony has downgraded the internal storage in the CFI-2116 revision is unknown. If it weren't for past leaks or the Amazon listings, we wouldn't even know about the revision, since Sony hasn't officially announced it. However, given the current market situation, it's plausible that the downgrade could be a way for Sony to optimize production costs without resorting to another price hike. Sony already <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/console-gaming/sony-hikes-ps5-prices-by-usd50-starting-tomorrow-sony-adds-up-to-10-percent-to-the-price-of-every-model-from-august-21">increased the pricing</a> for the different PlayStation 5 models by $50 last month, citing the "challenging economic environment." <br><br>This generation of gaming consoles is the first to experience <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/console-gaming/gaming-consoles-are-becoming-more-expensive-as-they-age-for-the-first-time-in-history-gamers-blame-tariffs-for-playstation-xbox-and-switch-price-increases">price increases</a> rather than decreases. Despite this, Sony has sold over 77 million units as of May 2025, so the company is eager to keep up the momentum. Instead of raising prices again, Sony may have decided that reducing internal storage was a better way to keep prices stable.<br><br>Sony silently released the revised PlayStation 5 Digital Edition in Europe on Sept. 13. The console has already surfaced on <a data-analytics-id="inline-link" href="https://www.amazon.es/dp/B0FN7ZG39D">Amazon Spain</a>, <a data-analytics-id="inline-link" href="https://www.amazon.it/dp/B0FN7ZG39D">Amazon Italy</a>, <a data-analytics-id="inline-link" href="https://www.amazon.fr/dp/B0FN7ZG39D">Amazon France</a>, and <a data-analytics-id="inline-link" href="https://www.amazon.de/dp/B0FN7ZG39D">Amazon Germany</a>. It's available for purchase at Amazon Italy and Amazon Germany for €499, which aligns with the European MSRP for the existing PlayStation 5 Digital Edition (Chassis D). Amazon Germany has set a delivery date by October 23.<br><br>Initial rumors suggest that the CFI-2116 revision may be exclusive to the European market. However, it's plausible that the revised console could make its way to the U.S. market. Fortunately, the revision only affects the PlayStation 5 Digital Edition, so the disc version is safe. Nonetheless, if you're set on buying the digital edition, it might be a good time to pull the trigger, as there's no telling if or when the 1TB SKUs will disappear from the shelves to be replaced by the 825GB variant.<br><br><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/playstation/playstation-5-digital-edition-with-1tb-ssd-downgraded-to-825gb-listed-at-the-same-price-cfi-2116-revision-emerges-overseas-on-amazon
The new revision (CFI-2116) of the PlayStation 5 Digital Edition has been listed on Amazon Italy, Amazon France, and Amazon Germany at the same MSRP.
oV2NsfSYWNJxnc9ojG8jJH
Wed, 17 Sep 2025 18:31:41 +0000 PlayStation
Video Games
Console Gaming
Zhiye Liu
Amazon Italy
PlayStation 5 Digital Edition
PlayStation 5 Digital Edition
<![CDATA[ Alibaba’s AI chip goes head-to-head with Nvidia H20 in state-backed benchmark demo ]]>
<p>Alibaba’s semiconductor unit, T-Head, has reportedly <a data-analytics-id="inline-link" href="https://www.reuters.com/business/media-telecom/china-spotlights-major-data-centre-project-using-domestic-chips-2025-09-17/" target="_blank"><u>developed a new AI processor</u></a> that it claims matches the performance of Nvidia’s H20 — the GPU built specifically for the Chinese market that’s currently <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-h20-gpus-reportedly-caught-up-in-u-s-commerce-departments-worst-export-license-backlog-in-30-years-billions-of-dollars-worth-of-gpus-and-other-products-in-limbo-due-to-staffing-cuts-communication-issues"><u>stuck in geopolitical purgatory</u></a>. <br><br>The demonstration aired Tuesday, September 16, on China Central Television (CCTV), during a broadcast covering Premier Li Qiang’s visit to China Umicom’s Sanjiangyuan Energy Intelligent Computing Centre in Qinghai. In the segment, T-Head’s new “PPU” accelerator was directly compared with Nvidia’s H20 and A800, as well as Huawei’s Ascend 910B, with a chart implying performance parity between the Alibaba and Nvidia parts. <br><br>The chip, an ASIC designed for AI workloads, features 96 GB of HBM2e, 700 GB/s chip-to-chip interconnect, PCIe support, and 400 W board power, according to the on-screen specs as reported by <a data-analytics-id="inline-link" href="https://www.scmp.com/tech/tech-war/article/3325894/tech-war-alibaba-developed-ai-processor-par-nvidias-h20-chip-cctv-report-shows" target="_blank"><u><em>South China Morning Post</em></u></a>. While the broadcast didn’t disclose the specifics of the testing methodology used or publish raw figures, it’s the first public benchmark placing Alibaba’s hardware in the same class as Nvidia’s datacenter GPUs. <br><br>According to <em>Reuters</em>, China Unicom has already deployed 16,384 of Alibaba’s PPU cards across its infrastructure, accounting for more than half of the almost 23,000 domestic accelerators currently installed at the Qinghai facility. Together, the cards deliver 3,579 petaflops of compute, with the site expected to scale to more than 20,000 petaflops once all phases are complete.<br><br>There’s just as much geopolitical context behind the CCTV demonstration as there is technical. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/the-tale-of-nvidias-hgx-h20-how-an-ai-gpu-became-a-political-lightning-rod"><u>Nvidia’s H20</u></a> was introduced to comply with U.S. export controls limiting the sale of high-performance silicon to China. Built on Hopper architecture but cut down to meet restrictions, the H20 ships with 96 GB of HBM3 and roughly 4.0 TB/s of memory bandwidth. That lends some perspective to Alibaba’s matching 96 GB HBM2e capacity, though not necessarily its real-world performance. <br><br>The biggest unknown right now is on the software side. While Alibaba is understandably <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d"><u>eager to show</u></a> it can meet AI hardware needs in-house, the company has not disclosed details about frameworks, toolchains, or compatibility with existing model stacks. Until independent benchmarks and developer support materialize, the PPU’s parity with Nvidia’s hardware is just a claim backed by Chinese state TV and endorsed by the Chinese government.<br><br><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><u><em> Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><u><em> add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/alibaba-ai-chip-goes-head-to-head-with-nvidia-h20
Alibaba’s semiconductor unit, T-Head, has reportedly developed a new AI processor that it claims matches the performance of Nvidia's H20 — the GPU built specifically for the Chinese market.
eTEwJwZgicad7UUJeZVdQU
Wed, 17 Sep 2025 18:02:57 +0000 GPUs
PC Components
lukejamesalden@gmail.com (Luke James)
Luke James
Getty/Bloomberg
Nvidia CEO Jensen Huang speaking to journalists in China.
Nvidia CEO Jensen Huang speaking to journalists in China.
<![CDATA[ MSI enters the US Electric Vehicle charger market with EV Life Series ]]>
<p>When I think of MSI, I think of<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/motherboards/msi-meg-x870e-godlike-motherboard-drops-at-an-eyewatering-usd1-099-unleashes-ddr5-9000-ram-five-m-2-slots-10-gbe-and-wi-fi-7-alongside-two-usb-4-0-40-gbps-ports"> <u>motherboards</u></a>,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/msi-skips-rdna-4-and-will-not-manufacture-amd-radeon-9000-series-gpus"> <u>video cards</u></a>,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/monitors/gaming-monitors/msis-new-500-hz-qd-oled-monitor-leverages-ai-tech-to-save-it-from-burn-in"> <u>gaming monitors</u></a>, and, more recently,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/msis-brings-amd-based-gaming-handheld-updated-mid-range-gaming-laptops-to-computex"> <u>PC gaming handhelds</u></a>. So, the thought of MSI entering the electric vehicle (EV) was a foreign concept to me. Unbeknownst to me, even as an enthusiast with two EVs, MSI has marketed EV chargers in other parts of the world for quite some time. However, the company is now ready to expand to North America with MSI's EV Life and EV Life Plus EV chargers.</p><p>The EV Life Series is available in four different models: you can opt for a SAE J1772 or NACS (Tesla) connector in NEMA 14-50 (think U.S. dryer outlet) or hardwired configurations. No matter which SKU you choose, you'll receive an incredibly long 24.6-foot, IP55-rated charging cable and 14.4kW/60A that will add between 43 and 59 miles of range per hour to the average EV (think Tesla Model 3 or Hyundai Ioniq 7). If you're driving something like a Chevrolet Silverado EV with a massive 200 kWh battery, you'll probably see those numbers halved.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.20%;"><img id="gt3mKdvxqy8ucyErnhWi8h" name="image1" alt="MSI EV Life Series" src="https://cdn.mos.cms.futurecdn.net/gt3mKdvxqy8ucyErnhWi8h.jpg" mos="" align="middle" fullscreen="" width="1920" height="1079" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: MSI)</span></figcaption></figure><p>When it comes to EVs, many owners like to geek out on charging stats and electricity running costs. With that in mind, the EV Life Series has built-in Bluetooth, which, when paired with the MSI aConnect app, provides a powerful tool for monitoring your EV and setting up scheduling routines. With aConnect, you can monitor current and historical charging times, how much you're saving by using electricity over a comparable gasoline- or diesel-powered vehicle, the total cost of the electricity you've pumped into your EV, and how much carbon emissions you've saved.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.20%;"><img id="kGYfASWF2L3JjNwwV42cxg" name="image2" alt="MSI EV Life Series" src="https://cdn.mos.cms.futurecdn.net/kGYfASWF2L3JjNwwV42cxg.jpg" mos="" align="middle" fullscreen="" width="1920" height="1079" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: MSI)</span></figcaption></figure><p>The EV Life Plus Series is in many ways similar to its lesser sibling. You'll find the same four connection options (NACS with NEMA 14-50 or hardwired, or SAE J1772 with NEMA 14-50 or hardwired). You also get the same 14.4KW/60A charging capabilities as on the EV Life. However, the EV Life Plus amps things up with RFID authentication support along with Wi-Fi and Ethernet connectivity. The latter two features allow you to monitor the charging progress of your vehicle from anywhere, instead of the short-range limitations of Bluetooth-only support.</p><p>The EV Life Plus Series also supports the OCPP 1.6J standard, which provides a secure, industry-standard communications protocol for charging. This helps avoid vendor lock-in through proprietary standards, which is why MSI's EV chargers can work not only with Tesla vehicles, which helped popularize the NACS connector, but also with vehicles that use the SAE J1772 connector.</p><p>The MSI EV Life with NACS or SAE J1772 connector is available for $449. If you want to connect to your home's grid with a NEMA 14-50 connection, the price increases to $499. The EV Life Plus starts at $549.99 for a hardwired connection with a NACS or SAE J1772 connector. You'll also pay a $50 premium for a NEMA 14-50 electrical hookup. The chargers are available directly from<a data-analytics-id="inline-link" href="https://us-store.msi.com/EV-Solution/EV-chargers"> <u>MSI</u></a> or from<a data-analytics-id="inline-link" href="https://www.amazon.com/stores/page/741AAEE3-21A0-46D6-9F0C-073BFB4E34DE"> <u>Amazon</u></a>. For comparison,<a data-analytics-id="inline-link" href="https://shop.tesla.com/product/wall-connector"> <u>Tesla's 11.5kW/48A Wall Connector is $420</u></a>.</p>
https://www.tomshardware.com/tech-industry/msi-enters-the-us-electric-vehicle-charger-market-with-ev-life-series
MSI's EV Life and EV Life Plus EV chargers support NACS and SAE J1772 vehicles.
Q8NGpFC6D8ZK7T4XeuKsyP
Wed, 17 Sep 2025 17:30:56 +0000 Tech Industry
brandon.hill@futurenet.com (Brandon Hill)
Brandon Hill
MSI
MSI EV Life Series
MSI EV Life Series
<![CDATA[ Exploring the RTX Pro 6000D, Nvidia's China-only GPU, which is now banned from sale — neutered specs cannot compete with grey-market chips ]]>
<p>Nvidia’s RTX 6000D was never going to be a hero product. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-preparing-rtx-6000d-for-chinese-market-to-comply-with-u-s-export-controls-fabricated-on-tsmc-n4-featuring-gddr7-memory-capable-of-delivering-1-100-gb-s-of-bidirectional-bandwidth">Built specifically for the Chinese market</a> to navigate U.S. export restrictions, it has a constrained design: a GDDR-based Blackwell GPU with no NVLink, targeting AI inference instead of full-scale training.</p><p>Two procurement sources speaking to the <a data-analytics-id="inline-link" href="https://www.scmp.com/tech/big-tech/article/3325740/nvidia-sees-tepid-demand-new-rtx6000d-ai-chip-chinese-tech-firms-sources"><em>South China Morning Post</em></a> say that demand is tepid and the chip’s value proposition is “expensive for what it does.” And that’s before you factor in the uncomfortable comparison to Nvidia’s own RTX 5090, the flagship gaming GPU that’s officially banned from China but widely available through grey-market channels. By some measure, that consumer card not only costs half as much but outperforms the 6000D in the same inference tasks Nvidia designed its export-safe silicon to run.</p><p>Shortly after this report, it was noted that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d">China has now banned its biggest tech companies from acquiring Nvidia chips</a>, which may be the real reason why 6000D interest was so tepid in the region. But there are still interesting observations to be made about the curious 6000D itself.</p><h2 id="blackwell-sans-bandwidth-2">Blackwell sans bandwidth</h2><p>Nvidia hasn’t published formal specifications for the RTX 6000D, but multiple sources indicate it uses <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-pro-6000d-b40-blackwell-gpus-reportedly-set-to-supersede-banned-h20-accelerators-in-china">Blackwell architecture with conventional GDDR memory</a>, and delivers around 1,398 GB/s of bandwidth — just under the 1.4 TB/s export limit. There are strong indications that it avoids HBM and high‑bandwidth interconnect packaging, suggesting simpler die structures and likely dependence on PCIe or external NICs, rather than NVLink.</p><p>In other words, the 6000D is a PCIe workstation card that looks a lot like the RTX 6000 Blackwell or a tweaked 5090, just with a different name and a dramatically higher price tag. But rather than specs, the core issue is what happens when you try to scale it.</p><p>Without NVLink, the 6000D may rely on PCIe or external NICs like ConnectX to communicate between GPUs. That puts it at an immediate disadvantage in large-model inference workloads. A 70B-parameter model at FP16 can easily require 140 GB or more just for weights. Even INT8 quantization struggles to fit under 50 GB once you add the KV cache, meaning that you’ll often need two or more GPUs just to serve a single model replica. At that point, GPU-to-GPU comms becomes the bottleneck.</p><p>PCIe 4.0 x16 delivers around 64 GB/s of bandwidth. Meanwhile, NVLink 5.0 is closer to 900 GB/s. Nvidia’s own documentation recommends keeping tensor parallelism inside the NVLink domain for exactly this reason — collective operations like all-reduce and activation exchange are latency-sensitive. Try doing that over PCIe or even 800Gbps Ethernet, and step time balloons. Add more 6000Ds to the cluster, and you can imagine how the payoff in throughput starts to collapse. From a buyer's perspective, what’s the point of doing that when you can do better with alternative hardware?</p><h2 id="grey-market-swarm-2">Grey-market swarm</h2><p>The RTX 6000D retailed in China for around $7,000 (¥50,000). That’s not far off the amount you might expect to pay for a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-next-gen-ai-chip-could-double-the-price-of-h20-if-china-export-is-approved-chinese-firms-still-consider-nvidias-b30a-a-good-deal">lower-end HBM-based GPU like the H20</a>, not a GDDR card with no NVLink. And unlike the H20, it’s not even pretending to be a training-class GPU.</p><p>Now compare that to what’s available unofficially. Grey-market RTX 5090s, which were designed for gamers but blessed with Nvidia’s full Blackwell silicon, are trading for as little as $3,500 (¥25,000). Some are even being <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-rtx-5090-gpus-with-blower-style-coolers-appear-in-china-design-optimizes-nvidias-fastest-gaming-gpus-for-use-in-ai-workloads">resold in blower-style enclosures</a> with expanded VRAM of up to 80GB or even 128GB in modded units. Despite being technically banned under U.S. export rules, they’re everywhere. And they perform.</p><p>So, why choose the 6000D when you can get a more powerful grey-market part without any issues? From a throughput-per-yuan perspective, the grey-market 5090 swarm makes the 6000D look like a bad joke. And with better options potentially being just over the horizon, why would Chinese buyers want to spend money on the 6000D at all?</p><h2 id="a-domestic-market-right-around-the-corner-2">A domestic market right around the corner?</h2><p>A big part of the 6000D’s lackluster reception is circumstantial. Chinese buyers were still waiting on shipments of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/the-tale-of-nvidias-hgx-h20-how-an-ai-gpu-became-a-political-lightning-rod">Nvidia’s sanctioned H20</a>, an HBM-based Hopper GPU approved for export in July but still stuck in limbo. It’s the chip many hyperscalers wanted for high-density inference, but the latest news of China's Nvidia ban calls into question whether those orders will even be fulfilled.</p><p>At the same time, there was a hope that the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-could-be-readying-b30a-accelerator-for-chinese-market-new-blackwell-chip-reportedly-beats-h20-and-even-h100-while-complying-with-u-s-export-controls">B30A</a> — a more powerful Blackwell part designed for training — would win approval for sale, too. However, with the new ban on acquiring Nvidia chips, this seems more unlikely than ever. The B30A was reportedly equipped with 144 GB of HBM3E and NVLink support, delivering up to six times the performance of the H20 for only double the price.</p><p>As evidenced by the Cyberspace Administration of China (CAC)'s latest actions, the country is clearly moving beyond reliance on Nvidia chips. There’s a deeper shift underway. China is pushing hard for domestic AI hardware adoption, mandating that state-backed clouds procure at least <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-mandates-domestic-firms-source-50-percent-of-chips-from-chinese-producers-beijing-continues-to-squeeze-companies-over-reliance-on-foreign-semiconductors">50% of their AI accelerators from Chinese vendors</a>. Huawei’s Ascend, Biren’s CloudMatrix, and Cambricon’s NPU lines are all on the table. So is CANN: Huawei’s CUDA alternative that just went fully open source.</p><p>This (in theory) allows developers to move workloads away from Nvidia and toward Ascend, but the transition has been rocky. Chinese LLM lab DeepSeek notoriously scrapped plans to train its next model on Ascend NPUs, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseek-reportedly-urged-by-chinese-authorities-to-train-new-model-on-huawei-hardware-after-multiple-failures-r2-training-to-switch-back-to-nvidia-hardware-while-ascend-gpus-handle-inference">much to the disdain of the Chinese government</a>, citing unstable performance and poor chip-to-chip communication.</p><p>For now, that leaves China’s major clouds effectively locked into CUDA. But the political and strategic pressure to break free is building. From China's perspective, there are too few advantages to justify doubling down on Nvidia’s ecosystem, which would be terminally behind Western counterparts, if export control rules continued.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/why-nobody-is-buying-nvidia-6000d-in-china
Nvidia's RTX 6000D might be banned from sale in China, but what lies inside the GPU itself? We explore why it's not as good a deal as grey-market RTX 5090s in the region.
SpJFSsot77AggpA5dYvRpU
Wed, 17 Sep 2025 17:17:05 +0000 Semiconductors
Tech Industry
Manufacturing
lukejamesalden@gmail.com (Luke James)
Luke James
Nvidia
Nvidia RTX 6000
Nvidia RTX 6000
<![CDATA[ These must-have accessories helped me power through my overseas trip to IFA 2025 ]]>
<p>I recently took a trip overseas to Germany for IFA 2025, which meant I had to bring some essential gear to keep my devices charged while on the go. These devices ranged from a multi-outlet USB-C wall adapter to a portable 25,000 mAh battery to a thin MagSafe battery for my iPhone to a Euro plug converter for keeping my devices charged in my hotel room in Berlin.</p><h3 class="article-body__section" id="section-ugreen-65-watt-retractable-usb-c-power-block"><span>Ugreen 65-watt Retractable USB-C Power Block</span></h3><p><strong>🧳 Ugreen USB-C Power Block</strong></p><p>I actually picked up both 45-watt and 65-watt Ugreen retractable USB-C power blocks during the last Amazon Prime Day event in July. I took the 65-watt version with me on my trip due to its higher power output.</p><div class="inlinegallery
mosaic-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="FQwGymMmGbnNEb5mhvWu9R" name="image5" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/FQwGymMmGbnNEb5mhvWu9R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="zgcb6782EcBgUob5cfuDBR" name="image1" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/zgcb6782EcBgUob5cfuDBR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>When using it to charge a single device, the retractable USB-C cable can deliver up to 60 watts. The USB-C port tops out at 60 watts, while the USB-A port doles out 22.5 watts. If you're charging two devices at once, either the retractable USB-C cable or the USB-C port can deliver a maximum of 45 watts, with the other topping out at 25 watts.</p><p>While in my hotel room, I used the Ugreen adapter to supply power to my 3-in-1 travel MagSafe charger via the retractable USB-C cable and to charge my MacBook Air with the USB-C port. Even with my MacBook Air, iPhone, Apple Watch, and AirPods Pro charging all at once, the adapter was just barely warm to the touch. The thermal performance is likely due to the Gallium Nitride (GaN) power transistors, which help improve efficiency and thus reduce heat output.</p><p>The<a data-analytics-id="inline-link" href="https://www.amazon.com/gp/product/B0DNSQCRJB"> <u>65-watt Green USB-C Power Bank</u></a> is currently on sale for $37.99. If you can get by with the lower-output<a data-analytics-id="inline-link" href="https://www.amazon.com/gp/product/B0DRP9HKKC"> <u>45-watt version</u></a>, it sells for $28.99.</p><h3 class="article-body__section" id="section-anker-737-power-bank"><span>Anker 737 Power Bank</span></h3><p><strong>🧳 Anker 737</strong></p><p>My Anker 737 is my go-to power source when flying, and I've had it for nearly two years at this point. The power bank features a 24,000 mAh internal battery that has enough juice to charge an iPhone 16 Pro from empty to full four times.</p><p>The Anker 737 has two USB-C ports, each of which can deliver up to 140 watts if just one device is attached. There's also a USB-A port that tops out at 18 watts. When you've fully depleted the power bank, if you have a 140-watt charger on hand, you can get it back to a 100 percent charge in 52 minutes.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="Y4D8fGbJagDGwmXRncqwDR" name="image3" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/Y4D8fGbJagDGwmXRncqwDR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:75.04%;"><img id="SzvhHKwwx74V2Xdn8iMaAR" name="image4" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/SzvhHKwwx74V2Xdn8iMaAR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1500" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>One of my favorite features of the power bank is the built-in OLED display, which provides information on the current charge capacity, the estimated time to deplete the battery based on the current output, and the wattage delivered to each port.</p><p>While crossing the Atlantic on my NYC to Berlin leg of my trip, I used the Anker 737 to charge my iPhone 16 Pro (not in use) and iPad Pro (as I binge-watched The Pitt). Granted, I could have used the power outlet near the floor, mounted on the seat in front of me. However, since I was in the aisle seat, and the two passengers beside me kept getting up to use the bathroom, which would have required me to keep unplugging to let them pass. With the Anker 737, I just set the battery beside me in my seat. It's also a lot easier than fumbling, trying to find the seat-mounted power outlet in the dark.</p><p>The Anker 737 is<a data-analytics-id="inline-link" href="https://www.amazon.com/Anker-PowerCore-Portable-Charger-Compatible/dp/B09VPHVT2Z/"> <u>currently priced at $87.99</u></a>, or 20 percent off its MSRP of $109.99.</p><h3 class="article-body__section" id="section-baseus-magsafe-portable-charger-for-iphone-10-000mah-20w-magnetic-power-bank"><span>Baseus MagSafe Portable Charger for iPhone, 10,000mAh 20W Magnetic Power Bank</span></h3><p><strong>🧳 Baseus MagSafe Portable Chargerk</strong></p><p>When on the ground in Berlin, I was in and out of meetings, in and out of Ubers, and walking around the show floor without easy access to power. It's easy to run through my phone's battery when taking tons of pictures, uploading those images to the cloud, and recording interviews for execs.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="Q9cvoTamgXCYF5eTu7wX7R" name="image6" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/Q9cvoTamgXCYF5eTu7wX7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="t7pR42fJ82iE9rSYGqWX7R" name="image2" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/t7pR42fJ82iE9rSYGqWX7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.23%;"><img id="4oTgfnyVuhsp9hufB4PW7R" name="image8" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/4oTgfnyVuhsp9hufB4PW7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1124" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>As a result, I use a Baseus 10,000 mAh 20W MagSafe battery. It magnetically attaches to the back of my phone, doubling its thickness. Despite the added heft, I still have no trouble fitting it in my front pants pocket. It has enough capacity to provide a 0-100 percent charge (and a bit more) on my iPhone 16 Pro. The battery pack recharges via its USB-C port. You can also recharge your phone or another device via the USB-C port at up to 20 watts if you don't want to bother with the MagSafe function.</p><p>I bought my Baseus MagSafe battery charger on clearance from<a data-analytics-id="inline-link" href="https://sellout.woot.com/offers/baseus-magsafe-10000mah-20w-power-bank"> <u>Woot.com for $18.99</u></a>. However, a newer, 22.5-watt version of the device is<a data-analytics-id="inline-link" href="https://www.amazon.com/Baseus-Portable-10000mAh-Wireless-Compatible/dp/B0DZWVN6GX"> <u>currently available from Amazon for $26.99</u></a>.</p><h3 class="article-body__section" id="section-vintar-international-power-plug-adapter"><span>VINTAR International Power Plug Adapter</span></h3><p><strong>🧳 Vintar Power Plug Adapter</strong></p><p>All of my U.S. plugs are useless in Europe without a travel plug adapter. I previously bought a VINTAR 2-pack of Euro travel adapters for a family vacation to Greece last year, and took one along for my trip to Berlin.</p><div class="inlinegallery
mosaic-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="KGxASjBnxZoxR5QRmkW39R" name="image7" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/KGxASjBnxZoxR5QRmkW39R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="ouoCKHwWTSSpNazsYrbnAR" name="image9" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/ouoCKHwWTSSpNazsYrbnAR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The plug is quite versatile, offering two U.S.-style outlets, three USB-A ports, and one USB-C port for your devices. The plug is sturdy and doesn't feature moving parts, making it less susceptible to breaking from continual use and being tossed in my carry-on bag.</p><p>The VINTAR European Travel Plug Adapter is available in a<a data-analytics-id="inline-link" href="https://docs.google.com/document/d/1dKE0okuJ9y2nEjas3J5U2rJF5EPG4MibXPYEiqDBoaM/edit?tab=t.0"> </a><a data-analytics-id="inline-link" href="https://www.amazon.com/European-VINTAR-International-Compatible-American/dp/B07WRWX15J"><u>two-pack for $19.99 at Amazon</u></a>.</p>
https://www.tomshardware.com/peripherals/cables-connectors/these-must-have-accessories-helped-me-power-through-my-overseas-trip-to-ifa-2025
From MagSafe batteries to a retractable cable USB-C charger, here’s what’s in my overseas travel bag.
kDaHzRL7PR2egyoQqa6mjG
Wed, 17 Sep 2025 17:10:11 +0000 Cables and Connectors
Peripherals
brandon.hill@futurenet.com (Brandon Hill)
Brandon Hill
Tom&#039;s Hardware
Travel Tech
Travel Tech
<![CDATA[ This upcoming Thunderbolt 5 eGPU dock lets you mount an entire mini-PC on the side — also features aftermarket ATX and SFX power supply support ]]>
<p>Mini-PC and eGPU maker Aoostar has added yet another eGPU dock to its arsenal of products. On <a data-analytics-id="inline-link" href="https://www.reddit.com/r/eGPU/comments/1nhzw0s/aoostar_ag02_vs_wait_for_aoostar_eg01/" target="_blank">Reddit</a>, the company announced the EG01, a Thunderbolt 5 graphics card dock that supports full-size desktop graphics cards, ATX/SFX power supplies, and features an optional mini-PC holder on top. Pricing and a release date have yet to be disclosed.</p><p>Not much information has been publicly revealed about the dock; however, <a data-analytics-id="inline-link" href="https://www.notebookcheck.net/Aoostar-EG01-Thunderbolt-5-and-OCuLink-eGPU-dock-revealed-globally-with-mini-PC-mount.1116086.0.html" target="_blank">Notebookcheck</a> was able to get some in-depth information about the EG01. But the outlet was able to find that the dock allegedly takes advantage of a Thunderbolt 5 interface and is compatible with OCuLink, making it one of the first docks to feature both connectivity standards. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/thunderbolt-5-debuts-120-gbps-speed-is-three-times-faster-than-previous-gen">Thunderbolt 5</a> and OCuLink provide a PCIe 4.0 x4 interface from the host system to the graphics card, offering the best connectivity you'll see on eGPU docks right now.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="wKBkGKqE6saJwuuoGBeDSZ" name="Aoostar EG01 eGPU dock" alt="Aoostar EG01 eGPU Dock" src="https://cdn.mos.cms.futurecdn.net/wKBkGKqE6saJwuuoGBeDSZ.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Aoostar)</span></figcaption></figure><p>The EG01 differs significantly from most of Aoostar's other eGPU docks, featuring full compatibility for desktop GPUs, but having no embedded PSU, instead having a mount for aftermarket units. For those who want to upgrade power supplies down the road, this is a great feature and allows the user to choose whatever ATX or SFX PSU they want to use, significantly increasing the dock's flexibility. Past iterations of Aoostar docks have featured embedded power supplies, ranging from 400W to 800W of power output. Besides its 800W offerings, Aoostar's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/aoostar-ag01-egpu-with-oculink-and-built-in-400w-psu-released-at-dollar150">older trims,</a> sporting 400W or <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/docking-stations-hubs/aoostar-ag02-egpu-dock-with-oculink-support-and-500w-psu-announced-for-usd219">500W</a> power supplies, would limit GPU power draw to a max of 250W or 350W, respectively, limiting GPU options to AMD or Nvidia's mid-range desktop graphics cards.</p><p>With the EG01, you have the option of building a GPU setup that only needs enough power for what you need in the current moment, with an upgrade path down the road. If you find you want to upgrade to a more power-hungry GPU in the future, you can easily swap out the PSU with a more potent unit if necessary.</p><p>The dock can be used with any Thunderbolt or OCuLink capable device; however, the dock has also been designed with mini-PCs in mind. The power supply bracket includes an optional mini-PC mounting solution on the top, turning the eGPU dock into a computer that mimics a mini-ITX PC. This will inevitably be a highly used feature for those who plan on daily driving the EG01 with a mini-PC or NUC-like device rather than a laptop or handheld gaming PC.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/desktops/mini-pcs/this-upcoming-thunderbolt-5-egpu-dock-lets-you-mount-an-entire-mini-pc-on-the-side-also-features-aftermarket-atx-and-sfx-power-supply-support
Aoostar has announced an upcoming eGPU dock that features Thunderbolt 5 connectivity and OCuLink connectivity, plus support for aftermarket ATX/SFX power supplies. The cherry on top is the included mini-PC mount for mini-PC daily drivers.
4ThCooLt9dGFS5h6rhyT8b
Wed, 17 Sep 2025 16:52:46 +0000 Mini PCs
Desktops
editors@tomshardware.com (Aaron Klotz)
Aaron Klotz
Aoostar
Aoostar EG01 eGPU Dock
Aoostar EG01 eGPU Dock
<![CDATA[ Modern memory is still vulnerable to Rowhammer vulnerabilities — Phoenix root privilege escalation attack proves that Rowhammer still smashes DDR5 security to bits ]]>
<p>Scientists from the Computer Security Group (COMSEC) at the ETH Zürich college, in conjunction with Google, have published a proof-of-concept attack on DDR5 RAM called <a data-analytics-id="inline-link" href="https://comsec.ethz.ch/research/dram/phoenix/" target="_blank">Phoenix</a>, with the CVE number 2025-6202. The new attack causes bit-flips in memory, leading to a set of vulnerabilities that includes high-level privilege escalation. Phoenix adeptly bypasses DDR5's preventive measures for<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/new-rowhammer-attack-silently-corrupts-ai-models-on-gddr6-nvidia-cards-gpuhammer-attack-drops-ai-accuracy-from-80-percent-to-0-1-percent-on-rtx-a6000"> Rowhammer-style attacks</a>, and neither ECC nor ODECC (on-die ECC) are of help.</p><p>It's worth noting that COMSEC only tested the attacks on an AMD Zen 4 platform, against 15 SK hynix DDR5 DIMMs from 2021-2024. The team states it chose the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/sk-hynix-dethrones-samsung-to-become-worlds-top-selling-memory-maker-for-the-first-time-success-mostly-attributed-to-its-hbm3-dominance-for-nvidias-ai-gpus">largest DRAM vendor</a>, as the analysis is time-intensive even with the help of dedicated FPGA test boards. Having said that, the research is part of a <a data-analytics-id="inline-link" href="https://security.googleblog.com/2025/09/supporting-rowhammer-research-to.html" target="_blank">Google-led effort</a> for better RAM security in cooperation with JEDEC, the consortium that defines memory standards. It's also not the first time that COMSEC has worked on RAM security, having previously cooperated with VUSec to create the <a data-analytics-id="inline-link" href="https://www.vusec.net/projects/trrespass/" target="_blank">TRRespass attack</a>.</p><p>Phoenix is a mix-up and evolution of existing Rowhammer-style attacks that repeatedly "hammer" a set of RAM locations with reads, in a specific pattern, in a bid to force at least one bit to flip via electromagnetic interference. This allows for extracting data or modifying code to an attacker's preference. The scenario is concerning enough in desktops and workstations, but particularly worrying in large-scale servers hosting thousands of clients. You can download the proof-of-concept software at <a data-analytics-id="inline-link" href="https://github.com/comsec-group/phoenix" target="_blank">COMSEC's Phoenix GitHub repository</a> if you want to test your systems.</p><p>Phoenix's creators tested specific scenarios and had a 100% success rate in replicating attacks that manipulate Page Table Entries (PTE), granting access to forbidden locations in memory; a 73% chance of extracting the SSH login keys from a virtual machine in the same server; and a 33% probability of straight up getting root access thanks to manipulating the in-memory binary for the <strong>sudo</strong> utility. The team replicated the privilege escalation scenario in the <a data-analytics-id="inline-link" href="https://github.com/comsec-group/rubicon" target="_blank">Rubicon suite</a> in only 5 minutes and 19 seconds flat. COMSEC's researchers revealed their findings past June 6 to SK hynix, CPU vendors, and the major cloud platforms, and will publish their findings at the IEEE Security & Privacy 2026 conference.</p><p>There's no bulletproof mitigation for this issue yet — at least for the tested SK hynix DIMMs — but the researchers state that increasing the row refresh rate (tREFI) in the machine's UEFI by 3 times down to around 1.3 μs makes the attacks unlikely to succeed. However, that comes at a steep cost, as a benchmark with the SPEC CPU2017 suite revealed a nasty 8.4% performance hit. COMSEC says there's an impending BIOS update for AMD client systems to address this problem, but couldn't verify its effectiveness as of the date of its publication.</p><p><a data-analytics-id="inline-link" href="https://security.googleblog.com/2025/09/supporting-rowhammer-research-to.html" target="_blank">Google points out in a related blog post</a> that DDR5's TRR (Target Row Refresh) and ECC/ODECC can't quite fix the problem as they're not deterministic. For example, TRR's mechanism of triggering a refresh of a memory row doesn't keep an exact count of the number of accesses to it, making it easy to exploit by increasing the attack surface. Meanwhile, even the mighty ODECC only corrects bit-flips when data is written, or after a certain time (usually hours), meaning that just keeping an attack going for a long while is enough.</p><p>Those circumstances led to the creation of the PRAC (Per-Row Activation Counting) JEDEC standard, <a data-analytics-id="inline-link" href="https://www.jedec.org/news/pressreleases/jedec-updates-jesd79-5c-ddr5-sdram-standard-elevating-performance-and-security" target="_blank">first announced</a> in April 2024 for a future DD5 revision. PRAC keeps an accurate count of sequential accesses to a memory row and alerts the host system if a limit is exceeded, so that mitigation measures (likely a refresh) are implemented. Predictably, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/dram/jedec-publishes-first-lpddr6-standard-new-interface-promises-double-the-effective-bandwidth-of-current-gen">upcoming LPDDR6 standard</a> is integrating PRAC from the get-go. Here's to hoping the new feature will finally put a stake through Rowhammer's heart.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/modern-memory-is-still-vulnerable-to-rowhammer-vulnerabilities-phoenix-root-privilege-escalation-attack-proves-that-rowhammer-still-smashes-ddr5-security-to-bits
A new attack on DDR5 further demonstrates that current countermeasures against Rowhammer-style assaults aren't enough.
bQRwTZSjALkiVeM5QEDxVj
Wed, 17 Sep 2025 16:40:53 +0000 Cyber Security
Tech Industry
Bruno Ferreira
Getty Images
A hammer smashing a laptop computer.
A hammer smashing a laptop computer.
<![CDATA[ I got excited for the idea of sub-$1,000 gaming laptops with integrated graphics — but there are more than a few reasons why that's probably not happening ]]>
<p>There's been a trend the last two or three years in gaming laptops (and elsewhere) that no one likes: Prices are going up. It hasn't been surprising to see systems with the most powerful graphics, along with high-refresh displays, mechanical keyboards, or tons of RAM, to cost anywhere between $3,000 and $5,000, and more. See some of our most powerful picks like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/msi-titan-18-hx-ai-review"><u>MSI Titan 18 HX AI</u></a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/razer-blade-18-review"><u>Razer Blade 18</u></a>.</p><p>But that same thing has been happening on the low end. Systems that used to be $999 or less are now often at least $1,100. Those laptops often use older processors and the lowest-end current GPUs.</p><p>Up until Lenovo announced that its Legion Go 2 handheld would start at $1,049, I was thinking that handhelds might replace the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-gaming-laptops-under-1000"><u>best gaming laptops under $1,000</u></a>.</p><p>We're in a place where it feels like we need something new to broaden what's available. Could gamers get a cheaper portable rig if they were willing to get handheld-style performance with integrated graphics?</p><p>That might not sound so appealing, but the best thing for PC gamers is to have options, including gaming laptops with discrete GPUs at $1,000 or less if that is the best they can afford. But with an increase in integrated GPU power that we've seen in everything from laptop chips to handheld <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/apu-accelerated-processing-unit-definition,37645.html"><u>APUs</u></a> to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-game-changing-strix-halo-apu-formerly-ryzen-ai-max-poses-for-new-die-shots"><u>Strix Halo</u></a>, along with the economies of scale that already build budget gaming PCs, could we finally see a new low-end gaming laptop with an iGPU?</p><p>It’s a nice idea, but the more I thought it through with my colleagues, the more quickly my dreams were dashed.</p><h2 id="there-s-precedent-but-it-makes-more-sense-now-2">There's precedent, but it makes more sense now</h2><p>Back in 2021, Adata released the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/xpg-xenia-xe"><u>XPG Xenia Xe</u></a>. It was a whitebox system from Intel, but more importantly, it used Intel's Core i7-1165G7 CPU with integrated Intel Iris Xe graphics. Still, Adata referred to it as a "gaming lifestyle notebook.” Its predecessor, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/xpg-xenia-15"><u>Xenia 15</u></a>, had used a GTX 1660 Ti. We saw a similar idea in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/alienware-concept-ufo-gaming-handheld-hands-on"><u>Alienware's Concept UFO</u></a>, which used a 10th Gen Intel CPU with integrated graphics to power the gaming handheld, but that never turned into a real product.</p><p>I scoffed at the idea. My colleague at the time, Michelle Ehrhardt, titled her review of the system "expensive and unbalanced." It was $1,600. She was right.</p><p>But what if the Xenia Xe hadn't been designed to be premium? I could see a version of that, using today's chips, making a new kind of low-end gaming laptop.</p><p>Imagine an ultraportable-sized system, perhaps with a 14-inch, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/what-is-fhd-full-hd,5741.html"><u>1080p</u></a> display up to 120 Hz, using something like the AMD Ryzen Z2 Extreme (or its most equivalent laptop part that exists). Other than some beefed up cooling, designs using largely plastic chassis that could work are probably already sitting on shelves.</p><h2 id="what-would-make-that-gaming-2">What would make that gaming?</h2><p>On the other hand, there are laptops out there now with chips using AMD's Radeon 890M, though they're generally in premium <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ultrabooks-premium-laptops"><u>ultrabooks</u></a>. People do play games on those, the same way Apple's entire product line doesn't include a dedicated "gaming" laptop. There's no reason you couldn't use one of them, but they're probably priced higher than I'm thinking.</p><p>So, now we have a theoretical plastic laptop with an otherwise strong chip that could, generously, play games at 1080p on medium settings. Hopefully, that could make for something affordable, even if it's not powerful.</p><p>With handhelds, gamers accept that lack of power because they get portability. Gamers expect portability from laptops already. So what would make
something like this a gaming laptop? What could give that <em>value?</em></p><p>For starters, I would want to see an OS focused on gaming. What if it was officially licensed to run SteamOS, and you could use Arch Linux for productivity? Or perhaps it could run Windows 11's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/microsoft-focusing-on-handheld-gaming-support-with-new-xbox-compact-mode"><u>upcoming handheld gaming mode</u></a> that will debut on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/asus-rog-xbox-ally-and-xbox-ally-x-to-launch-october-16-co-branded-handhelds-sport-new-cpus-game-friendly-windows-tweaks-but-pricing-is-still-unknown"><u>Asus ROG Xbox Ally</u></a>. In exchange for power, get rid of bloatware and give people a gaming-focused experience. Using one of these would also allow for game validation, like what the Steam Deck and upcoming Xbox Ally will offer, so you can get an idea if games will run. But honestly, these should be options on high-end machines, too.</p><p>The laptop companies could also team up with Nvidia or Microsoft to get lengthy trial subscriptions to streaming services for games that may have trouble running well on integrated graphics.</p><p>Lastly, if you're not paying for a discrete GPU, maybe toss a mechanical keyboard in there. It's not unprecedented. The Dell G16 previously had cheap configurations with a Cherry keyboard, and that was a great value-add (the Dell Gaming lineup has since been discontinued).</p><p>There's also the question of which companies might be bold enough to put their gaming brand on a laptop without a discrete GPU.</p><h2 id="counterpoint-the-market-is-complex-2">Counterpoint: The market is complex</h2><p>But enough brainstorming. When you start to think about how things really work, this idea is harder than it seems.</p><p>Take my thought that companies could stick a chip with strong integrated graphics in some existing system's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/pc-chassis-definition,37651.html"><u>chassis</u></a>. They probably could, to some degree, but more cooling would still be helpful. And existing gaming laptop designs are built around the idea that a dedicated GPU is there, so tweaking that would require more tooling. Additionally, using an existing gaming chassis defeats the idea that you could get a slimmer laptop if you don't have a GPU to cool.</p><p>Next up is that in most systems, the companies that make gaming laptops can pair the CPU and GPU they want together. That's why we're seeing so many RTX 50-series laptops with 13th Gen Intel Core CPUs — those CPUs are fast <em>enough</em>, and they're likely cheaper. Companies can mix and match to hit whatever price point they want. But APUs using graphics like Radeon 890M are the highest-end parts, paired with high-end CPU cores, and they're sold at a premium.</p><p>If a laptop company wanted to make something like what I described, they might have to go to Intel or AMD and ask for something custom, and that would require a big order. (This is also what happened with the Steam Deck. Valve got a custom chip.)</p><p>Otherwise, we're hoping Intel or AMD come around and make an SoC with a "good enough" CPU but the best GPU cores on the market. That doesn't seem likely, especially with limited fab capacity and a demand for higher-margin parts.</p><p>In an ideal world, maybe one day, the companies that make these systems could bring costs back down. But realistically, I don't see that happening. That makes each gaming laptop below $1,000 a rare bird these days. And while an RTX xx50-class mobile GPU might not excite you, it's all some people can afford, especially as prices creep up.</p><p>Despite the hardships of doing it cheaply, we are already seeing some steps down this road; they're just not cheap. After all, we saw AMD's Radeon 8060S with the Ryzen AI Max+ 395 in the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/gaming-pcs/framework-desktop-review"><u>Framework Desktop</u></a>, as well as in the Asus ROG Flow Z13. The latter is technically a laptop (well, tablet) and is selling <a data-analytics-id="inline-link" href="https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-64gb-ram-1tb-ssd-off-black/JJGGLHG8X9"><u>for $2,400 on Best Buy</u></a>. Admittedly, it would be easy to drop the price by using something without Strix Halo and 64GB of RAM. But even then, it’s probably not getting close to the sub $1,000 mark, at least until it goes on clearance.</p><p>And companies that make gaming handhelds have been <em>raising</em> prices. If those devices sell, no company is going to have much incentive to put those in laptops at a lower price.</p><h2 id="almost-but-not-quite-there-2">Almost, but not quite, there</h2><p>I think we're a lot closer to the idea of laptop gaming on integrated graphics than we've ever been. (You can argue it's been happening for a long time! People who want to game will find the means to play on any system they have.)</p><p>On the other hand, people buy gaming laptops because they want to play games – usually modern AAA titles. Some of the initial handheld chips, like the Steam Deck's Aerith, are showing their age. This happens to all systems, eventually.</p><p>That might just be the way games are now, with less optimization and more graphical capabilities (Nvidia and AMD are trying to sell their high-end GPUs, after all). But to call a laptop a gaming laptop, it really needs to play all of the games, at least for a while after launch. So maybe a few more iterations will be required before this idea is ready for prime time.</p><p>But it also means we need to wait for great integrated graphics to get even cheaper for market forces to get in line. I think it would be great to see a true "gaming lifestyle notebook" that’s slim, powerful enough for most games, and ready with plenty of gaming features at the OS level. But with all of the factors making it tough on the low end, we'll have to settle for pricier Strix Halo experiments, at least for now.</p>
https://www.tomshardware.com/laptops/gaming-laptops/i-got-excited-for-the-idea-of-sub-usd1-000-gaming-laptops-with-integrated-graphics-but-there-are-more-than-a-few-reasons-why-thats-probably-not-happening
The prices of gaming laptops have been going up. Is it possible for laptops with integrated graphics to bridge the gap? It sounds like a good idea until you look into market realities.
DCjNGnQ2Hpd2Wb4onrsCPC
Wed, 17 Sep 2025 16:38:13 +0000 Gaming Laptops
Laptops
Andrew E. Freedman
Tom&#039;s Hardware, AMD
Sub-$1,000 gaming laptops with integrated graphics
Sub-$1,000 gaming laptops with integrated graphics
<![CDATA[ DOOM left running on ASUS MyPal PDA for 2.5 years finally crashes — bug that crashes the game when gametic value hits 2,147,483,647 ticks likely to blame ]]>
<p>Released in December of 1993 after an unbelievable amount of hype among PC gamers hooked up to the then-novel Internet, <em>DOOM</em> codified the standards of the nascent first-person shooter genre and was so popular that "<em>Doom</em> clone" was the way we described first-person shooters for years after its release. Gamers have put millions, if not billions of hours, into the title in the nearly 32 years since its launch, thanks to a virtual cornucopia of mods and user levels, but it's pretty unlikely that many of them have left the game running for upwards of two years straight. <a data-analytics-id="inline-link" href="https://lenowo.org/viewtopic.php?t=31" target="_blank">At least one person did</a>, though, and the result is... that it crashed.</p><p>Posting at LenOwO, site admin Minki remarks that they have reproduced the expected crash by loading<em> WinDOOM</em> on what appears to be an ASUS MyPal A620 pocket PC from 2003 running the then-novel Windows Mobile on an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-builds-10nm-arm-chips,32501.html" target="_blank">Intel XScale ARMv5 SoC</a>. Minki says that the device was modified to use a "DIY 18650 [lithium cell] based UPS which was itself connected to the USB port of my router for a constant 5V supply." They left the system running and mostly forgot about it until yesterday, when they noticed a pop-up appearing on the device and complaining of an application crash:</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="sKBtPo78Th4NWPqyebDtuA" name="windoom-fatal-error-fhd-crop" alt="A cropped photo showing the WinDOOM crashing after over two years on Windows Mobile 2003." src="https://cdn.mos.cms.futurecdn.net/sKBtPo78Th4NWPqyebDtuA.png" mos="" align="middle" fullscreen="1" width="1920" height="1080" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">The crash, in the ASUS MyPal's Windows Mobile 2003 operating system. </span><span class="credit" itemprop="copyrightHolder">(Image credit: minki/LenOwO)</span></figcaption></figure><p>Like most source ports of the era, WinDOOM is based on the original source code release from 1997, and so it reproduces most features — and most bugs — of the original game. Like most large commercial software projects, <em>DOOM</em> has <a data-analytics-id="inline-link" href="https://doomwiki.org/wiki/Engine_bug" target="_blank">numerous known bugs</a> even in its final 1.9 release. Among them is a curious quirk where, when playing back "demo" files internally, usually for the game's "attract" loop, the "gametic" value does not reset upon starting a new demo playback. This value is used for tracking game timing for various purposes, and it increments at a rate of 35 Hz, or 35 times per second, independent of the game's render loop.</p><p>It doesn't take even high school-level math to figure out that the gametic value never resetting will eventually result in an enormous value over time. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/in-1991-after-a-28-hour-coding-spree-the-efforts-of-john-carmack-doomed-us-all" target="_blank">Principal <em>DOOM</em> engine coder John Carmack</a> was surely aware of this when he programmed it, but he likely reasoned that it simply didn't matter because the value is stored as a signed 32-bit integer. That means that it can reach a maximum value of 2,147,483,647 ticks before rolling over. Integer overflow behavior is undefined in C, but on x86 PCs it always results in a roll-over to the maximum negative value of -2,147,483,647. Unsurprisingly, the game doesn't handle this very gracefully, which is to say it crashes, at least on Windows Mobile 2003.</p><a href="https://github.com/chocolate-doom/chocolate-doom/issues/1287#issuecomment-636414423"><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:648px;"><p class="vanilla-image-block" style="padding-top:54.17%;"><img id="qCUbVy2Ac6u3H9fXxEdFCk" name="doom-dosbox-crash" alt="A screenshot of a more traditional DOOM crash in DOSBox." src="https://cdn.mos.cms.futurecdn.net/qCUbVy2Ac6u3H9fXxEdFCk.png" mos="" align="middle" fullscreen="1" width="648" height="351" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Of course, there are many other ways to crash DOOM, such as loading an invalid level. </span><span class="credit" itemprop="copyrightHolder">(Image credit: GitHub/AXDOOMER)</span></figcaption></figure></a><p>At 35 ticks per second, it takes about 1.95 years to overflow the gametic value. That's a bit less than Minki's estimate, but who knows how long the ASUS PDA sat before they noticed the error message on screen; from the photo, it doesn't look like the 22-year-old pocket computer gets a lot of attention. It's also possible that Doom4CE, the Windows CE port of WinDoom that Minki was likely using, reduces the game tick rate to 30 Hz for better frame pacing and reduced hardware demands; this was common in the console ports of DOOM, such as the Jaguar and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/hack-snes-classic-add-games" target="_blank">Super NES versions</a>. If that's the case, it would take around 2.26 years to overflow the gametic value, closer to the stated 2.5.</p><p>Whatever the case, the takeaway is this: don't leave <em>DOOM</em> running for two years — or any game, probably, at least if it's a game client and not a dedicated server. Other thoughts provoked by Minki's experiment include both an appreciation for scientific rigor (experimental testing of even irrelevant conclusions) and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/3d-printing/this-3d-printer-was-repurposed-as-a-robotic-camera-and-it-doubles-as-a-photogrammetry-rig-for-3d-scanning" target="_blank">clever re-use of "junk" hardware</a>. That appears to be a theme of the Len0w0 boards, so kudos to that gang for doing what nobody else bothered to do.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/video-games/pc-gaming/doom-left-running-on-asus-mypal-pda-for-2-5-years-finally-crashes-bug-that-crashes-the-game-when-gametic-value-hits-2-147-483-647-ticks-likely-to-blame
Gamers have put millions of hours into DOOM in the 32 years since its launch, but it's unlikely that many of them have left the game running for upwards of two years straight. At least one person did, though, and the result is... that it crashed.
x6y4rhXBfFAtmock42JjkE
Wed, 17 Sep 2025 16:23:25 +0000 PC Gaming
Video Games
Zak Killian
Zenimax/Id Software
The title screen of the classic video game DOOM.
The title screen of the classic video game DOOM.
<![CDATA[ China's largest chipmaker testing first homegrown immersion DUV litho tool — SMIC takes significant step on road to wafer fab equipment self-sufficiency ]]>
<p>SMIC, the largest foundry in China, is test-driving one of China's first domestic immersion DUV lithography tools, reports <a data-analytics-id="inline-link" href="https://www.ft.com/content/8fd79522-e34f-4633-bc87-ef0aae2d9159">Financial Times</a>. The system was developed by Shanghai Yuliangsheng Technology Co., which is linked to Huawei's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/chinas-sicarrier-challenges-u-s-and-eu-with-full-spectrum-of-chipmaking-equipment-huawei-linked-firm-makes-an-impressive-debut">SiCarrier</a>, and is believed to be a significant part of China's effort to become self-sufficient in wafer fab equipment.</p><p>SMIC's test platform from Yuliangsheng involves a DUV machine that uses immersion lithography and is reportedly designed for 28nm-class fabrication technologies, though it could be used for 7nm or even 5nm production nodes by applying multipatterning. The Yuliangsheng tool is mostly made from components sourced within China, although some parts are still imported. The company is actively working to localize the entire supply chain. Once that is achieved (though it is unclear when), it would allow China to operate outside the influence of the U.S. or European export policies in this segment of chip production.</p><p>If the description of the tool by <em>Financial Times</em> is accurate, then the Yuliangsheng immersion DUV system currently being tested by SMIC resembles ASML's <a data-analytics-id="inline-link" href="https://www.asml.com/en/news/press-releases/2008/asml-launches-the-twinscan-nxt1950i-immersion-lithography-system">Twinscan NXT:1950i</a> from 2008, which was designed for 32nm-class process technology in one exposure. The unit featured optics with 1.35 numerical aperture, a 2.5nm overlay, a 38nm resolution, and could be used for making chips on a 22nm-class fabrication node. While theoretically the NXT:1950i could be used to make chips on 7nm and 5nm-class nodes, ASML has developed NXT:2000i for such fabrication technologies, which is generations ahead of the NXT:1950i.</p><p>It is unclear whether the Yuliangsheng tool is being tested within SMIC's production flow (i.e., they are producing actual chips or patterns) or if the company is just beginning to test the scanner and has merely reached first light on the wafer or first patterning milestones (a more likely scenario). If it is the latter, then the scanner is years away from mass-producing actual chips. Indeed, the goal is reportedly to integrate domestic immersion DUV lithography machines into production lines starting in 2027, after their qualification. Before that, SMIC will continue to rely on tools from ASML.</p><p>It should be noted that while SMIC (and probably Yuliangsheng) believes that it is possible to build chips on 7nm and 5nm-class process technologies on the same tools that are used for 28nm-class production nodes, it remains to be seen whether this is possible without a dramatic improvement of a 28nm-class tool when it comes to overlay performance, precision, control, and complexity. Essentially, after the existing tool matures and gets inserted into SMIC's 28nm flow in 2027, it <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-injects-tens-of-billions-of-dollars-in-chipmaking-tools-but-its-easily-more-than-a-decade-behind-the-market-leaders-heres-why">will take Yuliangsheng years to jump to 16nm and then to 7nm fabrication nodes</a> with a significantly redesigned scanner, so do not expect SMIC's sub-10nm fabrication processes on domestic lithography systems earlier than in 2030.</p><p>The tool is codenamed 'Mount Everest,' after the world's tallest mountain, perhaps highlighting the importance of the project. Interestingly, but SiCarrier also tends to call its WFE projects after mountains, which perhaps proves that SiCarrier and Shanghai Yuliangsheng Technology Co. are not only affiliated (SiCarrier is reportedly an investor of Yuliangsheng), but likely belong to the same group working on the same goal. It is noteworthy that Shanghai Yuliangsheng Technology Co. is already known to the U.S. Department of Commerce, which put it into its <a data-analytics-id="inline-link" href="https://www.federalregister.gov/documents/2024/12/05/2024-28267/additions-and-modifications-to-the-entity-list-removals-from-the-validated-end-user-veu-program">Entity List in late 2024</a>.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/chinas-largest-foundry-testing-first-domestic-immersion-duv-lithography-tool-smic-takes-significant-step-on-road-to-wafer-fab-equipment-self-sufficiency
SMIC is testing a domestically built immersion DUV lithography system developed by Yuliangsheng and capable of 28nm-class process technology. But while it is said that the tool could be used to make 7nm or even 5nm-class chips with multipatterning, it remains to be seen whether this is going to happen any time soon.
3HazxWKQJrJJADHwtkLJG
Wed, 17 Sep 2025 14:11:15 +0000 Semiconductors
Tech Industry
Manufacturing
ashilov@gmail.com (Anton Shilov)
Anton Shilov
SMIC
SMIC
SMIC
<![CDATA[ Cooler Master debuts new 3D Heatpipe tech in new coolers — Hyper 212 3DHP promises reduced thermals and improved efficiency
]]>
<p>Cooler Master showed off its 3D Heatpipe technology back at Computex 2025, and now it's finally coming to the public just a few months later. This quick turnaround time can be attributed to perhaps the ingenious simplicity of this solution. Instead of having the heatpipes only go up against the edges of the heatsink fin stack, another one cuts through the center, allowing for more even heat dissipation — and now Cooler Master is bringing it to its legendary Hyper 212 lineup, as reported by <a data-analytics-id="inline-link" href="https://www.techpowerup.com/340696/cooler-master-intros-hyper-212-3dhp-cpu-cooler-with-3d-heat-pipe-technology" target="_blank">TechPowerUp</a>.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:800px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="dXUGmaPtsgULVTUFF8bpQE" name="inside-3dhp-p2-01-ezgif.com-video-to-gif-converter" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/dXUGmaPtsgULVTUFF8bpQE.gif" mos="" align="middle" fullscreen="" width="800" height="450" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure><p>If you've been in the PC community for even a smidge, the name Hyper 212 must ring a bell. Originally launched almost two decades ago, the Hyper 212 has gone through countless revisions, modernizing it for every generation of PC gaming. The latest in this line of iterations is the aforementioned 3D Heatpipe technology, dubbing the new cooler "Hyper 212 3DHP." To understand why this is special, we should first look at how (most) standard air coolers work. Generally, these tower coolers feature a dense heatsink with multiple fins stacked atop each other, through which a U-shaped heatpipe runs.</p><p>This pipe takes heat from the CPU's IHS and carries it across the finstack, where the mounted fans blow fresh air onto it to cool it down. This is a pretty decent thermal exchange, but it can be made better. Instead of just two heatpipes at the periphery of the fin stack, Cooler Master introduced a third one running through the middle, essentially forming a trident-like shape. This results in much more efficient distribution and dissipation of heat, since each pipe will not only be responsible for less heat now, but they will cover a larger area on the fin stack.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:741px;"><p class="vanilla-image-block" style="padding-top:74.09%;"><img id="TyDeStcnYhq7nX7HNbAdCU" name="RW6YnbvAXNkk1mHI" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/TyDeStcnYhq7nX7HNbAdCU.jpg" mos="" link="" align="" fullscreen="" width="741" height="549" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:871px;"><p class="vanilla-image-block" style="padding-top:62.23%;"><img id="Lkjor7X9aGycVsFeZHuwCU" name="GME5C0gyanRqHrFe" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/Lkjor7X9aGycVsFeZHuwCU.jpg" mos="" link="" align="" fullscreen="" width="871" height="542" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:756px;"><p class="vanilla-image-block" style="padding-top:55.56%;"><img id="uCTTzBVpRYmBLTAf8HXUv7" name="hp-3dhp-side-by-side" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/uCTTzBVpRYmBLTAf8HXUv7.png" mos="" link="" align="" fullscreen="" width="756" height="420" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div></div></div><p>It sounds simple (and it is), but it works, and that's why it's first appearing in the Hyper 212 series. Cooler Master is prepping two variants of the Hyper 212 3DHP: the standard one featuring ARGB fan(s) and the 3DHP Black, which, shocker, comes in black and with a non-LED-lit fan. Owing to the lineup's affordable nature, the Hyper 212 3DHP Black will cost just $29.99 — same as the standard non-3DHP model — and include a generous 5-year warranty, where Hyper 212s usually only get two.</p><p>The specs are otherwise identical between the ARGB and Black models; both come with two 3D Heatpipes, one on either side so totaling six ends at the top. While the company did not disclose weight, the dimensions are 133mm x 86mm x 158mm. The supplied fans will spin up to 2,050 RPM, moving 63.1 CFM of airflow with a static pressure of 2.69 mm. Noise is also kept under control with a max claimed output of just 27 dBA. The Hyper 212 3DHP is not available right away on Cooler Master's website, but it should start to show up soon at retailers.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em> Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em> add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/air-cooling/cooler-master-debuts-new-3d-heatpipe-tech-in-new-coolers-hyper-212-3dhp-promises-reduced-thermals-and-improved-efficiency
Cooler Master is updating its iconic lineup of Hyper 212 coolers with its proprietary 3D Heatpipe tech. Hyper 212 3DHP, therefore, has two trident-like heatpipes running through its finstack that dissipate heat more evenly while costing the same. They will come in two flavors: Black and ARGB, and will be backed with 5-year warranties.
KLbDDtbtNwcFAd2odhxixS
Wed, 17 Sep 2025 13:57:08 +0000 Air Cooling
PC Components
Cooling
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Cooler Master / Future
Cooler Master Hyper 212 3DHP
Cooler Master Hyper 212 3DHP