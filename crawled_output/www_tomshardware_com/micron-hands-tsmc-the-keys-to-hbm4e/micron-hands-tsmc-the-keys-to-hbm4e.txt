Micron teams up with TSMC to deliver HBM4E, targeted for 2027 — collaboration could enable further customization | Tom's Hardware
Skip to main content
Open menu
Close main menu
Tom's Hardware
US Edition
UK
US
Australia
Canada
RSS
Sign in
View Profile
Sign out
Search
Search Tom's Hardware
Best Picks
CPUs
GPUs
SSDs
News
Laptops
Premium
Coupons
More
Newsletter
Reviews
PC Components
PC Building
Motherboards
Cases
Cooling
Power Supplies
RAM
Desktops
3D Printers
Peripherals
Monitors
Windows 11
Gaming
Overclocking
About Us
Forums
TrendingBench Exclusive RoadmapsInside Intel's StrugglesThe tale of Nvidia's HGX H20
Don't miss these
RAM
YMTC and CXMT team up to accelerate Chinese domestic HBM production
Tech Industry
Sandisk and SK hynix join forces to standardize High Bandwidth Flash memory
Tech Industry
SK Hynix projects HBM market to expand 30% annually through 2030
Tech Industry
Samsung earns Nvidia's certification for its HBM3 memory
GPUs
Samsung reportedly slashes HBM3 prices to woo Nvidia
Artificial Intelligence
Intel jumps to HBM4 with Jaguar Shores, 2nd Gen MRDIMMs with Diamond Rapids
Semiconductors
Nvidia tipped to be TSMC's first A16 customer, ahead of Apple — Feynman GPUs could make full use of GAA transistors and backside power
DRAM
Micron details new U.S. fab projects — HBM production coming to America
Semiconductors
Nvidia rumored to ditch its first-gen custom memory form factor for newer version
Tech Industry
SK hynix dethrones Samsung to become world's top-selling memory maker for the first time
RAM
New 3D-stacked memory tech seeks to dethrone HBM in AI inference
Semiconductors
Huawei reveals long-range Ascend chip roadmap
Semiconductors
ASML and SK hynix assemble industry-first 'commercial' High-NA EUV system at fab in South Korea
GPUs
Kioxia’s new 5TB, 64 GB/s flash module puts NAND toward the memory bus for AI GPUs
Semiconductors
Why packaging is a huge part of Nvidia's $5B Intel deal — Foveros could speed up market delivery
Tech Industry
Manufacturing
Semiconductors
MEMBER EXCLUSIVE
Micron teams up with TSMC to deliver HBM4E, targeted for 2027 — collaboration could enable further customization
News
By
Luke James
published
25 September 2025
High bandwidth, high configurability
When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.
(Image credit: Getty / SOPA Images)
Micron has confirmed it will partner with TSMC to manufacture the base logic die for its next-generation HBM4E memory, with production targeted for 2027. The announcement, made during the company’s fiscal Q4 earnings call on September 23, adds yet more detail to an already busy roadmap.Micron is shipping early HBM4 samples at speeds above 11 Gbps per pin, providing up to 2.8TB/s of bandwidth, and it has already locked down most of its 2026 HBM3E supply agreements. But the big takeaway is that Micron will hand TSMC the task of fabricating both standard and custom HBM4E logic dies, opening the door to tailored memory solutions for AI workloads.The decision also places Micron squarely in the middle of the next wave of AI system design, aligning with previous reporting on HBM roadmaps across Micron, SK hynix, and Samsung, and with earlier analysis of how Micron views HBM4E as a platform for customization.
You may like
YMTC and CXMT team up to accelerate Chinese domestic HBM production
Sandisk and SK hynix join forces to standardize High Bandwidth Flash memory
SK Hynix projects HBM market to expand 30% annually through 2030
A semi-configurable subsystem
(Image credit: SK hynix)The industry is already familiar with the HBM cadence: HBM3E today, HBM4 in 2025–2026, and HBM4E around 2027, and each new generation brings higher per-pin data rates and taller stacks. SK hynix has already confirmed 12-Hi HBM4 with a full 2048-bit interface running at 10 GT/s, while Samsung is plotting similar capacities with its own logic processes. Micron is shipping its own HBM4 stacks and claims more than 20% better efficiency than HBM3E.HBM4E is the extension of that roadmap, but Micron is treating it as something more. The company highlighted that the base die will be fabricated at TSMC, not in-house, and that custom logic-die designs will be offered to customers willing to pay a premium. By opening the base die to customization, Micron is effectively turning HBM into a semi-configurable subsystem. Instead of a one-size-fits-all interface layer, GPU vendors could request additional SRAM, dedicated compression engines, or tuned signal paths.That approach mirrors what we have seen from SK hynix, which has already described customizable base dies as part of its HBM4 strategy. Given that customized memory is stickier, more profitable, and more important for customers trying to squeeze every watt and every cycle out of an AI accelerator, this is likely to become a lucrative segment of the market.The importance of AIThe timing of Micron’s plans for HBM4E looks to be no accident. Nvidia and AMD both have next-generation data center GPUs slated for 2026 that will introduce HBM4, and HBM4E looks perfectly aligned to their successors.Nvidia’s Rubin architecture, expected to follow Blackwell in 2026, is built around HBM4. Rubin-class GPUs are projected to deliver around 13 TB/s of memory bandwidth and up to 288GB of capacity, a jump from the 8 TB/s ceiling on Hopper with HBM3E. A follow-on platform, Rubin Ultra, is already on Nvidia’s roadmap for 2027. That platform specifically calls for HBM4E, with each GPU supporting up to a terabyte of memory and aggregate rack-level bandwidth measured in petabytes per second.AMD’s trajectory is just as aggressive. Its Instinct MI400 family, expected around the same time as Rubin, is also moving to HBM4. Leaks suggest as much as 432 GB of HBM4 and 19.6 TB/s of bandwidth, more than double what AMD's MI350 delivers today. Like Rubin, MI400 uses a chiplet design, bound by ultra-wide memory buses, making HBM4 a necessity. After that is HBM4E, which is set for 2027 or 2028, depending on yields and ecosystem readiness.This cadence makes Micron’s partnership with TSMC particularly important. By shifting the base die to a leading-edge logic process and offering customization, Micron can synchronize its roadmap with the needs of Rubin Ultra, MI400 successors, and whatever comes next in the accelerator space.Thinking of the bigger picture, Micron’s partnership with TSMC raises questions around how HBM4E might proliferate widely into AI data centers. Right now, only the highest-end GPUs and TPUs use HBM, with the majority of servers still relying on DDR5 or LPDDR. That could change dramatically as workloads keep ballooning in size.Micron has already said that its HBM customer base has grown to six, with Nvidia among them. The company is also working with Nvidia on deploying LPDDR in servers. The partnership with TSMC suggests that Micron intends to make HBM4E a broadly adopted piece of AI infrastructure, potentially making HBM4E the standard tier of memory for AI nodes in the second half of the decade.Follow Tom's Hardware on Google News to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.
TOPICS
Micron
TSMC
HBM4
Luke JamesSocial Links NavigationContributorLuke James is a freelance writer and journalist.  Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.
Read more
YMTC and CXMT team up to accelerate Chinese domestic HBM production
Sandisk and SK hynix join forces to standardize High Bandwidth Flash memory
SK Hynix projects HBM market to expand 30% annually through 2030
Samsung earns Nvidia's certification for its HBM3 memory
Samsung reportedly slashes HBM3 prices to woo Nvidia
Intel jumps to HBM4 with Jaguar Shores, 2nd Gen MRDIMMs with Diamond Rapids
Latest in Semiconductors
MediaTek reportedly mulling US chip production
Nvidia and Intel’s RTX SoCs could pose an existential threat to AMD’s APUs — if two companies can actually pull it off
China bets on DUV as EUV blockade reshapes chipmaking — but it won't dethrone ASML's advanced lithography, for now
Why packaging is a huge part of Nvidia's $5B Intel deal — Foveros could speed up market delivery
Huawei reveals long-range Ascend chip roadmap
China's largest chipmaker testing first homegrown immersion DUV litho tool
Latest in News
CoreWeave deal with OpenAI now worth $22.4 billion
Qualcomm shows off reference mini PCs almost as thin as a USB port, powered by Snapdragon X2 Elite Extreme
Micron teams up with TSMC to deliver HBM4E, targeted for 2027
SlimeMoldCrypt relies on gloopy living organism’s ever-changing network of tendrils for its dynamic, biological, encryption engine
Sony unveils first-ever wireless desktop speakers for PC gamers with planar magnetic drivers
Intel reportedly raising prices on ever-popular Raptor Lake chips
Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.
Terms and conditions
Contact Future's experts
Privacy policy
Cookies policy
Accessibility Statement
Advertise with us
About us
Coupons
Careers
©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036.