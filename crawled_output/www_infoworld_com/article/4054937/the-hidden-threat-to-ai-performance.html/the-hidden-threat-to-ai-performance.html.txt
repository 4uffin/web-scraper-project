The hidden threat to AI performance | InfoWorld
Topics
Spotlight: IT CareersVideosNewslettersResources
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersNewslettersContribute to InfoWorldReprintsPoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy RightsOur NetworkCIOComputerworldCSONetwork WorldMoreNewsFeaturesBlogsBrandPostsEventsVideosEnterprise Buyer’s Guides
Close
AnalyticsArtificial IntelligenceGenerative AICareersCloud ComputingData ManagementDatabasesEmerging TechnologyTechnology IndustrySecuritySoftware Development Microsoft .NETDevelopment ToolsDevopsOpen SourceProgramming LanguagesJavaJavaScriptPythonIT LeadershipEnterprise Buyer’s Guides
Back
Close
Back
Close
Popular Topics
Artificial IntelligenceCloud ComputingData ManagementSoftware Development
Search
Topics
Spotlight: IT CareersVideosNewslettersResourcesAboutPoliciesOur NetworkMore
Back
Topics
AnalyticsArtificial IntelligenceGenerative AICareersCloud ComputingData ManagementDatabasesEmerging TechnologyTechnology IndustrySecuritySoftware DevelopmentMicrosoft .NETDevelopment ToolsDevopsOpen SourceProgramming LanguagesJavaJavaScriptPythonIT LeadershipEnterprise Buyer’s Guides
Back
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersNewslettersContribute to InfoWorldReprints
Back
PoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy Rights
Back
Our NetworkCIOComputerworldCSONetwork World
Back
MoreNewsFeaturesBlogsBrandPostsEventsVideosEnterprise Buyer’s Guides
Home
Blogs
Cloud Insider
The hidden threat to AI performance
by									David Linthicum
The hidden threat to AI performance
analysis
Sep 12, 20256 minsArtificial IntelligenceCloud ComputingGPUs
Memory limitations have blindsided many cloud users. It’s crucial for enterprises to expand their focus beyond GPUs and for providers to fix memory problems to keep AI performance on track.
Credit: 															ML Robinson / Shutterstock
Most of us involved with AI are aware (or are quickly becoming aware) that memory bandwidth isn’t keeping pace with advancements in processing power. This imbalance creates a frustrating situation where GPUs are often underutilized, wasting compute power just as AI adoption is skyrocketing. For cloud users, this not only results in decreased performance but also higher bills as they process workloads less efficiently. The question is, will cloud providers step up to address this problem, or will they continue to focus solely on GPUs while ignoring other critical infrastructure issues?
Every time we discuss boosting AI capacity or performance, GPUs always take the spotlight. This emphasis has led to a surge in orders for AI chips, helping companies like Nvidia, AMD, Broadcom, and others. Public cloud providers have responded by expanding their infrastructure to include large GPU clusters, proudly showcasing their ability to run AI models at scale. Many businesses turned to these cloud providers to take advantage of AI opportunities without realizing that memory bandwidth would become the key bottleneck preventing these performance gains from being fully realized.
Simply put, memory bandwidth determines how quickly data can move between processors and external memory. GPUs continue to grow faster, but their ability to access the large amounts of data needed for AI workloads has not improved at the same pace. As a result, memory bandwidth has become a hidden cost that affects both performance and efficiency.
Imagine having a factory full of powerful machinery waiting to build products but only a small, rickety conveyor belt to deliver the raw materials to that machinery. That’s essentially what memory limitations do to AI performance. The processors (machinery) are more powerful than ever, and the workloads (raw materials) are growing exponentially. However, the conveyor belt (memory bandwidth) cannot keep up, leaving powerful GPU instances idle or underutilized.
The implications are shocking. Enterprises that leverage public clouds to scale AI workloads are now forced to spend more while getting less. Worse yet, most of these businesses—especially those caught in the GPU hype—have no idea that memory is the culprit.
Cloud-based AI is expensive
Executives love the promise of public clouds for AI: unlimited resources, enormous scalability, and access to cutting-edge technology without heavy upfront capital expenses. However, here’s the hard truth: the public cloud is not always the most cost-effective option for AI workloads. Cloud providers indeed offer physical infrastructure at scale, but it comes at a premium. And now, with memory bandwidth issues slowing down performance, that premium is even harder to justify.
AI workloads are already expensive due to the high cost of renting GPUs and the associated energy consumption. Memory bandwidth issues make things worse. When memory lags, workloads take longer to process. Longer runtimes result in higher costs, as cloud services charge based on hourly usage. Essentially, memory inefficiencies increase the time to compute, turning what should be cutting-edge performance into a financial headache.
Remember that the performance of an AI system is no better than its weakest link. No matter how advanced the processor is, limited memory bandwidth or storage access can restrict overall performance. Even worse, if cloud providers fail to clearly communicate the problem, customers might not realize that a memory bottleneck is reducing their ROI.
Will public clouds fix the problem?
Cloud providers are now at a critical juncture. If they want to remain the go-to platform for AI workloads, they’ll need to address memory bandwidth head-on—and quickly. Right now, all major players, from AWS to Google Cloud and Microsoft Azure, are heavily marketing the latest and greatest GPUs. But GPUs alone won’t cure the problem unless paired with advancements in memory performance, storage, and networking to ensure a seamless data pipeline for AI workloads.
We’re seeing some steps in the right direction. Nvidia has developed NVLink and Storage Next to optimize how GPUs interact with memory, while new technologies such as Compute Express Link (CXL) aim to improve memory bandwidth and reduce latency. Such solutions could help cloud providers adopt more balanced architectures in the future.
For enterprise customers, the question remains whether these improvements will trickle down fast enough to offset current inefficiencies. Will public cloud providers rebalance their infrastructure investments to focus on fixing the memory bottleneck? Or will they simply double down on marketing GPUs, leaving customers to deal with the messy and expensive reality of underperformance?
One thing is certain: Businesses must start asking their cloud providers the tough questions. How are they addressing memory bandwidth issues? What concrete steps are being taken to improve storage and network capacity? Are there more economical workloads that balance processor utilization with memory efficiency? Cloud users no longer have the luxury of passively trusting their providers to sort these issues out for them. In competitive markets where AI holds the potential to unlock true business value, even small inefficiencies in infrastructure can spiral into significant disadvantages.
Memory performance: A wake-up call
Public cloud providers blew the doors off with GPUs, creating infrastructure capable of supporting complex AI training and inference models that were unimaginable a few years ago. But with memory limitations now slowing down AI workloads, it’s clear that clouds are no longer a silver bullet for organizations looking to scale their AI ambitions. As we move forward, AI leaders must adopt a more pragmatic view of their infrastructure. Cost and performance are determined as much by compute power as by the intricate interplay of memory, storage, and networking.
Public cloud providers will remain key players in AI. However, without major investments to improve memory performance and bandwidth, organizations may need to rethink their reliance on cloud providers. It’s no longer just about keeping up with GPU trends; it’s about questioning whether your cloud provider can remove bottlenecks that slow down your workloads and drive up your costs.
As the race to scale AI accelerates, the ultimate message is clear: Your system is only as fast as its slowest component. Don’t let memory be the bottleneck.
Related content
analysis
Cloud computing has an ROI problem By David Linthicum
Sep 23, 2025 6 mins
Hybrid Cloud
Multicloud
ROI and Metrics
feature
Do vector-native databases beat add-ons for AI applications? By Bill Doerrfeld
Sep 22, 2025 10 mins
Analytics
Databases
Generative AI
analysis
How Oracle became a cloud player By David Linthicum
Sep 19, 2025 7 mins
Artificial Intelligence
Cloud Architecture
Cloud Computing
news
AI Alliance forges agent-native language, knowledge base By Paul Krill
Sep 18, 2025 2 mins
Artificial Intelligence
Generative AI
Programming Languages
Other Sections
Resources
Videos
Spotlight: IT Careers
by
David Linthicum
Follow David Linthicum on X
David S. Linthicum is an internationally recognized industry expert and thought leader. Dave has authored 13 books on computing, the latest of which is An Insider’s Guide to Cloud Computing. Dave’s industry experience includes tenures as CTO and CEO of several successful software companies, and upper-level management positions in Fortune 100 companies. He keynotes leading technology conferences on cloud computing, SOA, enterprise application integration, and enterprise architecture. Dave writes the Cloud Insider blog for InfoWorld. His views are his own.
More from this author
analysisAre cloud providers neglecting security to chase AI? Sep 16, 2025 6 minsanalysisConflicting opinions on the ROI of AI Sep 9, 2025 6 minsanalysisIs Meta’s $10 billion cloud deal a good idea for you? Sep 5, 2025 6 minsanalysisOverseas enterprises and US sovereign clouds Sep 2, 2025 6 minsanalysisThree tips for building agentic AI systems on cloud platforms Aug 29, 2025 5 minsanalysisHow does AI affect cloud attack vectors? Aug 26, 2025 6 minsanalysisFrom cloud migration to cloud optimization Aug 22, 2025 5 minsanalysisIBM can’t afford an unreliable cloud Aug 19, 2025 5 mins
Show me morePopularArticlesVideos
news
Microsoft Marketplace opens for AI apps, agents By Paul KrillSep 25, 20252 mins
Generative AIMicrosoft .NETMicrosoft Azure
news
GitHub Copilot-backed app modernization available for Java, .NET By Paul KrillSep 25, 20251 min
Generative AIGitHubJava
how-to
Introduction to Java records: Simplified data-centric programming in Java By Rafael del NeroSep 25, 202510 mins
JavaProgramming LanguagesSoftware Development
video
Python 3.14's live debugging interface Sep 23, 20254 mins
Python
video
Easier Chrome browser automation with PyDoll Sep 16, 20254 mins
Python
video
How to use Rust workspaces to speed up compile times Sep 9, 20253 mins
Python
Sponsored Links
Solve your most complex IT challenges with solutions that simplify your modernization journey.
Secure AI by Design: Unleash the power of AI and keep applications, usage and data secure.
Empower your cybersecurity team with expert insights from Palo Alto Networks.
About
About Us
Advertise
Contact Us
Editorial Ethics Policy
Foundry Careers
Reprints
Newsletters
BrandPosts
Policies
Terms of Service
Privacy Policy
Cookie Policy
Copyright Notice
Member Preferences
About AdChoices
Your California Privacy Rights
Privacy Settings
Our Network
CIO
Computerworld
CSO
Network World
FacebookXYouTubeGoogle NewsLinkedIn
© 2025
FoundryCo, Inc. All Rights Reserved.