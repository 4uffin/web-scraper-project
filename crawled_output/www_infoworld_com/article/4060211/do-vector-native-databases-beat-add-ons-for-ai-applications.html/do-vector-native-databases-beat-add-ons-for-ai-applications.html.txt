Do vector-native databases beat add-ons for AI applications? | InfoWorld
Topics
Spotlight: IT CareersVideosNewslettersResources
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersNewslettersContribute to InfoWorldReprintsPoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy RightsOur NetworkCIOComputerworldCSONetwork WorldMoreNewsFeaturesBlogsBrandPostsEventsVideosEnterprise Buyer’s Guides
Close
AnalyticsArtificial IntelligenceGenerative AICareersCloud ComputingData ManagementDatabasesEmerging TechnologyTechnology IndustrySecuritySoftware Development Microsoft .NETDevelopment ToolsDevopsOpen SourceProgramming LanguagesJavaJavaScriptPythonIT LeadershipEnterprise Buyer’s Guides
Back
Close
Back
Close
Popular Topics
Artificial IntelligenceCloud ComputingData ManagementSoftware Development
Search
Topics
Spotlight: IT CareersVideosNewslettersResourcesAboutPoliciesOur NetworkMore
Back
Topics
AnalyticsArtificial IntelligenceGenerative AICareersCloud ComputingData ManagementDatabasesEmerging TechnologyTechnology IndustrySecuritySoftware DevelopmentMicrosoft .NETDevelopment ToolsDevopsOpen SourceProgramming LanguagesJavaJavaScriptPythonIT LeadershipEnterprise Buyer’s Guides
Back
AboutAbout UsAdvertiseContact UsEditorial Ethics PolicyFoundry CareersNewslettersContribute to InfoWorldReprints
Back
PoliciesTerms of ServicePrivacy PolicyCookie PolicyCopyright NoticeMember PreferencesAbout AdChoicesYour California Privacy Rights
Back
Our NetworkCIOComputerworldCSONetwork World
Back
MoreNewsFeaturesBlogsBrandPostsEventsVideosEnterprise Buyer’s Guides
Home
Analytics
Do vector-native databases beat add-ons for AI applications?
by									Bill Doerrfeld
Contributing Writer
Do vector-native databases beat add-ons for AI applications?
feature
Sep 22, 202510 minsAnalyticsDatabasesGenerative AI
Even though traditional databases now support vector types, vector-native databases have the edge for AI development. Here’s how to choose.
Credit: 															Krot_Studio / Shutterstock
AI is turning the idea of a database on its head. Traditional databases, like relational and NoSQL databases, were designed for structured data and analytical queries—you have tidy tables, a schema, and clearly defined fields. Queries must match keywords and filters exactly, or else nothing is returned.
However, natural language processing applications powered by large language models (LLMs) are far looser. Instead of finding identical matches in structured records, these applications use techniques such as retrieval-augmented generation (RAG) to sift through massive amounts of unstructured data and find semantic similarities.
Enter the vector database. Vector databases are more dynamic than traditional databases, making them a good fit for AI use cases. New vector-native databases, like Qdrant, Pinecone, OpenSearch, Weaviate, and Chroma store and retrieve vector embeddings, enabling high-speed, context-aware, multi-modal data retrieval for AI agents, which is proving essential for RAG.
“Vector databases allow agentic AI systems to store and query massive amounts of unstructured embeddings—such as text or image features—with semantic similarity,” says Vagner Strapasson, tech lead AI engineer at Indicium, a data and AI consulting company.
As such, vector databases have become ubiquitous in the AI field. Nearly 70% of engineers are already using a vector database, according to an August 2025 survey conducted by HostingAdvice.com, which interviewed 300 US-based engineers holding roles in related data and AI and machine learning fields. Among those who aren’t using a vector database today, the majority (73%) are currently exploring one for future AI use cases.
But getting the most out of vectorization often requires more legwork than simply adding vectors as a new data type to pre-existing databases. Over three in four engineers say they are using a native vector database, as opposed to a traditional database with a vector add-on.
Below, we’ll explore how vector-native databases differ from traditional databases and weigh the benefits of going with a vector-native database versus using a relational or NoSQL database with vector storage support.
Interest in vector databases grows
The rise in interest around vector databases runs parallel to new requirements brought on by machine learning. As LLMs have made us all too aware, machine learning models can work with massive amounts of text or other unstructured data. However, machine learning models don’t work with this data directly, but with numerical representations of the data called vector embeddings.
These high-dimensional vector embeddings allow the semantic similarities between, say, words or paragraphs or other chunks of text to be measured in a vector space. The closer the vectors in this high-dimensional numerical space, the closer in meaning the corresponding chunks of text. Paired with approximate nearest neighbor (ANN) search, vector databases enable a quick, semantically-driven search and retrieval process, critical for working with generative AI and LLMs.
“In agentic AI applications, a vector database acts as external memory or a ‘knowledge index.’ It lets an AI agent recall relevant facts or past interactions,” says Indicium’s Strapasson. “Vector databases enable organizations to ingest, govern, and retrieve this coveted unstructured data—and then scale accurate, performant agentic AI systems,” adds Edward Calvesbert, vice president of IBM watsonx.
The nature of AI-based applications makes vector databases particularly useful for in-memory use cases, long-term storage for AI agents, and conveniently accessing lengthy queries. Embeddings, hybrid search, and other areas help vector databases go the extra mile.
“As AI applications shift from single-turn prompts to AI agents that reason, plan, and act over multiple steps, the challenge is no longer just model quality—it’s memory and retrieval,” says Andre Zayarni, co-founder and CEO of Qdrant, provider of an open-source vector database and vector search engine written in Rust.
Beyond the traditional DB
As of mid-2025, developer-favorite database options such as Postgres, MongoDB, and Elasticsearch have rolled in vector support. Microsoft’s SQL Server has added a native vector data type for storage, as has AWS with Amazon S3 Vectors. So, why use a specialized, vector-native database if these add-ons already exist?
Well, specialized vector databases provide better information retrieval mechanisms than typical databases, which enhance the speed and accuracy at which AI agents can reason over data. As IBM’s Calvesbert describes: “Fit-for-purpose vector databases provide greater flexibility combining multiple vector fields for dense, sparse, and multi-modal search—spanning text, images, and audio—to capture the full context and specific terms for the most comprehensive search results.”
Vector-native databases are also arguably a better fit in high-scale scenarios, requiring fewer adjustments. “Organizations handling billions of vectors, requiring sub-50ms latency, or needing specialized features like multi-modal search, benefit most from native vector databases,” says Janakiram MSV, principal analyst at Janakiram & Associates, an industry analyst and consulting firm. By contrast, traditional databases require extensive tuning and lack optimized performance for high-scale vector operations, he adds.
Vector database extensions, like Postgres’s pgvector or MongoDB’s Atlas Vector Search, could get the job done, says Indicium’s Strapasson. However, he adds, “For applications demanding high-speed, large-scale similarity searches with low latency and high throughput across millions or billions of vectors, standalone vector databases are optimal.”
About half of engineers (46%) working with vector databases are storing 10 to 100 million vectors, with most of the rest falling in the one million to 10 million range, found the HostingAdvice.com report.
In short, vector-native databases tend to outperform add-ons in large-scale, latency-sensitive, and multi-modal AI use cases. But for smaller workloads, built-in extensions could be a simpler, cost-effective choice.
Where native vector databases shine
Today’s new class of vector databases offers many features that differentiate them from classical databases. They’re explicitly designed as a back end for AI, purpose-built for AI use cases.
Some specific features include:
Advanced querying: Approximate nearest neighbor (ANN) or k-nearest neighbors (KNN) offer advanced querying processes to discover semantically similar entries.
Hybrid search: Options to combine semantic, metadata filtering, and keyword matching.
Multi-modal support: You can embed different file types, such as text, image, or audio, and the data can be unstructured.
Real-time indexing: Dynamic data updates or on-the-fly model inferencing capabilities.
Sparse vector and multi-vector support: Handle hybrid dense or sparse retrieval and multiple embeddings per object for finer-grained search.
Data management features unique to AI requirements: Specialized reranking algorithms, namespace partitioning, and semantic caching for repeated queries.
Quantization and memory optimization: Reduce vector size to save on storage and improve speed without major accuracy loss.
Integrations: RESTful APIs and native integrations with popular AI frameworks, such as LangChain, LlamaIndex, and Haystack.
Some vector databases take things a step further. “Advanced features include GPU acceleration, distributed scaling, sparse vector support, and integration with machine learning frameworks like PyTorch and TensorFlow,” adds Janakiram. Managed services may also include helpful capabilities, like automatic scaling, monitoring, built-in embedding models, or inferencing.
Choosing the right vector database
When choosing a vector database for your next project, the nuances of your AI workload, your performance and scale requirements, and integrations with your existing tech stack are going to be top priorities to evaluate.
According to Qdrant’s Zayarni, every AI project is unique, meaning adaptability is key: “The right vector database gives you the knobs to tune for your specific needs, not force a fixed architecture onto your application.” Certain features like quantization to reduce memory usage, advanced filtering, and configurable storage options help deliver that flexibility, he says.
Given the proliferation of hybrid, on-prem, and multi-cloud setups, interoperability and the degree of proprietary lock-in are important areas to assess. “Hybrid and open are two key traits to prioritize when choosing a vector database,” says IBM’s Calvesbert.
This makes open-source options a strong contender for hybrid use cases. “Weaviate is strong in hybrid and multi-modal use cases and on‑prem needs,” says Indicium’s Strapasson. Each vector database has different pricing implications, he adds, so it’s good to consider the trade-offs of managed services over open-source products, such as the cost-per-query and total storage and compute costs.
Other open-source native vector databases include Vespa, Vlad, and Milvus.
Janakiram notes that managed services, like Pinecone’s hosted database, will be easier to use, whereas open-source options, like Milvus or Weaviate, will offer more flexibility. There are also prototype-friendly lightweight options to consider that could run in-memory, like Chroma, he adds.
Different AI use cases will require different search styles and patterns to best determine relevance. As such, Janakiram recommends considering the data types and query patterns you’ll need: Do you require sparse or dense vectors, or real-time or batch options?
Janakiram stresses the need to view the total picture: “Assess the total cost of ownership, including operational overhead, scaling costs, and available developer resources, APIs, and SDKs for maintenance.”
One (of many) data sources for agents
As more and more companies ramp up AI development, building everything from RAG-based systems to innovative search engines to LLM-based virtual assistants to agentic AI applications, vector databases are fast becoming a standard part of the back-end infrastructure.
However, Tyler Jewel, CEO of Akka, providers of an agentic development platform, notes that vector databases are just one slice of the pie. AI agents must effectively connect with many sources: vector databases, conversation history, APIs, streaming data, and more.
Furthermore, vectorization comes at a cost. “Vector retrieval is computationally expensive and typically slower than traditional databases, which means agentic systems must use asynchronous, non-blocking I/O to avoid stalling on slow queries,” Jewel says. Essentially, AI-powered applications willlikely require an upstream engine to pull from diverse sources while maintaining reliability, he says.
Still, Jewel is confident in the role of vector databases in syncing AI agents with the right knowledge and data. “LLMs are only as good as the context you give them, and vector databases are essential for supplying that semantic context.”
As Qdrant’s Zayarni puts it, “Without vector databases, agents operate blindly. With them, they gain the context, continuity, and speed required to reason, adapt, and deliver on the promise of agentic AI.”
Related content
news
Teradata taps open source frameworks to offer agent-building capabilities By Anirban Ghoshal
Sep 23, 2025 4 mins
Generative AI
analysis
Vibe coding and the future of software development By Isaac Sacolick
Sep 23, 2025 11 mins
DevSecOps
Development Tools
Generative AI
feature
The productivity paradox of AI-assisted coding By Edgar Kussberg
Sep 23, 2025 8 mins
Artificial Intelligence
Development Tools
Generative AI
news
GitHub introduces registry for finding MCP servers By Paul Krill
Sep 22, 2025 2 mins
Generative AI
Software Development
Other Sections
Resources
Videos
Spotlight: IT Careers
by
Bill Doerrfeld
Contributing Writer
Follow Bill Doerrfeld on LinkedIn
Bill is a tech journalist specializing in state-of-the-art technologies in the enterprise cloud software space. He is also Editor in Chief for Nordic APIs, a knowledge center for API practitioners, and contributes to DevOps.com, Cloud Native Now (formerly Container Journal), and Acceleration Economy.
Bill is originally from Seattle, where he attended the University of Washington. He now lives and works in Portland, Maine.
More from this author
featureWhat alternative clouds are good for Aug 25, 2025 10 minsfeatureMulti-agent AI workflows: The next evolution of AI coding Aug 11, 2025 10 minsfeatureCan System Initiative fix devops? Jul 14, 2025 11 minsfeatureTop 6 multicloud management solutions Jun 16, 2025 11 minsfeature7 ways to improve your AI coding results Jun 2, 2025 9 minsfeatureKnowing when to use AI coding assistants May 5, 2025 8 minsfeature14 alternative managed Kubernetes platforms Mar 31, 2025 15 minsfeatureWhy AI-generated code isn’t good enough (and how it will get better) Mar 17, 2025 14 mins
Show me morePopularArticlesVideos
analysis
Cloud computing has an ROI problem By David LinthicumSep 23, 20256 mins
Hybrid CloudMulticloudROI and Metrics
news
Web Codegen Scorer evaluates AI-generated web code By Paul KrillSep 22, 20252 mins
AngularGenerative AIWeb Development
feature
Advanced debug logging techniques: A technical guide By Neel ShahSep 22, 20255 mins
Development ToolsJavaScriptPython
video
Easier Chrome browser automation with PyDoll Sep 16, 20254 mins
Python
video
How to use Rust workspaces to speed up compile times Sep 9, 20253 mins
Python
video
Getting encryption wrong (and getting it right, too) Sep 2, 20257 mins
Python
Sponsored Links
Solve your most complex IT challenges with solutions that simplify your modernization journey.
Empower your cybersecurity team with expert insights from Palo Alto Networks.
Secure AI by Design: Unleash the power of AI and keep applications, usage and data secure.
About
About Us
Advertise
Contact Us
Editorial Ethics Policy
Foundry Careers
Reprints
Newsletters
BrandPosts
Policies
Terms of Service
Privacy Policy
Cookie Policy
Copyright Notice
Member Preferences
About AdChoices
Your California Privacy Rights
Privacy Settings
Our Network
CIO
Computerworld
CSO
Network World
FacebookXYouTubeGoogle NewsLinkedIn
© 2025
FoundryCo, Inc. All Rights Reserved.