OpenAI’s research on AI models deliberately lying is wild  | TechCrunch
TechCrunch Desktop Logo
TechCrunch Mobile Logo
LatestStartupsVentureAppleSecurityAIAppsDisrupt 2025
EventsPodcastsNewsletters
SearchSubmit
Site Search Toggle
Mega Menu Toggle
Topics
Latest
AI
Amazon
Apps
Biotech & Health
Climate
Cloud Computing
Commerce
Crypto
Enterprise
EVs
Fintech
Fundraising
Gadgets
Gaming
Google
Government & Policy
Hardware
Instagram
Layoffs
Media & Entertainment
Meta
Microsoft
Privacy
Robotics
Security
Social
Space
Startups
TikTok
Transportation
Venture
More from TechCrunch
Staff
Events
Startup Battlefield
StrictlyVC
Newsletters
Podcasts
Videos
Partner Content
TechCrunch Brand Studio
Crunchboard
Contact Us
Image Credits:Getty Images
AI
OpenAI’s research on AI models deliberately lying is wild
Julie Bort
3:54 PM PDT · September 18, 2025
Every now and then, researchers at the biggest tech companies drop a bombshell. There was the time Google said its latest quantum chip indicated multiple universes exist. Or when Anthropic gave its AI agent Claudius a snack vending machine to run and it went amok, calling security on people and insisting it was human.
This week, it was OpenAI’s turn to raise our collective eyebrows.
OpenAI released on Monday some research that explained how it’s stopping AI models from “scheming.” It’s a
practice in which an “AI behaves one way on the surface while hiding its true goals,” OpenAI defined in its tweet about the research.
In the paper, conducted with Apollo Research, researchers went a bit further, likening AI scheming to a human stock broker breaking the law to make as much money as possible. The researchers, however, argued that most AI “scheming” wasn’t that harmful. “The most common failures involve simple forms of deception — for instance, pretending to have completed a task without actually doing so,” they wrote.
The paper was mostly published to show that “deliberative alignment⁠” — the anti-scheming technique they were testing — worked well.
But it also explained that AI developers haven’t figured out a way to train their models not to scheme. That’s because such training could actually teach the model how to scheme even better to avoid being detected.
“A major failure mode of attempting to ‘train out’ scheming is simply teaching the model to scheme more carefully and covertly,” the researchers wrote.
Techcrunch event
Join 10k+ tech and VC leaders for growth and connections at Disrupt 2025
Netflix, Box, a16z, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch, and a chance to learn from the top voices in tech. Grab your ticket before Sept 26 to save up to $668.
Join 10k+ tech and VC leaders for growth and connections at Disrupt 2025
Netflix, Box, a16z, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch, and a chance to learn from the top voices in tech. Grab your ticket before Sept 26 to save up to $668.
San Francisco
|
October 27-29, 2025
REGISTER NOW
Perhaps the most astonishing part is that, if a model understands that it’s being tested, it can pretend it’s not scheming just to pass the test, even if it is still scheming. “Models often become more aware that they are being evaluated. This situational awareness can itself reduce scheming, independent of genuine alignment,” the researchers wrote.
It’s not news that AI models will lie. By now most of us have experienced AI hallucinations, or the model confidently giving an answer to a prompt that simply isn’t true. But hallucinations are basically presenting guesswork with confidence, as OpenAI research released earlier this month documented.
Scheming is something else. It’s deliberate.
Even this revelation — that a model will deliberately mislead humans — isn’t new. Apollo Research first published a paper in December documenting how five models schemed when they were given instructions to achieve a goal “at all costs.”
The news here is actually good news: The researchers saw significant reductions in scheming by using “deliberative alignment⁠.” That technique involves teaching the model an “anti-scheming specification” and then making the model go review it before acting. It’s a bit like making little kids repeat the rules before allowing them to play.
OpenAI researchers insist that the lying they’ve caught with their own models, or even with ChatGPT, isn’t that serious. As OpenAI’s co-founder Wojciech Zaremba told TechCrunch’s Maxwell Zeff about this research: “This work has been done in the simulated environments, and we think it represents future use cases. However, today, we haven’t seen this kind of consequential scheming in our production traffic. Nonetheless, it is well known that there are forms of deception in ChatGPT. You might ask it to implement some website, and it might tell you, ‘Yes, I did a great job.’ And that’s just the lie. There are some petty forms of deception that we still need to address.”
The fact that AI models from multiple players intentionally deceive humans is, perhaps, understandable. They were built by humans, to mimic humans, and (synthetic data aside) for the most part trained on data produced by humans.
It’s also bonkers.
While we’ve all experienced the frustration of poorly performing technology (thinking of you, home printers of yesteryear), when was the last time your not-AI software deliberately lied to you? Has your inbox ever fabricated emails on its own? Has your CMS logged new prospects that didn’t exist to pad its numbers? Has your fintech app made up its own bank transactions?
It’s worth pondering this as the corporate world barrels toward an AI future where companies believe agents can be treated like independent employees. The researchers of this paper have the same warning.
“As AIs are assigned more complex tasks with real-world consequences and begin pursuing more ambiguous, long-term goals, we expect that the potential for harmful scheming will grow — so our safeguards and our ability to rigorously test must grow correspondingly,” they wrote.
Topics
AI, AI research, hallucinations, OpenAI
Julie Bort
Venture Editor
Julie Bort is the Startups/Venture Desk editor for TechCrunch.
You can contact or verify outreach from Julie by emailing julie.bort@techcrunch.com or via @Julie188 on X.
View Bio
October 27-29, 2025
San Francisco
Founders: land your investor and sharpen your pitch. Investors: discover your next breakout startup. Innovators: claim a front-row seat to the future. Join 10,000+ tech leaders at the epicenter of innovation. Register now and save up to $668.Regular Bird rates end September 26
Register Now
Most Popular
Google isn’t kidding around about cost cutting, even slashing its FT subscription
Connie Loizos
Meta CTO explains why the smart glasses demos failed at Meta Connect — and it wasn’t the Wi-Fi
Sarah Perez
OpenAI’s research on AI models deliberately lying is wild
Julie Bort
How AI startups are fueling Google’s booming cloud business
Maxwell Zeff
The 9 most sought-after startups from YC Demo Day
Marina Temkin
Apple’s iOS 26 with the new Liquid Glass design is now available to everyone
Ivan Mehta
Spotify will now let free users pick and play tracks
Sarah Perez
Loading the next article
Error loading the next article
X
LinkedIn
Facebook
Instagram
youTube
Mastodon
Threads
Bluesky
TechCrunchStaffContact UsAdvertiseCrunchboard JobsSite Map
Terms of ServicePrivacy PolicyRSS Terms of UseCode of Conduct
Meta ConnectGroqStubhubLiquid GlassVibe CodingTech LayoffsChatGPT
© 2025 TechCrunch Media LLC.