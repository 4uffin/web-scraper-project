How developers are using Apple's local AI models with iOS 26 | TechCrunch
TechCrunch Desktop Logo
TechCrunch Mobile Logo
LatestStartupsVentureAppleSecurityAIAppsDisrupt 2025
EventsPodcastsNewsletters
SearchSubmit
Site Search Toggle
Mega Menu Toggle
Topics
Latest
AI
Amazon
Apps
Biotech & Health
Climate
Cloud Computing
Commerce
Crypto
Enterprise
EVs
Fintech
Fundraising
Gadgets
Gaming
Google
Government & Policy
Hardware
Instagram
Layoffs
Media & Entertainment
Meta
Microsoft
Privacy
Robotics
Security
Social
Space
Startups
TikTok
Transportation
Venture
More from TechCrunch
Staff
Events
Startup Battlefield
StrictlyVC
Newsletters
Podcasts
Videos
Partner Content
TechCrunch Brand Studio
Crunchboard
Contact Us
Image Credits:Apple
Apps
How developers are using Appleâ€™s local AI models with iOS 26
Ivan Mehta
7:21 AM PDT Â· September 19, 2025
Earlier this year, Apple introduced its Foundation Models framework during WWDC 2025, which allows developers to use the companyâ€™s local AI models to power features in their applications.
The company touted that with this framework, developers gain access to AI models without worrying about any inference cost. Plus, these local models have capabilities such as guided generation and tool calling built in.
As iOS 26 is rolling out to all users, developers have been updating their apps to include features powered by Appleâ€™s local AI models. Appleâ€™s models are small compared with leading models from OpenAI, Anthropic, Google, or Meta. That is why local-only features largely improve quality of life with these apps rather than introducing major changes to the appâ€™s workflow.
Below are some of the first apps to tap into Appleâ€™s AI framework.
Lil Artist
The Lil Artist app offers various interactive experiences to help kids learn different skills like creativity, math, and music. Developer Arima Jain shipped an AI story creator with the iOS 26 update. This allows users to select a character and a theme, with the app generating a story using AI. The developer said that the text generation in the story is powered by the local model.
Image Credits:Lil Artist
Daylish
The developer of the Daylish app is working on a prototype for automatically suggesting emojis for timeline events based on the title for the daily planner app.
MoneyCoach
Finance tracking app MoneyCoach has two neat features powered by local models. First, the app shows insights about your spending, such as whether you spent more than average on groceries for that particular week. The other feature automatically suggests categories and subcategories for a spending item for quick entries.
Image Credits:MoneyCoach
LookUp
The word learning app LookUp has added two new modes using Appleâ€™s AI models. There is a new learning mode, which leverages a local model to create examples corresponding to a word. Plus, the example asks users to explain the usage of the word in a sentence.
Image Credits:LookUp
The developer is also using on-device models to generate a map view of a wordâ€™s origin.
Image Credits:Lookup
Tasks
Just like a few other apps, the Tasks app implemented a feature to suggest tags for an entry using local models automatically. Itâ€™s also using these models to detect a recurring task and schedule it accordingly. And the app lets users speak a few things and use the local model to break them down into various tasks without using the internet.
Image Credits:Tasks
Day One
Automattic-owned journaling app Day One is using Appleâ€™s models to get highlights and suggest titles for your entry. The team has also implemented a feature to generate prompts that nudge you to dive deeper and write more based on what you have already written.
Image Credits:Day One
Crouton
Recipe app Crouton is using Apple Intelligence to suggest tags for a recipe and assign names to timers. It also uses AI to break down a block of text into easy-to-follow steps for cooking.
Loving Foundation Model access on iOS 26, so many possibilities. Here are a few simple features that weâ€™ve added in Crouton so far ðŸ˜Š pic.twitter.com/8uU65VF8WLâ€” Devin (@JustMeDevin) September 15, 2025
SignEasy
Digital signing app SignEasy is using Appleâ€™s local models to extract key insights from a contract and give users a summary of the document they are signing.
We will continue updating this list as we discover more apps using Appleâ€™s local models.
Topics
AI, AI, Apple, Apple Intelligence, Apps, evergreen, evergreens, ios 26
Ivan Mehta
Ivan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web.
You can contact or verify outreach from Ivan by emailing im@ivanmehta.com or via encrypted message at ivan.42 on Signal.
View Bio
October 27-29, 2025
San Francisco
Founders: land your investor and sharpen your pitch. Investors: discover your next breakout startup. Innovators: claim a front-row seat to the future. Join 10,000+ tech leaders at the epicenter of innovation. Register now and save up to $668.Regular Bird rates end September 26
Register Now
Most Popular
Updates to Studio, YouTube Live, new GenAI tools, and everything else announced at Made on YouTube
Karyne Levy
Google isnâ€™t kidding around about cost cutting, even slashing its FT subscription
Connie Loizos
Meta CTO explains why the smart glasses demos failed at Meta Connect â€” and it wasnâ€™t the Wi-Fi
Sarah Perez
OpenAIâ€™s research on AI models deliberately lying is wild
Julie Bort
How AI startups are fueling Googleâ€™s booming cloud business
Maxwell Zeff
Appleâ€™s iOS 26 with the new Liquid Glass design is now available to everyone
Ivan Mehta
Spotify will now let free users pick and play tracks
Sarah Perez
Loading the next article
Error loading the next article
X
LinkedIn
Facebook
Instagram
youTube
Mastodon
Threads
Bluesky
TechCrunchStaffContact UsAdvertiseCrunchboard JobsSite Map
Terms of ServicePrivacy PolicyRSS Terms of UseCode of Conduct
H1-B VisaMBAsStubhubBlueskyTechCrunch DisruptTech LayoffsChatGPT
Â© 2025 TechCrunch Media LLC.