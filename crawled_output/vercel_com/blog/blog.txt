Blog - VercelProductsFrameworks
Next.jsThe native Next.js platform
TurborepoSpeed with Enterprise scaleAI SDKThe AI Toolkit for TypeScriptInfrastructureCI/CDHelping teams ship 6× fasterDelivery networkFast, scalable, and reliableFluid computeServers, in serverless form
AI InfrastructureAI Gateway, Sandbox, and moreObservabilityTrace every stepSecurityPlatform securityDDoS Protection, Firewall
Web Application FirewallGranular, custom protectionBot managementBotID, Bot ProtectionSolutionsUse Cases
AI AppsDeploy at the speed of AIComposable CommercePower storefronts that convertMarketing SitesLaunch campaigns fastMulti-tenant PlatformsScale apps with one codebaseWeb AppsShip features, not infrastructureUsersPlatform EngineersAutomate away repetition
Design EngineersDeploy for every ideaResourcesToolsResource CenterToday’s best practicesMarketplaceExtend and automate workflowsTemplatesJumpstart app developmentGuidesFind help quicklyPartner FinderGet help from solution partnersCompanyCustomersTrusted by the best teamsBlogThe latest posts and changesChangelogSee what shippedPressRead the latest newsEventsJoin us at an eventEnterpriseDocsPricingAll PostsAll PostsEngineeringCommunityCompany NewsCustomersv0ChangelogPressSearch postsNo results found for "".Try again with a different keyword.Featured articlesSep 9A more flexible Pro plan for modern teamsWe’re updating Vercel’s Pro plan to better align with how modern teams collaborate, how applications consume infrastructure, and how workloads are evolving with AI. Concretely, we’re making the following changes: A flexible spending model: Instead of discrete included allotments across 20+ infra products, we’re transitioning to a flexible usage balance. Free Viewer seats: Teammates can access previews, analytics, and more without needing to pay for a seat. Self-serve Enterprise features: SAML SSO, HIPAA BAAs, and more are now available to everyone on the Pro plan. No need to contact sales. Better Spend Management: Spend Management is now enabled by default, to provide peace of mind against rare runaway u...Tom OcchinoJul 10The AI Cloud: A unified platform for AI workloadsFor over a decade, Vercel has helped teams develop, preview, and ship everything from static sites to full-stack apps. That mission shaped the Frontend Cloud, now relied on by millions of developers and powering some of the largest sites and apps in the world. Now, AI is changing what and how we build. Interfaces are becoming conversations and workflows are becoming autonomous. We've seen this firsthand while building v0 and working with AI teams like Browserbase and Decagon. The pattern is clear: developers need expanded tools, new infrastructure primitives, and even more protections for their intelligent, agent-powered applications. At Vercel Ship, we introduced the AI Cloud: a unified platform that lets teams build AI features and apps with the right tools to stay flexible, move fast, and be secure, all while focusing on their products, not infrastructure.Dan FeinAug 21AI Gateway: Production-ready reliability for your AI appsBuilding an AI app can now take just minutes. With developer tools like the AI SDK, teams can build both AI frontends and backends that accept prompts and context, reason with an LLM, call actions, and stream back results. But going to production requires reliability and stability at scale. Teams that connect directly to a single LLM provider for inference create a fragile dependency: if that provider goes down or hits rate limits, so does the app.
As AI workloads become mission-critical, the focus shifts from integration to reliability and consistent model access. Fortunately, there's a better way to run. AI Gateway, now generally available, ensures availability when a provider fails, avoiding low rate limits and providing consistent reliability for AI workloads. It's the same system that has powered v0.app for millions of users, now battle-tested, stable, and ready for production for our customers.Walter and HarpreetLatest news.GeneralSep 18Scale to one: How Fluid solves cold startsCold starts have long been the Achilles’ heel of traditional serverless. It’s not just the delay itself, but when the delay happens. Cold starts happen when someone new discovers your app, when traffic is just starting to pick up, or during those critical first interactions that shape whether people stick around or convert. Traditional serverless platforms shut down inactive instances after a few minutes to save costs. But then when traffic returns, users are met with slow load times while new instances spin up. This is where developers would normally have to make a choice. Save money at the expense of unpredictable performance, or pay for always-on servers that increase costs and slow down scalability. But what if you didn't have to choose? That’s why we built a better way. Powered by Fluid compute, Vercel delivers zero cold starts for 99.37% of all requests. Fewer than one request in a hundred will ever experience a cold start. If they do happen, they are faster and shorter-lived than on a traditional serverless platform. Through a combination of platform-level optimizations, we've made cold starts a solved problem in practice. What follows is how that’s possible and why it works at every scale.Malte and Tomv0Sep 18What you need to know about vibe codingIn February 2025, Andrej Karpathy introduced the term vibe coding: a new way of coding with AI, “[where] you fully give in to the vibes, embrace exponentials, and forget that the code even exists.” Just months later, vibe coding is completely reshaping how developers and non-developers work. Over 90% of U.S. developers use AI coding tools, adoption is accelerating for other roles, and English has become the fastest growing programming language in the world. We explore this shift in detail in our new State of Vibe Coding. Here are a few of the key takeaways. Zeb and KeithGeneralSep 17Addressing security and quality issues with MCP tools in AI AgentModel Context Protocol
(MCP) is emerging as a standard protocol for federating tool calls between agents. Enterprises are starting to adopt MCP as a type of microservice architecture for teams to reuse each other's tools across different AI applications. But there are real risks with using MCP tools in production agents. Tool names, descriptions, and argument schemas become part of your agent's prompt and can change unexpectedly without warning. This can lead to security, cost, and quality issues even when the upstream MCP server has not been compromised or is not intentionally malicious. We built mcp-to-ai-sdk to reduce these issues. It is a CLI that generates static AI SDK tool definitions from any MCP server. Definitions become part of your codebase, so they only change when you explicitly update them. Malte and AndrewCustomersSep 16AI agents at scale: Rox’s Vercel-powered revenue operating systemRox is building the next-generation revenue operating system. By deploying intelligent AI agents that can research, prospect, and engage on behalf of sellers, Rox helps enterprises manage and grow revenue faster. From day one, Rox has built their applications on Vercel. With Vercel's infrastructure powering their web applications, Rox ships faster, scales globally, and delivers consistently fast experiences to every customer.
Jerry ZhouGeneralSep 15Introducing Vercel Drains: Complete observability data, anywhereVercel Log Drains are now Vercel Drains. Why? They’re not just for logs anymore, as you can now also export OpenTelemetry traces, Web Analytics events, and Speed Insights metrics. Drains give you a single way to stream observability data out of Vercel and into the systems your team already rely on.Dan FeinCustomersSep 15Helly Hansen migrated to Vercel and drove 80% Black Friday growth Founded in 1877, Helly Hansen is a global leader in technical apparel, but its digital experience wasn't living up to its legacy. Operating across 38 global markets with multiple brands (including HellyHansen.com, HHWorkwear.com, and Musto.com), the company was being held back by an outdated tech stack that slowed site speeds and frustrated customers. Through an incremental migration to Next.js and Vercel, Helly Hansen improved Core Web Vitals from red to green, increased developer agility, and delivered a record-breaking Black Friday Cyber Monday, building a foundation for future innovation. Alina WeinsteinGeneralSep 12Introducing x402-mcp: Open protocol payments for MCP toolsAI agents are improving at handling complex tasks, but a recurring limitation emerges when they need to access paid external services. The current model requires pre-registering with every API, managing keys, and maintaining separate billing relationships. This workflow breaks down if an agent needs to autonomously discover and interact with new services. x402 is an open protocol that addresses this by adding payment directly into HTTP requests. It uses the 402 Payment Required status code to let any API endpoint request payment without prior account setup. We built x402-mcp to integrate x402 payments with Model Context Protocol (MCP) servers and the Vercel AI SDK.Ethan NiserGeneralSep 10MongoDB Atlas is now available on the Vercel MarketplaceMongoDB Atlas is now available on the Vercel Marketplace. Developers can now provision a fully managed MongoDB database directly from your Vercel dashboard and connect it to your project without leaving the platform. Adding a database to your project typically means managing another account, working through connection setup, and coordinating billing across services. The Vercel Marketplace brings these tools into your existing workflow, so you can focus on building rather than configuring.Hedi ZandiCompany NewsSep 9A more flexible Pro plan for modern teamsWe’re updating Vercel’s Pro plan to better align with how modern teams collaborate, how applications consume infrastructure, and how workloads are evolving with AI. Concretely, we’re making the following changes:Tom OcchinoEngineeringSep 9The second wave of MCP: Building for LLMs, not developersWhen the MCP standard first launched, many teams rushed to ship something. Many servers ended up as thin wrappers around existing APIs with minimal changes. A quick way to say "we support MCP". At the time, this made sense. MCP was new, teams wanted to get something out quickly, and the obvious approach was mirroring existing API structures. Why reinvent when you could repackage? But the problem with this approach is LLMs don’t work like developers. They don’t reuse past code or keep long term state. Each conversation starts fresh. LLMs have to rediscover which tools exist, how to use them, and in what order. With low level API wrappers, this leads to repeated orchestration, inconsistent behavior, and wasted effort as LLMs repeatedly solve the same puzzles. MCP works best when tools handle complete user intentions rather than exposing individual API operations. One tool that deploys a project end-to-end works better than four tools that each handle a piece of the deployment pipeline.Boris and AndrewGeneralSep 8Critical npm supply chain attack response - September 8, 2025On September 9, 2025, the campaign extended to DuckDB-related packages after the duckdb_admin account was breached. These releases contained the same wallet-drainer malware, confirming this was part of a coordinated effort targeting prominent npm maintainers. While Vercel customers were not impacted by the DuckDB incident, we continue to track activity across the npm ecosystem with our partners to ensure deployments on Vercel remain secure by default.Aaron BrownEngineeringSep 4Stress testing Biome's noFloatingPromises lint ruleRecently we partnered with the Biome team to strengthen their noFloatingPromises lint rule to catch more subtle edge cases. This rule prevents unhandled Promises, which can cause silent errors and unpredictable behavior. Once Biome had an early version ready, they asked if we could help stress test it with some test cases. At Vercel, we know good tests require creativity just as much as attention to detail. To ensure strong coverage, we wanted to stretch the rule to its limits and so we thought it would be fun to turn this into a friendly internal competition. Who could come up with the trickiest examples that would still break the updated lint rule? Part of the fun was learning together, but before we dive into the snippets, let’s revisit what makes a Promise “float”.Dimitri MitropoulosShow more postsReady to deploy? Start building with a free account. Speak to an expert for your Pro or Enterprise needs.Start DeployingTalk to an ExpertExplore Vercel Enterprise with an interactive product tour, trial, or a personalized demo.Explore EnterpriseProductsAIEnterpriseFluid ComputeNext.jsObservabilityPreviewsRenderingSecurityTurbov0ResourcesCommunityDocsGuidesHelpIntegrationsPricingResourcesSolution PartnersStartupsTemplatesCompanyAboutBlogCareersChangelogEventsContact UsCustomersPartnersShippedPrivacy PolicyLegalSocialGitHubLinkedIn TwitterYouTubeLoading status…Select a display theme:system lightdark