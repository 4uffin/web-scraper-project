Fluid computeProductsFrameworks
Next.jsThe native Next.js platform
TurborepoSpeed with Enterprise scaleAI SDKThe AI Toolkit for TypeScriptInfrastructureCI/CDHelping teams ship 6× fasterDelivery networkFast, scalable, and reliableFluid computeServers, in serverless form
AI InfrastructureAI Gateway, Sandbox, and moreObservabilityTrace every stepSecurityPlatform securityDDoS Protection, Firewall
Web Application FirewallGranular, custom protectionBot managementScalable bot protectionBotIDInvisible CAPTCHASolutionsUse Cases
AI AppsDeploy at the speed of AIComposable CommercePower storefronts that convertMarketing SitesLaunch campaigns fastMulti-tenant PlatformsScale apps with one codebaseWeb AppsShip features, not infrastructureUsersPlatform EngineersAutomate away repetition
Design EngineersDeploy for every ideaResourcesToolsResource CenterToday’s best practicesMarketplaceExtend and automate workflowsTemplatesJumpstart app developmentGuidesFind help quicklyPartner FinderGet help from solution partnersCompanyCustomersTrusted by the best teamsBlogThe latest posts and changesChangelogSee what shippedPressRead the latest newsEventsJoin us at an eventEnterpriseDocsPricingNewLower Costs with Active CPU PricingThe power of servers, in serverless formFluid compute combines the efficiency of servers and the flexibility of serverless, enabling real-time, dynamic workloads like APIs, streaming, and AI.Enable FluidGet a demoEfficiency gains that pay offServer-like concurrency, in a serverless worldIn-function concurrency enables a single Vercel Function to handle multiple invocations simultaneously, optimizing resource usage and turning efficiency gains into savings.Traditional serverless wastes idle time. It fails to efficiently utilize available resources during periods of inactivity.Fluid runs invocations concurrently. Overall cloud resources provisioned are dramatically reduced while invocations are kept alive in-memory.CPU that’s charged when active, not idle. No more paying for I/O wait times, network latency, and API calls.“Many of our API endpoints were lightweight and involved external requests, resulting in idle compute time. By leveraging in-function concurrency, we were able to share compute resources between invocations, cutting costs by over 50% with zero code changes.”Lead Fullstack DeveloperAI workloadsEfficient idle time. Run tasks with reduced latency and higher concurrency, delivering faster, scalable results for all users—regardless of the workload size.Business-critical APIsReliable APIs. Ensure fast, resilient API responses under heavy traffic, keeping smooth and consistent experiences.Server-side and partial pre-renderingRapid-fire dynamic rendering.
Generate dynamic pages with minimal latency, allowing for faster load times and seamless interactions.MiddlewareDynamic routing.
Perform authentication checks and apply personalization, with the power of fluid computing.Vercel FunctionsBridging servers and serverless.Taking the best of servers and serverless to create a new model in computing, scaling business-critical workloads efficiently across global environments.Change selection to see feature set for computeServersTraditional ServerlessFluidShowing features for "Servers" compute.Showing features for "Traditional Serverless" compute.Showing features for "Fluid" compute.FeaturesServersTraditional ServerlessFluidCold start handlingN/ACold startsCold start preventionScalingManual scalingAuto-scalingEfficient auto-scalingConcurrencyHorizontalVerticalHorizontal & verticalOperational overheadHigh maintenanceMinimal overhead, inefficientMinimal overhead, automatically optimizedPricing modelUpfront cost per serverPay-as-you-go modelPay-as-you-compute (active CPU pricing)CPU efficiencyHigh efficiencyI/O bound inefficiencyOptimized I/O efficiencyFluid compute for dynamic web applicationsIn-function concurrencyRun multiple invocations on a single function instance, reducing idle compute time and lowering costs​.Cold-start reductionFunctions are pre-warmed and optimized with bytecode caching, ensuring faster response times​.StreamingSend data to users as it becomes available, improving performance for AI, media, and real-time apps​.Cross-region failoverEnsures high availability by rerouting traffic to backup regions during outages​.Dynamic scalingAutomatically adjusts concurrency and resource allocation based on real-time demand​.Post-response tasksKeep functions running after sending a response to handle tasks like logging or database updates​.Build something great.Start deploying todayContact salesProductsAIEnterpriseFluid ComputeNext.jsObservabilityPreviewsRenderingSecurityTurbov0ResourcesCommunityDocsGuidesHelpIntegrationsPricingResourcesSolution PartnersStartupsTemplatesSDKs by VercelCompanyAboutBlogCareersChangelogContact UsCustomersEventsPartnersShippedPrivacy PolicyLegalSocialGitHubLinkedIn TwitterYouTubeLoading status…Select a display theme:system lightdark