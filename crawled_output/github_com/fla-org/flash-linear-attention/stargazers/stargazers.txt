Stargazers · fla-org/flash-linear-attention · GitHub
Skip to content
Navigation Menu
Toggle navigation
Sign in
Appearance settings
Platform
GitHub Copilot
Write better code with AI
GitHub Spark
New
Build and deploy intelligent apps
GitHub Models
New
Manage and compare prompts
GitHub Advanced Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
Why GitHub
Documentation
GitHub Skills
Blog
Integrations
GitHub Marketplace
MCP Registry
View all features
Solutions
By company size
Enterprises
Small and medium teams
Startups
Nonprofits
By use case
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
Events & Webinars
Ebooks & Whitepapers
Customer Stories
Partners
Executive Insights
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
GitHub Advanced Security
Enterprise-grade security features
Copilot for business
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our documentation.
Cancel
Create saved search
Sign in
Sign up
Appearance settings
Resetting focus
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
fla-org
/
flash-linear-attention
Public
Notifications
You must be signed in to change notification settings
Fork
256
Star
3.4k
Code
Issues
33
Pull requests
8
Discussions
Actions
Projects
0
Wiki
Security
Uh oh!
There was an error while loading. Please reload this page.
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Projects
Wiki
Security
Insights
Stargazers
All 3,365
You know
CatCodeMe
Is from Beijing
Beijing
Follow
wxue1
Works for Huawei
Huawei
Follow
ipapal
Works for National Technical University of Athens
National Technical University of Athens
Follow
wlr737
Works for Institute of Information Engineering, Chinese Academy of Sciences
Institute of Information Engineering, Chinese Academy of Sciences
Follow
ZhiweiYan-96
Works for AMD, Sun Yat-sen University
AMD, Sun Yat-sen University
Follow
gogog01-29-2021
Works for ITM
ITM
Follow
0rzBen
Works for >_<
>_<
Follow
binbinzhm
Joined on Apr 19, 2018
Follow
youngsm
Is from SLAC & Stanford
SLAC & Stanford
Follow
tscholak
Works for @ServiceNow
@ServiceNow
Follow
schnurrd
Is from Zurich, Switzerland
Zurich, Switzerland
Follow
ahmadazakaria
Is from Cergy, france
Cergy, france
Follow
nraghuraman
Works for Mistral AI
Mistral AI
Follow
lovekdl
Joined on Nov 6, 2018
Follow
biofoolgreen
Joined on Aug 7, 2014
Follow
Shixiaowei02
Joined on May 15, 2018
Follow
it-dainb
Is from Thanh Hoá
Thanh Hoá
Follow
lucasjinreal
Works for Google
Google
Follow
tianchichi
Joined on Apr 5, 2019
Follow
MasterGodzilla
Joined on Feb 15, 2021
Follow
blackcat1402
Joined on Jul 26, 2020
Follow
troylhy1991
Joined on Sep 7, 2015
Follow
rickyxie2004
Is from 中国
中国
Follow
YXB-NKU
Works for MCG-NKU
MCG-NKU
Follow
PreviousNext
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.