GitHub - fla-org/flash-linear-attention: ðŸš€ Efficient implementations of state-of-the-art linear attention models
Skip to content
Navigation Menu
Toggle navigation
Sign in
Appearance settings
Platform
GitHub Copilot
Write better code with AI
GitHub Spark
New
Build and deploy intelligent apps
GitHub Models
New
Manage and compare prompts
GitHub Advanced Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
Why GitHub
Documentation
GitHub Skills
Blog
Integrations
GitHub Marketplace
MCP Registry
View all features
Solutions
By company size
Enterprises
Small and medium teams
Startups
Nonprofits
By use case
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
Events & Webinars
Ebooks & Whitepapers
Customer Stories
Partners
Executive Insights
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
GitHub Advanced Security
Enterprise-grade security features
Copilot for business
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our documentation.
Cancel
Create saved search
Sign in
Sign up
Appearance settings
Resetting focus
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
fla-org
/
flash-linear-attention
Public
Notifications
You must be signed in to change notification settings
Fork
256
Star
3.3k
ðŸš€ Efficient implementations of state-of-the-art linear attention models
github.com/fla-org/flash-linear-attention
License
MIT license
3.3k
stars
256
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
36
Pull requests
9
Discussions
Actions
Projects
0
Wiki
Security
Uh oh!
There was an error while loading. Please reload this page.
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Projects
Wiki
Security
Insights
fla-org/flash-linear-attention
mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commitÂ History1,683 Commits.github.githubÂ Â benchmarksbenchmarksÂ Â evalsevalsÂ Â examplesexamplesÂ Â flaflaÂ Â legacy/traininglegacy/trainingÂ Â scriptsscriptsÂ Â teststestsÂ Â utilsutilsÂ Â .flake8.flake8Â Â .gitignore.gitignoreÂ Â .pre-commit-config.yaml.pre-commit-config.yamlÂ Â CITATION.cffCITATION.cffÂ Â ENVs.mdENVs.mdÂ Â FAQs.mdFAQs.mdÂ Â LICENSELICENSEÂ Â README.mdREADME.mdÂ Â pyproject.tomlpyproject.tomlÂ Â setup.pysetup.pyÂ Â View all filesRepository files navigationREADMEMIT license
ðŸ’¥ Flash Linear Attention
This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. All implementations are written purely in PyTorch and Triton, making them platform-agnostic. Currently verified platforms include NVIDIA, AMD, and Intel. Any pull requests are welcome!
News
Models
Installation
Usage
Token Mixing
Fused Modules
Generation
Hybrid Models
Training
Evaluation
Benchmarks
Citation
Star History
Acknowledgments
News
$\texttt{[2025-09]}$: ðŸ» Thrilled to announce that GDN has been integrated into Qwen3-Next.
Check out the PR and their blog post for more infos!
$\texttt{[2025-08]}$: ðŸŒ² Add Log-Linear Attention implementation to fla (paper).
$\texttt{[2025-08]}$: ðŸŽ“ Add MoM implementation to fla (paper).
$\texttt{[2025-07]}$: ðŸ³ Add MLA implementation to fla (paper).
$\texttt{[2025-07]}$: ðŸ›£ï¸ Added PaTH Attention to fla (paper).
$\texttt{[2025-06]}$: ðŸŽ‰ Added MesaNet to fla (paper).
$\texttt{[2025-06]}$: ðŸ Add Comba implementation to fla (paper).
$\texttt{[2025-05]}$: ðŸŽ‰ Add Rodimus* implementation to fla (paper).
$\texttt{[2025-04]}$: ðŸŽ‰ Add DeltaProduct implementation to fla (paper).
$\texttt{[2025-04]}$: ðŸŽ‰ Add FoX implementation to fla (paper).
$\texttt{[2025-03]}$: We have changed the default initializer_range to the magic ðŸ³ 0.006 The initializer_range was rolled back to the default value of 0.02. For actual training, we recommend trying both.
$\texttt{[2025-02]}$: ðŸ³ Add NSA implementations to fla. See kernels here.
$\texttt{[2025-01]}$: ðŸ”¥ We are migrating to torchtitan-based training framework. Check out the flame repo for more details.
$\texttt{[2025-01]}$: ðŸ¦… Add RWKV7 implementations (both kernels and models) to fla.
$\texttt{[2024-12]}$: Integrated flash-bidirectional-attention to fla-org (repo)
$\texttt{[2024-12]}$: ðŸŽ‰ Add Gated DeltaNet implementation to fla (paper).
$\texttt{[2024-12]}$: ðŸš€ fla now officially supports kernels with variable-length inputs.
$\texttt{[2024-11]}$: The inputs are now switched from head-first to seq-first format.
$\texttt{[2024-11]}$: ðŸ’¥ fla now provides a flexible way for training hybrid models.
$\texttt{[2024-10]}$: ðŸ”¥ Announcing flame, a minimal and scalable framework for training fla models. Check out the details here.
$\texttt{[2024-09]}$: fla now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training.
$\texttt{[2024-09]}$: ðŸŽ‰ Add GSA implementation to fla (paper).
$\texttt{[2024-05]}$: ðŸŽ‰ Add DeltaNet implementation to fla (paper).
$\texttt{[2024-05]}$: ðŸ’¥ fla v0.1: a variety of subquadratic kernels/layers/models integrated (RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see Models).
$\texttt{[2023-12]}$: ðŸ’¥ Launched fla, offering a collection of implementations for state-of-the-art linear attention models.
Models
Roughly sorted according to the timeline supported in fla. The recommended training mode is chunk when available.
Year
Venue
Model
Paper
Code
2023
RetNet
Retentive network: a successor to transformer for large language models
official
fla
2024
ICML
GLA
Gated Linear Attention Transformers with Hardware-Efficient Training
official
fla
2024
ICML
Based
Simple linear attention language models balance the recall-throughput tradeoff
official
fla
2024
ACL
Rebased
Linear Transformers with Learnable Kernel Functions are Better In-Context Models
official
fla
2024
NeurIPS
DeltaNet
Parallelizing Linear Transformers with Delta Rule
over Sequence Length
official
fla
2022
ACL
ABC
ABC: Attention with Bounded-memory Control
fla
2023
NeurIPS
HGRN
Hierarchically Gated Recurrent Neural Network for Sequence Modeling
official
fla
2024
COLM
HGRN2
HGRN2: Gated Linear RNNs with State Expansion
official
fla
2024
COLM
RWKV6
Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
official
fla
2024
LightNet
You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet
official
fla
2025
ICLR
Samba
Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
official
fla
2024
ICML
Mamba2
Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
official
fla
2024
NeurIPS
GSA
Gated Slot Attention for Efficient Linear-Time Sequence Modeling
official
fla
2025
ICLR
Gated DeltaNet
Gated Delta Networks: Improving Mamba2 with Delta Rule
official
fla
2025
RWKV7
RWKV-7 "Goose" with Expressive Dynamic State Evolution
official
fla
2025
NSA
Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention
fla
2025
ICLR
FoX
Forgetting Transformer: Softmax Attention with a Forget Gate
official
fla
2025
DeltaProduct
DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products
fla
2025
ICLR
Rodimus*
Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions
official
fla
2025
MesaNet
MesaNet: Sequence Modeling by Locally Optimal Test-Time Training
fla
2025
Comba
Comba: Improving Bilinear RNNs with Closed-loop Control
official
fla
2025
PaTH
PaTH Attention: Position Encoding via Accumulating Householder Transformations
fla
2025
MoM
MoM: Linear Sequence Modeling with Mixture-of-Memories
official
fla
2025
Log-Linear Attention
Log-Linear Attention
official
fla
Installation
The following requirements should be satisfied
PyTorch >= 2.5
Triton >=3.0 (or nightly version, see FAQs)
einops
transformers >=4.45.0
datasets >=3.3.0
Starting from v0.3.2, the packages published on PyPI are fla-core and flash-linear-attention. The former contains all our customized kernels and only depends on PyTorch, Triton, and einops. The latter is an extension package of the former, containing fla/layers and fla/models, and depends on transformers. We also provide Triton implementations for conv1d operations, so causal-conv1d is not required.
You can install fla with pip:
pip install flash-linear-attention
As fla is actively developed now, for the latest features and updates, an alternative way is to install the package from source. Note that installing from git uses the default mode, so you need to uninstall both fla-core and flash-linear-attention first:
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y && pip install -U git+https://github.com/fla-org/flash-linear-attention
or manage fla with submodules
git submodule add https://github.com/fla-org/flash-linear-attention.git 3rdparty/flash-linear-attention
ln -s 3rdparty/flash-linear-attention/fla fla
If you have installed triton-nightly and torch pre version, please use the following command:
pip install einops ninja datasets transformers numpy
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y && pip install -U --no-use-pep517 git+https://github.com/fla-org/flash-linear-attention --no-deps
Usage
Token Mixing
We provide ``token mixing'' linear attention layers in fla.layers for you to use.
You can replace the standard multihead attention layer in your model with other linear attention layers.
Example usage is as follows:
>>> import torch
>>> from fla.layers import MultiScaleRetention
>>> batch_size, num_heads, seq_len, hidden_size = 32, 4, 2048, 1024
>>> device, dtype = 'cuda:0', torch.bfloat16
>>> retnet = MultiScaleRetention(hidden_size=hidden_size, num_heads=num_heads).to(device=device, dtype=dtype)
>>> retnet
MultiScaleRetention(
(q_proj): Linear(in_features=1024, out_features=1024, bias=False)
(k_proj): Linear(in_features=1024, out_features=1024, bias=False)
(v_proj): Linear(in_features=1024, out_features=2048, bias=False)
(g_proj): Linear(in_features=1024, out_features=2048, bias=False)
(o_proj): Linear(in_features=2048, out_features=1024, bias=False)
(g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-05, activation=swish)
(rotary): RotaryEmbedding(dim=256, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
)
>>> x = torch.randn(batch_size, seq_len, hidden_size).to(device=device, dtype=dtype)
>>> y, *_ = retnet(x)
>>> y.shape
torch.Size([32, 2048, 1024])
We provide the implementations of models that are compatible with ðŸ¤— Transformers library.
Here's an example of how to initialize a GLA model from the default configs in fla:
>>> from fla.models import GLAConfig
>>> from transformers import AutoModelForCausalLM
>>> config = GLAConfig()
>>> config
GLAConfig {
"attn": null,
"attn_mode": "chunk",
"bos_token_id": 1,
"clamp_min": null,
"conv_size": 4,
"elementwise_affine": true,
"eos_token_id": 2,
"expand_k": 0.5,
"expand_v": 1,
"feature_map": null,
"fuse_cross_entropy": true,
"fuse_norm": true,
"fuse_swiglu": true,
"hidden_act": "swish",
"hidden_ratio": 4,
"hidden_size": 2048,
"initializer_range": 0.006,
"intermediate_size": null,
"max_position_embeddings": 2048,
"model_type": "gla",
"norm_eps": 1e-06,
"num_heads": 4,
"num_hidden_layers": 24,
"num_kv_heads": null,
"tie_word_embeddings": false,
"transformers_version": "4.50.1",
"use_cache": true,
"use_gk": true,
"use_gv": false,
"use_output_gate": true,
"use_short_conv": false,
"vocab_size": 32000
}
>>> AutoModelForCausalLM.from_config(config)
GLAForCausalLM(
(model): GLAModel(
(embeddings): Embedding(32000, 2048)
(layers): ModuleList(
(0-23): 24 x GLABlock(
(attn_norm): RMSNorm(2048, eps=1e-06)
(attn): GatedLinearAttention(
(q_proj): Linear(in_features=2048, out_features=1024, bias=False)
(k_proj): Linear(in_features=2048, out_features=1024, bias=False)
(v_proj): Linear(in_features=2048, out_features=2048, bias=False)
(g_proj): Linear(in_features=2048, out_features=2048, bias=False)
(gk_proj): Sequential(
(0): Linear(in_features=2048, out_features=16, bias=False)
(1): Linear(in_features=16, out_features=1024, bias=True)
)
(o_proj): Linear(in_features=2048, out_features=2048, bias=False)
(g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-06, activation=swish)
)
(mlp_norm): RMSNorm(2048, eps=1e-06)
(mlp): GatedMLP(
(gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
(up_proj): Linear(in_features=2048, out_features=5632, bias=False)
(down_proj): Linear(in_features=5632, out_features=2048, bias=False)
(swiglu_linear): SwiGLULinear()
)
)
)
(norm): RMSNorm(2048, eps=1e-06)
)
(lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
Fused Modules
We offer a collection of fused modules in fla.modules to facilitate faster training:
Rotary Embedding: rotary positional embeddings as adopted by the Llama architecture, a.k.a., Transformer++.
Norm Layers:
RMSNorm, LayerNorm and GroupNorm
RMSNormLinear, LayerNormLinear and GroupNormLinear to reduce memory usage of intermediate tensors for improved memory efficiency.
Norm Layers with Gating: combine norm layers with element-wise sigmoid or swish gating, as used by RetNet/GLA.
Cross Entropy: faster Triton implementation of cross entropy loss.
Linear Cross Entropy: fused linear layer and cross entropy loss to avoid the materialization of large logits tensors. Also refer to implementations by mgmalek and Liger-Kernel.
Linear KL Divergence: fused linear layer and KL divergence loss in a similar vein as CE loss.
ImportantYou can control using fuse_linear_cross_entropy in the model configuration to enable/disable the fused linear cross entropy loss.
This fused implementation is more memory-efficient but may reduce numerical precision. Due to this trade-off, it is disabled by default.
If you enable this feature and encounter training instability (e.g., loss divergence), we recommend disabling it to see if the issue is resolved.
Generation
Upon successfully pretraining a model, it becomes accessible for generating text using the ðŸ¤— text generation APIs.
In the following, we give a generation example:
>>> import fla
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> name = 'fla-hub/gla-1.3B-100B'
>>> tokenizer = AutoTokenizer.from_pretrained(name)
>>> model = AutoModelForCausalLM.from_pretrained(name).cuda()
>>> input_prompt = "Power goes with permanence. Impermanence is impotence. And rotation is castration."
>>> input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.cuda()
>>> outputs = model.generate(input_ids, max_length=64)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
We also provide a simple script here for benchmarking the generation speed.
Simply run it by:
$ python -m benchmarks.benchmark_generation \
--path 'fla-hub/gla-1.3B-100B' \
--repetition_penalty 2. \
--prompt="Hello everyone, I'm Songlin Yang"
Prompt:
Hello everyone, I'm Songlin Yang
Generated:
Hello everyone, I'm Songlin Yang.
I am a 20 year old girl from China who is currently studying in the United States of America for my Master degree and also working as an English teacher at school here on campus since last summer (1st semester). My main goal to be able do well with this course so that we can have
Prompt length: 10, generation length: 64
Total prompt processing + decoding time: 4593ms
All of the pretrained models currently available can be found in fla-hub.
>>> from huggingface_hub import list_models
>>> for model in list_models(author='fla-hub'): print(model.id)
Hybrid Models
fla provides a flexible method to incorporate standard attention layers into existing linear attention models.
This is easily achieved by specifying the attn argument in the model configuration.
For example, to create a 2-layer Samba model with interleaved Mamba and local attention layers, using a sliding window size of 2048:
>>> from fla.models import SambaConfig
>>> from transformers import AutoModelForCausalLM
>>> config = SambaConfig(num_hidden_layers=2)
>>> config.attn = {
'layers': [1],
'num_heads': 18,
'num_kv_heads': 18,
'qkv_bias': False,
'rope_theta': 10000.,
'window_size': 2048
}
>>> config
SambaConfig {
"attn": {
"layers": [
1
],
"num_heads": 18,
"num_kv_heads": 18,
"qkv_bias": false,
"rope_theta": 10000.0,
"window_size": 2048
},
"bos_token_id": 1,
"conv_kernel": 4,
"eos_token_id": 2,
"expand": 2,
"fuse_cross_entropy": true,
"fuse_norm": true,
"fuse_swiglu": true,
"hidden_act": "swish",
"hidden_ratio": 4,
"hidden_size": 2304,
"initializer_range": 0.02,
"intermediate_size": 4608,
"max_position_embeddings": 2048,
"model_type": "samba",
"norm_eps": 1e-05,
"num_hidden_layers": 2,
"pad_token_id": 0,
"rescale_prenorm_residual": false,
"residual_in_fp32": false,
"state_size": 16,
"tie_word_embeddings": false,
"time_step_floor": 0.0001,
"time_step_init_scheme": "random",
"time_step_max": 0.1,
"time_step_min": 0.001,
"time_step_rank": 144,
"time_step_scale": 1.0,
"transformers_version": "4.50.1",
"use_bias": false,
"use_cache": true,
"use_conv_bias": true,
"vocab_size": 32000
}
>>> AutoModelForCausalLM.from_config(config)
SambaForCausalLM(
(backbone): SambaModel(
(embeddings): Embedding(32000, 2304)
(layers): ModuleList(
(0): SambaBlock(
(mixer_norm): RMSNorm(2304, eps=1e-05)
(mixer): Mamba(
(conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)
(in_proj): Linear(in_features=2304, out_features=9216, bias=False)
(x_proj): Linear(in_features=4608, out_features=176, bias=False)
(dt_proj): Linear(in_features=144, out_features=4608, bias=True)
(out_proj): Linear(in_features=4608, out_features=2304, bias=False)
)
(mlp_norm): RMSNorm(2304, eps=1e-05)
(mlp): GatedMLP(
(gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
(up_proj): Linear(in_features=2304, out_features=6144, bias=False)
(down_proj): Linear(in_features=6144, out_features=2304, bias=False)
(swiglu_linear): SwiGLULinear()
)
)
(1): SambaBlock(
(mixer_norm): RMSNorm(2304, eps=1e-05)
(mixer): Attention(
(q_proj): Linear(in_features=2304, out_features=2304, bias=False)
(k_proj): Linear(in_features=2304, out_features=2304, bias=False)
(v_proj): Linear(in_features=2304, out_features=2304, bias=False)
(o_proj): Linear(in_features=2304, out_features=2304, bias=False)
(rotary): RotaryEmbedding(dim=128, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
)
(mlp_norm): RMSNorm(2304, eps=1e-05)
(mlp): GatedMLP(
(gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
(up_proj): Linear(in_features=2304, out_features=6144, bias=False)
(down_proj): Linear(in_features=6144, out_features=2304, bias=False)
(swiglu_linear): SwiGLULinear()
)
)
)
(norm_f): RMSNorm(2304, eps=1e-05)
)
(lm_head): Linear(in_features=2304, out_features=32000, bias=False)
)
During inference, you DO NOT need to revise anything for generation!
The model will produce output as-is, without any need for additional configurations or modifications.
Training
We provide a minimal framework called ðŸ”¥ flame built on top of torchtitan, for efficient training of fla models.
Checkout the GLA example for more details.
Evaluation
The lm-evaluation-harness library allows you to easily perform (zero-shot) model evaluations.
Follow the steps below to use this library:
Install lm_eval following their instructions.
Run evaluation with:
$ MODEL='fla-hub/gla-1.3B-100B'
$ python -m evals.harness --model hf \
--model_args pretrained=$MODEL,dtype=bfloat16 \
--tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
--batch_size 64 \
--num_fewshot 0 \
--device cuda \
--show_config
We've made fla compatible with hf-style evaluations, you can call evals.harness to finish the evaluations.
Running the command above will provide the task results reported in the GLA paper.
Multi-GPU Evaluation with Hugging Face accelerate ðŸš€
To perform data-parallel evaluation (where each GPU loads a separate full copy of the model), we leverage the accelerate launcher as follows:
$ MODEL='fla-hub/gla-1.3B-100B'
$ accelerate launch -m evals.harness --model hf
\
--model_args pretrained=$MODEL,dtype=bfloat16,trust_remote_code=True
\
--tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
--batch_size 64
\
--num_fewshot 0
\
--device cuda
\
--show_config
\
--trust_remote_code
ðŸ“ RULER Benchmark suite
The RULER benchmarks are commonly used for evaluating model performance on long-context tasks.
You can evaluate fla models on RULER directly using lm-evaluation-harness. RULER is only available in a relatively recent version of lm-evaluation-harness, so make sure you have the latest version installed.
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
Then, install the necessary dependencies for RULER:
pip install lm_eval["ruler"]
and run evaluation by (e.g., 32k contexts):
$ accelerate launch -m evals.harness \
--output_path $OUTPUT \
--tasks niah_single_1,niah_single_2,niah_single_3,niah_multikey_1,niah_multikey_2,niah_multikey_3,niah_multiquery,niah_multivalue,ruler_vt,ruler_cwe,ruler_fwe,ruler_qa_hotpot,ruler_qa_squad \
--model_args pretrained=$MODEL,dtype=bfloat16,max_length=32768,trust_remote_code=True \
--metadata='{"max_seq_lengths":[4096,8192,16384,32768]}' \
--batch_size 2 \
--show_config
\
--trust_remote_code
If a GPU can't load a full copy of the model, please refer to this link for FSDP settings.
TipIf you are using lm-evaluation-harness as an external library and can't find (almost) any tasks available, before calling lm_eval.evaluate() or lm_eval.simple_evaluate(), simply run the following to load the library's stock tasks!
>>> from lm_eval.tasks import TaskManager; TaskManager().initialize_tasks()
Benchmarks
We compared our Triton-based RetNet implementation with CUDA-based FlashAttention2, using a batch size of 8, 32 heads, and a head dimension of 128, across different sequence lengths.
These tests were conducted on a single H100 80GB GPU, as illustrated in the following graph
# you might have to first install `fla` to enable its import via `pip install -e .`
$ python benchmark_retention.py
Performance:
T
chunk_fwd
parallel_fwd
flash_fwd
chunk_fwdbwd
parallel_fwdbwd
flash_fwdbwd
0
128.0
0.264032
0.243536
0.083488
1.301856
1.166784
0.320704
1
256.0
0.273472
0.252848
0.094304
1.345872
1.300608
0.807936
2
512.0
0.303600
0.278896
0.098112
1.503168
1.433184
0.857216
3
1024.0
0.357248
0.367360
0.156528
1.773552
2.303424
1.160864
4
2048.0
0.454624
0.605616
0.340928
2.283728
4.483360
1.955936
5
4096.0
0.638960
1.378016
1.004992
3.374720
12.271215
4.813776
6
8192.0
1.012352
4.201344
3.625008
5.581808
40.833618
15.023697
7
16384.0
1.748512
14.489664
13.710080
10.191552
153.093765
54.336864
Citation
If you find this repository helpful, please cite our work:
@software{yang2024fla,
title
= {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
author = {Yang, Songlin and Zhang, Yu},
url
= {https://github.com/fla-org/flash-linear-attention},
month
= jan,
year
= {2024}
}
@inproceedings{yang2024gdn,
title
= {Gated Delta Networks: Improving Mamba2 with Delta Rule},
author
= {Songlin Yang and Jan Kautz and Ali Hatamizadeh},
booktitle = {Proceedings of ICLR},
year
= {2025}
}
@inproceedings{yang2024deltanet,
title
= {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
author
= {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
booktitle = {Proceedings of NeurIPS},
year
= {2024}
}
@inproceedings{zhang2024gsa,
title
= {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
author
= {Zhang, Yu and Yang, Songlin and Zhu, Ruijie and Zhang, Yue and Cui, Leyang and Wang, Yiqiao and Wang, Bolun and Shi, Freda and Wang, Bailin and Bi, Wei and Zhou, Peng and Fu, Guohong},
booktitle = {Proceedings of NeurIPS},
year
= {2024}
}
@inproceedings{qin2024hgrn2,
title
= {HGRN2: Gated Linear RNNs with State Expansion},
author
= {Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
booktitle = {Proceedings of COLM},
year
= {2024}
}
@inproceedings{yang2024gla,
title
= {Gated Linear Attention Transformers with Hardware-Efficient Training},
author
= {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
booktitle = {Proceedings of ICML},
year
= {2024}
}
Star History
Acknowledgments
We extend our gratitude to Bitdeer for providing CI server resources that power our infrastructure.
About
ðŸš€ Efficient implementations of state-of-the-art linear attention models
github.com/fla-org/flash-linear-attention
Topics
natural-language-processing
machine-learning-systems
large-language-models
Resources
Readme
License
MIT license
Uh oh!
There was an error while loading. Please reload this page.
Activity
Custom properties
Stars
3.3k
stars
Watchers
29
watching
Forks
256
forks
Report repository
Releases
9
v0.3.2
Latest
Sep 10, 2025
+ 8 releases
Packages
0
No packages published
Uh oh!
There was an error while loading. Please reload this page.
Contributors
60
+ 46 contributors
Languages
Python
99.8%
Shell
0.2%
Footer
Â© 2025 GitHub,Â Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You canâ€™t perform that action at this time.