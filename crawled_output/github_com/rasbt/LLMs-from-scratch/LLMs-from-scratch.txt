GitHub - rasbt/LLMs-from-scratch: Implement a ChatGPT-like LLM in PyTorch from scratch, step by step
Skip to content
Navigation Menu
Toggle navigation
Sign in
Appearance settings
Platform
GitHub Copilot
Write better code with AI
GitHub Spark
New
Build and deploy intelligent apps
GitHub Models
New
Manage and compare prompts
GitHub Advanced Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
Why GitHub
Documentation
GitHub Skills
Blog
Integrations
GitHub Marketplace
MCP Registry
View all features
Solutions
By company size
Enterprises
Small and medium teams
Startups
Nonprofits
By use case
App Modernization
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
Events & Webinars
Ebooks & Whitepapers
Customer Stories
Partners
Executive Insights
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
GitHub Advanced Security
Enterprise-grade security features
Copilot for business
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our documentation.
Cancel
Create saved search
Sign in
Sign up
Appearance settings
Resetting focus
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
rasbt
/
LLMs-from-scratch
Public
Notifications
You must be signed in to change notification settings
Fork
10.6k
Star
72.8k
Implement a ChatGPT-like LLM in PyTorch from scratch, step by step
amzn.to/4fqvn0D
License
View license
72.8k
stars
10.6k
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
4
Pull requests
1
Discussions
Actions
Security
Uh oh!
There was an error while loading. Please reload this page.
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Security
Insights
rasbt/LLMs-from-scratch
mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commit History958 Commits.github.github  appendix-Aappendix-A  appendix-Dappendix-D  appendix-Eappendix-E  ch01ch01  ch02ch02  ch03ch03  ch04ch04  ch05ch05  ch06ch06  ch07ch07  pkg/llms_from_scratchpkg/llms_from_scratch  reasoning-from-scratch @ 2fc0f00reasoning-from-scratch @ 2fc0f00  setupsetup  .gitignore.gitignore  .gitmodules.gitmodules  CITATION.cffCITATION.cff  LICENSE.txtLICENSE.txt  README.mdREADME.md  pixi.tomlpixi.toml  pyproject.tomlpyproject.toml  requirements.txtrequirements.txt  View all filesRepository files navigationREADMELicenseBuild a Large Language Model (From Scratch)
This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book Build a Large Language Model (From Scratch).
In Build a Large Language Model (From Scratch), you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.
The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.
Link to the official source code repository
Link to the book at Manning (the publisher's website)
Link to the book page on Amazon.com
ISBN 9781633437166
To download a copy of this repository, click on the Download ZIP button or execute the following command in your terminal:
git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at https://github.com/rasbt/LLMs-from-scratch for the latest updates.)
Table of Contents
Please note that this README.md file is a Markdown (.md) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven't installed a Markdown editor yet, Ghostwriter is a good free option.
You can alternatively view this and other files on GitHub at https://github.com/rasbt/LLMs-from-scratch in your browser, which renders Markdown automatically.
Tip:
If you're seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the README.md file located in the setup directory.
Chapter Title
Main Code (for Quick Access)
All Code + Supplementary
Setup recommendations
-
-
Ch 1: Understanding Large Language Models
No code
-
Ch 2: Working with Text Data
- ch02.ipynb- dataloader.ipynb (summary)- exercise-solutions.ipynb
./ch02
Ch 3: Coding Attention Mechanisms
- ch03.ipynb- multihead-attention.ipynb (summary) - exercise-solutions.ipynb
./ch03
Ch 4: Implementing a GPT Model from Scratch
- ch04.ipynb- gpt.py (summary)- exercise-solutions.ipynb
./ch04
Ch 5: Pretraining on Unlabeled Data
- ch05.ipynb- gpt_train.py (summary) - gpt_generate.py (summary) - exercise-solutions.ipynb
./ch05
Ch 6: Finetuning for Text Classification
- ch06.ipynb - gpt_class_finetune.py - exercise-solutions.ipynb
./ch06
Ch 7: Finetuning to Follow Instructions
- ch07.ipynb- gpt_instruction_finetuning.py (summary)- ollama_evaluate.py (summary)- exercise-solutions.ipynb
./ch07
Appendix A: Introduction to PyTorch
- code-part1.ipynb- code-part2.ipynb- DDP-script.py- exercise-solutions.ipynb
./appendix-A
Appendix B: References and Further Reading
No code
-
Appendix C: Exercise Solutions
No code
-
Appendix D: Adding Bells and Whistles to the Training Loop
- appendix-D.ipynb
./appendix-D
Appendix E: Parameter-efficient Finetuning with LoRA
- appendix-E.ipynb
./appendix-E
The mental model below summarizes the contents covered in this book.
Prerequisites
The most important prerequisite is a strong foundation in Python programming.
With this knowledge, you will be well prepared to explore the fascinating world of LLMs
and understand the concepts and code examples presented in this book.
If you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.
This book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs, helpful for learning about the essentials.
Hardware Requirements
The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the setup doc for additional recommendations.)
Video Course
A 17-hour and 15-minute companion video course where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book's structure so that it can be used as a standalone alternative to the book or complementary code-along resource.
Companion Book / Sequel
Build A Reasoning Model (From Scratch), while a standalone book, can be considered as a sequel to Build A Large Language Model (From Scratch).
It starts with a pretrained model and implements different reasoning approaches, including inference-time scaling, reinforcement learning, and distillation, to improve the model's reasoning capabilities.
Similar to Build A Large Language Model (From Scratch), Build A Reasoning Model (From Scratch) takes a hands-on approach implementing these methods from scratch.
Amazon link (TBD)
Manning link
GitHub repository
Exercises
Each chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example,
./ch02/01_main-chapter-code/exercise-solutions.ipynb.
In addition to the code exercises, you can download a free 170-page PDF titled
Test Yourself On Build a Large Language Model (From Scratch) from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.
Bonus Material
Several folders contain optional materials as a bonus for interested readers:
Setup
Python Setup Tips
Installing Python Packages and Libraries Used In This Book
Docker Environment Setup Guide
Chapter 2: Working with text data
Byte Pair Encoding (BPE) Tokenizer From Scratch
Comparing Various Byte Pair Encoding (BPE) Implementations
Understanding the Difference Between Embedding Layers and Linear Layers
Dataloader Intuition with Simple Numbers
Chapter 3: Coding attention mechanisms
Comparing Efficient Multi-Head Attention Implementations
Understanding PyTorch Buffers
Chapter 4: Implementing a GPT model from scratch
FLOPS Analysis
KV Cache
Chapter 5: Pretraining on unlabeled data:
Alternative Weight Loading Methods
Pretraining GPT on the Project Gutenberg Dataset
Adding Bells and Whistles to the Training Loop
Optimizing Hyperparameters for Pretraining
Building a User Interface to Interact With the Pretrained LLM
Converting GPT to Llama
Llama 3.2 From Scratch
Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch
Gemma 3 From Scratch
Memory-efficient Model Weight Loading
Extending the Tiktoken BPE Tokenizer with New Tokens
PyTorch Performance Tips for Faster LLM Training
Chapter 6: Finetuning for classification
Additional experiments finetuning different layers and using larger models
Finetuning different models on 50k IMDb movie review dataset
Building a User Interface to Interact With the GPT-based Spam Classifier
Chapter 7: Finetuning to follow instructions
Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries
Evaluating Instruction Responses Using the OpenAI API and Ollama
Generating a Dataset for Instruction Finetuning
Improving a Dataset for Instruction Finetuning
Generating a Preference Dataset with Llama 3.1 70B and Ollama
Direct Preference Optimization (DPO) for LLM Alignment
Building a User Interface to Interact With the Instruction Finetuned GPT Model
Questions, Feedback, and Contributing to This Repository
I welcome all sorts of feedback, best shared via the Manning Forum or GitHub Discussions. Likewise, if you have any questions or just want to bounce ideas off others, please don't hesitate to post these in the forum as well.
Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.
Citation
If you find this book or code useful for your research, please consider citing it.
Chicago-style citation:
Raschka, Sebastian. Build A Large Language Model (From Scratch). Manning, 2024. ISBN: 978-1633437166.
BibTeX entry:
@book{build-llms-from-scratch-book,
author
= {Sebastian Raschka},
title
= {Build A Large Language Model (From Scratch)},
publisher
= {Manning},
year
= {2024},
isbn
= {978-1633437166},
url
= {https://www.manning.com/books/build-a-large-language-model-from-scratch},
github
= {https://github.com/rasbt/LLMs-from-scratch}
}
About
Implement a ChatGPT-like LLM in PyTorch from scratch, step by step
amzn.to/4fqvn0D
Topics
python
machine-learning
ai
deep-learning
pytorch
artificial-intelligence
transformer
gpt
language-model
from-scratch
large-language-models
llm
chatgpt
Resources
Readme
License
View license
Uh oh!
There was an error while loading. Please reload this page.
Activity
Stars
72.8k
stars
Watchers
629
watching
Forks
10.6k
forks
Report repository
Uh oh!
There was an error while loading. Please reload this page.
Contributors
51
+ 37 contributors
Languages
Jupyter Notebook
76.2%
Python
23.8%
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.