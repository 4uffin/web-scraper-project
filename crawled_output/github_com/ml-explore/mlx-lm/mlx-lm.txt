GitHub - ml-explore/mlx-lm: Run LLMs with MLX
Skip to content
Navigation Menu
Toggle navigation
Sign in
Appearance settings
Platform
GitHub Copilot
Write better code with AI
GitHub Spark
New
Build and deploy intelligent apps
GitHub Models
New
Manage and compare prompts
GitHub Advanced Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
Why GitHub
Documentation
GitHub Skills
Blog
Integrations
GitHub Marketplace
View all features
Solutions
By company size
Enterprises
Small and medium teams
Startups
Nonprofits
By use case
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
Events & Webinars
Ebooks & Whitepapers
Customer Stories
Partners
Executive Insights
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
GitHub Advanced Security
Enterprise-grade security features
Copilot for business
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our documentation.
Cancel
Create saved search
Sign in
Sign up
Appearance settings
Resetting focus
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
ml-explore
/
mlx-lm
Public
Notifications
You must be signed in to change notification settings
Fork
236
Star
2k
Run LLMs with MLX
License
MIT license
2k
stars
236
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
38
Pull requests
27
Discussions
Security
Uh oh!
There was an error while loading. Please reload this page.
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Security
Insights
ml-explore/mlx-lm
mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commit History559 Commits.circleci.circleci  mlx_lmmlx_lm  teststests  .gitignore.gitignore  .pre-commit-config.yaml.pre-commit-config.yaml  ACKNOWLEDGMENTS.mdACKNOWLEDGMENTS.md  CODE_OF_CONDUCT.mdCODE_OF_CONDUCT.md  CONTRIBUTING.mdCONTRIBUTING.md  LICENSELICENSE  MANIFEST.inMANIFEST.in  README.mdREADME.md  requirements.txtrequirements.txt  setup.pysetup.py  View all filesRepository files navigationREADMECode of conductContributingMIT licenseMLX LM
MLX LM is a Python package for generating text and fine-tuning large language
models on Apple silicon with MLX.
Some key features include:
Integration with the Hugging Face Hub to easily use thousands of LLMs with a
single command.
Support for quantizing and uploading models to the Hugging Face Hub.
Low-rank and full model
fine-tuning
with support for quantized models.
Distributed inference and fine-tuning with mx.distributed
The easiest way to get started is to install the mlx-lm package:
With pip:
pip install mlx-lm
With conda:
conda install -c conda-forge mlx-lm
Quick Start
To generate text with an LLM use:
mlx_lm.generate --prompt "How tall is Mt Everest?"
To chat with an LLM use:
mlx_lm.chat
This will give you a chat REPL that you can use to interact with the LLM. The
chat context is preserved during the lifetime of the REPL.
Commands in mlx-lm typically take command line options which let you specify
the model, sampling parameters, and more. Use -h to see a list of available
options for a command, e.g.:
mlx_lm.generate -h
Python API
You can use mlx-lm as a module:
from mlx_lm import load, generate
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
prompt = "Write a story about Einstein"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
messages, add_generation_prompt=True
)
text = generate(model, tokenizer, prompt=prompt, verbose=True)
To see a description of all the arguments you can do:
>>> help(generate)
Check out the generation
example
to see how to use the API in more detail.
The mlx-lm package also comes with functionality to quantize and optionally
upload models to the Hugging Face Hub.
You can convert models using the Python API:
from mlx_lm import convert
repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"
convert(repo, quantize=True, upload_repo=upload_repo)
This will generate a 4-bit quantized Mistral 7B and upload it to the repo
mlx-community/My-Mistral-7B-Instruct-v0.3-4bit. It will also save the
converted model in the path mlx_model by default.
To see a description of all the arguments you can do:
>>> help(convert)
Streaming
For streaming generation, use the stream_generate function. This yields
a generation response object.
For example,
from mlx_lm import load, stream_generate
repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)
prompt = "Write a story about Einstein"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
messages, add_generation_prompt=True
)
for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
print(response.text, end="", flush=True)
print()
Sampling
The generate and stream_generate functions accept sampler and
logits_processors keyword arguments. A sampler is any callable which accepts
a possibly batched logits array and returns an array of sampled tokens.
The
logits_processors must be a list of callables which take the token history
and current logits as input and return the processed logits. The logits
processors are applied in order.
Some standard sampling functions and logits processors are provided in
mlx_lm.sample_utils.
Command Line
You can also use mlx-lm from the command line with:
mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
This will download a Mistral 7B model from the Hugging Face Hub and generate
text using the given prompt.
For a full list of options run:
mlx_lm.generate --help
To quantize a model from the command line run:
mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
For more options run:
mlx_lm.convert --help
You can upload new models to Hugging Face by specifying --upload-repo to
convert. For example, to upload a quantized Mistral-7B model to the
MLX Hugging Face community you can do:
mlx_lm.convert \
--hf-path mistralai/Mistral-7B-Instruct-v0.3 \
-q \
--upload-repo mlx-community/my-4bit-mistral
Models can also be converted and quantized directly in the
mlx-my-repo Hugging
Face Space.
Long Prompts and Generations
mlx-lm has some tools to scale efficiently to long prompts and generations:
A rotating fixed-size key-value cache.
Prompt caching
To use the rotating key-value cache pass the argument --max-kv-size n where
n can be any integer. Smaller values like 512 will use very little RAM but
result in worse quality. Larger values like 4096 or higher will use more RAM
but have better quality.
Caching prompts can substantially speedup reusing the same long context with
different queries. To cache a prompt use mlx_lm.cache_prompt. For example:
cat prompt.txt | mlx_lm.cache_prompt \
--model mistralai/Mistral-7B-Instruct-v0.3 \
--prompt - \
--prompt-cache-file mistral_prompt.safetensors
Then use the cached prompt with mlx_lm.generate:
mlx_lm.generate \
--prompt-cache-file mistral_prompt.safetensors \
--prompt "\nSummarize the above text."
The cached prompt is treated as a prefix to the supplied prompt. Also notice
when using a cached prompt, the model to use is read from the cache and need
not be supplied explicitly.
Prompt caching can also be used in the Python API in order to avoid
recomputing the prompt. This is useful in multi-turn dialogues or across
requests that use the same context. See the
example
for more usage details.
Supported Models
mlx-lm supports thousands of Hugging Face format LLMs. If the model you want to
run is not supported, file an
issue or better yet,
submit a pull request.
Here are a few examples of Hugging Face models that work with this example:
mistralai/Mistral-7B-v0.1
meta-llama/Llama-2-7b-hf
deepseek-ai/deepseek-coder-6.7b-instruct
01-ai/Yi-6B-Chat
microsoft/phi-2
mistralai/Mixtral-8x7B-Instruct-v0.1
Qwen/Qwen-7B
pfnet/plamo-13b
pfnet/plamo-13b-instruct
stabilityai/stablelm-2-zephyr-1_6b
internlm/internlm2-7b
tiiuae/falcon-mamba-7b-instruct
Most
Mistral,
Llama,
Phi-2,
and
Mixtral
style models should work out of the box.
For some models (such as Qwen and plamo) the tokenizer requires you to
enable the trust_remote_code option. You can do this by passing
--trust-remote-code in the command line. If you don't specify the flag
explicitly, you will be prompted to trust remote code in the terminal when
running the model.
For Qwen models you must also specify the eos_token. You can do this by
passing --eos-token "<|endoftext|>" in the command
line.
These options can also be set in the Python API. For example:
model, tokenizer = load(
"qwen/Qwen-7B",
tokenizer_config={"eos_token": "<|endoftext|>", "trust_remote_code": True},
)
Large Models
NoteThis requires macOS 15.0 or higher to work.
Models which are large relative to the total RAM available on the machine can
be slow. mlx-lm will attempt to make them faster by wiring the memory
occupied by the model and cache. This requires macOS 15 or higher to
work.
If you see the following warning message:
[WARNING] Generating with a model that requires ...
then the model will likely be slow on the given machine. If the model fits in
RAM then it can often be sped up by increasing the system wired memory limit.
To increase the limit, set the following sysctl:
sudo sysctl iogpu.wired_limit_mb=N
The value N should be larger than the size of the model in megabytes but
smaller than the memory size of the machine.
About
Run LLMs with MLX
Topics
mlx
llms
Resources
Readme
License
MIT license
Code of conduct
Code of conduct
Contributing
Contributing
Uh oh!
There was an error while loading. Please reload this page.
Activity
Custom properties
Stars
2k
stars
Watchers
29
watching
Forks
236
forks
Report repository
Releases
11
v0.27.1
Latest
Sep 4, 2025
+ 10 releases
Packages
0
No packages published
Used by 634
+ 626
Contributors
51
+ 37 contributors
Languages
Python
100.0%
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.