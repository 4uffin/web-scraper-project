GitHub - OpenMind/OM1: Modular AI runtime for robots
Skip to content
Navigation Menu
Toggle navigation
Sign in
Appearance settings
Platform
GitHub Copilot
Write better code with AI
GitHub Spark
New
Build and deploy intelligent apps
GitHub Models
New
Manage and compare prompts
GitHub Advanced Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
Why GitHub
Documentation
GitHub Skills
Blog
Integrations
GitHub Marketplace
MCP Registry
View all features
Solutions
By company size
Enterprises
Small and medium teams
Startups
Nonprofits
By use case
App Modernization
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
Events & Webinars
Ebooks & Whitepapers
Customer Stories
Partners
Executive Insights
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
GitHub Advanced Security
Enterprise-grade security features
Copilot for business
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our documentation.
Cancel
Create saved search
Sign in
Sign up
Appearance settings
Resetting focus
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
OpenMind
/
OM1
Public
Notifications
You must be signed in to change notification settings
Fork
116
Star
592
Modular AI runtime for robots
openmind.org
License
MIT license
592
stars
116
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
14
Pull requests
20
Discussions
Actions
Projects
0
Wiki
Security
Uh oh!
There was an error while loading. Please reload this page.
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Projects
Wiki
Security
Insights
OpenMind/OM1
mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commit History2,065 Commits.github.github  configconfig  cycloneddscyclonedds  docsdocs  gazebogazebo  mintlifymintlify  srcsrc  system_hw_testsystem_hw_test  teststests  .dockerignore.dockerignore  .gitignore.gitignore  .gitmodules.gitmodules  .pre-commit-config.yaml.pre-commit-config.yaml  .python-version.python-version  CONTRIBUTING.mdCONTRIBUTING.md  DockerfileDockerfile  LICENSELICENSE  README.mdREADME.md  docker-compose.ymldocker-compose.yml  env.exampleenv.example  libzenoh_backend_fs.dyliblibzenoh_backend_fs.dylib  mintlify.shmintlify.sh  pyproject.tomlpyproject.toml  robot_storage.json5robot_storage.json5  uv.lockuv.lock  yolov8n.ptyolov8n.pt  View all filesRepository files navigationREADMEContributingMIT license
Technical Paper |
Documentation |
X | Discord
OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.
Capabilities of OM1
Modular Architecture: Designed with Python for simplicity and seamless integration.
Data Input: Easily handles new data and sensors.
Hardware Support via Plugins: Supports new hardware through plugins for API endpoints and specific robot hardware connections to ROS2, Zenoh, and CycloneDDS. (We recommend Zenoh for all new development).
Web-Based Debugging Display: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
Pre-configured Endpoints: Supports Voice-to-Speech, OpenAI’s gpt-4o, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.
Architecture Overview
Getting Started - Hello World
To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to OpenAI 4o, which returns movement, speech and face action commands. These commands are displayed on WebSim along with basic timing and other debugging information.
Package Management and VENV
You will need the uv package manager.
Clone the Repo
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
Install Dependencies
For MacOS
brew install portaudio ffmpeg
For Linux
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
Obtain an OpenMind API Key
Obtain your API Key at OpenMind Portal. Copy it to config/spot.json5, replacing the openmind_free placeholder. Or, cp env.example .env and add your key to the .env.
Launching OM1
Run
uv run src/run.py spot
After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see getting started.
What's Next?
Try out some examples
Add new inputs and actions.
Design custom agents and robots by creating your own json5 config files with custom combinations of inputs and actions.
Change the system prompts in the configuration files (located in /config/) to create new behaviors.
Interfacing with New Robot Hardware
OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as backflip, run, gently pick up the red apple, move(0.37, 0, 0), and smile. An example is provided in actions/move_safe/connector/ros2.py:
...
elif output_interface.action == "shake paw":
if self.sport_client:
self.sport_client.Hello()
...
If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.
OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see Unitree's C++ SDK. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.
Recommended Development Platforms
OM1 is developed on:
Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)
Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
Generic Linux machines (running Ubuntu 22.04)
OM1 should run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.
Full Autonomy Guidance
We're excited to introduce full autonomy mode, where three services work together in a loop without manual intervention:
om1
unitree_go2_ros2_sdk – A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
om1-avatar – A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.
Intro to Backpack?
From research to real-world autonomy, a platform that learns, moves, and builds with you.
We'll shortly be releasing the BOM and details on DIY for the it.
Stay tuned!
Clone the following repos -
https://github.com/OpenMind/OM1.git
https://github.com/OpenMind/unitree_go2_ros2_sdk.git
https://github.com/OpenMind/OM1-avatar.git
Starting the system
To start all services, run the following commands:
For OM1
Setup the API key
For Bash: vim ~/.bashrc or ~/.bash_profile.
For Zsh: vim ~/.zshrc.
Add
export OM_API_KEY="your_api_key"
Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.
command: ["unitree_go2_autonomy_advance"]
cd OM1
docker-compose up om1 -d --no-build
For unitree_go2_ros2_sdk
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
For OM1-avatar
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
Detailed Documentation
More detailed documentation can be accessed at docs.openmind.org.
Contributing
Please make sure to read the Contributing Guide before making a pull request.
License
This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.
About
Modular AI runtime for robots
openmind.org
Topics
robotics
multiagent
ros2
zenoh
llm
Resources
Readme
License
MIT license
Contributing
Contributing
Uh oh!
There was an error while loading. Please reload this page.
Activity
Custom properties
Stars
592
stars
Watchers
14
watching
Forks
116
forks
Report repository
Releases
3
v1.0.0-beta.3
Latest
Sep 20, 2025
+ 2 releases
Packages
0
No packages published
Uh oh!
There was an error while loading. Please reload this page.
Contributors
26
+ 12 contributors
Languages
Python
64.7%
C++
19.7%
MDX
9.0%
C
2.2%
CMake
1.7%
HTML
1.4%
Other
1.3%
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.