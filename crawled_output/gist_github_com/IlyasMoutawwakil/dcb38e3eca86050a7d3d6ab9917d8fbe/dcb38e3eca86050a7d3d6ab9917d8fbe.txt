A self-contained performance benchmarking script, comparing SmolVLM2 on PyTorch and Optimum-Intel's OpenVINO integration · GitHub
Skip to content
Search Gists
Search Gists
All gists
Back to GitHub
Sign in
Sign up
Sign in
Sign up
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
Instantly share code, notes, and snippets.
IlyasMoutawwakil/openvino-vlm-benchmark.py
Last active
September 24, 2025 07:42
Show Gist options
Download ZIP
Star
1
(1)
You must be signed in to star a gist
Fork
0
(0)
You must be signed in to fork a gist
Embed
Embed
Embed this gist in your website.
Share
Copy sharable link for this gist.
Clone via HTTPS
Clone using the web URL.
Learn more about clone URLs
Clone this repository at &lt;script src=&quot;https://gist.github.com/IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe.js&quot;&gt;&lt;/script&gt;
Save IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe to your computer and use it in GitHub Desktop.
Code
Revisions
14
Stars
1
Embed
Embed
Embed this gist in your website.
Share
Copy sharable link for this gist.
Clone via HTTPS
Clone using the web URL.
Learn more about clone URLs
Clone this repository at &lt;script src=&quot;https://gist.github.com/IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe.js&quot;&gt;&lt;/script&gt;
Save IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe to your computer and use it in GitHub Desktop.
Download ZIP
A self-contained performance benchmarking script, comparing SmolVLM2 on PyTorch and Optimum-Intel's OpenVINO integration
Raw
openvino-vlm-benchmark.py
# /// script
# dependencies = [
#
"optimum-benchmark[openvino]@git+https://github.com/huggingface/optimum-benchmark.git@main",
#
"optimum-intel@git+https://github.com/huggingface/optimum-intel.git@main",
# ]
# ///
from argparse import ArgumentParser
from huggingface_hub import create_repo, upload_file
from optimum_benchmark import (
Benchmark,
BenchmarkConfig,
BenchmarkReport,
InferenceConfig,
OpenVINOConfig,
ProcessConfig,
PyTorchConfig,
)
from optimum_benchmark.logging_utils import setup_logging
from optimum_benchmark.plot_utils import plot_latencies, plot_throughputs
if __name__ == "__main__":
setup_logging(level="INFO", prefix="MAIN-PROCESS")
parser = ArgumentParser()
parser.add_argument(
"--model_id",
type=str,
default="HuggingFaceTB/SmolLM2-360M-Instruct",
help="The model to benchmark.",
)
parser.add_argument(
"--benchmark_repo_id",
type=str,
default="optimum-benchmark/OpenVINO-LLM-Benchmark",
help="The repository to store the benchmark results. Pass an empty string to disable pushing to the hub.",
)
args = parser.parse_args()
model_id = args.model_id
benchmark_repo_id = args.benchmark_repo_id
if benchmark_repo_id:
create_repo(benchmark_repo_id, repo_type="dataset", exist_ok=True)
# Defining benchmark configurations
launcher_config = ProcessConfig()
scenario_config = InferenceConfig(
latency=True,
input_shapes={"batch_size": 1, "sequence_length": 16},
generate_kwargs={"max_new_tokens": 16, "min_new_tokens": 16},
)
configs = {
"pytorch": PyTorchConfig(device="cpu", model=model_id, no_weights=True),
"openvino": OpenVINOConfig(device="cpu", model=model_id, no_weights=True),
"openvino-8bit-woq": OpenVINOConfig(device="cpu", model=model_id, no_weights=True, load_in_8bit=True),
"openvino-4bit-woq": OpenVINOConfig(device="cpu", model=model_id, no_weights=True, load_in_4bit=True),
}
# Running benchmarks (saved locally and pushed to the hub if benchmark_repo_id is not None)
for config_name, backend_config in configs.items():
benchmark_config = BenchmarkConfig(
name=f"{config_name}",
launcher=launcher_config,
scenario=scenario_config,
backend=backend_config,
)
benchmark_report = Benchmark.launch(benchmark_config)
benchmark = Benchmark(config=benchmark_config, report=benchmark_report)
benchmark_report.save_json(f"{config_name}_report.json")
benchmark_config.save_json(f"{config_name}_config.json")
benchmark.save_json(f"{config_name}_benchmark.json")
if benchmark_repo_id:
benchmark_report.push_to_hub(repo_id=benchmark_repo_id, filename=f"{config_name}_report.json")
benchmark_config.push_to_hub(repo_id=benchmark_repo_id, filename=f"{config_name}_config.json")
benchmark.push_to_hub(repo_id=benchmark_repo_id, filename=f"{config_name}_benchmark.json")
# Loading reports (from local files or from the hub if benchmark_repo_id is not None)
reports = {}
for config_name in configs.keys():
if benchmark_repo_id:
reports[config_name] = BenchmarkReport.from_hub(
repo_id=benchmark_repo_id, filename=f"{config_name}_report.json"
)
else:
reports[config_name] = BenchmarkReport.from_json(f"{config_name}_report.json")
# Plotting results (saved locally and uploaded to the hub if benchmark_repo_id is not None)
fig1, ax1 = plot_latencies(
reports,
target_name="prefill",
title=f"{model_id} - Prefill Latencies",
xlabel="Configurations",
ylabel="Latency",
)
fig1.savefig("prefill_latencies_boxplot.png")
fig2, ax2 = plot_throughputs(
reports,
target_name="decode",
title=f"{model_id} - Decode Throughput",
xlabel="Configurations",
ylabel="Throughput",
)
fig2.savefig("decode_throughput_barplot.png")
if benchmark_repo_id:
upload_file(
path_or_fileobj="prefill_latencies_boxplot.png",
path_in_repo="prefill_latencies_boxplot.png",
repo_id=benchmark_repo_id,
repo_type="dataset",
)
upload_file(
path_or_fileobj="decode_throughput_barplot.png",
path_in_repo="decode_throughput_barplot.png",
repo_id=benchmark_repo_id,
repo_type="dataset",
)
Copy link
Author
IlyasMoutawwakil
commented
Sep 22, 2025
•
edited
Loading
Uh oh!
There was an error while loading. Please reload this page.
HF repository with all configs/results/plots:
https://huggingface.co/datasets/optimum-benchmark/OpenVINO-VLM-Benchmark/tree/main
How to reproduce locally:
hf auth login
uv run "https://gist.githubusercontent.com/IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe/raw/openvino-vlm-benchmark.py" --benchmark_repo_id=NameSpace/RepoName
How to reproduce using HF Jobs on an Intel CPU:
hf auth login
hf jobs uv run --flavor "cpu-performance" --secrets HF_TOKEN=your_hf_token "https://gist.githubusercontent.com/IlyasMoutawwakil/dcb38e3eca86050a7d3d6ab9917d8fbe/raw/openvino-vlm-benchmark.py" --benchmark_repo_id=NameSpace/RepoName
Plots (taken directly from the hub):
Prefill Latency / TTFT (time to first token): from 372ms to 168ms (2.2x speedup)
Decode Throughput / TPS
(tokens per second): from 29tok/s to 77 tok/s (2.6x speedup)
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Sign up for free
to join this conversation on GitHub.
Already have an account?
Sign in to comment
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.