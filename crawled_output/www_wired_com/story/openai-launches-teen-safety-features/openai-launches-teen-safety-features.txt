OpenAI's Teen Safety Features Will Walk a Thin Line | WIREDSkip to main contentMenuSECURITYPOLITICSTHE BIG STORYBUSINESSSCIENCECULTUREREVIEWSMenuAccountAccountNewslettersSecurityPoliticsThe Big StoryBusinessScienceCultureReviewsChevronMoreExpandThe Big InterviewMagazineThe New Era of Business TravelEventsWIRED InsiderWIRED ConsultingNewslettersPodcastsVideoMerchSearchSearchSign InSign InBy Kylie RobisonBusinessSep 16, 2025 4:53 PMOpenAI's Teen Safety Features Will Walk a Thin LineCEO Sam Altman announced an age-prediction system and new parental controls in a blog post on Tuesday.FacebookXEmailSave StoryPhoto-Illustration: WIRED Staff; Getty ImagesCommentLoaderSave StorySave this storyCommentLoaderSave StorySave this storyOpenAI announced new teen safety features for ChatGPT on Tuesday as part of an ongoing effort to respond to concerns about how minors engage with chatbots. The company is building an age-prediction system that identifies if a user is under 18 years old and routes them to an “age-appropriate” system that blocks graphic sexual content. If the system detects that the user is considering suicide or self-harm, it will contact the user’s parents. In cases of imminent danger, if a user's parents are unreachable, the system may contact the authorities.In a blog post about the announcement, CEO Sam Altman wrote that the company is attempting to balance freedom, privacy, and teen safety.“We realize that these principles are in conflict, and not everyone will agree with how we are resolving that conflict,” Altman wrote. “These are difficult decisions, but after talking with experts, this is what we think is best and want to be transparent in our intentions.”While OpenAI tends to prioritize privacy and freedom for adult users, for teens the company says it puts safety first. By the end of September, the company will roll out parental controls so that parents can link their child’s account to their own, allowing them to manage the conversations and disable features. Parents can also receive notifications when “the system detects their teen is in a moment of acute distress,” according to the company’s blog post, and set limits on the times of day their children can use ChatGPT.The moves come as deeply troubling headlines continue to surface about people dying by suicide or committing violence against family members after engaging in lengthy conversations with AI chatbots. Lawmakers have taken notice, and both Meta and OpenAI are under scrutiny. Earlier this month, the Federal Trade Commission asked Meta, OpenAI, Google, and other AI firms to hand over information about how their technologies impact kids, according to Bloomberg.At the same time, OpenAI is still under a court order mandating that it preserve consumer chats indefinitely—a fact that the company is extremely unhappy about, according to sources I’ve spoken to. Today’s news is both an important step toward protecting minors and a savvy PR move to reinforce the idea that conversations with chatbots are so personal that consumer privacy should only be breached in the most extreme circumstances.“A Sexbot Avatar in ChatGPT”From the sources I’ve spoken to at OpenAI, the burden of protecting users weighs heavily on many researchers. They want to create a user experience that is fun and engaging, but it can quickly veer into becoming disastrously sycophantic. It's positive that companies like OpenAI are taking steps to protect minors. At the same time, in the absence of federal regulation, there's still nothing forcing these firms to do the right thing.In a recent interview, Tucker Carlson pushed Altman to answer exactly who is making these decisions that impact the rest of us. The OpenAI chief pointed to the model behavior team, which is responsible for tuning the model for certain attributes. “The person I think you should hold accountable for those calls is me,” Altman added. “Like, I’m a public face. Eventually, like, I’m the one that can overrule one of those decisions or our board.”He’s right, yet some of the imminent harms seem to escape him. In another podcast interview with YouTuber Cleo Abrams, Altman said that “sometimes we do get tempted” to launch products “that would really juice growth.” He added: “We haven’t put a sexbot avatar in ChatGPT yet.” Yet! How strange.OpenAI recently released research on who uses ChatGPT, and how they use it. That research excluded users who were under the age of 18. We don’t yet have a full understanding of how teens are using AI, and it’s an important question to answer before the situation grows more dire.Sources SayElon Musk’s xAI is suing a former staffer who left the company to join OpenAI, alleging in a complaint that he misappropriated trade secrets and confidential information. In the current era of AI companies swapping staffers for multimillion-dollar compensation packages, I’m sure we’ll see more of these types of lawsuits pop up.The staffer in question, Xuechen Li, never made it to OpenAI’s internal Slack, according to two sources at the company. It’s unclear whether his offer was rescinded, or if he was onboarded only to be let go. OpenAI and Li did not respond to WIRED’s request for comment.This is an edition of Kylie Robison’s Model Behavior newsletter. Read previous newsletters here.CommentsBack to topTriangleYou Might Also Like …In your inbox: Will Knight's AI Lab explores advances in AITesla's trillion-dollar bet that it's more than just carsBig Interview: Hasan Piker will never run for officeThe doomers who insist AI will kill us allWatch: Uncanny Valley live with Jack Conte, CEO of PatreonKylie Robison is a senior correspondent at WIRED covering the business of artificial intelligence. She was previously a reporter at The Verge, Fortune, and Business Insider. Please send story tips (no PR pitches) to @kylie.01 on Signal. ... Read MoreSenior correspondentTopicsModel Behaviorartificial intelligenceOpenAISafetyprivacyalgorithmsSam AltmanTeensRead MoreThe United Arab Emirates Releases a Tiny But Powerful AI ModelK2 Think compares well with reasoning models from OpenAI and DeepSeek but is smaller and more efficient, say researchers based in Abu Dhabi.Distillation Can Make AI Models Smaller and CheaperA fundamental technique lets researchers use a big, expensive model to train another model for less.The Doomers Who Insist AI Will Kill Us AllEliezer Yudkowsky, AI’s prince of doom, explains why computers will kill us and provides an unrealistic plan to stop it.Researchers Are Already Leaving Meta’s New Superintelligence LabCEO Mark Zuckerberg went on a recruiting blitz to lure top AI researchers to Meta. WIRED has confirmed that three recent hires have now resigned.Watch Our Livestream Replay: WIRED’s AI Power SummitWIRED gathered a panel of leaders across technology, politics, and media to tell you everything you need to know about the future of generative AI.Anthropic Settles High-Profile AI Copyright Lawsuit Brought by Book AuthorsAnthropic faced the prospect of more than $1 trillion in damages, a sum that could have threatened the company’s survival if the case went to trial.YouTube Thinks AI Is Its Next Big BangOn its 20th anniversary, YouTube is venturing into an era of AI-generated video, and may never be the same.This AI-Powered Robot Keeps Going Even if You Attack It With a ChainsawA single AI model trained to control numerous robotic bodies can operate unfamiliar hardware and adapt eerily well to serious injuries.I Wasn’t Sure I Wanted Anthropic to Pay Me for My Books—I Do NowAnthropic agreed to a $1.5 billion settlement for authors whose books were used to train its AI model. As an author who fits that description, I’ve come around to the idea.Should AI Get Legal Rights?Model welfare is an emerging field of research that seeks to determine whether AI is conscious and, if so, how humanity should respond.Meet the Guys Betting Big on AI Gambling AgentsOnline gambling is a massive industry. The AI boom keeps booming. It was only a matter of time before people tried to put them together.Microsoft’s AI Chief Says Machine Consciousness Is an ‘Illusion’Mustafa Suleyman says that designing AI systems to exceed human intelligence—and to mimic behavior that suggests consciousness—would be “dangerous and misguided.”WIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives—from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries.SubscribeNewslettersTravelFAQWIRED StaffWIRED EducationEditorial StandardsArchiveRSSAccessibility HelpReviewsBuying GuidesMattressesElectric BikesSoundbarsStreaming GuidesWearablesTVsCouponsGift GuidesAdvertiseContact UsManage AccountJobsPress CenterCondé Nast StoreUser AgreementPrivacy PolicyYour California Privacy Rights© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad ChoicesSelect international siteUnited StatesLargeChevronItaliaJapónCzech Republic & SlovakiaFacebookXPinterestYouTubeInstagramTiktok