Sole-tenancy overview  |  Compute Engine Documentation  |  Google Cloud
Skip to main content
Documentation
Technology areas
close
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
close
Access and resources management
Costs and usage management
Google Cloud SDK, languages, frameworks, and tools
Infrastructure as code
Migration
Related sites
close
Google Cloud Home
Free Trial and Free Tier
Architecture Center
Blog
Contact Sales
Google Cloud Developer Center
Google Developer Center
Google Cloud Marketplace
Google Cloud Marketplace Documentation
Google Cloud Skills Boost
Google Cloud Solution Center
Google Cloud Support
Google Cloud Tech Youtube Channel
/
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Console
Sign in
Compute Engine
Guides
Reference
Samples
Resources
Contact Us
Start free
Documentation
Guides
Reference
Samples
Resources
Technology areas
More
Cross-product tools
More
Related sites
More
Console
Contact Us
Start free
Discover
Product overview
Compute Engine instances
Instance groups
Compute Engine machine resources
Machine resource guide
Machine type families
General-purpose machinesStorage-optimized machinesCompute-optimized machinesMemory-optimized machinesAccelerator-optimized machinesCoreMark scores of VM instances by familyCPU platforms
GPUs
About GPUs on Google CloudGPU machine typesArm VMsBare metal instances
Regions and zones
About regions and zonesGPU regions and zonesGlobal, regional, and zonal resources
Get started
Plan and prepare
Work with regions and zones
View available regions and zonesChange the default region or zone
Review VM deployment options
Choose a deployment strategyAbout VM provisioning modelsAbout VM tenancyDesign resilient systemsNetworking overview for VMs
Images and operating systems
OS images
About OS imagesOperating system detailsOS image lifecycleSupport policy
Premium operating systems
RHEL FAQSLES FAQUbuntu Pro FAQMicrosoft Licensing on Google CloudMicrosoft licenses FAQ
License Manager
About License ManagerUse License Manager for Microsoft OfficeView audit logs
Access control
Access control overviewManage access to Compute Engine resourcesManage resources by using custom constraintsIAM roles and permissionsService accountsName resources
Quickstarts
Create a Linux VMCreate a Windows Server VMCreate a managed instance group
Create instances
Instance creation overview
Create an instance
Create and start an instance
Create an instance and customize machine configuration
Create an instance with a custom hostnameCreate an instance with a custom machine typeSpecify a minimum CPU platform for an instance
Create an instance with attached GPUs
Overview
Accelerator-optimized instances
Create an A3 Ultra or A4 instanceCreate an A3, A2, or G2 instanceCreate an A3 instance with GPUDirect-TCPX enabledCreate an N1 instance that has attached GPUs
Create an instance and customize OS configuration
Create an instance from a public imageCreate an instance from a custom imageCreate an instance from a shared imageCreate an instance using a RHEL BYOS image
Create an instance and customize networking configuration
Create an instance in a specific subnetCreate an instance with multiple network interfacesCreate an instance with IPv6 addressesCreate an instance that uses Cloud RDMA
Create an instance and customize observability configuration
Create an instance for Ops Agent monitoring and loggingEnable virtual displays on an instance
Create an instance and customize security configuration
Create an instance that uses a user-managed service account
Create an instance using an existing configuration
Create an instance from an instance templateCreate an instance similar to an existing instance
Create a Spot VM
Spot VMs
About Spot VMsCreate and use Spot VMs
Preemptible VMs
About preemptible VMsCreate and use preemptible VMs
Create instances for specific workload types
Create a Google-configured, workload-optimized instanceCreate an HPC-ready instanceCreate and manage a Windows Server instanceCreate a SQL Server instance
Create custom images
Requirements to build custom imagesCreate custom imagesCreate custom Windows BYOL base imagesCreate custom Windows Server images
Create and manage instance templates
About instance templatesCreate instance templatesDeterministic instance templatesGet, list, and delete instance templates
Create multiple VMs
Create a managed instance group (MIG)
Basic scenarios for creating MIGsCreate a MIG in a single zoneCreate a MIG in multiple zones in a regionCreate a MIG with multiple machine typesCreate a MIG from an existing VMCreate a MIG with autoscalingCreate a MIG with preemptible VMsCreate a MIG with GPU VMsCreate a MIG with stateful configuration
Bulk creation of VMs
About bulk creation of VMsCreate VMs in bulkCreate GPU VMs in bulk
Create HPC clusters
Overview of HPC cluster creationCreate an RDMA-enabled HPC Slurm clusterBulk create HPC-optimized instances that use RDMA
Create MIGs for HPC workloads
Create a MIG that uses Flex-startCreate a MIG that uses reservations
Create sole-tenant VMs
Sole-tenancy overviewProvision a sole-tenant VMAdvanced maintenance control for sole-tenant nodesSole-tenancy best practicesSole-tenancy accounting FAQ
Create a virtual workstation
About creating virtual workstationsCreate a virtual Linux workstationCreate a virtual Windows workstationCreate a virtual Linux workstation with an attached GPUCreate a virtual Windows workstation with an attached GPU
Use nested virtualization
About nested virtualizationManage the nested virtualization constraintEnable nested virtualizationCreate nested VMs
Manage VM boot disks
Detach and reattach a boot diskCreate a customized boot disk
Migrate VMs
Choose a migration path
Bring your own licenses
Import disks and images
Prerequisites for importing and exporting VM images
Automatic import
Import virtual disksImport virtual appliances
Manual import
Manually import boot disksManually configure imported disksCreate a persistent disk image from an ISO file
Move a VM within Google Cloud
Move a VM between zonesMigrate a VM between networksCopy VMs between projects
Move an existing VM to a new VM
Connect to VMs
Connect to a VM
About SSH connections
Linux VMs
Connect to VMs
Connect through internal IP addresses
Connection options for internal-only VMsConnect using IAPConnect using a bastion hostConnect using Cloud VPNConnect as the root userConnect using service accountsConfigure apps to use SSH
Best practices
Securely connect to VMs
Windows VMs
Connect to Windows VMs using RDPConnect to a Windows VM's SACConnect to Windows VMs using SSHConnect to Windows VMs using PowerShell
Manage access to VMs
Linux VMs
Choose an access management methodAbout OS LoginSet up OS LoginSet up OS Login to require SSH certificatesEnable security keys with OS LoginManage OS Login in an organizationMonitor OS Login audit logs
Windows VMs
Manage accounts and credentials on Windows VMsAutomate Windows password generation
Manually manage SSH keys
Create SSH keysAdd SSH keys to VMsRestrict SSH keys from VMs
Best practices for securing SSH access
OverviewControl network accessControl SSH login accessProtect SSH credentialsAudit SSH accessManage tags for resources
Transfer files to or from a VM
Transfer files to Linux VMsTransfer files to Windows VMs
IP addresses
Internal DNS
Overview of internal DNSAccess VMs using internal DNS names
Use zonal DNS
Overview of zonal DNSSet zonal DNS as the defaultMigrate to zonal DNS
Create a PTR record for a VM
Verify VM identity
Manage storage
Choose a disk type
Disk types
About Hyperdisk
Hyperdisk overview
Choose a Hyperdisk type
Hyperdisk BalancedHyperdisk Balanced High AvailabilityHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputAbout Hyperdisk Storage PoolsAbout Persistent DiskExtreme Persistent DiskAbout Local SSD
Configure storage pools
Create a storage poolManage storage pools
Add disks to VMs
Create a VM with Local SSD disksCreate a VM with additional non-boot disksCreate a new HyperdiskCreate a new Persistent DiskAdd disks from a storage pool to VMsShare a disk between VMsAttach a disk to a VMMount in-memory RAM disks
Configure disks
Format and mount a non-boot disk on LinuxFormat and prepare a non-boot disk on Windows
Access disks attached to a VM
Best practice: Use persistent device namesSymbolic links to disks
Transfer data to disks attached to a VM
Transfer files to Linux VMsTransfer files to Windows VMs
View disk details
Encrypt disks
About disk encryptionEncrypt disks with customer-supplied encryption keysHelp protect resources by using Cloud KMS keys
Modify disks
Modify a HyperdiskChange the disk typeIncrease the size of a Persistent DiskModify a Persistent Disk
Evaluate disk performance
Hyperdisk performance overviewHyperdisk performance and size limitsPersistent Disk performance overview
Review disk performance
Review storage pool metricsReview disk performance metricsAnalyze provisioned IOPS and throughput
Benchmark disk performance
Benchmark Hyperdisk performanceBenchmark Persistent Disk performance on a Linux VMBenchmark Persistent Disk performance on a Windows VMBenchmark Local SSD performance
Make disks highly available
Replicate disks across regions
About Asynchronous ReplicationConfigure replicationManage replicationFailover and failback disksManage asynchronous disksManage consistency groupsReview performance metrics
Cross-zonal synchronous disk replication
About regional disksBuild high availability services using regional disksDesign considerations for resilient workloads with regional disksCreate and manage regional disksManage failures for regional disks
Back up and restore
Data protection options
Configure the default backup setting
Back up VMs
Use machine images
About machine imagesCreate machine imagesImport machine images from virtual appliances
Use Backup and DR backup plans
About backup plansCreate an instance with a backup planChange your instance's backup plan
Back up disks
Back up a disk in place
About instant snapshotsCreate and manage instant snapshotsCopy an instant snapshot to a different location
Back up a disk for disaster recovery
About disk snapshotsBest practices for disk snapshotsSet default storage location for globally scoped snapshotsSet creation and restore locations for regionally scoped snapshotsCreate disk snapshotsManage disk snapshots
Create application consistent snapshots
Create Linux application consistent snapshotsCreate a Windows disk snapshot (VSS snapshots)
Schedule disk backups
About snapshot schedulesCreate snapshot schedulesManage snapshot schedulesConfigure alerts for snapshot schedulesDuplicate a disk with clones
Restore from a backup
Create VMs from machine imagesRestore from a standard snapshotRestore from an instant snapshot
Recover a VM with a corrupted or full disk
Manage VMs
Basic operations and lifecycle
VM instance lifecycle
Schedule VM operations
Schedule a VM to start and stopLimit the run time of a VM
View VM properties
Detect if a VM is running in Compute EngineGet a list of VMsGet the details of a VMGet the UUID of a VMView the source image of a VMView referrers to VMsView network configuration of an instanceView the number of visible CPU cores
Stop or suspend a VM
Stop or suspend VMs overviewStop or restart a VM
Increase or decrease VM shutdown time
Increase shutdown time
Graceful shutdown overviewEnable graceful shutdownView graceful shutdownDisable graceful shutdownDecrease shutdown timeSuspend or resume a VMReset a VM
Update VM details
Rename a VMUpdate VM propertiesEdit the machine type of a VMAdd or remove GPUsChange the attached service account
Update the physical location of a VM
About placement policiesCreate and apply spread placement policies to VMsView placement policiesRemove or delete placement policies
Update network configuration for instances
Configure static external IP addressesConfigure static internal IP addressesConfigure IPv6 for instances and instance templatesUpdate network interfaces
Delete VMs
Delete a VMPrevent accidental VM deletion
Update VM tenancy
Manage groups of VMs
Work with managed VMs in a MIGView info about MIGs and managed instancesAdd or remove VMs in a MIGLimit the run time of VMs in a MIG
Add GPU VMs all at once in a MIG
About resize requestsCreate resize requestsView, cancel, or delete resize requests
Configure instance flexibility in a MIG
About instance flexibilityAdd instance flexibilityView instance flexibilityChange or remove instance flexibility
Distribute VMs across zones in a regional MIG
About regional MIGsAbout target distribution shapeSet a target distribution for VMs across zonesDisable and reenable proactive instance redistributionManually rebalance a regional MIGSimulate a zone outage for a regional MIG
Work with suspended and stopped VMs in a MIG
OverviewManually suspend or stop VMs in a MIGAccelerate scale out with suspended and stopped VMs
Apply new VM configurations in a MIG
About applying new VM configurations to VMs in a MIGAutomatically apply VM configuration updatesSelectively apply VM configuration updatesApply configuration updates during repairsOverride instance template properties with an all-instances configurationPerform one-click OS image upgrades
Maintain high availability during VM failures
About repairing VMs for high availability
Repair a VM when an application fails
Set up an application-based health check and autohealingMonitor VM health state changesDisable and enable health state change logsTurn off repairs in a MIG
Support a stateful workload with a MIG
About stateful MIGs
Configure stateful MIGs
Configure a stateful MIGConfigure stateful persistent disksConfigure stateful metadataConfigure stateful IP addressesApply, view, and remove stateful configurationHow stateful MIGs workHow operations affect preserved state
Group VMs together
Migrate an existing workload to a stateful managed instance groupGroup unmanaged VMs togetherDelete a MIG
Host maintenance events
About host eventsLive migration processSet the host maintenance policyQuery metadata server for noticesSimulate a host maintenance eventHandle GPU host maintenance eventsMonitor and plan for a host maintenance eventManually start host maintenance
Manage metadata
About VM metadataPredefined metadata keysSet and remove custom metadataView and query VM metadataSet and query guest attributes
Securing VMs
About Shielded VMsAbout Confidential VMsProtect resources with VPC Service ControlsMonitor security risks with Security Command Center
Manage operating systems
Guest environment
About the guest environmentInstall the guest environment
Guest agent
About the guest agentGuest agent functionalityConfigure the guest agent
Manage operating systems using VM Manager
Manage OS images
Image management best practicesImage families best practicesAccess Red Hat KnowledgebaseManage access to custom imagesSet up trusted image policiesExport a custom image to Cloud StorageSet image versions in an image familyDeprecate a custom imageDelete a custom image
Manage OS packages
Manage licenses
About licensesManage licensesLicense changes and restrictionsSwitch between PAYG and BYOSAppend RHEL ELS licensesUpgrade from Ubuntu to Ubuntu Pro
Use startup scripts
Startup scripts overviewUse startup scripts on Linux VMsUse startup scripts on Windows VMs
Run shutdown scripts
Configure NTP
Enable the virtual random number generator (Virtio RNG)
Deploy workloads
Set up authentication for workloads
Choose a workload authentication methodAuthenticate workloads to Google Cloud API using service accountsAuthenticate workloads to other workloads over mTLS
Agent for Compute Workloads overview
Web servers
Deploy an Apache serverDeploy an IIS serverDeploy a Flask server by using Terraform
Applications
Interactive: Build a to-do app with MongoDBDeploy an ASP.NET applicationSet up JoomlaSet up LAMPPerform blue/green deployments using Cloud Build
Send email from a VM
About sending emailSend email with SendGridSend email with MailgunSend email with Mailjet
Databases
MySQL
MySQL on Compute EngineInstall MySQL on Compute EngineConfigure MySQL on Compute EngineSet up client access with a private IP addressCloning a MySQL database on Compute EngineArchitectures for high availability of MySQL clusters on Compute EngineDeploying a highly available MySQL 5.6 cluster with DRBD on Compute Engine
PostgreSQL
Set up PostgreSQL on Compute EngineSet up a PostgreSQL data diskSet up PostgreSQL with hot standby
SQL Server
Best practices for SQL Server VMs
Create
Create a high-performance SQL Server VMAdd a SQL Server license to an existing Linux serverAdd a SQL Server license to an existing Windows serverConfigure SQL Server on Google Cloud Platform using Cloud Volumes Service
Configure
Set up AlwaysOn availability groups using an internal load balancerSet up AlwaysOn availability groups using a distributed network nameSet up a failover cluster VM that uses S2DSet up a failover cluster VM with multi-writer disksSet up a SQL Server cluster on Linux with Always On availability groups and Pacemaker
Migrate
Migrate a SQL Server database from AWS EC2 to Compute EngineCloning a Microsoft SQL Server database on Compute EngineBackup SQL Server databases to a Google Cloud Storage bucketLoad test SQL Server using HammerDBDisaster recovery for Microsoft SQL ServerDisaster recovery for Microsoft SQL server on Persistent diskDisaster recovery for Microsoft SQL server on HyperdiskDeploying Microsoft SQL Server for multi-regional disaster recoveryMigrate a SQL Server database from Windows to Linux
Redis
Deployment Options for Redis on Google Cloud
Containers
Containers on Compute EngineDeploy containers on VMs and managed instance groupsConfigure options to run your container
Transition from the container startup agent
Prepare for the shutdown of the startup agentMigrate containers that were deployed on VMs during VM creation
OpenShift workloads
Best practices for high availability with OpenShift
Microsoft Windows
Windows workloadsBest practices for Windows Server VMsSetting up Active DirectoryBest practices for running Active Directory on Google CloudDeploy Microsoft SharePoint Server on Compute EngineDeploying Microsoft Exchange Server 2016 on Compute Engine
Windows Server
Perform an in-place upgrade of Windows ServerRun Windows Server failover clustering
Others
Load testing
Distributed load testing using KubernetesSSH port forwarding and load testing
Analytics
Monte Carlo methods using Apache Spark
Machine learning
Run TensorFlow inference workloads with TensorRT5 and NVIDIA T4 GPU
Monitor
Monitor logs
View audit logsView usage reportsView Compute Engine operationsMigrate from activity logs to audit logsView activity logs
Monitor resources
Monitor VM and sole-tenant node usageObserve and monitor VMs
Monitor GPU performance
Monitor GPU performance on Linux VMsMonitor GPU performance on Windows VMs
Monitor disks
Monitor disk healthMonitor the replica states of regional persistent disk volumesMonitor Hyperdisk Storage PoolsMonitor disksMonitor reservations
Organize resources using labels
Scale
Autoscale groups of VMs
About autoscaling groups of VMs
Create and manage autoscalers
Scale based on CPU utilizationScale based on predictionsScale based on load balancing serving capacityScale based on Monitoring metricsScale based on schedulesUse an autoscaling policy with multiple signalsManage autoscalersUnderstand autoscaler decisionsView autoscaler logs
Autoscale node groups
Reserve VM capacity
Choose a reservation type
On-demand reservations
About on-demand reservations
Create an on-demand reservation
For a single projectFor multiple projectsCombine an on-demand reservation with a CUDModify an on-demand reservationDelete an on-demand reservation
Future reservations
About future reservations
Create a reservation request
For a single projectFor multiple projectsModify a reservation requestDelete a reservation request
Future reservations in calendar mode
About future reservations in calendar modeCreate a reservation request in calendar modeView reservations or reservation requestsConsume a reservationPrevent VMs from consuming reservations
Load balancing
About load balancing and scalingAdd an instance group to a load balancerRequest routing to a multi-region external HTTPS load balancerCross-region load balancing for Microsoft IIS backendsSet up Internal TCP/UDP Load Balancing
Build reliable and scalable applications
Use autohealing for highly available applicationsUse load balancing for highly available applicationsUse autoscaling for highly scalable applicationsGlobally autoscale a web service on Compute EnginePatterns for scalable and resilient applicationsPatterns for using floating IP addresses on Compute Engine
Optimize
Resource utilization
Use recommendations to manage resources
Apply machine type recommendations to VMsConfigure machine type recommendationsApply machine type recommendations to MIGsView and apply idle resources recommendationsView and understand VM insightsView and understand MIG insights
Manage idle VM recommendations
Idle VM recommendations overviewView and apply idle VM recommendationsConfigure idle VM recommendations
Manage reservation recommendations
Reservation recommendations overviewView and apply idle reservation recommendationsView and apply underutilized reservation recommendationsConfigure idle reservation recommendationsConfigure underutilized reservation recommendationsOvercommit CPUs on sole-tenant VMs
Manual live migration
About manual live migrationManually live migrate VMsShare sole-tenant node groupsNext generation dynamic resource management
Cost savings
Get discounts for committed usage
About commitments and committed use discounts (CUDs)Resource-based CUDs
Manage resource-based commitments
Renew commitments automaticallyExtend the term length of commitmentsMerge and split commitmentsUpgrade the term of commitmentsShare resource-based CUDs across projectsGet discounts for sustained usage
Disk performance
Optimize Hyperdisk performanceOptimize Persistent Disk performanceOptimize Local SSD performance
Workload performance
Set the number of threads per coreCustomize the number of visible CPU cores
Analyze the CPU performance using the PMU
PMU overviewEnable the PMU in VMsManage the PMU in VMs
Accelerated workloads with GPUs
GPUs on Compute Engine
About GPUs
Install drivers
Install GPU driversInstall drivers for NVIDIA RTX Virtual Workstations (vWS)Drivers for NVIDIA RTX Virtual Workstations (vWS)
Network performance
Network bandwidthUse Google Virtual NICUse IRDMA network driverUse IDPF network interfaceConfigure a VM with higher bandwidthReduce latency by using compact placement policiesOptimize TCP network performance and resiliencyBenchmark higher bandwidth VMsOptimize app latency with load balancingUse DPDK to improve network performance
Network performance and GPU VMs
Networking and GPU machinesUse higher network bandwidthPatterns for using multiple host NICs
Troubleshoot
General tips
Troubleshoot connectivity
Troubleshoot RDPTroubleshoot SSHTroubleshoot OS Login
Troubleshoot VMs
Troubleshoot VM operations
Troubleshoot VM creationTroubleshoot resource availability errorsTroubleshoot bulk API VM creationTroubleshoot VM reboots and shutdownsTroubleshoot VM suspension
Troubleshoot unresponsive VMs
Troubleshoot VM startupTroubleshoot fstab errorsTroubleshoot kernel panicCollecting core dumpsRescue an inaccessible VMTroubleshoot CPU soft lockups
Troubleshoot VM configurations
Troubleshoot Arm VMs
Troubleshoot GPU VMs
Troubleshoot NVIDIA GPU errorsGenerate a NVIDIA bug report for Blackwell GPUsTroubleshoot nested virtualizationTroubleshoot using VM screenshotsTroubleshoot sole-tenant nodesTroubleshoot VM performance issuesTroubleshoot sudoers files
Troubleshoot Windows VMs
Troubleshoot Windows VMsCollecting diagnostic information
Troubleshoot using the serial console
Troubleshoot using the serial consoleViewing serial port output
Troubleshoot instance groups
Troubleshoot managed instance groups (MIGs)
Troubleshoot OS management
Troubleshoot licensesTroubleshoot image import and exportTroubleshooting SLES pay-as-you-go registrationTroubleshooting Ubuntu Pro Registration
Troubleshoot metadata server
Troubleshoot metadata server
Troubleshoot networking issues
Troubleshoot common networking issuesTroubleshoot gVNICTroubleshoot VM performance issues
Troubleshoot storage
Troubleshoot full disks and disk resizingTroubleshoot NVMe disksTroubleshoot instant snapshotsTroubleshoot standard snapshots
Troubleshoot reservations and commitments
Troubleshoot reservation creationTroubleshoot reservation consumptionTroubleshooting reservation monitoringTroubleshoot reservation updatesTroubleshoot future reservation creation and updatesTroubleshoot automatic commitment renewal
Troubleshoot quota errors
Troubleshoot concurrent operation quota errors
Troubleshoot workload authentication
Troubleshoot default service accountsTroubleshoot workload to workload authentication
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Google Cloud SDK, languages, frameworks, and tools
Infrastructure as code
Migration
Google Cloud Home
Free Trial and Free Tier
Architecture Center
Blog
Contact Sales
Google Cloud Developer Center
Google Developer Center
Google Cloud Marketplace
Google Cloud Marketplace Documentation
Google Cloud Skills Boost
Google Cloud Solution Center
Google Cloud Support
Google Cloud Tech Youtube Channel
Home
Compute Engine
Documentation
Guides
Send feedback
Sole-tenancy overview
Stay organized with collections
Save and categorize content based on your preferences.
Linux
Windows
This document describes sole-tenant nodes. For information about how to provision
VMs on sole-tenant nodes, see Provisioning VMs on sole-tenant
nodes.
Sole-tenancy lets you have exclusive access to a sole-tenant node, which is a
physical Compute Engine server that is dedicated to hosting only your
project's VMs. Use sole-tenant nodes to keep your VMs physically separated from
VMs in other projects, or to group your VMs together on the same host hardware
as shown in the following diagram. You can also create a sole-tenant node group
and specify whether you want to
share it with other projects or with the entire organization.
Figure 1: A multi-tenant host versus a sole-tenant node.
VMs running on sole-tenant nodes can use the same Compute Engine
features as other VMs, including transparent scheduling and block storage, but
with an added layer of hardware isolation. To give you full control over the VMs
on the physical server, each sole-tenant node maintains a one-to-one mapping to
the physical server that is backing the node.
Within a sole-tenant node, you can provision multiple VMs on machine types of
various sizes, which lets you efficiently use the underlying resources
of the dedicated host hardware. Also, if you choose not to share the host
hardware with other projects, you can meet security or compliance requirements
with workloads that require physical isolation from other workloads or VMs. If
your workload requires sole tenancy only temporarily, you can modify
VM tenancy as necessary.
Sole-tenant nodes can help you meet dedicated hardware requirements for
bring your own license (BYOL)
scenarios that require per-core or per-processor licenses. When you use
sole-tenant nodes, you have some visibility into the underlying hardware, which
lets you track core and processor usage. To track this usage,
Compute Engine reports the ID of the physical server on which a VM is
scheduled. Then, by using Cloud Logging, you can
view the historical server usage of a VM.
To optimize the use of the host hardware, you can do the following:
Overcommit CPUs on sole-tenant VMs
Share sole-tenant node groups
Manually live migrate sole-tenant VMs
Through a configurable host maintenance policy, you can control the behavior of
sole-tenant VMs while their host is undergoing maintenance. You can specify when
maintenance occurs, and whether the VMs maintain affinity with a specific
physical server or are moved to other sole-tenant nodes within a node group.
Workload considerations
The following types of workloads might benefit from using sole-tenant nodes:
Gaming workloads with performance requirements
Finance or healthcare workloads with security and compliance requirements
Windows workloads with licensing requirements
Machine learning, data processing, or image rendering workloads. For these
workloads, consider reserving GPUs.
Workloads requiring increased I/O operations per second (IOPS) and
decreased latency, or workloads that use temporary storage in the form of
caches, processing space, or low-value data. For these workloads, consider
reserving Local SSD disks.
Node templates
A node template is a regional resource that defines the properties of each node
in a node group. When you create a node group from a node template, the
properties of the node template are immutably copied to each node in the node
group.
When you create a node template you must specify a node type. You can optionally
specify node affinity labels when you create a node template. You can only
specify node affinity labels on a node template. You can't specify node affinity
labels on a node group.
Node types
When configuring a node template, specify a node type to apply to all nodes
within a node group created based on the node template. The sole-tenant node
type, referenced by the node template, specifies the total amount of vCPU cores
and memory for nodes created in node groups that use that template. For example,
the n2-node-80-640 node type has 80 vCPUs and 640 GB of memory.
The VMs that you add to a sole-tenant node must have the same machine type as
the node type that you specify in the node template. For example, n2
sole-tenant node types are only compatible with VMs created with the n2
machine type. You can add VMs to a sole-tenant node until the total amount of
vCPUs or memory exceeds the capacity of the node.
When you create a node group using a node template, each node in the node group
inherits the node template's node type specifications. A node type applies to
each individual node within a node group, not to all of the nodes in the group
uniformly. So, if you create a node group with two nodes that are both of the
n2-node-80-640 node type, each node is allocated 80 vCPUs and 640 GB of
memory.
Depending on your workload requirements, you might fill the node with multiple
smaller VMs running on machine types of various sizes, including
predefined machine types,
custom machine types, and
machine types with extended memory.
When a node is full, you cannot schedule additional VMs on that node.
The following table displays the available node types. To see a list of the
node types available for your project, run the
gcloud compute sole-tenancy node-types list
command or create a
nodeTypes.list REST request.
Node type
Processor
vCPU
GB
vCPU:GB
Sockets
Cores:Socket
Total cores
Max VMs allowed
c2-node-60-240
Cascade Lake
60
240
1:4
2
18
36
15
c3-node-176-352
Sapphire Rapids
176
352
1:2
2
48
96
44
c3-node-176-704
Sapphire Rapids
176
704
1:4
2
48
96
44
c3-node-176-1408
Sapphire Rapids
176
1408
1:8
2
48
96
44
c3d-node-360-708
AMD EPYC Genoa
360
708
1:2
2
96
192
34
c3d-node-360-1440
AMD EPYC Genoa
360
1440
1:4
2
96
192
40
c3d-node-360-2880
AMD EPYC Genoa
360
2880
1:8
2
96
192
40
c4-node-192-384
Emerald Rapids
192
384
1:2
2
60
120
40
c4-node-192-720
Emerald Rapids
192
720
1:3.75
2
60
120
30
c4-node-192-1488
Emerald Rapids
192
1,488
1:7.75
2
60
120
30
c4a-node-72-144
Google Axion
72
144
1:2
1
80
80
22
c4a-node-72-288
Google Axion
72
288
1:4
1
80
80
22
c4a-node-72-576
Google Axion
72
576
1:8
1
80
80
36
c4d-node-384-720
AMD EPYC Turin
384
744
1:2
2
96
192
24
c4d-node-384-1488
AMD EPYC Turin
384
1488
1:4
2
96
192
25
c4d-node-384-3024
AMD EPYC Turin
384
3024
1:8
2
96
192
25
g2-node-96-384
Cascade Lake
96
384
1:4
2
28
56
8
g2-node-96-432
Cascade Lake
96
432
1:4.5
2
28
56
8
h3-node-88-352
Sapphire Rapids
88
352
1:4
2
48
96
1
m1-node-96-1433
Skylake
96
1433
1:14.93
2
28
56
1
m1-node-160-3844
Broadwell E7
160
3844
1:24
4
22
88
4
m2-node-416-8832
Cascade Lake
416
8832
1:21.23
8
28
224
1
m2-node-416-11776
Cascade Lake
416
11776
1:28.31
8
28
224
2
m3-node-128-1952
Ice Lake
128
1952
1:15.25
2
36
72
2
m3-node-128-3904
Ice Lake
128
3904
1:30.5
2
36
72
2
m4-node-224-2976
Emerald Rapids
224
2976
1:13.3
2
112
224
1
m4-node-224-5952
Emerald Rapids
224
5952
1:26.7
2
112
224
1
n1-node-96-624
Skylake
96
624
1:6.5
2
28
56
96
n2-node-80-640
Cascade Lake
80
640
1:8
2
24
48
80
n2-node-128-864
Ice Lake
128
864
1:6.75
2
36
72
128
n2d-node-224-896
AMD EPYC Rome
224
896
1:4
2
64
128
112
n2d-node-224-1792
AMD EPYC Milan
224
1792
1:8
2
64
128
112
n4-node-224-1372
Emerald Rapids
224
1372
1:6
2
60
120
90
For information about the prices of these node types, see
sole-tenant node pricing.
All nodes let you schedule VMs of different shapes. Node n type are
general-purpose nodes, on which you can schedule
custom machine type
instances. For recommendations about which node type to choose, see
Recommendations for machine types.
For information about performance, see
CPU platforms.
Node groups and VM provisioning
Sole-tenant node templates define the properties of a node group, and you must
create a node template before creating a node group in a Google Cloud
zone. When you create a group, specify the host maintenance policy for VMs
on the node group, the number of nodes for the node group, and whether to
share it with other projects or with the entire organization.
A node group can have zero or more nodes; for example, you can reduce the number
of nodes in a node group to zero when you don't need to run any VMs on
nodes in the group, or you can enable the
node group autoscaler to manage
the size of the node group automatically.
Before provisioning VMs on sole-tenant nodes, you must create a sole-tenant node
group. A node group is a homogeneous set of sole-tenant nodes in a specific
zone. Node groups can contain multiple VMs from the same
machine series running on
machine types of various sizes, as long as the machine type has 2 or more vCPUs.
When you create a node group, enable autoscaling
so that the size of the group adjusts automatically to meet the requirements of
your workload. If your workload requirements are static, you can manually
specify the size of the node group.
After creating a node group, you can provision VMs on the group or on a specific
node within the group. For further control, use node affinity labels to schedule
VMs on any node with matching affinity labels.
After you've provisioned VMs on node groups, and optionally assigned affinity
labels to provision VMs on specific node groups or nodes, consider
labeling your resources to help manage your
VMs. Labels are key-value pairs that can help you categorize your VMs so that
you can view them in aggregate for reasons such as billing. For example, you can
use labels to mark the role of a VM, its tenancy, the license type, or its
location.
Host maintenance policy
Depending on your licensing scenarios and workloads, you might want to limit the
number of physical cores used by your VMs. The host maintenance policy you choose
might depend on, for example, your licensing or compliance requirements, or, you
might want to choose a policy that lets you limit usage of physical servers.
With all of these policies, your VMs remain on dedicated hardware.
When you schedule VMs on
sole-tenant nodes, you can choose from
the following three different host maintenance policy options, which let you
determine how and whether Compute Engine
live migrates VMs during
host events,
which occur approximately every 4 to 6 weeks. During maintenance,
Compute Engine live migrates, as a group, all of the VMs on the host
to a different sole-tenant node, but, in some cases, Compute Engine
might break up the VMs into smaller groups and live migrate each smaller group
of VMs to separate sole-tenant nodes.
Default host maintenance policy
This is the default host maintenance policy, and VMs on nodes groups configured with
this policy follow
traditional maintenance behavior for non-sole-tenant VMs.
That is, depending on the on-host maintenance setting of the VM's host,
VMs live migrate to a new
sole-tenant node in the node group before a host maintenance
event, and this new sole-tenant node only runs the customer's VMs.
This policy is most suitable for per-user or per-device licenses that require
live migration during host events. This setting doesn't restrict
migration of VMs to within a fixed pool of physical servers, and is recommended
for general workloads without physical server requirements and that don't
require existing licenses.
Because VMs live migrate to any server without considering existing server
affinity with this policy, this policy is not suitable for scenarios requiring
minimization of the use of physical cores during host events.
The following figure shows an animation of the Default host maintenance
policy.
Figure 2: Animation of the Default host
maintenance policy.
Restart in place host maintenance policy
When you use this host maintenance policy, Compute Engine stops VMs
during host events, and then restarts the VMs on the same physical server after
the host event. You must set the VM's on host maintenance setting to
TERMINATE when using this policy.
This policy is most suitable for workloads that are fault-tolerant and can
experience approximately one hour of downtime during host events,
workloads that must remain on the same physical server, workloads that don't
require live migration, or if you have licenses that are based on the number of
physical cores or processors.
With this policy, the instance can be assigned to the node group using
node-name, node-group-name, or node affinity label.
The following figure shows an animation of the Restart in place maintenance
policy.
Figure 3: Animation of the Restart in place host
maintenance policy.
Migrate within node group host maintenance policy
When using this host maintenance policy, Compute Engine live migrates
VMs within a fixed-sized group of physical servers during host events, which
helps limit the number of unique physical servers used by the VM.
This policy is most suitable for high-availability workloads with licenses that
are based on the number of physical cores or processors, because with this host
maintenance policy, each sole-tenant node in the group is pinned to a fixed set
of physical servers, which is different than the default policy that lets VMs
migrate to any server.
To confirm the capacity for live migration, Compute Engine reserves 1
holdback node for every 20 nodes that you reserve.
The following figure shows an animation of the Migrate within node group
host maintenance policy.
Figure 4: Animation of the Migrate within node group
host maintenance policy.
The following table shows how many holdback nodes Compute Engine
reserves depending on how many nodes you reserve for your node group.
Total nodes in group
Holdback nodes reserved for live migration
1
Not applicable. Must reserve at least 2 nodes.
2 to 20
1
21 to 40
2
41 to 60
3
61 to 80
4
81 to 100
5
Pin an instance to multiple node groups
You can pin an instance to multiple node groups using the node-group-name
affinity label
under the following conditions:
The instance that you want to pin is using a
default host maintenance policy (Migrate VM instance).
The host maintenance policy of all the node groups that you want to pin the
instance to is migrate within node group. If you try to pin an instance to
node groups with different host maintenance policies, the operation fails with
an error.
For example, if you want to pin an instance test-node to two node groups
node-group1 and node-group2, verify the following:
The host maintenance policy of test-node is Migrate VM instance.
The host maintenance policy of node-group1 and node-group2 is
migrate within node group.
Note: If the instance's on host maintenance policy is Terminate, you can pin
the instance to only a single node group.
You cannot assign an instance to any specific node with the affinity label
node-name. You can use any custom node affinity labels for your instances as
long as they are assigned the node-group-name and not the node-name.
Maintenance windows
If you are managing workloads—for example—finely tuned databases,
that might be sensitive to the performance impact of
live migration, then you can
determine when maintenance begins on a sole-tenant node group by specifying a
maintenance window when you create the node group. You can't modify the
maintenance window after you create the node group.
Maintenance windows are 4-hour blocks of time that you can use to specify when
Google performs maintenance
on your sole-tenant VMs. Maintenance events occur
approximately once every 4 to 6 weeks.
The maintenance window applies to all VMs in the sole-tenant node group, and it
only specifies when the maintenance begins. Maintenance is not guaranteed to
finish during the maintenance window, and there is no guarantee on how
frequently maintenance occurs. Maintenance windows are not supported on
node groups with the Migrate within node group host maintenance policy.
Simulate a host maintenance event
You can
simulate a host maintenance event
to test how your workloads that are running on sole-tenant nodes behave during
a host maintenance event. This lets you see the effects of the sole-tenant VM's
host maintenance policy on the applications running on the VMs.
Host errors
When there is a rare critical hardware failure on the host—sole-tenant or
multi-tenant—Compute Engine does the following:
Retires the physical server and its unique identifier.
Revokes your project's access to the physical server.
Replaces the failed hardware with a new physical server that has a new unique
identifier.
Moves the VMs from the failed hardware to the replacement node.
Restarts the affected VMs if you configured them to automatically restart.
Node affinity and anti-affinity
Sole-tenant nodes make sure that your VMs don't share host with VMs from other
projects unless you use shared sole-tenant node groups. With
shared sole-tenant node groups,
other projects within the organization can provision VMs on the same host.
However, you still might want to group several workloads together on the same
sole-tenant node or isolate your workloads from one another on different nodes.
For example, to help meet some compliance requirements, you might need to use
affinity labels to separate sensitive workloads from non-sensitive workloads.
When you create a VM, you request sole-tenancy by specifying node affinity or
anti-affinity, referencing one or more node affinity labels. You specify custom
node affinity labels when you create a node template, and Compute Engine
automatically includes some default affinity labels on each node. By specifying
affinity when you create a VM, you can schedule VMs together on a specific node
or nodes in a node group. By specifying anti-affinity when you create a VM, you
can make sure that certain VMs are not scheduled together on the same node or
nodes in a node group.
Node affinity labels are key-value pairs assigned to nodes, and are inherited
from a node template. Affinity labels let you:
Control how individual VM instances are assigned to nodes.
Control how VM instances created from a template, such as those created by a
managed instance group, are assigned to nodes.
Group sensitive VM instances on specific nodes or node groups, separate from
other VMs.
Default affinity labels
Compute Engine assigns the following default affinity labels to each
node:
A label for the node group name:
Key: compute.googleapis.com/node-group-name
Value: Name of the node group.
A label for the node name:
Key: compute.googleapis.com/node-name
Value: Name of the individual node.
A label for the projects the node group is shared with:
Key: compute.googleapis.com/projects
Value: Project ID of the project containing the node group.
Custom affinity labels
You can create custom node affinity labels when you
create a node template.
These affinity labels are assigned to all nodes in node groups created from the
node template. You can't add more custom affinity labels to nodes in a node
group after the node group has been created.
For information about how to use affinity labels, see
Configuring node affinity.
Pricing
To help you to minimize the
cost of your sole-tenant nodes,
Compute Engine provides
committed use discounts (CUDs)
and sustained use discounts (SUDs).
Note that for sole-tenancy premium charges, you can receive only flexible CUDs
and SUDs, but not resource-based CUDs.
Because you are already billed for the vCPU and memory of your
sole-tenant nodes, you don't pay extra for the VMs that you create on those
nodes.
If you provision sole-tenant nodes with GPUs or Local SSD disks, you are
billed for all of the GPUs or Local SSD disks on each node that you provision.
The sole-tenancy premium is based only on the vCPUs and memory that you use
for the sole-tenant node, and doesn't include GPUs or Local SSD disks.
Availability
Sole-tenant nodes are
available in select zones. To
verify high-availability, schedule VMs on sole-tenant nodes in different
zones.
Before using GPUs or Local SSD disks on sole-tenant nodes, make sure you have
enough GPU or Local SSD quota in the zone where
you are reserving the resource.
Compute Engine supports GPUs on n1 and g2 sole-tenant node
types that are in
zones with GPU support. The
following table shows the types of GPUs that you can attach to n1 and
g2 nodes and how many GPUs you must attach when you create the node
template.
GPU type
GPU quantity
Sole-tenant node type
NVIDIA L4
8
g2
NVIDIA P100
4
n1
NVIDIA P4
4
n1
NVIDIA T4
4
n1
NVIDIA V100
8
n1
Compute Engine supports Local SSD disks on n1, n2, n2d, and
g2 sole-tenant node types that are used in
zones that support those machine series.
Restrictions
You can't use sole-tenant VMs with the follow machine series and types:
T2D,
T2A,
E2,
C2D,
A2,
A3,
A4,
A4X,
G4, or
bare metal instances.
Sole-tenant VMs can't specify a minimum CPU platform.
You can't migrate a VM to a sole-tenant
node if that VM specifies a minimum
CPU platform. To migrate a VM to a sole-tenant node, remove the minimum CPU
platform specification by setting it to
automatic before updating
the VM's node affinity labels.
Sole-tenant nodes don't support preemptible VM instances.
For information about the limitations of using Local SSD disks on sole-tenant
nodes, see Local SSD data persistence.
For information about how using GPUs affects live migration, see the limitations of live
migration.
Sole-tenant nodes with GPUs don't support VMs without GPUs.
Only N1, N2, N2D, and N4 sole-tenant nodes support overcommitting CPUs.
C3, C3D, C4, C4A, C4D, and N4 VMs are scheduled with alignment to the
underlying NUMA architecture of the sole tenant node. Scheduling full and
sub-NUMA VM shapes on the same node might lead to fragmentation, whereby
a larger shape can't be run while several smaller shapes totaling the same
resource requirements can.
C3 and C4 sole-tenant nodes require that VMs have the same vCPU-to-memory
ratio as the node type—for example—you can't place a c3-standard
VM on a -highmem node type.
You can't update the maintenance policy on a live node group.
What's next
Learn how to create, configure, and consume your sole-tenant nodes.
Learn how to overcommit CPUs on sole-tenant VMs.
Learn how to bring your own licenses.
Review our best practices for using sole-tenant nodes to run VM workloads.
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-09-18 UTC.
Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어